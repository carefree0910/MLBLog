<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[“卷积神经网络”小结]]></title>
      <url>/posts/18671318/</url>
      <content type="html"><![CDATA[<ul>
<li>相比起 NN 的全连接来说、CNN 使用了局部视野和权值共享，这使得 CNN 更适合处理结构性的数据（比如图像）</li>
<li>Tensorflow 框架能帮助我们处理梯度并更新参数，这可以给实现带来极大的便利</li>
<li>CNN 大体上可分为“卷积块”与“NN 块”两部分，其中卷积块为特征提取器、NN 块为“附带”的分类器</li>
<li>比起 NN 而言、CNN 的参数量会少很多，这也是许多近现代的 CNN 网络不采用全连接层而采用全局平均池化层（GAP）的原因之一</li>
<li>CNN 的强大之处更多在于其提取特征的能力而非分类的能力，使用 CNN 进行特征提取后、再使用其它模型（比如 NN）进行相应的训练是一种常见的做法。事实上、这种做法有个学名叫做“迁移学习（Transfer Learning）”，感兴趣的观众老爷可以参见<a href="http://journalofbigdata.springeropen.com/articles/10.1186/s40537-016-0043-6" target="_blank" rel="external">这里</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> 卷积神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 小结 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[将 NN 扩展为 CNN]]></title>
      <url>/posts/433ed5d6/</url>
      <content type="html"><![CDATA[<p>往简单里说、CNN 只是多了卷积层、池化层和 FC 的 NN 而已，虽然卷积、池化对应的前向传导算法和反向传播算法的高效实现都很不平凡，但得益于 Tensorflow 的强大、我们可以在仅仅知道它们思想的前提下进行相应的实现，因为 Tensorflow 能够帮我们处理所有数学与技术上的细节</p>
<a id="more"></a>
<h1 id="实现卷积层"><a href="#实现卷积层" class="headerlink" title="实现卷积层"></a>实现卷积层</h1><p>回忆我们说过的卷积层和普通层的性质、不难发现它们的表现极其相似，区别大体上来说只在于如下三点：</p>
<ul>
<li>普通层自身对数据的处理只有“激活”（<script type="math/tex">v^{\left( i \right)} = \phi_{i}\left( u^{\left( i \right)} \right)</script>）这一个步骤，层与层（<script type="math/tex">L_{i}</script>、<script type="math/tex">L_{i + 1}</script>）之间的数据传递则是通过权值矩阵、偏置量（<script type="math/tex">w^{\left( i \right)}</script>、<script type="math/tex">b^{\left( i \right)}</script>）和线性变换（<script type="math/tex">u^{\left( i + 1 \right)} = v^{\left( i \right)} \times w^{\left( i \right)} + b^{\left( i \right)}</script>）来完成的；卷积层自身对数据的处理则多了“卷积”这个步骤（通常来说是先卷积再激活：<script type="math/tex">v^{\left( i \right)} = \phi_{i}\left( \text{conv}\left( u^{\left( i \right)} \right) \right)</script>）、同时层与层之间的数据传递是直接传递的（<script type="math/tex">u^{\left( i + 1 \right)} = v^{\left( i \right)}</script>）</li>
<li>卷积层自身多了 Kernel 这个属性并因此带来了诸如 Stride、Padding 等属性，不过与此同时、卷积层之间没有权值矩阵</li>
<li>卷积层和普通层的<code>shape</code>属性记录的东西不同，具体而言：<ul>
<li>普通层的<code>shape</code>记录着上个 Layer 和该 Layer 所含神经元的个数</li>
<li>卷积层的<code>shape</code>记录着上个卷积层的输出和该卷积层的 Kernel 的信息（注意卷积层的上一层必定还是卷积层）</li>
</ul>
</li>
</ul>
<p>接下来就看看具体实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvLayer</span><span class="params">(Layer)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self.shape：记录着上个卷积层的输出和该Layer的Kernel的信息，具体而言：</div><div class="line">            self.shape[0] = 上个卷积层的输出的形状（频道数×高×宽）</div><div class="line">                常简记为self.shape[0] =(c,h_old,w_old)</div><div class="line">            self.shape[1] = 该卷积层Kernel的信息（Kernel数×高×宽）</div><div class="line">                常简记为self.shape[1] =(f,h_new,w_new)</div><div class="line">        self.stride、self.padding：记录Stride、Padding的属性</div><div class="line">        self.parent：记录父层的属性</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, shape, stride=<span class="number">1</span>, padding=<span class="string">"SAME"</span>, parent=None)</span>:</span></div><div class="line">        <span class="keyword">if</span> parent <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            _parent = parent.root <span class="keyword">if</span> parent.is_sub_layer <span class="keyword">else</span> parent</div><div class="line">            shape = _parent.shape</div><div class="line">        Layer.__init__(self, shape)</div><div class="line">        self.stride = stride</div><div class="line">        <span class="comment"># 利用Tensorflow里面对Padding功能的封装、定义self.padding属性</span></div><div class="line">        <span class="keyword">if</span> isinstance(padding, str):</div><div class="line">            <span class="comment"># "VALID"意味着输出的高、宽会受Kernel的高、宽影响，具体公式后面会说</span></div><div class="line">            <span class="keyword">if</span> padding.upper() == <span class="string">"VALID"</span>:</div><div class="line">                self.padding = <span class="number">0</span></div><div class="line">                self.pad_flag = <span class="string">"VALID"</span></div><div class="line">            <span class="comment"># "SAME"意味着输出的高、宽与Kernel的高、宽无关、只受Stride的影响</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                self.padding = self.pad_flag = <span class="string">"SAME"</span></div><div class="line">        <span class="comment"># 如果输入了一个整数、那么就按照VALID情形设置Padding相关的属性</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.padding = int(padding)</div><div class="line">            self.pad_flag = <span class="string">"VALID"</span></div><div class="line">        self.parent = parent</div><div class="line">        <span class="keyword">if</span> len(shape) == <span class="number">1</span>:</div><div class="line">            self.n_channels = self.n_filters = self.out_h = self.out_w = <span class="keyword">None</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.feed_shape(shape)</div><div class="line"></div><div class="line">    <span class="comment"># 定义一个处理shape属性的方法</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feed_shape</span><span class="params">(self, shape)</span>:</span></div><div class="line">        self.shape = shape</div><div class="line">        self.n_channels, height, width = shape[<span class="number">0</span>]</div><div class="line">        self.n_filters, filter_height, filter_width = shape[<span class="number">1</span>]</div><div class="line">        <span class="comment"># 根据Padding的相关信息、计算输出的高、宽</span></div><div class="line">        <span class="keyword">if</span> self.pad_flag == <span class="string">"VALID"</span>:</div><div class="line">            self.out_h = ceil((height - filter_height + <span class="number">1</span>) / self.stride)</div><div class="line">            self.out_w = ceil((width - filter_width + <span class="number">1</span>) / self.stride)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.out_h = ceil(height / self.stride)</div><div class="line">            self.out_w = ceil(width / self.stride)</div></pre></td></tr></table></figure>
<p>上述代码的最后几行对应着下述两个公式、这两个公式在 Tensorflow 里面有着直接对应的实现：</p>
<ul>
<li>当 Padding 设置为 VALID 时，输出的高、宽分别为：  <script type="math/tex; mode=display">
h^{\text{out}} = \left\lceil \frac{h^{\text{old}} - h^{\text{new}} + 1}{\text{stride}} \right\rceil,\ \ w^{\text{out}} = \left\lceil \frac{w^{\text{old}} - w^{\text{new}} + 1}{\text{stride}} \right\rceil</script>其中，符号“<script type="math/tex">\lceil\ \rceil</script>”代表着“向上取整”，stride 代表着步长</li>
<li>当 Padding 设置为 SAME 时，输出的高、宽分别为：  <script type="math/tex; mode=display">
h^{\text{out}} = \left\lceil \frac{h^{\text{old}}}{\text{stride}} \right\rceil,\ \ w^{\text{out}} = \left\lceil \frac{w^{\text{old}}}{\text{stride}} \right\rceil</script></li>
</ul>
<p>同时不难看出、上述代码其实没有把 CNN 的前向传导算法囊括进去，这是因为考虑到卷积层会利用到普通层的激活函数、所以期望能够合理复用代码。所以期望能够把上述代码定义的 ConvLayer 和前文重写的 Layer 整合在一起以成为具体用在 CNN 中的卷积层，为此我们需要利用到 Python 中一项比较高级的技术——元类（元类的介绍可以参见<a href="https://zhuanlan.zhihu.com/p/24633374" target="_blank" rel="external">这里</a>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvLayerMeta</span><span class="params">(type)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(mcs, *args, **kwargs)</span>:</span></div><div class="line">        name, bases, attr = args[:<span class="number">3</span>]</div><div class="line">        <span class="comment"># 规定继承的顺序为ConvLayer→Layer</span></div><div class="line">        conv_layer, layer = bases</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, shape, stride=<span class="number">1</span>, padding=<span class="string">"SAME"</span>)</span>:</span></div><div class="line">            conv_layer.__init__(self, shape, stride, padding)</div><div class="line"></div><div class="line">        <span class="comment"># 利用Tensorflow的相应函数定义计算卷积的方法</span></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_conv</span><span class="params">(self, x, w)</span>:</span></div><div class="line">            <span class="keyword">return</span> tf.nn.conv2d(x, w, strides=[self.stride] * <span class="number">4</span>, padding=self.pad_flag)</div><div class="line"></div><div class="line">        <span class="comment"># 依次进行卷积、激活的步骤</span></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x, w, bias, predict)</span>:</span></div><div class="line">            res = self._conv(x, w) + bias</div><div class="line">            <span class="keyword">return</span> layer._activate(self, res, predict)</div><div class="line"></div><div class="line">        <span class="comment"># 在正式进行前向传导算法之前、先要利用Tensorflow相应函数进行Padding</span></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">activate</span><span class="params">(self, x, w, bias=None, predict=False)</span>:</span></div><div class="line">            <span class="keyword">if</span> self.pad_flag == <span class="string">"VALID"</span> <span class="keyword">and</span> self.padding &gt; <span class="number">0</span>:</div><div class="line">                _pad = [self.padding] * <span class="number">2</span></div><div class="line">                x = tf.pad(x, [[<span class="number">0</span>, <span class="number">0</span>], _pad, _pad, [<span class="number">0</span>, <span class="number">0</span>]], <span class="string">"CONSTANT"</span>)</div><div class="line">            <span class="keyword">return</span> _activate(self, x, w, bias, predict)</div><div class="line">        <span class="comment"># 将打包好的类返回</span></div><div class="line">        <span class="keyword">for</span> key, value <span class="keyword">in</span> locals().items():</div><div class="line">            <span class="keyword">if</span> str(value).find(<span class="string">"function"</span>) &gt;= <span class="number">0</span>:</div><div class="line">                attr[key] = value</div><div class="line">        <span class="keyword">return</span> type(name, bases, attr)</div></pre></td></tr></table></figure>
<p>在定义好基类和元类后、定义实际应用在 CNN 中的卷积层就非常简洁了。以在深度学习中应用最广泛的 ReLU 卷积层为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvReLU</span><span class="params">(ConvLayer, ReLU, metaclass=ConvLayerMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<h1 id="实现池化层"><a href="#实现池化层" class="headerlink" title="实现池化层"></a>实现池化层</h1><p>池化层比起卷积层而言要更简单一点：对于最常见的两种池化——极大池化和平均池化而言，它们所做的只是取输入的极大值和均值而已、本身并没有可以更新的参数。是故对池化层而言，我们无需维护其 Kernel、而只用定义相应的池化方法（极大、平均）即可，因此我们要求用户在调用池化层时、只提供“高”和“宽”而不提供“Kernel 个数”</p>
<p><strong><em>注意：Kernel 个数从数值上来说与输出频道个数一致，所以对于池化层的实现而言、我们应该直接用输入频道数来赋值 Kernel 数，因为池化不会改变数据的频道数</em></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvPoolLayer</span><span class="params">(ConvLayer)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feed_shape</span><span class="params">(self, shape)</span>:</span></div><div class="line">        shape = (shape[<span class="number">0</span>], (shape[<span class="number">0</span>][<span class="number">0</span>], *shape[<span class="number">1</span>]))</div><div class="line">        ConvLayer.feed_shape(self, shape)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">activate</span><span class="params">(self, x, w, bias=None, predict=False)</span>:</span></div><div class="line">        pool_height, pool_width = self.shape[<span class="number">1</span>][<span class="number">1</span>:]</div><div class="line">        <span class="comment"># 处理Padding</span></div><div class="line">        <span class="keyword">if</span> self.pad_flag == <span class="string">"VALID"</span> <span class="keyword">and</span> self.padding &gt; <span class="number">0</span>:</div><div class="line">            _pad = [self.padding] * <span class="number">2</span></div><div class="line">            x = tf.pad(x, [[<span class="number">0</span>, <span class="number">0</span>], _pad, _pad, [<span class="number">0</span>, <span class="number">0</span>]], <span class="string">"CONSTANT"</span>)</div><div class="line">        <span class="comment"># 利用self._activate方法进行池化</span></div><div class="line">        <span class="keyword">return</span> self._activate(<span class="keyword">None</span>)(</div><div class="line">            x, ksize=[<span class="number">1</span>, pool_height, pool_width, <span class="number">1</span>],</div><div class="line">            strides=[<span class="number">1</span>, self.stride, self.stride, <span class="number">1</span>], padding=self.pad_flag)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x, *args)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>同样的，由于 Tensorflow 已经帮助我们做好了封装、我们可以直接调用相应的函数来完成极大池化和平均池化的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 实现极大池化</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaxPool</span><span class="params">(ConvPoolLayer)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x, *args)</span>:</span></div><div class="line">        <span class="keyword">return</span> tf.nn.max_pool</div><div class="line"></div><div class="line"><span class="comment"># 实现平均池化</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AvgPool</span><span class="params">(ConvPoolLayer)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x, *args)</span>:</span></div><div class="line">        <span class="keyword">return</span> tf.nn.avg_pool</div></pre></td></tr></table></figure>
<h1 id="实现-CNN-中的特殊层结构"><a href="#实现-CNN-中的特殊层结构" class="headerlink" title="实现 CNN 中的特殊层结构"></a>实现 CNN 中的特殊层结构</h1><p>在 CNN 中同样有着 Dropout 和 Normalize 这两种特殊层结构。它们的表现和 NN 中相应特殊层结构的表现是完全一致的，区别只在于作用的对象不同</p>
<p>我们知道，CNN 每一层数据的维度要比 NN 中每一层数据的维度多一维：一个典型的 NN 中每一层的数据通常是<script type="math/tex">N \times p \times q</script>的，而 CNN 则通常是<script type="math/tex">N \times p \times q \times r</script>的、其中<script type="math/tex">r</script>是当前数据的频道数。为了让适用于 NN 的特殊层结构适配于 CNN，一个自然而合理的做法就是将<script type="math/tex">r</script>个频道的数据当做一个整体来处理、或说将 CNN 中<script type="math/tex">r</script>个频道的数据放在一起并视为 NN 中的一个神经元，这样做的话就能通过简易的封装来直接利用上我们对 NN 定义的特殊层结构。封装的过程则仍要用到元类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义作为封装的元类</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvSubLayerMeta</span><span class="params">(type)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(mcs, *args, **kwargs)</span>:</span></div><div class="line">        name, bases, attr = args[:<span class="number">3</span>]</div><div class="line">        conv_layer, sub_layer = bases</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, parent, shape, *_args, **_kwargs)</span>:</span></div><div class="line">            conv_layer.__init__(self, <span class="keyword">None</span>, parent=parent)</div><div class="line">            <span class="comment"># 与池化层类似、特殊层输出数据的形状应保持与输入数据的形状一致</span></div><div class="line">            self.out_h, self.out_w = parent.out_h, parent.out_w</div><div class="line">            sub_layer.__init__(self, parent, shape, *_args, **_kwargs)</div><div class="line">            self.shape = ((shape[<span class="number">0</span>][<span class="number">0</span>], self.out_h, self.out_w), shape[<span class="number">0</span>])</div><div class="line">            <span class="comment"># 如果是CNN中的Normalize、则要提前初始化好γ、β</span></div><div class="line">            <span class="keyword">if</span> name == <span class="string">"ConvNorm"</span>:</div><div class="line">                self.tf_gamma = tf.Variable(tf.ones(self.n_filters), name=<span class="string">"norm_scale"</span>)</div><div class="line">                self.tf_beta = tf.Variable(tf.zeros(self.n_filters), name=<span class="string">"norm_beta"</span>)</div><div class="line"></div><div class="line">        <span class="comment"># 利用NN中的特殊层结构的相应方法获得结果</span></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x, predict)</span>:</span></div><div class="line">            <span class="keyword">return</span> sub_layer._activate(self, x, predict)</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">activate</span><span class="params">(self, x, w, bias=None, predict=False)</span>:</span></div><div class="line">            <span class="keyword">return</span> _activate(self, x, predict)</div><div class="line">        <span class="comment"># 将打包好的类返回</span></div><div class="line">        <span class="keyword">for</span> key, value <span class="keyword">in</span> locals().items():</div><div class="line">            <span class="keyword">if</span> str(value).find(<span class="string">"function"</span>) &gt;= <span class="number">0</span> <span class="keyword">or</span> str(value).find(<span class="string">"property"</span>):</div><div class="line">                attr[key] = value</div><div class="line">        <span class="keyword">return</span> type(name, bases, attr)</div><div class="line"></div><div class="line"><span class="comment"># 定义CNN中的Dropout，注意继承顺序</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvDrop</span><span class="params">(ConvLayer, Dropout, metaclass=ConvSubLayerMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="comment"># 定义CNN中的Normalize，注意继承顺序</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvNorm</span><span class="params">(ConvLayer, Normalize, metaclass=ConvSubLayerMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<h1 id="实现-LayerFactory"><a href="#实现-LayerFactory" class="headerlink" title="实现 LayerFactory"></a>实现 LayerFactory</h1><p>我们在前三节讲述了 CNN 中卷积层、池化层和特殊层的实现，这一节我们将介绍如何定义一个简单的工厂来“生产”NN 中的层和前文介绍的这些层以方便进行应用（与上个系列中生产优化器的工厂差不多）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerFactory</span>:</span></div><div class="line">    <span class="comment"># 使用一个字典记录下所有的Root Layer</span></div><div class="line">    available_root_layers = &#123;</div><div class="line">        <span class="string">"Tanh"</span>: Tanh, <span class="string">"Sigmoid"</span>: Sigmoid,</div><div class="line">        <span class="string">"ELU"</span>: ELU, <span class="string">"ReLU"</span>: ReLU, <span class="string">"Softplus"</span>: Softplus,</div><div class="line">        <span class="string">"Identical"</span>: Identical,</div><div class="line">        <span class="string">"CrossEntropy"</span>: CrossEntropy, <span class="string">"MSE"</span>: MSE,</div><div class="line">        <span class="string">"ConvTanh"</span>: ConvTanh, <span class="string">"ConvSigmoid"</span>: ConvSigmoid,</div><div class="line">        <span class="string">"ConvELU"</span>: ConvELU, <span class="string">"ConvReLU"</span>: ConvReLU, <span class="string">"ConvSoftplus"</span>: ConvSoftplus,</div><div class="line">        <span class="string">"ConvIdentical"</span>: ConvIdentical,</div><div class="line">        <span class="string">"MaxPool"</span>: MaxPool, <span class="string">"AvgPool"</span>: AvgPool</div><div class="line">    &#125;</div><div class="line">    <span class="comment"># 使用一个字典记录下所有特殊层</span></div><div class="line">    available_special_layers = &#123;</div><div class="line">        <span class="string">"Dropout"</span>: Dropout,</div><div class="line">        <span class="string">"Normalize"</span>: Normalize,</div><div class="line">        <span class="string">"ConvDrop"</span>: ConvDrop,</div><div class="line">        <span class="string">"ConvNorm"</span>: ConvNorm</div><div class="line">    &#125;</div><div class="line">    <span class="comment"># 使用一个字典记录下所有特殊层的默认参数</span></div><div class="line">    special_layer_default_params = &#123;</div><div class="line">        <span class="string">"Dropout"</span>: (<span class="number">0.5</span>,),</div><div class="line">        <span class="string">"Normalize"</span>: (<span class="string">"Identical"</span>, <span class="number">1e-8</span>, <span class="number">0.9</span>),</div><div class="line">        <span class="string">"ConvDrop"</span>: (<span class="number">0.5</span>,),</div><div class="line">        <span class="string">"ConvNorm"</span>: (<span class="string">"Identical"</span>, <span class="number">1e-8</span>, <span class="number">0.9</span>)</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p>以上是一些准备工作，如果由于特殊需求（比如想实验某种激活函数是否好用）实现了新的 Layer 的话、就需要更新上面对应的字典。<br>接下来看看核心的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义根据“名字”获取（Root）Layer的方法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_root_layer_by_name</span><span class="params">(self, name, *args, **kwargs)</span>:</span></div><div class="line">    <span class="comment"># 根据字典判断输入的名字是否是Root Layer的名字</span></div><div class="line">    <span class="keyword">if</span> name <span class="keyword">in</span> self.available_root_layers:</div><div class="line">        <span class="comment"># 若是、则返回相应的Root Layer</span></div><div class="line">        layer = self.available_root_layers[name]</div><div class="line">        <span class="keyword">return</span> layer(*args, **kwargs)</div><div class="line">    <span class="comment"># 否则返回None</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">None</span></div><div class="line"></div><div class="line"><span class="comment"># 定义根据“名字”获取（任何）Layer的方法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_layer_by_name</span><span class="params">(self, name, parent, current_dimension, *args, **kwargs)</span>:</span></div><div class="line">    <span class="comment"># 先看输入的是否是Root Layer</span></div><div class="line">    _layer = self.get_root_layer_by_name(name, *args, **kwargs)</div><div class="line">    <span class="comment"># 若是、直接返回相应的Root Layer</span></div><div class="line">    <span class="keyword">if</span> _layer:</div><div class="line">        <span class="keyword">return</span> _layer, <span class="keyword">None</span></div><div class="line">    <span class="comment"># 否则就根据父层和相应字典进行初始化后、返回相应的特殊层</span></div><div class="line">    _current, _next = parent.shape[<span class="number">1</span>], current_dimension</div><div class="line">    layer_param = self.special_layer_default_params[name]</div><div class="line">    _layer = self.available_special_layers[name]</div><div class="line">    <span class="keyword">if</span> args <span class="keyword">or</span> kwargs:</div><div class="line">        _layer = _layer(parent, (_current, _next), *args, **kwargs)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        _layer = _layer(parent, (_current, _next), *layer_param)</div><div class="line">    <span class="keyword">return</span> _layer, (_current, _next)</div></pre></td></tr></table></figure>
<p>至此，所有 CNN 会用到的、和层结构相关的东西就已经全部实现完毕了，接下来只需在网络结构上做一些简单的更新后、CNN 的实现便能大功告成</p>
<h1 id="扩展网络结构"><a href="#扩展网络结构" class="headerlink" title="扩展网络结构"></a>扩展网络结构</h1><p>将网络结构迁移到 Tensorflow 框架中并扩展出 CNN 的功能这个过程、虽然不算困难却也相当繁琐。本节将会节选出其中比较重要的部分进行说明，对于其余和上个系列实现的网络结构几乎一致的地方则不再进行注释或叙述</p>
<p>首先是初始化，由于我们使用的是 Tensorflow 框架、所以相应变量名的前面会一概加上“tf”两个字母：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">NN</span><span class="params">(ClassifierBase)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(NN, self).__init__()</div><div class="line">        self._layers = []</div><div class="line">        self._optimizer = <span class="keyword">None</span></div><div class="line">        self._current_dimension = <span class="number">0</span></div><div class="line">        self._available_metrics = &#123;</div><div class="line">            key: value <span class="keyword">for</span> key, value <span class="keyword">in</span> zip([<span class="string">"acc"</span>, <span class="string">"f1-score"</span>], [NN.acc, NN.f1_score])</div><div class="line">        &#125;</div><div class="line">        self.verbose = <span class="number">0</span></div><div class="line">        self._metrics, self._metric_names, self._logs = [], [], &#123;&#125;</div><div class="line">        self._layer_factory = LayerFactory()</div><div class="line">        <span class="comment"># 定义Tensorflow中的相应变量</span></div><div class="line">        self._tfx = self._tfy = <span class="keyword">None</span>  <span class="comment"># 记录每个Batch的样本、标签的属性</span></div><div class="line">        self._tf_weights, self._tf_bias = [], []  <span class="comment"># 记录w、b的属性</span></div><div class="line">        self._cost = self._y_pred = <span class="keyword">None</span>  <span class="comment"># 记录损失值、输出值的属性</span></div><div class="line">        self._train_step = <span class="keyword">None</span>  <span class="comment"># 记录“参数更新步骤”的属性</span></div><div class="line">        self._sess = tf.Session()  <span class="comment"># 记录Tensorflow Session的属性</span></div></pre></td></tr></table></figure>
<p>然后我们要解决的就是上篇文章最后遗留下来的、在初始化各个权值矩阵时要把从初始化为 Numpy 数组改为初始化为 Tensorflow 数组、同时要注意兼容 CNN 的问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 利用Tensorflow相应函数初始化参数</span></div><div class="line"><span class="meta">@staticmethod</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_w</span><span class="params">(shape)</span>:</span></div><div class="line">    initial = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</div><div class="line">    <span class="keyword">return</span> tf.Variable(initial, name=<span class="string">"w"</span>)</div><div class="line"></div><div class="line"><span class="meta">@staticmethod</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_b</span><span class="params">(shape)</span>:</span></div><div class="line">    <span class="keyword">return</span> tf.Variable(np.zeros(shape, dtype=np.float32) + <span class="number">0.1</span>, name=<span class="string">"b"</span>)</div><div class="line"></div><div class="line"><span class="comment"># 做一个初始化参数的封装，要注意兼容CNN</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_add_params</span><span class="params">(self, shape, conv_channel=None, fc_shape=None, apply_bias=True)</span>:</span></div><div class="line">    <span class="comment"># 如果是FC的话、就要根据铺平后数据的形状来初始化数据</span></div><div class="line">    <span class="keyword">if</span> fc_shape <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        w_shape = (fc_shape, shape[<span class="number">1</span>])</div><div class="line">        b_shape = shape[<span class="number">1</span>],</div><div class="line">    <span class="comment"># 如果是卷积层的话、就要定义Kernel而非权值矩阵</span></div><div class="line">    <span class="keyword">elif</span> conv_channel <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">if</span> len(shape[<span class="number">1</span>]) &lt;= <span class="number">2</span>:</div><div class="line">            w_shape = shape[<span class="number">1</span>][<span class="number">0</span>], shape[<span class="number">1</span>][<span class="number">1</span>], conv_channel, conv_channel</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            w_shape = (shape[<span class="number">1</span>][<span class="number">1</span>], shape[<span class="number">1</span>][<span class="number">2</span>], conv_channel, shape[<span class="number">1</span>][<span class="number">0</span>])</div><div class="line">        b_shape = shape[<span class="number">1</span>][<span class="number">0</span>],</div><div class="line">    <span class="comment"># 其余情况和普通NN无异</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        w_shape = shape</div><div class="line">        b_shape = shape[<span class="number">1</span>],</div><div class="line">    self._tf_weights.append(self._get_w(w_shape))</div><div class="line">    <span class="keyword">if</span> apply_bias:</div><div class="line">        self._tf_bias.append(self._get_b(b_shape))</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        self._tf_bias.append(<span class="keyword">None</span>)</div><div class="line"></div><div class="line"><span class="comment"># 由于特殊层不会用到w和b、所以要定义一个生成占位符的方法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_add_param_placeholder</span><span class="params">(self)</span>:</span></div><div class="line">    self._tf_weights.append(tf.constant([<span class="number">.0</span>]))</div><div class="line">    self._tf_bias.append(tf.constant([<span class="number">.0</span>]))</div></pre></td></tr></table></figure>
<p>以上就是和 NN 中网络结构相比有比较大改动的地方、其余的部分则都是一些琐碎的细节。完整的代码可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/g_CNN/Networks.py" target="_blank" rel="external">这里</a>，功能更为齐全、在许多细节上都进行了优化的版本则可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/NN/TF/Networks.py" target="_blank" rel="external">这里</a></p>
]]></content>
      
        <categories>
            
            <category> 卷积神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[利用 Tensorflow 重写 NN]]></title>
      <url>/posts/24ed2586/</url>
      <content type="html"><![CDATA[<p>本文将会使用 Tensorflow 框架来重写我们上个系列中实现过的 NN、观众老爷们可能会需要知道 Tensorflow 的基本知识之后才能比较顺畅地阅读接下来的内容；如果对 Tensorflow 基本不了解的话、可以先参见我写的一篇 <a href="https://zhuanlan.zhihu.com/p/26645181" target="_blank" rel="external">Tensorflow 的应用式入门教程</a></p>
<a id="more"></a>
<h1 id="重写-Layer-结构"><a href="#重写-Layer-结构" class="headerlink" title="重写 Layer 结构"></a>重写 Layer 结构</h1><p>使用 Tensorflow 来重写 NN 的流程和上个系列中我们介绍过的实现流程是差不多的，不过由于 Tensorflow 帮助我们处理了更新参数这一部分的细节，所以我们能增添许多功能、同时也能把接口写得更漂亮一些。</p>
<p>首先还是要来实现 NN 的基本单元——Layer 结构。鉴于 Tensorflow 能够自动获取梯度、同时考虑到要扩展出 CNN 的功能，我们需要做出如下微调：</p>
<ul>
<li>对于激活函数，只用定义其原始形式、不必定义其导函数形式</li>
<li>解决上一章遗留下来的、特殊层结构的实现问题</li>
<li>要考虑当前层为 FC（全连接层）时的表现</li>
<li>让用户可以选择是否给 Layer 加偏置量</li>
</ul>
<p>其中的第四点可能有些让人不明所以：上个系列不是刚说过、偏置量对破坏对称性是很重要的吗？为什么要让用户选择是否使用偏置量呢？这主要是因为特殊层结构中 Normalize 的特殊性会使偏置量显得冗余。具体细节会在后文讨论特殊层结构处进行说明，这里就暂时按下不表</p>
<p>以下是 Layer 结构基类的具体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> ceil</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Layer</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self.shape：记录该Layer和上个Layer所含神经元的个数，具体而言：</div><div class="line">            self.shape[0] = 上个Layer所含神经元的个数</div><div class="line">            self.shape[1] = 该Layer所含神经元的个数</div><div class="line">        self.is_fc、self.is_sub_layer：记录该Layer是否为FC、特殊层结构的属性</div><div class="line">        self.apply_bias：记录是否对该Layer加偏置量的属性</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, shape, **kwargs)</span>:</span></div><div class="line">        self.shape = shape</div><div class="line">        self.is_fc = self.is_sub_layer = <span class="keyword">False</span></div><div class="line">        self.apply_bias = kwargs.get(<span class="string">"apply_bias"</span>, <span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.__class__.__name__</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> str(self)</div><div class="line"></div><div class="line"><span class="meta">    @property</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">name</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> str(self)</div><div class="line"></div><div class="line"><span class="meta">    @property</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">root</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self</div><div class="line"></div><div class="line">    <span class="comment"># 定义兼容特殊层结构和CNN的、前向传导算法的封装</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">activate</span><span class="params">(self, x, w, bias=None, predict=False)</span>:</span></div><div class="line">        <span class="comment"># 如果当前层是FC、就需要先将输入“铺平”</span></div><div class="line">        <span class="keyword">if</span> self.is_fc:</div><div class="line">            x = tf.reshape(x, [<span class="number">-1</span>, int(np.prod(x.get_shape()[<span class="number">1</span>:]))])</div><div class="line">        <span class="comment"># 如果是特殊的层结构、就调用相应的方法获得结果</span></div><div class="line">        <span class="keyword">if</span> self.is_sub_layer:</div><div class="line">            <span class="keyword">return</span> self._activate(x, predict)</div><div class="line">        <span class="comment"># 如果不加偏置量的话、就只进行矩阵相乘和激活函数的作用</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.apply_bias:</div><div class="line">            <span class="keyword">return</span> self._activate(tf.matmul(x, w), predict)</div><div class="line">        <span class="comment"># 否则就进行“最正常的”前向传导算法</span></div><div class="line">        <span class="keyword">return</span> self._activate(tf.matmul(x, w) + bias, predict)</div><div class="line"></div><div class="line">    <span class="comment"># 前向传导算法的核心、留待子类定义</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x, predict)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>注意到我们前向传导算法中有一项“predict”参数，这主要是因为特殊层结构的训练过程和预测过程表现通常都会不一样、所以要加一个标注。该标注的具体意义会在后文进行特殊层结构 SubLayer 的相关说明时体现出来、这里暂时按下不表</p>
<p>在实现好基类后、就可以实现具体要用在神经网络中的 Layer 了。以 Sigmoid 激活函数对应的 Layer 为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sigmoid</span><span class="params">(Layer)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x, predict)</span>:</span></div><div class="line">        <span class="keyword">return</span> tf.nn.sigmoid(x)</div></pre></td></tr></table></figure>
<p>得益于 Tensorflow 框架的强大（你除了这句话就没别的话说了吗……）、我们甚至连激活函数的形式都无需手写，因为它已经帮我们封装好了（事实上、绝大多数常用的激活函数在 Tensorflow 里面都有封装）</p>
<h1 id="实现特殊层"><a href="#实现特殊层" class="headerlink" title="实现特殊层"></a>实现特殊层</h1><p>这一节我们将介绍如何利用 Tensorflow 框架实现上个系列没有实现的特殊层结构——SubLayer，同时也会对十分常用的两种 SubLayer（Dropout、Normalize）做比上个系列深入一些的介绍</p>
<p>先来看看应该如何定义 SubLayer 的基类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 让SubLayer继承Layer以合理复用代码</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SubLayer</span><span class="params">(Layer)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self.shape：和Layer相应属性意义一致</div><div class="line">        self.parent：记录该Layer的父层的属性</div><div class="line">        self.description：用于可视化的属性，记录着对该SubLayer的“描述”</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, parent, shape)</span>:</span></div><div class="line">        Layer.__init__(self, shape)</div><div class="line">        self.parent = parent</div><div class="line">        self.description = <span class="string">""</span></div><div class="line"></div><div class="line">    <span class="comment"># 辅助获取Root Layer的property</span></div><div class="line"><span class="meta">    @property</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">root</span><span class="params">(self)</span>:</span></div><div class="line">        _root = self.parent</div><div class="line">        <span class="keyword">while</span> _root.parent:</div><div class="line">            _root = _root.parent</div><div class="line">        <span class="keyword">return</span> _root</div></pre></td></tr></table></figure>
<p>可以看到，得益于 Tensorflow 框架（Tensorflow 就是很厉害嘛……），本来难以处理的SubLayer 的实现变得非常简洁清晰。在实现好基类后、就可以实现具体要用在神经网络中的 SubLayer 了，先来看 Dropout：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dropout</span><span class="params">(SubLayer)</span>:</span></div><div class="line">    <span class="comment"># self._prob：训练过程中每个神经元被“留下”的概率</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, parent, shape, drop_prob=<span class="number">0.5</span>)</span>:</span></div><div class="line">        <span class="comment"># 神经元被Drop的概率必须大于等于0和小于1</span></div><div class="line">        <span class="keyword">if</span> drop_prob &lt; <span class="number">0</span> <span class="keyword">or</span> drop_prob &gt;= <span class="number">1</span>:</div><div class="line">            <span class="keyword">raise</span> ValueError(</div><div class="line">                <span class="string">"(Dropout) Probability of Dropout should be a positive float smaller than 1"</span>)</div><div class="line">        SubLayer.__init__(self, parent, shape)</div><div class="line">        <span class="comment"># 被“留下”的概率自然是1-被Drop的概率</span></div><div class="line">        self._prob = tf.constant(<span class="number">1</span> - drop_prob, dtype=tf.float32)</div><div class="line">        self.description = <span class="string">"(Drop prob: &#123;&#125;)"</span>.format(drop_prob)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x, predict)</span>:</span></div><div class="line">        <span class="comment"># 如果是在训练过程，那么就按照设定的、被“留下”的概率进行Dropout</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> predict:</div><div class="line">            <span class="keyword">return</span> tf.nn.dropout(x, self._prob)</div><div class="line">        <span class="comment"># 如果是在预测过程，那么直接返回输入值即可</span></div><div class="line">        <span class="keyword">return</span> x</div></pre></td></tr></table></figure>
<p>Dropout 的详细说明自然是看<a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="external">原 paper </a>最好，这里我就大概翻译、总结一下主要内容。Dropout 的核心思想在于提高模型的泛化能力：它会在每次迭代中依概率去掉对应 Layer 的某些神经元，从而每次迭代中训练的都是一个小的神经网络。这个过程可以通过下图进行说明：</p>
<img src="/posts/24ed2586/p1.png" alt="p1.png" title="">
<p>上图所示的即为当<code>drop_prob</code>为 50%（我们所设的默认值）时、Dropout 的一种可能的表现。左图所示为原网络、右图所示的为 Dropout 后的网络，可以看到神经元 a、b、e、g、j 都被 Drop 了</p>
<p>Dropout 过程的合理性需要概率论上一些理论的支撑，不过鉴于 Tensorflow 框架有封装好的相应函数、我们就不深入介绍其具体的数学原理而仅仅说明其直观（以<code>drop_prob</code>为 50%为例，其余<code>drop_prob</code>的情况是同理的）：</p>
<ul>
<li>在训练过程中，由于 Dropout 后留下来的神经元可以理解为“在 50%死亡概率下幸存”的神经元，所以给将它们对应的输出进行“增幅”是合理的。具体而言，假设一个神经元<script type="math/tex">n_{i}</script>的输出本来是<script type="math/tex">o_{i}</script>，那么如果 Dropout 后它被留下来了的话、其输出就应该变成<script type="math/tex">o_{i} \times \frac{1}{50\%} = 2o_{i}</script>（换句话说、应该让带 Dropout 的期望输出和原输出一致：对于任一个神经元<script type="math/tex">n_{i}</script>，设<code>drop_prob</code>为<script type="math/tex">p</script>而其原输出为<script type="math/tex">o_{i}</script>，那么当带 Dropout 的输出为<script type="math/tex">o_{i} \times \frac{1}{p}</script>时、<script type="math/tex">n_{i}</script>的期望输出即为<script type="math/tex">p \times o_{i} \times \frac{1}{p} = o_{i}</script>）</li>
<li>由于在训练时我们保证了神经网络的期望输出不变、所以在预测过程中我们还是应该让整个网络一起进行预测而不进行 Dropout（关于这一点，原论文似乎也表示这是一种“经试验证明行之有效”的办法而没有给出具体的、原理层面的说明）</li>
</ul>
<p>接下来介绍一下 Normalize。Normalize 这个特殊层结构的学名叫 Batch Normalization、常简称为 BN，顾名思义，它用于对每个 Batch 对应的数据进行规范化处理。这样做的意义是直观的：对于 NN、CNN 乃至任何机器学习分类器来说，其目的可以说都是从训练样本集中学出样本在样本空间中的分布、从而可以用这个分布来预测未知数据所属的类别。如果不对每个 Batch 的数据进行任何操作的话，不难想象它们彼此对应的“极大似然分布（极大似然估计意义下的分布）”是各不相同的（因为训练集只是样本空间中的一个小抽样、而 Batch 又只是训练集的一个小抽样）；这样的话，分类器在接受每个 Batch 时都要学习一个新的分布、然后最后还要尝试从这些分布中总结出样本空间的总分布，这无疑是相当困难的。如果存在一种规范化处理方法能够使每个 Batch 的分布都贴近真实分布的话、对分类器的训练来说无疑是至关重要的</p>
<p>传统的做法是对输入<script type="math/tex">X</script>进行很久以前提到过的归一化处理、亦即：</p>
<script type="math/tex; mode=display">
X = \frac{X - \bar{X}}{std(X)}</script><p>其中<script type="math/tex">\bar{X}</script>表示<script type="math/tex">X</script>的均值、<script type="math/tex">std(X)</script>表示<script type="math/tex">X</script>的标准差（Standard Deviation）。这种做法虽然能保证输入数据的质量、但是却无法保证NN里面中间层输出数据的质量。试想NN中的第一个隐藏层<script type="math/tex">L_{2}</script>，它接收的输入<script type="math/tex">u^{\left( 2 \right)}</script>是输入层<script type="math/tex">L_{1}</script>的输出<script type="math/tex">v^{\left( 1 \right)} = \phi_{1}(u^{\left( 1 \right)})</script>和权值矩阵<script type="math/tex">w^{\left( 1 \right)}</script>相乘后、加上偏置量<script type="math/tex">b^{\left( 1 \right)}</script>后的结果；在训练过程中，虽然<script type="math/tex">v^{\left( 1 \right)}</script>的质量有保证，但由于<script type="math/tex">w^{\left( 1 \right)}</script>和<script type="math/tex">b^{\left( 1 \right)}</script>在训练过程中会不断地被更新、所以<script type="math/tex">u^{\left( 2 \right)} = v^{\left( 1 \right)} \times w^{\left( 1 \right)} + b^{\left( 1 \right)}</script>的分布其实仍然不断在变。换句话说、<script type="math/tex">u^{\left( 2 \right)}</script>的质量其实就已经没有保证了</p>
<p>BN 打算解决的正是随着前向传导算法的推进、得到的数据的质量会不断变差的问题，它能通过对中间层数据进行某种规范化处理以达到类似对输入归一化处理的效果。事实上回忆上一章的内容、我们已经提到过 Normalize 的核心思想在于把父层的输出进行“归一化”了，下面我们就简单看看它具体是怎么做到这一点的</p>
<p>首先需要指出的是，简单地将每层得到的数据进行上述归一化操作显然是不可行的、因为这样会破坏掉每层自身学到的数据特征。设想如果某一层<script type="math/tex">L_{i}</script>学到了“数据基本都分布在样本空间的边缘”这一特征，这时如果强行做归一化处理并把数据都中心化的话、无疑就摈弃了<script type="math/tex">L_{i}</script>所学到的、可能是非常有价值的知识</p>
<p>为了使得中心化之后不破坏 Layer 本身学到的特征、BN 采取了一个简单却十分有效的方法：引入两个可以学习的“重构参数”以期望能够从中心化的数据重构出 Layer 本身学到的特征。具体而言：</p>
<ol>
<li><strong>输入</strong>：某一层<script type="math/tex">L_{i}</script>在当前 Batch 上的输出<script type="math/tex">v^{\left( i \right)}</script>、增强数值稳定性所用的小值<script type="math/tex">\epsilon</script></li>
<li><strong>过程</strong>：<ol>
<li>计算当前 Batch 的均值、方差：  <script type="math/tex; mode=display">
\mu_{i} = \bar{v^{\left( i \right)}}</script><script type="math/tex; mode=display">
\sigma_{i}^{2} = \left\lbrack \text{std}\left( v^{\left( i \right)} \right) \right\rbrack^{2}</script></li>
<li>归一化：  <script type="math/tex; mode=display">
\hat{v^{\left( i \right)}} = \frac{v^{\left( i \right)} - \mu_{i}}{\sqrt{\sigma_{i}^{2} + \epsilon}}</script></li>
<li>线性变换：  <script type="math/tex; mode=display">
y^{\left( i \right)} = \gamma\hat{v^{\left( i \right)}} + \beta</script></li>
</ol>
</li>
<li><strong>输出</strong>：规范化处理后的输出<script type="math/tex">y^{\left( i \right)}</script></li>
</ol>
<p>BN 的核心即在于<script type="math/tex">\gamma</script>、<script type="math/tex">\beta</script>这两个参数的应用上。关于如何利用反向传播算法来更新这两个参数的数学推导会稍显繁复、我们就不展开叙述了，取而代之、我们会直接利用 Tensorflow 来进行相关的实现</p>
<p>需要指出的是、对于算法中均值和方差的计算其实还有一个被广泛使用的小技巧，该小技巧某种意义上可以说是用到了“动量”的思想：我们会分别维护两个储存“运行均值（Running<br>Mean）”和“运行方差（Running Variance）”的变量。具体而言：</p>
<ol>
<li><strong>输入</strong>：某一层<script type="math/tex">L_{i}</script>在当前 Batch 上的输出<script type="math/tex">v^{\left( i \right)}</script>、增强数值稳定性所用的小值<script type="math/tex">\epsilon</script>；动量值<script type="math/tex">m</script>（一般取<script type="math/tex">m = 0.9</script>）</li>
<li><strong>过程</strong>：<br>首先要初始化 Running Mean、Running Variance 为 0 向量：  <script type="math/tex; mode=display">
\mu_{run} = \sigma_{run}^{2} = 0</script>并初始化<script type="math/tex">\gamma</script>、<script type="math/tex">\beta</script>为 1、0 向量：  <script type="math/tex; mode=display">
\gamma = 1,\ \ \beta = 0</script>然后进行如下操作：<ol>
<li>计算当前 Batch  的均值、方差：  <script type="math/tex; mode=display">
\mu_{i} = \bar{v^{\left( i \right)}}</script><script type="math/tex; mode=display">
\sigma_{i}^{2} = \left\lbrack \text{std}\left( v^{\left( i \right)} \right) \right\rbrack^{2}</script></li>
<li>利用<script type="math/tex">\mu_{i}</script>、<script type="math/tex">\sigma_{i}^{2}</script>和动量值<script type="math/tex">m</script>更新<script type="math/tex">\mu_{run}</script>、<script type="math/tex">\sigma_{run}^{2}</script>：  <script type="math/tex; mode=display">
\mu_{run} \leftarrow m \cdot \mu_{run} + \left( 1 - m \right) \cdot \mu_{i}</script><script type="math/tex; mode=display">
\sigma_{run}^{2} \leftarrow m \cdot \sigma_{run}^{2} + \left( 1 - m \right) \cdot \sigma_{i}^{2}</script></li>
<li>利用<script type="math/tex">\mu_{run}</script>、<script type="math/tex">\sigma_{run}^{2}</script>规范化处理输出：  <script type="math/tex; mode=display">
\hat{v^{\left( i \right)}} = \frac{v^{\left( i \right)} - \mu_{run}}{\sqrt{\sigma_{run}^{2} + \epsilon}}</script></li>
<li>线性变换：  <script type="math/tex; mode=display">
y^{\left( i \right)} = \gamma\hat{v^{\left( i \right)}} + \beta</script></li>
</ol>
</li>
<li><strong>输出</strong>：规范化处理后的输出<script type="math/tex">y^{\left( i \right)}</script></li>
</ol>
<p>最后提三点使用 Normalize 时需要注意的事项：</p>
<ul>
<li>无论是上述的哪种算法、BN 的训练过程和预测过程的表现都是不同的。具体而言，训练过程和算法中所叙述的一致、均值和方差都是根据当前 Batch 来计算的；但测试过程中的均值和方差不能根据当前 Batch 来计算、而应该根据训练样本集的某些特征来进行计算。对于第二个算法来说，<script type="math/tex">\mu_{\text{run}}</script>和<script type="math/tex">\sigma_{\text{run}}^{2}</script>天然就是很好的、可以用来当测试过程中的均值和方差的变量，对于第一个算法而言就需要额外的计算</li>
<li>对于 Normalize 这个特殊层结构来说、偏置量是一个冗余的变量；这是因为规范化操作（去均值）本身会将偏置量的影响抹去、同时 BN 本身的<script type="math/tex">\beta</script>参数可以说正是破坏对称性的参数，它能比较好地完成原本偏置量所做的工作</li>
<li>Normalize 这个层结构是可以加在许多不同地方的（如下图所示的 A、B 和 C 处），原论文将它加在了 A 处、但其实现在很多主流的深层 CNN 结构都将它加在了 C 处；相对而言、加在 B 处的做法则会少一些</li>
</ul>
<img src="/posts/24ed2586/p2.png" alt="p2.png" title="">
<p>在基本了解了 Normalize 对应的 BN 算法之后、我们就可以着手进行实现了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Normalize</span><span class="params">(SubLayer)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._eps：记录增强数值稳定性所用的小值的属性</div><div class="line">        self._activation：记录自身的激活函数的属性，主要是为了兼容图7.17 A的情况</div><div class="line">        self.tf_rm、self.tf_rv：记录μ_run、σ_run^2的属性</div><div class="line">        self.tf_gamma、self.tf_beta：记录γ、β的属性</div><div class="line">        self._momentum：记录动量值m的属性</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, parent, shape, activation=<span class="string">"Identical"</span>, eps=<span class="number">1e-8</span>, momentum=<span class="number">0.9</span>)</span>:</span></div><div class="line">        SubLayer.__init__(self, parent, shape)</div><div class="line">        self._eps, self._activation = eps, activation</div><div class="line">        self.tf_rm = self.tf_rv = <span class="keyword">None</span></div><div class="line">        self.tf_gamma = tf.Variable(tf.ones(self.shape[<span class="number">1</span>]), name=<span class="string">"norm_scale"</span>)</div><div class="line">        self.tf_beta = tf.Variable(tf.zeros(self.shape[<span class="number">1</span>]), name=<span class="string">"norm_beta"</span>)</div><div class="line">        self._momentum = momentum</div><div class="line">        self.description = <span class="string">"(eps: &#123;&#125;, momentum: &#123;&#125;)"</span>.format(eps, momentum)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x, predict)</span>:</span></div><div class="line">        <span class="comment"># 若μ_run、σ_run^2还未初始化，则根据输入x进行相应的初始化</span></div><div class="line">        <span class="keyword">if</span> self.tf_rm <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> self.tf_rv <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            shape = x.get_shape()[<span class="number">-1</span>]</div><div class="line">            self.tf_rm = tf.Variable(tf.zeros(shape), trainable=<span class="keyword">False</span>, name=<span class="string">"norm_mean"</span>)</div><div class="line">            self.tf_rv = tf.Variable(tf.ones(shape), trainable=<span class="keyword">False</span>, name=<span class="string">"norm_var"</span>)</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> predict:</div><div class="line">            <span class="comment"># 利用Tensorflow相应函数计算当前Batch的举止、方差</span></div><div class="line">            _sm, _sv = tf.nn.moments(x, list(range(len(x.get_shape()) - <span class="number">1</span>)))</div><div class="line">            _rm = tf.assign(</div><div class="line">                self.tf_rm, self._momentum * self.tf_rm + (<span class="number">1</span> - self._momentum) * _sm)</div><div class="line">            _rv = tf.assign(</div><div class="line">                self.tf_rv, self._momentum * self.tf_rv + (<span class="number">1</span> - self._momentum) * _sv)</div><div class="line">            <span class="comment"># 利用Tensorflow相应函数直接得到Batch Normalization的结果</span></div><div class="line">            <span class="keyword">with</span> tf.control_dependencies([_rm, _rv]):</div><div class="line">                _norm = tf.nn.batch_normalization(</div><div class="line">                    x, _sm, _sv, self.tf_beta, self.tf_gamma, self._eps)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            _norm = tf.nn.batch_normalization(</div><div class="line">                x, self.tf_rm, self.tf_rv, self.tf_beta, self.tf_gamma, self._eps)</div><div class="line">        <span class="comment"># 如果指定了激活函数、就再用相应激活函数作用在BN结果上以得到最终结果</span></div><div class="line">        <span class="comment"># 这里只定义了ReLU和Sigmoid两种，如有需要可以很方便地进行拓展</span></div><div class="line">        <span class="keyword">if</span> self._activation == <span class="string">"ReLU"</span>:</div><div class="line">            <span class="keyword">return</span> tf.nn.relu(_norm)</div><div class="line">        <span class="keyword">if</span> self._activation == <span class="string">"Sigmoid"</span>:</div><div class="line">            <span class="keyword">return</span> tf.nn.sigmoid(_norm)</div><div class="line">        <span class="keyword">return</span> _norm</div></pre></td></tr></table></figure>
<h1 id="重写-CostLayer-结构"><a href="#重写-CostLayer-结构" class="headerlink" title="重写 CostLayer 结构"></a>重写 CostLayer 结构</h1><p>在上个系列中，为了整合特殊变换函数和损失函数以更高效地计算梯度、我们花了不少代码来做繁琐的封装；不过由于 Tensorflow 中已经有了这些封装好的、数值性质更优的函数、所以 CostLayer 的实现将会变得非常简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义一个简单的基类</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CostLayer</span><span class="params">(Layer)</span>:</span></div><div class="line">    <span class="comment"># 定义一个方法以获取损失值</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calculate</span><span class="params">(self, y, y_pred)</span>:</span></div><div class="line">        <span class="keyword">return</span> self._activate(y_pred, y)</div><div class="line"></div><div class="line"><span class="comment"># 定义Cross Entropy对应的CostLayer（整合了Softmax变换）</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossEntropy</span><span class="params">(CostLayer)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x, y)</span>:</span></div><div class="line">        <span class="keyword">return</span> tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=x, labels=y))</div><div class="line"></div><div class="line"><span class="comment"># 定义MSE准则对应的CostLayer</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MSE</span><span class="params">(CostLayer)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x, y)</span>:</span></div><div class="line">        <span class="keyword">return</span> tf.reduce_mean(tf.square(x - y))</div></pre></td></tr></table></figure>
<p>短短 15 行代码就实现了上个系列中用 113 行代码才实现的所有功能，由此可窥见 Tensorflow 框架的强大</p>
<p>（话说我这么卖力地安利 Tensorflow，Google 是不是应该给我些广告费什么的）（喂</p>
<h1 id="重写网络结构"><a href="#重写网络结构" class="headerlink" title="重写网络结构"></a>重写网络结构</h1><p>由于 Tensorflow 重写的是算法核心部分，作为封装的网络结构其实并不用进行太大的变动；具体而言、整个网络结构需要做比较大的改动的地方只有如下两个：</p>
<ul>
<li>初始化各个权值矩阵时，从初始化为 Numpy 数组改为初始化为 Tensorflow 数组、同时要注意兼容 CNN 的问题</li>
<li>不用记录所有 Layer 的激活值而只用关心输出 Layer 的输出值和 CostLayer 的损失值（在上个系列中、我们是要记录所有中间结果以进行反向传播算法的）</li>
</ul>
<p>关于第一点我们会在后面介绍 CNN 的实现时进行说明，这里就仅看看第二点怎么做到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义一个只获取输出Layer的输出值的方法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_rs</span><span class="params">(self, x, predict=True)</span>:</span></div><div class="line">    <span class="comment"># 先获取第一层的激活值并用一个 _cache变量进行存储</span></div><div class="line">    _cache = self._layers[<span class="number">0</span>].activate(x, self._tf_weights[<span class="number">0</span>], self._tf_bias[<span class="number">0</span>], predict)</div><div class="line">    <span class="comment"># 遍历剩余的Layer</span></div><div class="line">    <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(self._layers[<span class="number">1</span>:]):</div><div class="line">        <span class="comment"># 如果到了倒数第二层（输出层）、就进行相应的处理并输出结果</span></div><div class="line">        <span class="keyword">if</span> i == len(self._layers) - <span class="number">2</span>:</div><div class="line">            <span class="comment"># 如果输出层是卷积层、就要把结果铺平</span></div><div class="line">            <span class="keyword">if</span> isinstance(self._layers[<span class="number">-2</span>], ConvLayer):</div><div class="line">                _cache = tf.reshape(_cache, [<span class="number">-1</span>, int(np.prod(_cache.get_shape()[<span class="number">1</span>:]))])</div><div class="line">            <span class="keyword">if</span> self._tf_bias[<span class="number">-1</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">                <span class="keyword">return</span> tf.matmul(_cache, self._tf_weights[<span class="number">-1</span>]) + self._tf_bias[<span class="number">-1</span>]</div><div class="line">            <span class="keyword">return</span> tf.matmul(_cache, self._tf_weights[<span class="number">-1</span>])</div><div class="line">        <span class="comment"># 否则、进行相应的前向传导算法</span></div><div class="line">        _cache = layer.activate(_cache, self._tf_weights[i + <span class="number">1</span>], self._tf_bias[i + <span class="number">1</span>], predict)</div></pre></td></tr></table></figure>
<p><strong><em>注意：不难看出、<code>get_rs</code>是兼容 CNN 的</em></strong></p>
<p>有了<code>get_rs</code>这个方法后、Tensorflow 下的网络结构的核心训练步骤就非常简洁了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 获取输出值</span></div><div class="line">self._y_pred = self._get_rs(self._tfx, predict=<span class="keyword">False</span>)</div><div class="line"><span class="comment"># 利用输出值和CostLayer的calculate方法、计算出损失值</span></div><div class="line">self._cost = self._layers[<span class="number">-1</span>].calculate(self._tfy, self._y_pred)</div><div class="line"><span class="comment"># 利用Tensorflow帮我们封装的优化器、直接定义出参数的更新步骤</span></div><div class="line">self._train_step = self._optimizer.minimize(self._cost)</div></pre></td></tr></table></figure>
<p>完整的、Tensorflow 版本的网络结构的代码可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/g_CNN/Networks.py" target="_blank" rel="external">这里</a>，对其深入一些的介绍则在下篇文章的最后一节中进行。此外、我对 Tensorflow 提供的诸多优化器做了一个简单的封装以兼容上个系列实现的优化器的一些接口，具体的代码可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/g_CNN/Optimizers.py" target="_blank" rel="external">这里</a></p>
]]></content>
      
        <categories>
            
            <category> 卷积神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> 算法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[从 NN 到 CNN]]></title>
      <url>/posts/7990dadf/</url>
      <content type="html"><![CDATA[<p>从名字也可以看出、卷积神经网络（CNN）其实是神经网络（NN）的一种拓展，而事实上从结构上来说，朴素的 CNN 和朴素的 NN 没有任何区别（当然，引入了特殊结构的、复杂的 CNN 会和 NN 有着比较大的区别）。本文主要会说一下 CNN 的思想以及它到底在 NN 的基础上做了哪些改进，同时也会说一下 CNN 能够解决的任务类型</p>
<a id="more"></a>
<h1 id="“视野”的共享"><a href="#“视野”的共享" class="headerlink" title="“视野”的共享"></a>“视野”的共享</h1><p>CNN 的主要思想可以概括为如下两点：</p>
<ul>
<li>局部连接（Sparse Connectivity）</li>
<li>权值共享（Shared Weights）</li>
</ul>
<p>它们具有很好的直观。举一个从学术上可能不太严谨的例子：我们平时看风景时，由于视野有限、我们通常并不能将整个风景收入眼中；取而代之、我们每次只能接受视野中的、整个风景的一块“局部风景”（所谓的【局部感受野】）。如果想要欣赏整个风景的话、我们就会不断地“四处张望”。在这个过程中，我们的思想在看的过程中通常是不怎么变的；而在看完后可能整合该过程中所有视野所看到的“局部风景”并发出“这风景真美”的感慨、然后可能会根据这个感慨来调整我们的思想。在这个例子中，我们的视野就可以看作所谓的“局部连接”，我们的思想则可以看作是“共享的权值”（注：这个栗子是我开脑洞开出来的、完全不能保证其学术严谨性、还请各位观众老爷们带着批判的眼光去看待它……如果有这方面专长的观众老爷发现我完全就在瞎扯淡、还望不吝指出 ( σ’ω’)σ）</p>
<p>光用文字叙述可能还是有些懵懂，我来画张图（参考了一张被引用烂了的图；但由于原图有一定的误导性、所以还是打算自己画一个）（虽然很丑）：</p>
<img src="/posts/7990dadf/p1.png" alt="p1.png" title="">
<p>这张图比较了 NN 和 CNN 的思想差别。左图为 NN，可以看到它在处理输入时是全连接的、亦即它采用的是全局感受野；同时由于各个神经元又是相对独立的、这直接导致它难以将原数据样本翻译成一个“视野”。而正如上面所说、CNN 采用的是局部感受野和共享权值，这在右图中的表现为它的神经元可以看成是“一整块”的“视野”，这块视野的每一个组成部分都是共享的权值（右图中的绿线；换句话说、右图中的四条绿线其实是同一个东西）在原数据样本的某一个局部上“看到”的东西</p>
<p>用上文中看风景的例子来说的话，CNN 的行为比较像一个正常人的表现、而 NN 的行为就更像是很多个能把整个风景都看在眼底的人同时看了同一个风景、然后分别感慨了一下并把这个感慨传递下去这种表现（？？？）</p>
<h1 id="前向传导算法"><a href="#前向传导算法" class="headerlink" title="前向传导算法"></a>前向传导算法</h1><p>CNN 的前向传导算法和上一章说明过的 NN 的前向传导算法有许多相似之处，至少从实现的层面来说它们的结构几乎一模一样。它们之间的不同之处则主要体现在如下两点：</p>
<ul>
<li>接收的输入的形式不同</li>
<li>层与层之间的连接方式不同</li>
</ul>
<p>先看第一点：对于 NN 而言、输入是一个<script type="math/tex">N \times n</script>的矩阵</p>
<script type="math/tex; mode=display">
X = \left( x_{1},\ldots,x_{N} \right)^{T}</script><p>其中<script type="math/tex">x_{1},\ldots,x_{N}</script>都是<script type="math/tex">n \times 1</script>的列向量；当输入是图像时，NN 的处理方式是将图像拉直成一个列向量。以<script type="math/tex">3 \times 3 \times 3</script>的图像为例（第一个 3 代指 RGB 通道，后两个 3 分别是高和宽），NN 会先把各个图像变成<script type="math/tex">27 \times 1</script>的列向量（亦即<script type="math/tex">n = 3 \times 3 \times 3</script>），然后再把它们合并、转置成一个<script type="math/tex">N \times 27</script>的大矩阵以当作输入</p>
<p>CNN 则不会这么大费周章——它会直接以原始的数据作为输入。换句话说、CNN 接收的输入是<script type="math/tex">N \times 3 \times 3 \times 3</script>的矩阵</p>
<p>可以用下图来直观认知一下该区别：</p>
<img src="/posts/7990dadf/p2.png" alt="p2.png" title="">
<p>所以两者的前向传导算法就可以用以下两张图来进行直观说明了：</p>
<img src="/posts/7990dadf/p3.png" alt="NN 的前向传导算法" title="NN 的前向传导算法">
<img src="/posts/7990dadf/p4.png" alt="CNN 的前向传导算法" title="CNN 的前向传导算法">
<p>（我已经尽我全力来画得好看一点了……）</p>
<p>下面进行进一步的说明：</p>
<ul>
<li>对于一个<script type="math/tex">3 \times 3 \times 3</script>的输入，我们可以把它拆分成 3 个<script type="math/tex">3 \times 3</script>的输入的堆叠（如果把<script type="math/tex">3 \times 3 \times 3</script>的输入看成是一个“图像”的话，我们可以把拆分后的 3 个输入看成是该图像的 3 个“频道”；对于原始输入来讲，这 3 个频道通常就是 RGB 通道）</li>
<li>由于 NN 是全连接的，所以输入的所有信息都会直接输入给下一层的某个神经元</li>
<li>由于 CNN 是局部连接、共享权值的，一个合理的做法就是给拆分后的每个“频道”分配一个共享的“局部视野”（注意上图中三个“频道”中间都有 4 个相同颜色的正方形、且三个频道中正方形的颜色彼此不同，这就是局部共享视野的意义）（谁注意得到啊喂）。我们通常会把这三个局部视野视为一个整体并把它称作一个 Kernel 或一个 Filter</li>
<li>上面 CNN 那张图中我们用的是<script type="math/tex">2 \times 2</script>的局部视野，该局部视野从相应频道左上看到右上、然后看左下、最后看右下，这个过程中一共看了四次、每看一次就会生成一个输出。所以三个局部视野会分别在对应的频道上生成四个输出、亦即一个 Kernel（或说一个 Filter）会生成 3 个<script type="math/tex">2 \times 2</script>的输出，将它们直接相加就得到了该 Kernel 的最终输出——一个<script type="math/tex">2 \times 2</script>的频道</li>
</ul>
<p>上面最后提到的“左上<script type="math/tex">\rightarrow</script>右上<script type="math/tex">\rightarrow</script>左下<script type="math/tex">\rightarrow</script>右下”这个“看”的过程其实就是所谓的“卷积”，这也正是卷积神经网络名字的由来。卷积本身的数学定义要比上面这个简单的描述要繁复得多，但幸运的是、实现和应用 CNN 本身并不需要具备这方面的数学理论知识（当然如果想开发更好的 CNN 结构与算法的话、是需要进行相关研究的，不过这些都已超出我们的讨论范围了）</p>
<p><strong><em>注意：上面 CNN 的那张图中的情形为只有一个 Kernel 的情形，通常来说在实际应用中、我们会使用几十甚至几百个 Kernel 以期望网络能够学习出更好的特征——这是因为一个 Kernel 会生成一个频道，几十、几百个 Kernel 就意味着会生成几十、几百个频道，由此可以期待这大量不同的频道能够对数据进行足够强的描述（要知道原始数据可只有 3 个频道）</em></strong></p>
<p>不难根据上文和上个系列的内容总结出 NN 和 CNN 目前为止的异同：</p>
<ul>
<li>NN 和 CNN 的主要结构都是层，但是 NN 的层结构是一维的（<script type="math/tex">1 \times n_{i}</script>）、CNN 的层结构是高维的</li>
<li>NN 处理的一般是“线性”的数据，CNN 则从直观上更适合处理“结构性的”数据</li>
<li>NN 层结构间会有权值矩阵作为连接的桥梁，CNN 则没有层结构之间的权值矩阵、取而代之的是层结构本身的局部视野。该局部视野会在前向传导算法中与层结构进行卷积运算来得到结果、并会直接将这个结果（或将被激活函数作用后的结果）传给下一层。因此我们常称 NN 中的层结构为“普通层”、称 CNN 中拥有局部视野的层结构为“卷积层”</li>
</ul>
<p>可以看出、CNN 与 NN 区别之关键正在于“卷积”二字。虽然卷积的直观形式比较简单、但是它的实现却并不平凡。常用的解决方案有如下两种：</p>
<ul>
<li>将卷积步骤变换成比较大规模的矩阵相乘（cs231n 里面的 stride trick 把我看哭了……）</li>
<li>利用快速傅里叶变换（Fast Fourier Transform，简称FFT）求解（只听说过，没实践过）</li>
</ul>
<p>展开叙述它们需要用到比较深的知识、所以从略</p>
<p>最后介绍一下 Stride 和 Padding 的概念。Stride 可以翻译成“步长”，它描述了局部视野在频道上的“浏览速度”。设想现在有一个<script type="math/tex">5 \times 5</script>的频道而我们的局部视野是<script type="math/tex">2 \times 2</script>的，那么不同 Stride 下的表现将如下面两张图所示（只以第一排为例）：</p>
<img src="/posts/7990dadf/p5.png" alt="p5.png" title="">
<img src="/posts/7990dadf/p6.png" alt="p6.png" title="">
<p>（……）</p>
<p>可以看到上图中局部视野每次前进“一步”而下图中每次会前进“三步”</p>
<p>Padding 可以翻译成“填充”、其存在意义有许多种解释，一种最好理解的就是——它能保持输入和输出的频道形状一致。注意目前为止展示过的栗子中，输入频道在被卷积之后、输出的频道都会“缩小”一点。这样在经过相当有限的卷积操作后、输入就会变得过小而不适合再进行卷积，从而就会大大限制了整个网络结构的深度。Padding 正是这个问题的一种解决方案：它会在输入频道进行卷积之前、先在频道的周围“填充”上若干圈的“0”。设想现在有一个<script type="math/tex">3\times3</script>的频道而我们的局部视野也是<script type="math/tex">3\times3</script>的，如果按照之前所说的卷积来做的话、不难想象输出将会是<script type="math/tex">1\times1</script>的频道；不过如果我们将 Padding 设置为 1、亦即在输入的频道周围填充一圈 0 的话，那么卷积的表现将如下图所示：</p>
<img src="/posts/7990dadf/p7.png" alt="p7.png" title="">
<p>可以看到当我们在输入频道外面 Pad 上一圈 0 之后、输出就变成<script type="math/tex">3 \times 3</script>的了，这为超深层 CNN 的搭建创造了可能性（比如有名的 ResNet）</p>
<p>在 cs231n 的<a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="external">这篇文章</a>里面有一张很好很好很好的动图（大概位于页面中央），请允许我偷个懒不自己动手画了…… ( σ’ω’)σ</p>
<h1 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h1><p>全连接层是 Fully Connected Layer 的直译，常简称为 FC，它是可能会出现在 CNN 中的、一个比较特殊的结构；从名字就可以大概猜想到、FC 应该和普通层息息相关，事实上也正是如此。直观地说、FC 是连接卷积层和普通层的普通层，它将从父层（卷积层）那里得到的高维数据铺平以作为输入、进行一些非线性变换（用激活函数作用）、然后将结果输进跟在它后面的各个普通层构成的系统中：</p>

<p>上图中的 FC 一共有<script type="math/tex">n_{1} = 3 \times 2 \times 2 = 12</script>个神经元，自 FC 之后的系统其实就是上一章所介绍的 NN。换句话说、我们可以把 CNN 拆分成如下两块结构：</p>
<ul>
<li>自输入开始、至 FC 终止的“卷积块”，组成卷积块的都是卷积层</li>
<li>自 FC 开始、至输出终止的“NN 块”，组成 NN 块的都是普通层</li>
</ul>
<p><strong><em>注意：值得一提的是，在许多常见的网络结构中、NN 块里都只含有 FC 这个普通层</em></strong></p>
<p>那么为什么 CNN 会有 FC 这个结构呢？或者问得更具体一点、为什么要将总体分成卷积块和NN块两部分呢？这其实从直观上来说非常好解释：卷积块中的卷积的基本单元是局部视野，用它类比我们的眼睛的话、就是将外界信息翻译成神经信号的工具，它能将接收的输入中的各个特征提取出来；至于 NN（神经网络）块、则可以类比我们的神经网络（甚至说、类比我们的大脑），它能够利用卷积块得到的信号（特征）来做出相应的决策。概括地说、CNN 视卷积块为“眼”而视 NN 块为“脑”，眼脑结合则决策自成（？？？）。用机器学习的术语来说、则卷积块为“特征提取器”而 NN 块为“决策分类器”</p>
<p>而事实上，CNN 的强大之处其实正在于其卷积块强大的特征提取能力上、NN 块甚至可以说只是用于分类的一个附属品而已。我们完全可以利用 CNN 将特征提取出来后、用前面几章介绍过的决策树、支持向量机等等来进行分类这一步而无须使用 NN 块</p>
<h1 id="池化（Pooling）"><a href="#池化（Pooling）" class="headerlink" title="池化（Pooling）"></a>池化（Pooling）</h1><p>池化是 NN 中完全没有的、只属于 CNN 的特殊演算。虽然名字听上去可能有些高大上的感觉，但它的本质其实就是“对局部信息的总结”。常见的池化有如下两种：</p>
<ul>
<li>极大池化（Max Pooling），它会输出接收到的所有输入中的最大值</li>
<li>平均池化（Average Pooling），它会输出接收到的所有输入的均值</li>
</ul>
<p>池化过程其实与卷积过程类似、可以看成是局部视野对输入信息的转换，只不过卷积过程做的是卷积运算、池化过程做的是极大或平均运算而已</p>
<p>不过池化与卷积有一点通常是差异较大的——池化的 Stride 通常会比卷积的 Stride 要大。比如对于一个<script type="math/tex">3 \times 3</script>的输入频道和一个<script type="math/tex">3 \times 3</script>的局部视野而言：</p>
<ul>
<li>卷积常常选取 Stride 和 Padding 都为 1，从而输出频道是<script type="math/tex">3 \times 3</script>的</li>
<li>池化常常选取 Stride 为 2、Padding 为1，从而输出频道是<script type="math/tex">2 \times 2</script>的</li>
</ul>
<p>将 Stride 选大是符合池化的内涵的：池化是对局部信息的总结、所以自然希望池化能够将得到的信息进行某种“压缩处理”。如果将 Stride 选得比较小的话、总结出来的信息就很可能会产生“冗余”，这就违背了池化的本意</p>
<p>不过为什么最常见的两种池化——极大池化和平均池化确实能够压缩信息呢？这主要是因为 CNN 一般处理的都是图像数据。由经验可知、图像在像素级间隔上的差异是很小的，这就为上述两种池化提供了一定的合理性</p>
]]></content>
      
        <categories>
            
            <category> 卷积神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[卷积神经网络综述]]></title>
      <url>/posts/78be0d7d/</url>
      <content type="html"><![CDATA[<p>卷积神经网络是 Convolutional Neural Network 的直译、常简称为 CNN，它是当今非常火热的话题——深度学习中的一种具有代表性的结构。如果提起卷积神经网络的话、许多人可能都会觉得是一个非常深奥的魔法，但如果不考虑其背后的数学理论而只想理解并应用的话、学习 CNN 其实也并不是特别困难（而且事实上相比起比较传统的机器学习算法而言、CNN 经常会由于其缺乏理论支撑而受到批评）</p>
<p>以下是目录：</p>
<ul>
<li><a href="/posts/7990dadf/" title="从 NN 到 CNN">从 NN 到 CNN</a></li>
<li><a href="/posts/24ed2586/" title="利用 Tensorflow 重写 NN">利用 Tensorflow 重写 NN</a></li>
<li><a href="/posts/433ed5d6/" title="将 NN 扩展为 CNN">将 NN 扩展为 CNN</a></li>
<li><a href="/posts/18671318/" title="“卷积神经网络”小结">“卷积神经网络”小结</a>
</li>
</ul>
<p>需要特别指出的是，本系列的文章基本不会涉及任何卷积神经网络数学上的细节；一方面是因为它们相当繁复、另一方面则是因为它们也并不完全 Make Sense。卷积神经网络在因其效果拔群而大红大紫的同时、其“黑箱”程度也是非常著名的，因此我们会较多地从实现和应用层面来介绍卷积神经网络、理论方面的叙述则大多采取直观说明的方式来进行</p>
<p>此外，CNN 的性能分析会放在<a href="https://github.com/carefree0910/MachineLearning/tree/master/_Dist" target="_blank" rel="external">具体的应用实例（Applications）</a>中进行，故本系列将略去这部分的内容</p>
]]></content>
      
        <categories>
            
            <category> 卷积神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
            <tag> 目录 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[“神经网络”小结]]></title>
      <url>/posts/66bacb27/</url>
      <content type="html"><![CDATA[<ul>
<li>神经网络的基本单位是层（Layer）、它是一个非常强大的多分类模型</li>
<li>神经网络的每一层（<script type="math/tex">L_{i}</script>）都会有一个激活函数<script type="math/tex">\phi_{i}</script>、它是模型的非线性扭曲力</li>
<li>神经网络通过权值矩阵<script type="math/tex">w^{\left( i \right)}</script>和偏置量<script type="math/tex">b^{\left( i \right)}</script>来连接相邻两层<script type="math/tex">L_{i}</script>、<script type="math/tex">L_{i + 1}</script>，其中<script type="math/tex">w^{\left( i \right)}</script>能将结果从原来的维度空间线性映射到新的维度空间、<script type="math/tex">b^{\left( i \right)}</script>则能打破对称性</li>
<li>神经网络通过前向传导算法获取各层的激活值、通过输出层的激活值<script type="math/tex">v^{\left( m \right)}</script>和损失函数<script type="math/tex">L^{*}\left( x \right) = L\left( y,v^{\left( m \right)} \right)</script>来做决策并获得损失、通过反向传播算法算出各个 Layer 的局部梯度<script type="math/tex">\delta^{\left( i \right)}</script>并用各种优化器更新参数</li>
<li>合理利用一些特殊的层结构能使模型表现提升</li>
<li>当任务规模较大时、就需要考虑内存等诸多和算法无关的问题了</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 小结 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[相关数学理论]]></title>
      <url>/posts/613bbb2f/</url>
      <content type="html"><![CDATA[<p>本文会叙述之前没有解决的纯数学问题，虽然它们仅会涉及到求导相关的知识、但是仍然具有一定难度</p>
<a id="more"></a>
<h1 id="BP-算法的推导"><a href="#BP-算法的推导" class="headerlink" title="BP 算法的推导"></a>BP 算法的推导</h1><p>要想知道 BP 算法的推导，我们需要且仅需要知道两点知识：求导及其链式法则。由前文的诸多说明可知、我们至少需要知道如下几件事：</p>
<ul>
<li>梯度是向量函数<script type="math/tex">f</script>在某点<script type="math/tex">x</script>上升最快的方向、其数学定义为  <script type="math/tex; mode=display">
\nabla_{x}f\left( x \right) = \left\lbrack \frac{\partial f\left( x \right)}{\partial x_{1}},\frac{\partial f\left( x \right)}{\partial x_{2}},\ldots,\frac{\partial f\left( x \right)}{\partial x_{n}} \right\rbrack^{T}</script>它是向量函数<script type="math/tex">f</script>对 n 个分量的偏导组成的向量。需要指出的是，我个人的习惯是在推导向量函数的梯度时，先把它分拆成单个的函数进行普通函数的求偏导计算、最后再把它们合成梯度。后文的推导也会采取这种形式</li>
<li>BP 算法的初始输入是真实的类别向量<script type="math/tex">y</script></li>
<li>我们的目标是让模型的输出尽可能拟合<script type="math/tex">y</script>。为此我们会定义：<ul>
<li>预测函数<script type="math/tex">f(x)</script>，它是一个向量函数、会根据输入矩阵<script type="math/tex">x</script>输出预测向量<script type="math/tex">v^{\left( m \right)}</script></li>
<li>损失函数<script type="math/tex">L^{*}\left( x \right) \triangleq L\left( y,v^{\left( m \right)} \right) = L\left( y,f\left( x \right) \right)</script>，它是一个标量函数、其函数值能反映<script type="math/tex">v^{\left( m \right)}</script>和<script type="math/tex">y</script>的差异；差异越大、<script type="math/tex">L^{*}\left( x \right) = L\left( y,v^{\left( m \right)} \right)</script>的值就越大</li>
</ul>
</li>
</ul>
<p>接下来就可以进行具体的推导了。如前所述，我们会把求解梯度的过程化为若干个求解偏导数的问题、然后再把结果进行整合；换句话说，我们会先以单个的神经元为基本单元进行分析、然后再把神经元上的结果整合成 Layer 上的结果</p>
<p>先来通过下图来进行一些符号约定：</p>
<img src="/posts/613bbb2f/p1.png" alt="p1.png" title="">
<p>其中</p>
<script type="math/tex; mode=display">
u_{q}^{\left( i \right)} = \sum_{p = 1}^{n_{i - 1}}{v_{p}^{\left( i - 1 \right)}w_{pq}^{\left( i - 1 \right)}}</script><p>代表着第 k 层第 j 个神经元接收的输入；</p>
<script type="math/tex; mode=display">
v_{q}^{\left( i \right)} = \phi_{i}\left( u_{q}^{(i)} \right)</script><p>代表着对应的激活值。注意我们在前文已经说过、局部梯度的定义可以写为</p>
<script type="math/tex; mode=display">
\delta_{q}^{\left( i \right)} = \frac{\partial L\left( x \right)}{\partial u_{q}^{\left( i \right)}}</script><p>接下来我们尝试把它转化成 BP 算法中相应的公式。首先由链式法直接可得：</p>
<script type="math/tex; mode=display">
\frac{\partial L^{*}\left( x \right)}{\partial w_{pq}^{\left( i - 1 \right)}} = \frac{\partial L^{*}\left( x \right)}{\partial u_{q}^{\left( i \right)}} \cdot \frac{\partial u_{q}^{\left( i \right)}}{\partial w_{pq}^{\left( i - 1 \right)}} = \delta_{q}^{\left( i \right)}v_{p}^{\left( i - 1 \right)}</script><p>这就可以直接导出最朴素的 SGD 算法。继续往下推导的话会遇到两种情况：</p>
<ul>
<li>当前 Layer 是 CostLayer、也就是说最后一层，此时有：  <script type="math/tex; mode=display">
\delta_{q}^{\left( m \right)} = \frac{\partial L^{*}\left( x \right)}{\partial u_{q}^{\left( m \right)}} = \frac{\partial L\left( y,v_{q}^{\left( m \right)} \right)}{\partial u_{q}^{\left( m \right)}} = \frac{\partial L\left( y,u_{q}^{\left( m \right)} \right)}{\partial v_{q}^{\left( m \right)}} \cdot \frac{\partial v_{q}^{\left( m \right)}}{\partial u_{q}^{\left( m \right)}} = \frac{\partial L\left( y,v_{q}^{\left( m \right)} \right)}{\partial v_{q}^{\left( m \right)}} \cdot \phi_{m}^{'}\left( u_{q}^{\left( m \right)} \right)</script>相当长的式子、里面涉及到的定义也挺多，不过其实每一步的本质都只是链式法则而已。注意最后出现了<script type="math/tex">\phi_{m}^{'}\left( u_{q}^{\left( m \right)} \right)</script>一项，它其实是输出层激活函数对应的导函数</li>
<li>当前 Layer 不是最后一层时，同样由链式法则可知（注意：该层的每个神经元对下一层所有神经元都会有影响）  <script type="math/tex; mode=display">
\delta_{q}^{\left( i \right)} = \frac{\partial L^{*}\left( x \right)}{\partial u_{q}^{\left( i \right)}} = \sum_{p = 1}^{n_{i + 1}}{\frac{\partial L^{*}\left( x \right)}{\partial u_{p}^{\left( i + 1 \right)}} \cdot \frac{\partial u_{p}^{\left( i + 1 \right)}}{\partial u_{q}^{\left( i \right)}}}</script>其中  <script type="math/tex; mode=display">
\frac{\partial L^{*}\left( x \right)}{\partial u_{p}^{\left( i + 1 \right)}} = \delta_{p}^{\left( i + 1 \right)}</script>即为下一层传播回来的局部梯度；且由于  <script type="math/tex; mode=display">
u_{p}^{\left( i + 1 \right)} = \sum_{q = 1}^{n_{i}}{v_{q}^{\left( i \right)}w_{qp}^{\left( i \right)}} = \sum_{q = 1}^{n_{i}}{\phi_{i}\left( u_{q}^{\left( i \right)} \right)w_{qp}^{\left( i \right)}}</script>从而可知  <script type="math/tex; mode=display">
\frac{\partial u_{p}^{\left( i + 1 \right)}}{\partial u_{q}^{\left( i \right)}} = \sum_{k = 1}^{n_{i}}\frac{\partial\left\lbrack \phi_{i}\left( u_{k}^{\left( i \right)} \right)w_{ki}^{\left( i \right)} \right\rbrack}{\partial u_{q}^{\left( i \right)}} = \phi_{i}^{'}\left( u_{q}^{\left( i \right)} \right)w_{qp}^{\left( i \right)}</script></li>
</ul>
<p>以上就是所有的推导过程，将结果进行整合之后、不难得出前文出现过的这些公式：</p>
<ul>
<li>对 CostLayer 而言、有  <script type="math/tex; mode=display">
\delta^{\left( m \right)} = \frac{\partial L\left( y,v^{\left( m \right)} \right)}{\partial v^{\left( m \right)}}*\phi_{m}^{'}\left( u^{\left( m \right)} \right)</script></li>
<li>对其余 Layer 而言、有  <script type="math/tex; mode=display">
\delta^{\left( i \right)} = \delta^{\left( i + 1 \right)} \times w^{\left( i \right)T}*\phi_{i}^{'}\left( u^{\left( i \right)} \right)</script></li>
</ul>
<h1 id="Softmax-log-likelihood-组合"><a href="#Softmax-log-likelihood-组合" class="headerlink" title="Softmax+log-likelihood 组合"></a>Softmax<script type="math/tex">+</script>log-likelihood 组合</h1><p>这一节主要说明下常见组合——Softmax<script type="math/tex">+</script>log-likelihood 的梯度公式的推导，不过在此之前可能需要复习一下符号约定：</p>
<ul>
<li>假设输入为<script type="math/tex">x</script>、输出为<script type="math/tex">y \in c_{k}</script></li>
<li>假设模型在 Softmax 之前的输出为  <script type="math/tex; mode=display">
v^{\left( m - 1 \right)} = \left( v_{1}^{\left( m - 1 \right)},\ldots,v_{K}^{\left( m - 1 \right)} \right)^{T}</script></li>
<li>假设模型的 Softmax 接受的输入为  <script type="math/tex; mode=display">
u^{\left( m \right)} = v^{\left( m - 1 \right)} \times w^{\left( m - 1 \right)} + b^{\left( m - 1 \right)}</script></li>
<li>假设模型在 Softmax 之后的输出为  <script type="math/tex; mode=display">
v^{\left( m \right)} = \varphi\left( u^{\left( m \right)} \right) = \left( \varphi_{1},\ldots,\varphi_{K} \right)^{T}</script>其中  <script type="math/tex; mode=display">
\varphi_{i} = \frac{e^{u_{i}^{\left( m \right)}}}{\sum_{j = 1}^{K}e^{u_{j}^{\left( m \right)}}}</script></li>
<li>假设模型的损失为 log-likelihood：  <script type="math/tex; mode=display">
L^{*}\left( x \right) = - \ln\varphi_{k}</script></li>
</ul>
<p>接下来开始正式的推导。同样先以神经元为基本单位进行分析、可知：</p>
<script type="math/tex; mode=display">
\frac{\partial L^{*}\left( x \right)}{\partial w_{pq}^{\left( m - 1 \right)}} = \frac{\partial L^{*}\left( x \right)}{\partial\varphi_{p}} \cdot \frac{\partial\varphi_{p}}{\partial u_{p}^{\left( m \right)}} \cdot \frac{\partial u_{p}^{\left( m \right)}}{\partial w_{pq}^{\left( m - 1 \right)}}</script><p>注意到</p>
<script type="math/tex; mode=display">
\frac{\partial L^{*}\left( x \right)}{\partial\varphi_{p}} = \left\{ \begin{matrix}
 - \frac{1}{\varphi_{p}},\ \ & p = k \\
0,\ \ & p \neq k \\
\end{matrix} \right.\</script><p>所以我们只需要考虑<script type="math/tex">p = k</script>的情况、此时有</p>
<script type="math/tex; mode=display">
\frac{\partial L^{*}\left( x \right)}{\partial w_{kq}^{\left( m - 1 \right)}} = - \frac{1}{\varphi_{k}} \cdot \frac{\partial\varphi_{k}}{\partial u_{k}^{\left( m \right)}} \cdot \frac{\partial u_{k}^{\left( m \right)}}{\partial w_{kq}^{\left( m - 1 \right)}}</script><p>注意到</p>
<script type="math/tex; mode=display">
\frac{\partial\varphi_{k}}{\partial u_{k}^{\left( m \right)}} = \varphi_{k}\left( 1 - \varphi_{k} \right)</script><p>以及</p>
<script type="math/tex; mode=display">
u^{\left( m \right)} = v^{\left( m - 1 \right)} \times w^{\left( m - 1 \right)} + b^{\left( m - 1 \right)}</script><p>故</p>
<script type="math/tex; mode=display">
\frac{\partial u_{k}^{\left( m \right)}}{\partial w_{kq}^{\left( m - 1 \right)}} = v_{q}^{\left( m - 1 \right)}</script><p>综上所述、即得</p>
<script type="math/tex; mode=display">
\frac{\partial L^{*}(x)}{\partial w_{pq}^{\left( m - 1 \right)}} = \left\{ \begin{matrix}
\left( \varphi_{p} - 1 \right)v_{q}^{\left( m - 1 \right)},\ \ & p = k \\
0,\ \ & p \neq k \\
\end{matrix} \right.\</script><p>亦即</p>
<script type="math/tex; mode=display">
\delta_{p}^{\left( m \right)} = \left\{ \begin{matrix}
\varphi_{p} - 1,\ \ & p = k \\
0,\ \ & p \neq k \\
\end{matrix} \right.\</script><p>若将 log-likelihood 改进为</p>
<script type="math/tex; mode=display">
L^{*}\left( x \right) = \left\{ \begin{matrix}
 - \ln v_{p},\ \ & p = k \\
 - \ln\left( 1 - v_{p} \right),\ \ & p \neq k \\
\end{matrix} \right.\</script><p>即得</p>
<script type="math/tex; mode=display">
\delta_{p}^{\left( m \right)} = \left\{ \begin{matrix}
\varphi_{p} - 1,\ \ & p = k \\
\varphi_{p},\ \ & p \neq k \\
\end{matrix} \right.\</script>]]></content>
      
        <categories>
            
            <category> 神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[“大数据”下的网络结构]]></title>
      <url>/posts/65c8a24f/</url>
      <content type="html"><![CDATA[<p>本文标题处的“大数据”打上了引号，是因为我们所要讨论的不是当今十分火热的、真正的大数据问题、而是讨论当问题规模“相当大”时应该如何处理。我们虽然在上篇文章中实现了一个切实可用的神经网络、但它确实显得过于朴实。本文会说明如何在这个朴实模型的基础上进行拓展，这些拓展的手法不单适用于神经网络、还适用于诸多旨在解决现实生活中规模相对较大的任务的模型</p>
<a id="more"></a>
<h1 id="分批（Batch）的思想"><a href="#分批（Batch）的思想" class="headerlink" title="分批（Batch）的思想"></a>分批（Batch）的思想</h1><p>回忆上一节实现的朴素神经网络中的<code>fit</code>方法、可以发现每次迭代时我们都只会用整个训练集进行一次参数的更新；以 Vanilla Update 为例的话、我们进行的就是 BGD 而非 MBGD。在数据量比较大时，姑且不论 MBGD 算法和 BGD 算法本身孰优孰劣，单从内存问题来看、BGD 就不是一个可以接受的做法。因此与 MBGD 算法的思想类似、我们需要将训练集“分批（Batch）”进行训练</p>
<p>同样的道理，目前我们做预测时是将整个预测数据集扔给模型让它做前传算法的。当数据量比较大时、这样做显然也会引发内存不足的问题，为此我们需要分 Batch 进行前向传导并在最后做一个整合</p>
<p>总之在数据量变大的情况下、我们要时刻有着分 Batch 的思想。先来看看如何在训练过程中引入 Batch（以下代码需定义在<code>fit</code>方法中的相关位置、仅写出关键部分）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 得到总样本数</span></div><div class="line">train_len = len(x)</div><div class="line"><span class="comment"># 得到单个Batch中的样本数，其中batch_size是传进来的参数</span></div><div class="line">batch_size = min(batch_size, train_len)</div><div class="line"><span class="comment"># 先判断是否有必要分Batch；若Batch中的样本数多于总样本数、自然没有必要分Batch</span></div><div class="line">do_random_batch = train_len &gt;= batch_size</div><div class="line"><span class="comment"># 算出需要分多少次Batch</span></div><div class="line">train_repeat = int(train_len / batch_size) + <span class="number">1</span></div><div class="line"><span class="comment"># 训练的主循环</span></div><div class="line"><span class="keyword">for</span> counter <span class="keyword">in</span> range(epoch):</div><div class="line">    <span class="comment"># 进行train_repeat次子迭代、每次子迭代中会利用一个Batch来训练模型</span></div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(train_repeat):</div><div class="line">        <span class="keyword">if</span> do_random_batch:</div><div class="line">            <span class="comment"># np.random.choice(n, m)：随机从[0,1,...,n-1]中选出m个数</span></div><div class="line">            batch = np.random.choice(train_len, batch_size)</div><div class="line">            x_batch, y_batch = x_train[batch], y_train[batch]</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            x_batch, y_batch = x_train, y_train</div><div class="line">        self._w_optimizer.update()</div><div class="line">        self._b_optimizer.update()</div><div class="line">        _activations = self._get_activations(x_batch)</div><div class="line">        _deltas = [self._layers[<span class="number">-1</span>].bp_first(y_batch, _activations[<span class="number">-1</span>])]</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">-1</span>, -len(_activations), <span class="number">-1</span>):</div><div class="line">            _deltas.append(</div><div class="line">                self._layers[i - <span class="number">1</span>].bp(_activations[i - <span class="number">1</span>], self._weights[i], _deltas[<span class="number">-1</span>])</div><div class="line">            )</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(layer_width - <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>):</div><div class="line">            self._opt(i, _activations[i - <span class="number">1</span>], _deltas[layer_width - i - <span class="number">1</span>])</div><div class="line">        self._opt(<span class="number">0</span>, x_batch, _deltas[<span class="number">-1</span>])</div></pre></td></tr></table></figure>
<p>然后是在预测过程中引入 Batch，实现的方法有两种：一种是比较常见的按个数分 Batch、一种是我们打算采用的按数据大小分 Batch。换句话说：</p>
<ul>
<li>常见的做法是在每个 Batch 中放 k 个数据</li>
<li>我们的做法是在每个 Batch 中放 m 个数据、它们一共大概包含 N 个数字</li>
</ul>
<p>其中常见做法有一个显而易见的缺点：如果单个数据很庞大的话、这样做可能还是会引发内存不足的问题。接下来就看看我们的做法相对应的具体实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 参数batch_size即为N</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_prediction</span><span class="params">(self, x, batch_size=<span class="number">1e6</span>)</span>:</span></div><div class="line">    <span class="comment"># 计算Batch中的数据个数m、默认</span></div><div class="line">    single_batch = int(batch_size / np.prod(x.shape[<span class="number">1</span>:]))</div><div class="line">    <span class="comment"># 如果单个样本的数据量比N还大、那么将m设为1</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> single_batch:</div><div class="line">        single_batch = <span class="number">1</span></div><div class="line">    <span class="comment"># 如果m大于样本总数、直接将所有样本输入前向传导算法即可</span></div><div class="line">    <span class="keyword">if</span> single_batch &gt;= len(x):</div><div class="line">        <span class="keyword">return</span> self._get_activations(x).pop()</div><div class="line">    <span class="comment"># 否则、计算需要重复调用前向传导算法的次数</span></div><div class="line">    epoch = int(len(x) / single_batch)</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> len(x) % single_batch:</div><div class="line">        epoch += <span class="number">1</span></div><div class="line">    <span class="comment"># 反复调用前向传导并获得一系列结果</span></div><div class="line">    rs, count = [self._get_activations(x[:single_batch]).pop()], single_batch</div><div class="line">    <span class="keyword">while</span> count &lt; len(x):</div><div class="line">        count += single_batch</div><div class="line">        <span class="keyword">if</span> count &gt;= len(x):</div><div class="line">            rs.append(self._get_activations(x[count - single_batch:]).pop())</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            rs.append(self._get_activations(x[count - single_batch:count]).pop())</div><div class="line">    <span class="comment"># 利用np.vstack将这一系列结果进行合并</span></div><div class="line">    <span class="keyword">return</span> np.vstack(rs)</div></pre></td></tr></table></figure>
<p>实现完毕后、我们就能得到如下图所示的结果（以在上一篇文章最后所用的螺旋线数据集上的训练过程为例）：</p>
<img src="/posts/65c8a24f/p1.png" alt="p1.png" title="">
<p>其中左图的准确率为 99.0%、右图的准确率为 100%。神经网络的结构仍都是两层含 24 个神经元的 ReLU 加 Softmax<script type="math/tex">+</script>Cross Entropy 组合的这个结构、迭代次数仍为 1000 次、平均训练时间则分别变为 2.36秒（左图）和 3.84秒（右图）</p>
<h1 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h1><p>由于针对现实任务训练出来的神经网络通常来说是很难直接进行可视化的，所以如果想要评估它的表现的话、就必须要用交叉验证。这里我们提供一种简易交叉验证的实现方法（以下代码需定义在<code>fit</code>方法中的相关位置、仅写出关键部分）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># train_rate是传进来的参数、代表着训练集在整个数据集中占的比例</span></div><div class="line"><span class="keyword">if</span> train_rate <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">    train_rate = float(train_rate)</div><div class="line">    train_len = int(len(x) * train_rate)</div><div class="line">    shuffle_suffix = np.random.permutation(len(x))</div><div class="line">    x, y = x[shuffle_suffix], y[shuffle_suffix]</div><div class="line">    x_train, y_train = x[:train_len], y[:train_len]</div><div class="line">    x_test, y_test = x[train_len:], y[train_len:]</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    x_train = x_test = x</div><div class="line">    y_train = y_test = y</div></pre></td></tr></table></figure>
<p>仅仅简单地把数据集分开并没有意义，如果想要进行评估的话、就必须切实利用到那分出来的测试集。一种常见的做法是实时记录模型在测试集上的表现并在最后以图表的形式画出，这正是我们之前展示过的各种训练曲线的由来；要想实现这种实时记录的功能、我们需要额外地定义一些属性和方法。思路大致如下：</p>
<ul>
<li>定义一个属性<code>self._logs</code>以存储我们的记录。该属性是一个字典、结构大致为：  <script type="math/tex; mode=display">
\text{self._logs} = \text{\{"train": train_log, "test":test_log\}}</script>其中<script type="math/tex">\text{train_log}</script>和<script type="math/tex">\text{test_log}</script>为训练集和测试集的实时表现</li>
<li>常见的对模型实时表现的评估有三种：损失（cost）、准确率（acc）和 F1-score，其中前两种是通用的评估、F1-score 则针对二类分类问题（F1-score 的相关数学定义可以参见<a href="https://en.wikipedia.org/wiki/F1_score" target="_blank" rel="external">这里</a>）</li>
<li>定义三个方法，一个拿来实时记录这些评估、一个拿来输出最新的评估、一个拿来可视化评估</li>
</ul>
<p>实现的话不难但繁、需要综合考虑许多东西并微调已有的代码；由于如果把所有变动的地方都写出来会有大量的冗余、所以这里就不写出所有细节了。感兴趣的观众老爷们可以尝试自己进行实现，我个人实现的版本则可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/f_NN/Networks.py" target="_blank" rel="external">这里</a></p>
<p>实现完毕后、我们就能得到如下图所示的结果（以之前二分类螺旋线数据集上的训练过程为例）：</p>
<img src="/posts/65c8a24f/p2.png" alt="p2.png" title="">
<p>从左到右依次为损失、准确率和 F1-score 的曲线，其中绿线为训练集上的表现、蓝线为测试集上的表现</p>
<h1 id="进度条"><a href="#进度条" class="headerlink" title="进度条"></a>进度条</h1><p>当我们在解决现实生活中一个比较大型的问题时（比如网络爬虫或机器学习）、模型的耗时有时会达数十分钟甚至几个小时。在此期间如果程序什么都不输出的话、不免会感到些许不安：程序的运行到底到了哪个步骤？大概还需多久程序才能跑完呢？为了能在大型任务中获得即时的反馈、设计一个进度条是相当有必要的。本节拟介绍一种简单实用的进度条的实现方法，它支持记录并发程序的进度且损耗基本只来源于 Python 本身</p>
<p>先来看看我们的进度条是怎样的：</p>
<img src="/posts/65c8a24f/p3.png" alt="p3.png" title="">
<p>其中每一行对应着一个单独任务的进度条、它有如下属性：</p>
<ul>
<li>任务名字（“Test”、“Test2”和“Test3”）</li>
<li>一个形如“[- - - - - - ]”的进度显示器（紧跟在任务名字后面）</li>
<li>已完成任务数和总任务数（紧跟在进度显示器后面、以 m /n 的形式出现，其中 m 为已完成任务数、n 为总任务数）</li>
<li>总耗时和单个任务的平均耗时（紧跟在任务数后面，其中“Time Cost”后显示的是总耗时、“Average”后显示的是平均耗时，格式都是“时-分-秒”）</li>
</ul>
<p>可以看到功能还算完备。不过虽说看上去有些复杂、但其实核心的实现只用到了<code>time</code>这个 Python 标准库和<code>print</code>这个 Python 自带的函数。总代码量虽说不算太大（110 行左右）、但有许多地方都是些琐碎的细节；所以我们这里就只说一个思路、具体的代码则可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/ProgressBar.py" target="_blank" rel="external">这里</a></p>
<p>实现的大纲大概如下：</p>
<ul>
<li>要记录任务开始时的已完成的任务数和未完成的任务数</li>
<li>要定义一个计数器，记录着总共已完成的任务数</li>
<li>要定义一个<code>start</code>函数和一个<code>update</code>函数作为初始化进度条和更新进度条的接口</li>
<li>要定义一个<code>_flush</code>函数来控制输出流</li>
</ul>
<p>调用的方法也非常直观，这里举一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义一个返回函数的函数</span></div><div class="line"><span class="comment"># 参数cost为任务耗时（秒）、epoch为迭代次数、name为任务名、_sub_task为子任务</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">task</span><span class="params">(cost=<span class="number">0.5</span>, epoch=<span class="number">3</span>, name=<span class="string">""</span>, _sub_task=None)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_sub</span><span class="params">()</span>:</span></div><div class="line">        bar = ProgressBar(max_value=epoch, name=name)</div><div class="line">        <span class="comment"># 调用start方法进行进度条的初始化</span></div><div class="line">        bar.start()</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</div><div class="line">            <span class="comment"># 利用time.sleep方法模拟任务耗时</span></div><div class="line">            time.sleep(cost)</div><div class="line">            <span class="comment"># 如果有子任务的话就执行子任务</span></div><div class="line">            <span class="keyword">if</span> _sub_task <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">                _sub_task()</div><div class="line">            <span class="comment"># 调用update方法更新进度条</span></div><div class="line">            bar.update()</div><div class="line">    <span class="keyword">return</span> _sub</div><div class="line"></div><div class="line"><span class="comment"># 定义三个任务Task1、Task2、Task3</span></div><div class="line"><span class="comment"># 其中Task2、Task3分别为Task1、Task2的子任务</span></div><div class="line">task(name=<span class="string">"Task1"</span>, _sub_task=task(</div><div class="line">    name=<span class="string">"Task2"</span>, _sub_task=task(</div><div class="line">        name=<span class="string">"Task3"</span>)))()</div></pre></td></tr></table></figure>
<p>这段代码的运行效果正如上图所示</p>
<h1 id="计时器"><a href="#计时器" class="headerlink" title="计时器"></a>计时器</h1><p>对于现实生活中的任务来说，我们往往需要让模型更可控、高效；这就使得我们需要知道程序运行的各个细节、或说各个部分的时间开销。Python 有一个自带的分析程序运行开销的工具 profile、它能满足我们大部分的要求。本节拟介绍 profile 的一种更灵活的轻量级替代品——Timing 的使用，其代码量仅 60 行左右、且可以比较简单地进行各种改进、拓展（Timing 的实现会放在今后介绍 Python 装饰器时进行简要的说明，观众老爷们也可以直接参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Timing.py" target="_blank" rel="external">这里</a>）</p>
<p>先来看一下它的效果：</p>
<img src="/posts/65c8a24f/p4.png" alt="p4.png" title="">
<p>该图反映的正是之前二分类螺旋线数据集上的训练过程。可以看到它将神经网络中各个组成部分的各个函数的开销情况都记录了下来、总体上来说已足够我们进行性能分析。此外、这里我们采取的是按名字排序，如有必要、完全可以定义成按总开销排序或是按平均开销排序（另外虽然我们没有记录平均开销、但是添加上平均开销这一项是平凡的）</p>
<p>应用 Timing 是比较简单的一件事，举一个小例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义一个测试类来进行简单的测试</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span>:</span></div><div class="line">    timing = Timing()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rate)</span>:</span></div><div class="line">        self.rate = rate</div><div class="line"></div><div class="line">    <span class="comment"># 以装饰器的形式、调用Timing中的timeit方法来计时</span></div><div class="line">    <span class="comment"># 默认迭代三次且单次迭代中调用self._test方法</span></div><div class="line"><span class="meta">    @timing.timeit()</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, cost=<span class="number">0.1</span>, epoch=<span class="number">3</span>)</span>:</span></div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</div><div class="line">            self._test(cost * self.rate)</div><div class="line"></div><div class="line">    <span class="comment"># 使用time.sleep模拟任务耗时</span></div><div class="line"><span class="meta">    @timing.timeit(prefix="[Core] ")</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_test</span><span class="params">(self, cost)</span>:</span></div><div class="line">        time.sleep(cost)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test1</span><span class="params">(Test)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        Test.__init__(self, <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test2</span><span class="params">(Test)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        Test.__init__(self, <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test3</span><span class="params">(Test)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        Test.__init__(self, <span class="number">3</span>)</div><div class="line"></div><div class="line">test1 = Test1()</div><div class="line">test2 = Test2()</div><div class="line">test3 = Test3()</div><div class="line">test1.test()</div><div class="line">test2.test()</div><div class="line">test3.test()</div><div class="line">test1.timing.show_timing_log()</div></pre></td></tr></table></figure>
<p>这段代码的运行效果如下图所示：</p>
<img src="/posts/65c8a24f/p5.png" alt="p5.png" title="">]]></content>
      
        <categories>
            
            <category> 神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[朴素的网络结构]]></title>
      <url>/posts/3bb962a6/</url>
      <content type="html"><![CDATA[<p>这一节主要介绍一下如何进行最简单的封装，对于更加完善的实现则会放在下一节。由于我本人实现的最终版本有上千行，囿于篇幅、无法在这里进行叙述，感兴趣的观众老爷们可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/NN/Basic/Networks.py" target="_blank" rel="external">这里</a></p>
<p>总结前文说明过的诸多子结构、不难得知我们用于封装它们的朴素网络结构至少需要实现如下这些功能：</p>
<ul>
<li>加入一个 Layer</li>
<li>获取各个模型参数对应的优化器</li>
<li>协调各个子结构以实现前向传导算法和反向传播算法</li>
</ul>
<a id="more"></a>
<p>接下来就看看具体的实现。先看其基本框架：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> f_NN.Layers <span class="keyword">import</span> *</div><div class="line"><span class="keyword">from</span> f_NN.Optimizers <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="keyword">from</span> Util.Bases <span class="keyword">import</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">NaiveNN</span><span class="params">(ClassifierBase)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._layers、self._weights、self._bias：记录着所有Layer、权值矩阵、偏置量</div><div class="line">        self._w_optimizer、self._b_optimizer：记录着所有权值矩阵的和偏置量的优化器</div><div class="line">        self._current_dimension：记录着当前最后一个Layer所含的神经元个数</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(NaiveNN, self).__init__()</div><div class="line">        self._layers, self._weights, self._bias = [], [], []</div><div class="line">        self._w_optimizer = self._b_optimizer = <span class="keyword">None</span></div><div class="line">        self._current_dimension = <span class="number">0</span></div></pre></td></tr></table></figure>
<p>接下来实现加入 Layer 的功能。由于我们只打算进行朴素实现、所以应该对输入模型的 Layer 的格式做出一些限制以减少代码量。具体而言、我们对输入模型的 Layer 做出如下三个约束：</p>
<ul>
<li>如果该 Layer 是第一次输入模型的 Layer 的话（亦即<script type="math/tex">L_{1}</script>）、则要求 Layer 的<code>shape</code>属性是一个二元元组，此时<code>shape[0]</code>即为输入数据的维度、<code>shape[1]</code>即为<script type="math/tex">L_{1}</script>的神经元个数<script type="math/tex">n_{1}</script></li>
<li>否则（亦即<script type="math/tex">L_{i},i \geq 2</script>）、我们要求 Layer 输入模型时的<code>shape</code>属性是一元元组，其唯一的元素记录的就是该<code>Layer</code>的神经元个数<script type="math/tex">n_{i}</script></li>
</ul>
<p>比如说、如果我们想设计含有如下结构的神经网络：</p>
<ul>
<li>含有一层 ReLU 隐藏层，该层有 24 个神经元</li>
<li>损失函数为 Sigmoid<script type="math/tex">+</script>Cross Entropy 的组合</li>
</ul>
<p>那么在实现完毕后、需要能够通过如下三行代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">nn = NaiveNN()</div><div class="line">nn.add(ReLU((x.shape[<span class="number">1</span>], <span class="number">24</span>)))</div><div class="line">nn.add(CostLayer((y.shape[<span class="number">1</span>],), <span class="string">"CrossEntropy"</span>, transform=<span class="string">"Sigmoid"</span>))</div></pre></td></tr></table></figure>
<p>来把对应的结构搭建完毕（其中 x、y 是训练集）。以下即为具体实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, layer)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._layers:</div><div class="line">        <span class="comment"># 如果是第一次加入layer、则初始化相应的属性</span></div><div class="line">        self._layers, self._current_dimension = [layer], layer.shape[<span class="number">1</span>]</div><div class="line">        <span class="comment"># 调用初始化权值矩阵和偏置量的方法</span></div><div class="line">        self._add_params(layer.shape)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        _next = layer.shape[<span class="number">0</span>]</div><div class="line">        layer.shape = (self._current_dimension, _next)</div><div class="line">        <span class="comment"># 调用进一步处理Layer的方法</span></div><div class="line">        self._add_layer(layer, self._current_dimension, _next)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_add_params</span><span class="params">(self, shape)</span>:</span></div><div class="line">    self._weights.append(np.random.randn(*shape))</div><div class="line">    self._bias.append(np.zeros((<span class="number">1</span>, shape[<span class="number">1</span>])))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_add_layer</span><span class="params">(self, layer, *args)</span>:</span></div><div class="line">    _current, _next = args</div><div class="line">    self._add_params((_current, _next))</div><div class="line">    self._current_dimension = _next</div><div class="line">    self._layers.append(layer)</div></pre></td></tr></table></figure>
<p>然后就需要获取各个模型参数对应的优化器并实现前向传导算法和反向传播算法了。鉴于我们实现的是朴素的版本、我们只允许用户自定义学习速率、优化器使用的算法及总的迭代次数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, lr=<span class="number">0.001</span>, optimizer=<span class="string">"Adam"</span>, epoch=<span class="number">10</span>)</span>:</span></div><div class="line">    <span class="comment"># 调用相应方法来初始化优化器</span></div><div class="line">    self._init_optimizers(optimizer, lr, epoch)</div><div class="line">    layer_width = len(self._layers)</div><div class="line">    <span class="comment"># 训练的主循环</span></div><div class="line">    <span class="comment"># 需要注意的是，在每次迭代中、我们是用训练集中所有样本来进行训练的</span></div><div class="line">    <span class="keyword">for</span> counter <span class="keyword">in</span> range(epoch):</div><div class="line">        self._w_optimizer.update()</div><div class="line">        self._b_optimizer.update()</div><div class="line">        <span class="comment"># 调用相应方法来进行前向传导算法、把所得的激活值都存储下来</span></div><div class="line">        _activations = self._get_activations(x)</div><div class="line">        <span class="comment"># 调用CostLayer的bp_first方法来进行BP算法的第一步</span></div><div class="line">        _deltas = [self._layers[<span class="number">-1</span>].bp_first(y, _activations[<span class="number">-1</span>])]</div><div class="line">        <span class="comment"># BP算法主体</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">-1</span>, -len(_activations), <span class="number">-1</span>):</div><div class="line">            _deltas.append(self._layers[i - <span class="number">1</span>].bp(</div><div class="line">                _activations[i - <span class="number">1</span>], self._weights[i], _deltas[<span class="number">-1</span>]</div><div class="line">            ))</div><div class="line">        <span class="comment"># 利用各个局部梯度来更新模型参数</span></div><div class="line">        <span class="comment"># 注意由于最后一个是CostLayer对应的占位符、所以无需对其更新</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(layer_width - <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>):</div><div class="line">            self._opt(i, _activations[i - <span class="number">1</span>], _deltas[layer_width - i - <span class="number">1</span>])</div><div class="line">        self._opt(<span class="number">0</span>, x, _deltas[<span class="number">-1</span>])</div></pre></td></tr></table></figure>
<p>这里用到了三个方法、它们的作用为：</p>
<ul>
<li><code>self._init_optimizers</code>：根据优化器的名字、学习速率和迭代次数来初始化优化器</li>
<li><code>self._get_activations</code>：进行前向传导算法</li>
<li><code>self._opt</code>：利用局部梯度和优化器来更新模型的各个参数</li>
</ul>
<p>它们的具体实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_init_optimizers</span><span class="params">(self, optimizer, lr, epoch)</span>:</span></div><div class="line">    <span class="comment"># 利用定义好的优化器工厂来初始化优化器</span></div><div class="line">    <span class="comment"># 注意由于最后一层是CostLayer对应的占位符、所以无需把它输进优化器</span></div><div class="line">    _opt_fac = OptFactory()</div><div class="line">    self._w_optimizer = _opt_fac.get_optimizer_by_name(</div><div class="line">        optimizer, self._weights[:<span class="number">-1</span>], lr, epoch)</div><div class="line">    self._b_optimizer = _opt_fac.get_optimizer_by_name(</div><div class="line">        optimizer, self._bias[:<span class="number">-1</span>], lr, epoch)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_activations</span><span class="params">(self, x)</span>:</span></div><div class="line">    _activations = [self._layers[<span class="number">0</span>].activate(x, self._weights[<span class="number">0</span>], self._bias[<span class="number">0</span>])]</div><div class="line">    <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(self._layers[<span class="number">1</span>:]):</div><div class="line">        _activations.append(layer.activate(</div><div class="line">            _activations[<span class="number">-1</span>], self._weights[i + <span class="number">1</span>], self._bias[i + <span class="number">1</span>]))</div><div class="line">    <span class="keyword">return</span> _activations</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_opt</span><span class="params">(self, i, _activation, _delta)</span>:</span></div><div class="line">    self._weights[i] += self._w_optimizer.run(</div><div class="line">        i, _activation.T.dot(_delta)</div><div class="line">    )</div><div class="line">    self._bias[i] += self._b_optimizer.run(</div><div class="line">        i, np.sum(_delta, axis=<span class="number">0</span>, keepdims=<span class="keyword">True</span>)</div><div class="line">    )</div></pre></td></tr></table></figure>
<p>最后就是模型的预测了，这一部分的实现非常直观易懂：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x, get_raw_results=False)</span>:</span></div><div class="line">    y_pred = self._get_prediction(np.atleast_2d(x))</div><div class="line">    <span class="keyword">if</span> get_raw_results:</div><div class="line">        <span class="keyword">return</span> y_pred</div><div class="line">    <span class="keyword">return</span> np.argmax(y_pred, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_prediction</span><span class="params">(self, x)</span>:</span></div><div class="line">    <span class="comment"># 直接取前向传导算法得到的最后一个激活值即可</span></div><div class="line">    <span class="keyword">return</span> self._get_activations(x)[<span class="number">-1</span>]</div></pre></td></tr></table></figure>
<p>至此、一个朴素的神经网络结构就实现完了；虽说该模型有诸多不足之处，但其基本的框架和模式却都是有普适性的、且它的表现也已经相当不错。可以通过在螺旋线数据集上做几组实验来直观地感受一下这个朴素神经网络的分类能力、结果如下图所示：</p>
<img src="/posts/3bb962a6/p1.png" alt="p1.png" title="">
<p>左图是 4 条螺旋线的二类分类问题、准确率为 92.75%；右图为 7 条螺旋线的七类分类问题、准确率为 100%；神经网络的结构则都是两层含 24 个神经元的 ReLU 加 Softmax<script type="math/tex">+</script>Cross Entropy 组合的这个结构，迭代次数则为 1000 次、平均训练时间分别为 0.74秒（左图）和 1.04秒（右图）。注意到虽然我们使用的螺旋线数据集的“旋转程度”比之前使用过的螺旋线数据集的都要大不少、但是神经网络的表现仍然相当不错</p>
]]></content>
      
        <categories>
            
            <category> 神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[参数的更新]]></title>
      <url>/posts/55a23cf0/</url>
      <content type="html"><![CDATA[<p>我们之前曾简单地描述过如何使用随机梯度下降来更新参数，本文则主要会介绍一些应用得更多、效果更好的算法。正如上个系列最后所提及的，这些梯度下降的拓展算法从思想上来说和梯度下降法类似、区别则可以简练地概括如下两点：</p>
<ul>
<li>更新方向不是简单地取为梯度</li>
<li>学习速率不是简单地取为常值</li>
</ul>
<p>虽然我们不会深入地叙述这些算法背后复杂的数学基础、但我们会对每种算法都提供一些直观的解释。需要指出的是、这些算法都是利用局部梯度来获得一个更好的“梯度”、从而使得“梯度下降”变得更优</p>
<a id="more"></a>
<p>具体而言、原始的梯度为：</p>
<script type="math/tex; mode=display">
\frac{\partial L^{*}\left( x \right)}{\partial w_{pq}^{\left( i - 1 \right)}} = \delta_{q}^{\left( i \right)}v_{p}^{\left( i - 1 \right)}</script><p>若想把它向量化、就不得不考虑上训练集中的样本数<script type="math/tex">N</script>，此时：</p>
<ul>
<li>权值矩阵：<script type="math/tex">w^{\left( i - 1 \right)}</script>的维度为<script type="math/tex">n_{i - 1} \times n_{i}</script></li>
<li>输出向量：<script type="math/tex">v^{\left( i - 1 \right)}</script>的维度为<script type="math/tex">N \times n_{i - 1}</script></li>
<li>局部梯度：<script type="math/tex">\delta^{\left( i \right)}</script>的维度为<script type="math/tex">N \times n_{i}</script></li>
</ul>
<p>且有</p>
<script type="math/tex; mode=display">
\frac{\partial L^{*}\left( x \right)}{\partial w^{\left( i - 1 \right)}} = v^{\left( i - 1 \right)^{T}} \times \delta^{\left( i \right)}</script><p>换句话说、原始梯度的向量化形式即为：</p>
<script type="math/tex; mode=display">
\Delta w^{\left( i - 1 \right)} = v^{\left( i - 1 \right)^{T}} \times \delta^{\left( i \right)}</script><p>而本节所要说明的诸多算法、大多都是利用<script type="math/tex">\Delta w^{\left( i - 1 \right)}</script>和其它属性来得到一个比<script type="math/tex">\Delta w^{\left( i - 1 \right)}</script>更好的“梯度”<script type="math/tex">\Delta^{*}w^{\left( i - 1 \right)}</script>、进而把梯度下降从</p>
<script type="math/tex; mode=display">
w^{\left( i - 1 \right)} \leftarrow w^{\left( i - 1 \right)} - \eta\Delta w^{\left( i - 1 \right)}</script><p>变成</p>
<script type="math/tex; mode=display">
w^{\left( i - 1 \right)} \leftarrow w^{\left( i - 1 \right)} - \eta\Delta^{*}w^{\left( i - 1 \right)}</script><p>在接下来的讨论中，我们统一使用<script type="math/tex">w</script>代指要更新的参数、用<script type="math/tex">\Delta w_{t}</script>和<script type="math/tex">\Delta^{*}w_{t}</script>代指第 t 步迭代中得到的原始梯度和优化后的梯度、用<script type="math/tex">\eta</script>代指学习速率。首先需要指出的是，在众多深度学习的成熟框架中、参数的更新过程常常会被单独抽象成若干个模型，我们常常会称这些模型为“优化器（Optimizer）”。顾名思义、优化器能够根据模型的参数和损失来“优化”模型；具体而言，优化器至少需要能够利用各种算法并根据输入的参数与对应的梯度来进行参数的更新。对于有自身 Graph 结构的深度学习框架而言（比如 Tensorflow），用户甚至只需将参数更新的算法和最终的损失值提供给其优化器、然后该优化器就能够利用 Graph 结构来自动更新各个部分的参数</p>
<p>我们所打算实现的优化器属于最朴素的优化器——根据算法与梯度来更新相应参数；由后文的讨论可知，比较优秀的算法在每一步迭代中计算梯度时都不是独立的、而会利用上以前的计算结果。综上所述、可知优化器的框架应该包括如下三个方法：</p>
<ul>
<li>接收欲更新的参数并进行相应处理的方法</li>
<li>利用梯度和自身属性来更新参数的方法</li>
<li>在完成参数更新后更新自身属性的方法</li>
</ul>
<p>尽管一个朴素优化器的实现比较平凡，但对于帮助我们理解各种算法而言还是足够的。考虑到不同算法对应的优化器有许多行为一致的地方，为了合理重复利用代码、我们需要把它们的共性所对应的实现抽象出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self.lr：记录学习速率的参数，默认为0.01</div><div class="line">        self._cache：储存中间结果的参数，在不同算法中的表现会不同</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, lr=<span class="number">0.01</span>, cache=None)</span>:</span></div><div class="line">        self.lr = lr</div><div class="line">        self._cache = cache</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.__class__.__name__</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> str(self)</div><div class="line"></div><div class="line">    <span class="comment"># 接收欲更新的参数并进行相应处理，注意有可能传入多个参数</span></div><div class="line">    <span class="comment"># 默认行为是创建若干个和传入的各个参数形状相等的0矩阵并把它们存在self._cache中</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feed_variables</span><span class="params">(self, variables)</span>:</span></div><div class="line">        self._cache = [</div><div class="line">            np.zeros(var.shape) <span class="keyword">for</span> var <span class="keyword">in</span> variables</div><div class="line">        ]</div><div class="line"></div><div class="line">    <span class="comment"># 利用负梯度和优化器自身的属性来返回最终更新步伐的方法</span></div><div class="line">    <span class="comment"># 注意这里的i是指优化器中的第i个参数</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, i, dw)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div><div class="line"></div><div class="line">    <span class="comment"># 完成参数更新后、更新自身属性的方法</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>接下来就看看各种常用的参数更新算法的说明和相应实现</p>
<h1 id="Vanilla-Update"><a href="#Vanilla-Update" class="headerlink" title="Vanilla Update"></a>Vanilla Update</h1><p>Vanilla 在机器学习中常用来表示“朴实的”、“平凡的”，换句话说、Vanilla<br>Update 和最普通的梯度下降法别无二致，亦即：</p>
<script type="math/tex; mode=display">
\Delta^{*}w_{t} \triangleq \Delta w_{t}</script><p>在实际实现中、Vanilla Update 通常以小批量梯度下降法（MBGD）的形式出现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MBGD</span><span class="params">(Optimizer)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, i, dw)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.lr * dw</div></pre></td></tr></table></figure>
<p>其中<script type="math/tex">dw</script>通常会是一个矩阵（对应 MBGD 算法）而非一个数（对应 SGD 算法）。</p>
<p><strong><em>注意：即使是 SGD、其实也属于 Vanilla Update</em></strong></p>
<h1 id="Momentum-Update"><a href="#Momentum-Update" class="headerlink" title="Momentum Update"></a>Momentum Update</h1><p>Vanilla Update 的缺点是比较明显的：以 MBGD 为例，它每一步迭代中参数的更新是完全独立的、亦即第t步参数的更新方向只依赖于当前所用的 batch，这在物理意义上是不太符合直观的。可以进行如下设想：</p>
<ul>
<li>将损失函数的图像想象成一个山谷、我们的目的是达到谷底</li>
<li>将损失函数某一点的梯度想象成该点对应的坡度</li>
<li>将学习速率想象成沿坡度行走的速度</li>
</ul>
<p>如果是 Vanilla Update 的话，就相当于可能会出现明明前一秒还在以很快的速度往左走、这一秒就突然开始以很快的速度往右走。这种“行进模式”之所以违背直观、是因为没有考虑到我们都很熟悉的“惯性”。Momentum Update 正是通过尝试模拟物体运动时的“惯性”以期望增加算法收敛的速度和稳定性，其优化公式为：</p>
<script type="math/tex; mode=display">
\Delta^{*}w_{t} \triangleq - \frac{\rho}{\eta}v_{t - 1} + \Delta w_{t}</script><p>其中梯度<script type="math/tex">\Delta w_{t}</script>的物理意义即为“动力”、<script type="math/tex">v_{t}</script>的物理意义即为第 t 步迭代中参数的“行进速度”、<script type="math/tex">\rho</script>的物理意义即为惯性，它描述了上一步的行进速度会在多大程度上影响到这一步的行进速度。易知当<script type="math/tex">\rho = 0</script>时、Momentum Update等价于 Vanilla Update</p>
<p>一般来说我们不会把<script type="math/tex">\rho</script>设置为一个常量、而会把它设置成一个会随训练过程的推进而变动的变量；同时一般来说、我们会将<script type="math/tex">\rho</script>的初始值设为 0.5 并逐步将它加大至 0.99。该做法蕴含着如下两个思想：</p>
<ul>
<li>认为训练刚开始时的梯度会比较大而训练后期时梯度会变小，通过逐步调大<script type="math/tex">\rho</script>、我们能够使更新的步伐一直保持在比较大的水平</li>
<li>认为当我们接近谷底时、我们应该尽量减少“动力”带来的影响而保持原有的方向前进。这是因为如果每一步都直接往谷底方向走（亦即运动仅受动力影响）的话、就会很容易由于动力大小难以拿捏而引发震荡</li>
</ul>
<p>该做法所对应的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Momentum</span><span class="params">(Optimizer, metaclass=TimingMeta)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构（Momentum Update版本）</div><div class="line">        self._momentum：记录“惯性”的属性</div><div class="line">        self._step：每一步迭代后“惯性”的增量</div><div class="line">        self._floor、self._ceiling：“惯性”的最小、最大值</div><div class="line">        self._cache：对于Momentum Update而言、该属性记录的就是“行进速度”</div><div class="line">        self._is_nesterov：处理Nesterov Momentum Update的属性，这里暂时按下不表</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, lr=<span class="number">0.01</span>, cache=None, epoch=<span class="number">100</span>, floor=<span class="number">0.5</span>, ceiling=<span class="number">0.999</span>)</span>:</span></div><div class="line">        Optimizer.__init__(self, lr, cache)</div><div class="line">        self._momentum = floor</div><div class="line">        self._step = (ceiling - floor) / epoch</div><div class="line">        self._floor, self._ceiling = floor, ceiling</div><div class="line">        self._is_nesterov = <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, i, dw)</span>:</span></div><div class="line">        dw *= self.lr</div><div class="line">        velocity = self._cache</div><div class="line">        velocity[i] *= self._momentum</div><div class="line">        velocity[i] += dw</div><div class="line">        <span class="keyword">return</span> velocity[i]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">if</span> self._momentum &lt; self._ceiling:</div><div class="line">            self._momentum += self._step</div></pre></td></tr></table></figure>
<p>当然也不是说只能用这种方法来调整<script type="math/tex">\rho</script>的值，对于一些特殊的情况、确实是会有更好且更具针对性的更新策略的</p>
<h1 id="Nesterov-Momentum-Update"><a href="#Nesterov-Momentum-Update" class="headerlink" title="Nesterov Momentum Update"></a>Nesterov Momentum Update</h1><p>从名字不难想象，Nesterov Momentum Update 方法是基于 Momentum Update 方法的，它由 Ilya Sutskever 在 Nesterov 相关工作（Nesterov Accelerated Gradient，常简称为 NAG）的启发下提出。它在凸优化问题下的收敛性会比传统的 Momentum Update 要更好，而在实际任务中它也确实经常表现得更优</p>
<p>Nesterov Momentum Update 的核心思想在于想让算法具有“前瞻性”。简单来说、它会利用“下一步”的梯度而不是“这一步”的梯度来合成出最终的更新步伐（所谓更新步伐、可以直观地理解为“更新方向<script type="math/tex">\times</script>更新幅度”）。可以通过下图来直观地认知这个过程：</p>
<img src="/posts/55a23cf0/p1.png" alt="p1.png" title="">
<p>左图为普通的 Momentum Update、<script type="math/tex">v_{t}</script>经由如下两部分合成而得：</p>
<ul>
<li>起点<script type="math/tex">w_{t - 1}</script>处的行进速度<script type="math/tex">\rho v_{t - 1}</script></li>
<li>中继点<script type="math/tex">{\hat{w}}_{t - 1}</script>处的更新步伐<script type="math/tex">- \eta\Delta w_{t}</script>（<script type="math/tex">w_{t - 1}</script>处的负梯度与学习速率的乘积）</li>
</ul>
<p>右图则为 Nesterov Momentum Update、<script type="math/tex">v_{t}</script>经由如下两部分合成而得：</p>
<ul>
<li>起点<script type="math/tex">w_{t - 1}</script>处的行进速度<script type="math/tex">\rho v_{t - 1}</script></li>
<li>中继点<script type="math/tex">{\hat{w}}_{t - 1}</script>处的更新步伐<script type="math/tex">- \eta\Delta{\hat{w}}_{t}</script>（<script type="math/tex">{\hat{w}}_{t - 1}</script>处的负梯度与学习速率的乘积）</li>
</ul>
<p>于是不难写出 Nesterov Momentum Update 的优化公式：</p>
<script type="math/tex; mode=display">
\Delta^{*}w_{t} \triangleq - \frac{\rho}{\eta}v_{t - 1} + \Delta{\hat{w}}_{t}</script><p>但是这里<script type="math/tex">\Delta{\hat{w}}_{t}</script>的计算却不是一个平凡的问题。对此、Yoshua Bengio 等人在论文《Advances In Optimizing Recurrent Networks》里面提出了一个利用到换参法的解决方案。具体而言、令：</p>
<script type="math/tex; mode=display">
{\hat{w}}_{t - 1} \triangleq w_{t - 1} + \rho v_{t - 1}</script><p>注意到</p>
<script type="math/tex; mode=display">
v_{t} = \rho v_{t - 1} - \eta\Delta{\hat{w}}_{t}</script><p>从而</p>
<script type="math/tex; mode=display">
\begin{align}
\hat{w}_{t} &= w_{t} + \rho v_{t} \\
&= \left( w_{t - 1} + v_{t} \right) + \rho v_{t} \\
&= \left( {\hat{w}}_{t - 1} - \rho v_{t - 1} + \rho v_{t - 1} - \eta\Delta{\hat{w}}_{t} \right) + \rho v_{t} \\
&= {\hat{w}}_{t - 1} + \rho v_{t} - \eta\Delta{\hat{w}}_{t} \\
\end{align}</script><p>综上所述、不难得到换参后的优化公式：</p>
<script type="math/tex; mode=display">
\begin{align}
v_{t} &= \rho v_{t - 1} - \eta\Delta{\hat{w}}_{t} \\
\Delta^{*}w_{t} &\triangleq - \frac{\rho}{\eta}v_{t} + \Delta{\hat{w}}_{t}
\end{align}</script><p>可以看出该更新公式和 Momentum Update 中的更新公式非常类似、从而在实现层面上也基本相同。事实上、只需将 Momentum 优化器中的<code>run</code>方法改写为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, i, dw)</span>:</span></div><div class="line">    dw *= self.lr</div><div class="line">    velocity = self._cache</div><div class="line">    velocity[i] *= self._momentum</div><div class="line">    velocity[i] += dw</div><div class="line">    <span class="comment"># 如果不是Nesterov Momentum Update、可以直接把当成更新步伐</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._is_nesterov:</div><div class="line">        <span class="keyword">return</span> velocity[i]</div><div class="line">    <span class="comment"># 否则、调用公式来计算更新步伐</span></div><div class="line">    <span class="keyword">return</span> self._momentum * velocity[i] + dw</div></pre></td></tr></table></figure>
<p>然后再让 Nesterov Momentum Update 对应的优化器（NAG 优化器）继承 Momentum 优化器、并把<code>self._is_nesterov</code>这项属性设为 True 即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">NAG</span><span class="params">(Momentum)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, lr=<span class="number">0.01</span>, cache=None, epoch=<span class="number">100</span>, floor=<span class="number">0.5</span>, ceiling=<span class="number">0.999</span>)</span>:</span></div><div class="line">        Momentum.__init__(self, lr, cache, epoch, floor, ceiling)</div><div class="line">        self._is_nesterov = <span class="keyword">True</span></div></pre></td></tr></table></figure>
<h1 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h1><p>RMSProp 方法与 Momentum 系的方法最根本的不同在于：Momentum 系算法是通过搜索更优的更新方向来进行优化、而 RMSProp 则是通过实时调整学习速率来进行优化。具体而言、它的优化公式为：</p>
<script type="math/tex; mode=display">
\nabla^{2} \leftarrow \rho\nabla^{2} + \left( 1 - \rho \right)\Delta w_{t}</script><script type="math/tex; mode=display">
\Delta^{*}w_{t} \triangleq \frac{\Delta w_{t}}{\nabla + \epsilon}</script><p>其中有两个变量是需要注意的：</p>
<ul>
<li>中间变量<script type="math/tex">\nabla^{2}</script>，它是从算法开始到当前步骤的所有梯度的某种“累积”</li>
<li>衰减系数<script type="math/tex">\rho</script>，它反映了比较早的梯度对当前梯度的影响、<script type="math/tex">\rho</script>越小则影响越小</li>
</ul>
<p>换句话说、在 RMSProp 算法中，“累积”的梯度越小会导致当前更新步伐越大、反之则会越小。关于这种做法的合理性有许多种解释，我可以提供一个仅供参考的说法：如果徘徊回了原点自然需要奋发图强地开辟新天地、如果已经走了很远自然应该谨小慎微（？？？）</p>
<p>值得一提的是，RMSProp 其实可以算是 AdaGrad（Adaptive Gradient）方法的改进；深入的讨论会牵扯到许多数学理论、这里就只看看应该怎样实现它：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RMSProp</span><span class="params">(Optimizer)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构（RMSProp版本）</div><div class="line">        self.decay_rate：记录的属性，一般会取0.9、0.99或0.999</div><div class="line">        self.eps：算法的平滑项、用于增强算法稳定性，通常取中的某个数</div><div class="line">        self._cache：对于RMSProp而言、该属性记录的就是中间变量</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, lr=<span class="number">0.01</span>, cache=None, decay_rate=<span class="number">0.9</span>, eps=<span class="number">1e-8</span>)</span>:</span></div><div class="line">        Optimizer.__init__(self, lr, cache)</div><div class="line">        self.decay_rate, self.eps = decay_rate, eps</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, i, dw)</span>:</span></div><div class="line">        self._cache[i] = self._cache[i] * self.decay_rate + (<span class="number">1</span> - self.decay_rate) * dw ** <span class="number">2</span></div><div class="line">        <span class="keyword">return</span> self.lr * dw / (np.sqrt(self._cache[i] + self.eps))</div></pre></td></tr></table></figure>
<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>Adam 算法是应用最广泛的、一般而言效果最好的算法，它高效、稳定、适用于绝大多数的应用场景。一般来说如果不知道该选哪种优化算法的话、使用Adam常常会是个不错的选择。它的数学理论背景是相当复杂的、这里就只写出它的一个简化版的优化公式：</p>
<script type="math/tex; mode=display">
\begin{align}
\Delta &\leftarrow \beta_{1}\Delta + \left( 1 - \beta_{1} \right)\Delta w_{t} \\
\nabla^{2} &\leftarrow \beta_{2}\nabla^{2} + \left( 1 - \beta_{2} \right)\Delta^{2}w_{t}
\end{align}</script><script type="math/tex; mode=display">
\Delta^{*}w_{t} \triangleq \frac{\Delta}{\nabla + \epsilon}</script><p>从直观上来说、Adam 算法很像是 Momentum 系算法和 RMSProp 算法的结合（中间变量<script type="math/tex">\Delta</script>的相关计算类似于 Momentum 系算法对更新方向的选取、中间变量<script type="math/tex">\nabla</script>的相关计算则类似于 RMSProp 算法对学习速率的调整）。同样的、我们跳过其背后的那一套数学理论并仅说明如何进行实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span><span class="params">(Optimizer)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构（Adam版本）</div><div class="line">        self.beta1、self.beta2：记录、的属性，一般会取、</div><div class="line">        self.eps：意义与RMSProp中的eps一致、常取</div><div class="line">        self._cache：对于Adam而言、该属性记录的就是中间变量和中间变量</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, lr=<span class="number">0.01</span>, cache=None, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.999</span>, eps=<span class="number">1e-8</span>)</span>:</span></div><div class="line">        Optimizer.__init__(self, lr, cache)</div><div class="line">        self.beta1, self.beta2, self.eps = beta1, beta2, eps</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feed_variables</span><span class="params">(self, variables)</span>:</span></div><div class="line">        self._cache = [</div><div class="line">            [np.zeros(var.shape) <span class="keyword">for</span> var <span class="keyword">in</span> variables],</div><div class="line">            [np.zeros(var.shape) <span class="keyword">for</span> var <span class="keyword">in</span> variables],</div><div class="line">        ]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, i, dw)</span>:</span></div><div class="line">        self._cache[<span class="number">0</span>][i] = self._cache[<span class="number">0</span>][i] * self.beta1 + (<span class="number">1</span> - self.beta1) * dw</div><div class="line">        self._cache[<span class="number">1</span>][i] = self._cache[<span class="number">1</span>][i] * self.beta2 + (<span class="number">1</span> - self.beta2) * (dw ** <span class="number">2</span>)</div><div class="line">        <span class="keyword">return</span> self.lr * self._cache[<span class="number">0</span>][i] / (np.sqrt(self._cache[<span class="number">1</span>][i] + self.eps))</div></pre></td></tr></table></figure>
<h1 id="Factory"><a href="#Factory" class="headerlink" title="Factory"></a>Factory</h1><p>前 5 小节分别介绍了 5 种常用的优化算法及对应的优化器的实现、这一小节主要介绍的就是如何应用这些实现好的优化器。虽说直接对它们进行调用也无不可，但是考虑到编程中的一些“套路”、我们可以实现一个简单的工厂来“生产”这些优化器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">OptFactory</span>:</span></div><div class="line">    <span class="comment"># 将所有能用的优化器存进一个字典</span></div><div class="line">    available_optimizers = &#123;</div><div class="line">        <span class="string">"MBGD"</span>: MBGD,</div><div class="line">        <span class="string">"Momentum"</span>: Momentum, <span class="string">"NAG"</span>: NAG,</div><div class="line">        <span class="string">"RMSProp"</span>: RMSProp, <span class="string">"Adam"</span>: Adam,</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment"># 定义一个能通过优化器名字来获取优化器的方法</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_optimizer_by_name</span><span class="params">(self, name, variables, lr, epoch)</span>:</span></div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            _optimizer = self.available_optimizers[name](lr)</div><div class="line">            <span class="keyword">if</span> variables <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">                _optimizer.feed_variables(variables)</div><div class="line">            <span class="keyword">if</span> epoch <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> isinstance(_optimizer, Momentum):</div><div class="line">                _optimizer.epoch = epoch</div><div class="line">            <span class="keyword">return</span> _optimizer</div></pre></td></tr></table></figure>
<p>至此、我们就对如何更新神经网络中的参数进行了比较全面的说明；结合上一节所实现的 Layer 结构、我们接下来要做的事情就很明确了：定义一个总的框架、把 Layer、Optimizer 有机地结合在一起、从而得到最终能用的 NN 模型</p>
]]></content>
      
        <categories>
            
            <category> 神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> 算法 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[特殊的层结构]]></title>
      <url>/posts/a33ff165/</url>
      <content type="html"><![CDATA[<p>在神经网络模型中有一类特殊的 Layer 结构——它们不会独立地存在、而会“依附”在某个 Layer 之后以实现某种特定的功能。一般我们会称这种特殊的 Layer 结构为附加层（SubLayer）</p>
<a id="more"></a>
<p>CostLayer 算是一个比较特殊的 SubLayer：它附加在输出层的后面、能够根据输出进行相应的变换并得到模型的损失。“根据输出得到损失”即是 CostLayer 实现的特定的功能。对于一般的 SubLayer、它的思想是清晰的：为了在 Layer 的输出的基础上进行一些变换以得到更好的输出；换句话说、SubLayer 通常可以优化 Layer 的输出</p>
<p>对于 SubLayer 和 SubLayer、SubLayer 和 Layer 之间的关系，我们可以类比于决策树中的根节点（Root）、叶节点（Leaf）等概念来提出“根层（Root Layer）”和“叶层（Leaf Layer）”的概念。不妨以下图为例：</p>
<img src="/posts/a33ff165/p1.png" alt="p1.png" title="">
<p>其中<script type="math/tex">L_{i}</script>为第 i 层 Layer、<script type="math/tex">SL_{1}^{\left( i \right)},SL_{2}^{\left( i \right)},SL_{3}^{\left( i \right)}</script>为附加在<script type="math/tex">L_{i}</script>后的三个 SubLayer，且：</p>
<ul>
<li><script type="math/tex">L_{i},SL_{1}^{\left( i \right)},SL_{2}^{\left( i \right)}</script>分别为<script type="math/tex">SL_{1}^{\left( i \right)},SL_{2}^{\left( i \right)},SL_{3}^{\left( i \right)}</script>的父层</li>
<li><script type="math/tex">SL_{1}^{\left( i \right)},SL_{2}^{\left( i \right)},SL_{3}^{\left( i \right)}</script>分别为<script type="math/tex">L_{i},SL_{1}^{\left( i \right)},SL_{2}^{\left( i \right)}</script>的子层</li>
<li><script type="math/tex">L_{i}</script>为<script type="math/tex">SL_{1}^{\left( i \right)},SL_{2}^{\left( i \right)},SL_{3}^{\left( i \right)}</script>的 Root Layer</li>
<li><script type="math/tex">SL_{3}^{\left( i \right)}</script>为<script type="math/tex">L_{i}</script>的 Leaf Layer</li>
</ul>
<p>从 SubLayer 的思想可以看出、SubLayer 很像一个“局部优化器”；不过和下一节中要介绍的优化器不同，它不是通过更新模型参数来优化模型、而是通过变换 Layer 的输出来优化模型</p>
<p>在进一步叙述之前、我们需要先定义一下层结构之间的“关联”是什么。具体而言：</p>
<ul>
<li>Layer 和 Layer 之间的关联即为相应的权值矩阵，比如<script type="math/tex">L_{i},L_{i + 1}</script>之间的关联即为<script type="math/tex">w^{\left( i \right)}</script></li>
<li>SubLayer 之间的关联亦即 SubLayer 和 Root Layer 之间的关联都只是“占位符”、它们没有任何实际的作用。这其实是符合 SubLayer 作为“局部优化器”的定位的</li>
</ul>
<p>从而 SubLayer 的所有行为大体上可以概括如下：</p>
<ul>
<li>在前向传导中、它会根据自身的属性和算法来优化从父层处得到的更新</li>
<li>在反向传播中、它会有如下三种行为：<ul>
<li>SubLayer 之间的关联以及 SubLayer 和 Root Layer 之间的关联不会被更新、因为它们仅仅是占位符</li>
<li>SubLayer 作为“局部优化器”、本身可能会有一些参数，这些参数则可能会被 BP 算法更新、但影响域仅在该 SubLayer 的内部（Normalize 会是一个很好的例子）</li>
<li>Layer 之间的关联的更新是通过 Leaf Layer 完成的。具体而言、<script type="math/tex">L_{i}</script>的 Leaf Layer 会利用<script type="math/tex">L_{i}</script>的激活函数来完成局部梯度的计算</li>
</ul>
</li>
</ul>
<p>最后这里所谓的“利用 Leaf Layer”可以通过下面两张图来直观认知在存在 SubLayer 的情况下、前向传导算法和反向传播算法的表现：</p>
<img src="/posts/a33ff165/p2.png" alt="p2.png" title="">
<img src="/posts/a33ff165/p3.png" alt="p3.png" title="">
<p>典型的 SubLayer 有前文提到过的 Dropout 和 Normalize。它们都是近年来才提出的技术，其中 Dropout 是由 Srivastava 等人在 Journal of Machine Learning Research 15 (2014） 上的一篇论文中最先提出的、全文共 30 页，感兴趣的读者可以直接参见<a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf" target="_blank" rel="external">这里</a>；Normalize 则是 Batch Normalization 对应的特殊层结构、它是由 Sergey loffe 和 Christian Szegedy 在 2015 年最先提出的，感兴趣的读者可以直接参见<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="external">这里</a>，这里仅直观地进行一些说明：</p>
<ul>
<li>Dropout 的核心思想在于提高模型的泛化能力：它会在每次迭代中依概率去掉对应 Layer 的某些神经元，从而每次迭代中训练的都是一个小的神经网络</li>
<li>Normalize 的核心思想在于把父层的输出进行“归一化”、从而期望能够解决由于网络结构过深而引起的“梯度消失”等问题</li>
</ul>
<p>虽说实现 SubLayer 本身并不是一个特别困难的任务，但是处理 SubLayer 之间的关联、SubLayer 与 Layer 之间的关联以及反向传播算法却是一件相当麻烦的事；具体的实现细节比较繁杂、这里就不进行叙述了。观众老爷们可以尝试按照上文相关的思想和定义来进行实现、我个人实现的版本则可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/NN/Basic/Layers.py" target="_blank" rel="external">这里</a></p>
<p><strong><em>注意：我们会在下个系列的文章中利用 Tensorflow 框架进行相关的实现，彼时我们会结合具体实现对 Dropout 和 Normalize 进行深入一些的介绍</em></strong></p>
<p>至此、神经网络会用到的所有层结构就都大致说明了一遍，接下来就要解决一个至关重要但又还没解决的问题了：如何使用局部梯度来更新相应 Layer 中的参数</p>
]]></content>
      
        <categories>
            
            <category> 神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[反向传播算法]]></title>
      <url>/posts/437097cd/</url>
      <content type="html"><![CDATA[<p>本文要讲的就是可能最让我们头疼的反向传播（Backpropagation，常简称为 BP）算法了。事实上，如果不是要做理论研究而只是想快速应用神经网络来干活的话，了解如何使用 Tensorflow 等帮我们处理梯度的成熟的框架可能会比了解算法细节要更好一些（我们会把本章实现的模型的 Tensorflow 版本放在下一个系列中进行说明）。但即使如此，了解神经网络背后的原理总是有益的，在某种意义上它也能告诉我们应该选择怎样的神经网络结构来进行具体的训练</p>
<a id="more"></a>
<h1 id="算法概述"><a href="#算法概述" class="headerlink" title="算法概述"></a>算法概述</h1><p>顾名思义、BP 算法和前向传导算法的“方向”其实刚好相反：前向传导是由后往前（将激活值）一路传导，反向传播则是由前往后（将梯度）一路传播</p>
<p><strong><em>注意：这里的“前”和“后”的定义是由 Layer 和输出层的相对位置给出的。具体而言，越靠近输出层的 Layer 我们称其越“前”、反之就称其越“后”</em></strong></p>
<p>先从直观上理解一下 BP 算法的原理。总体上来说，BP 算法的目的是利用梯度来更新结构中的参数以使得损失函数最小化。这里面就涉及两个问题：</p>
<ul>
<li>如何获得（局部）梯度？</li>
<li>如何使用梯度进行更新？</li>
</ul>
<p>本节会简要介绍第一个问题应该如何解决、并说一种第二个问题的解决方案，对第二个问题的详细讨论会放在第 5 节中；正如前面提到的，BP 是在前向传导之后进行的、从前往后传播的算法，所以我们需要时刻记住这么一个要求——对于每个 Layer（<script type="math/tex">L_{i}</script>）而言、其（局部）梯度的计算除了能利用它自身的数据外、仅会利用到（假设包括输入、输出层在内一共有 m 个 Layer、符号约定与上述符号约定一致）：</p>
<ul>
<li>上一层（<script type="math/tex">L_{i - 1}</script>）传过来的激活值<script type="math/tex">v^{\left( i - 1 \right)}</script>和下一层（<script type="math/tex">L_{i + 1}</script>）传回来的（局部）梯度<script type="math/tex">\delta^{\left( i + 1 \right)}</script></li>
<li>该层与下一层之间的线性变换矩阵（亦即权值矩阵）<script type="math/tex">w^{\left( i \right)}</script></li>
</ul>
<p>其中出现的“局部梯度”的概念即为 BP 算法获得梯度的核心。其数学定义为：</p>
<script type="math/tex; mode=display">
\delta_{j}^{\left( i \right)} = \frac{\partial L\left( x \right)}{\partial u_{j}^{\left( i \right)}}</script><p>一般而言我们会用其向量形式：</p>
<script type="math/tex; mode=display">
\delta^{\left( i \right)} = \frac{\partial L\left( x \right)}{\partial u^{\left( i \right)}}</script><p>需要注意的是、此时数据样本数<script type="math/tex">N</script>不可忽视，亦即<script type="math/tex">u^{\left( i \right)}</script>、<script type="math/tex">\delta^{\left( i \right)}</script>其实都是<script type="math/tex">N \times n_{i}</script>的矩阵。</p>
<p>由名字不难想象、局部梯度<script type="math/tex">\delta^{\left( i \right)}</script>仅在局部起作用且能在局部进行计算，事实上 BP 算法也正是通过将局部梯度进行传播来计算各个参数在全局的梯度、从而使参数的更新变得非常高效的。有关局部梯度的推导是相当繁复的工作、其中的细节我们会在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>中进行说明，这里就只叙述最终结果：</p>
<ul>
<li>BP 算法的第一步为得到损失函数的梯度：  <script type="math/tex; mode=display">
\delta^{\left( m \right)} = \frac{\partial L\left( y,v^{\left( m \right)} \right)}{\partial v^{\left( m \right)}}*\phi_{m}^{'}\left( u^{\left( m \right)} \right)</script>注意式中运算符“<em>”两边都是维的矩阵（其中即为输出层所含神经元的个数）、运算符“</em>”本身代表的则是 element wise 操作，亦即若  <script type="math/tex; mode=display">
x = \left( x_{1},\ldots,x_{m} \right)^{T},\ \ y = \left( y_{1},\ldots,y_{m} \right)^{T}</script>则有  <script type="math/tex; mode=display">
x*y = \left( x_{1}y_{1},\ldots,x_{m}y_{m} \right)^{T}</script>同理若  <script type="math/tex; mode=display">x = \begin{bmatrix}
x_{11} & \cdots & x_{1q} \\
\vdots & \ddots & \vdots \\
x_{p1} & \cdots & x_{\text{pq}} \\
\end{bmatrix},\ \ y = \begin{bmatrix}
y_{11} & \cdots & y_{1q} \\
\vdots & \ddots & \vdots \\
y_{p1} & \cdots & y_{\text{pq}} \\
\end{bmatrix}</script></li>
<li>BP 算法剩下的步骤即为局部梯度的反向传播过程：  <script type="math/tex; mode=display">
\delta^{\left( i \right)} = \delta^{\left( i + 1 \right)} \times w^{\left( i \right)T}*\phi_{i}^{'}\left( u^{\left( i \right)} \right)</script>这里列举出各个变量的维度以便理解：<ul>
<li>局部梯度、激活函数的导数：<script type="math/tex">\delta^{\left( i \right)}</script>、<script type="math/tex">\phi_{i}^{'}\left( u^{\left( i \right)} \right)</script>的维度为<script type="math/tex">N \times n_{i}</script></li>
<li>权值矩阵的转置：<script type="math/tex">w^{\left( i \right)T}</script>的维度为<script type="math/tex">n_{i + 1} \times n_{i}</script></li>
<li>局部梯度：<script type="math/tex">\delta^{\left( i + 1 \right)}</script>的维度为<script type="math/tex">N \times n_{i + 1}</script></li>
</ul>
</li>
</ul>
<p>如果不管推导的话、求局部梯度的过程本身其实是相当清晰简洁的；如果所用的编程语言（比如 Python）能够直接支持矩阵操作的话、求解局部梯度的过程完全可以用一行实现</p>
<h1 id="损失函数的选择"><a href="#损失函数的选择" class="headerlink" title="损失函数的选择"></a>损失函数的选择</h1><p>我们在上一篇文章中说过、损失函数通常需要结合输出层的激活函数来讨论，这是因为在 BP 算法的第一步所计算的局部梯度<script type="math/tex">\delta^{\left( m \right)}</script>正是由损失函数对模型输出<script type="math/tex">v^{\left( m \right)}</script>的梯度<script type="math/tex">\frac{\partial L\left( y,v^{\left( m \right)} \right)}{\partial v^{\left( m \right)}}</script>和激活函数的导数<script type="math/tex">\phi_{m}^{'}\left( u^{\left( m \right)} \right)</script>通过 element<br>wise 操作“*”得到的。不难想象对于固定的损失函数而言、会有相对“适合它”的激活函数，而事实上、结合激活函数来选择损失函数确实是一个常见的做法。用得比较多的组合有以下四个：</p>
<ul>
<li>Sigmoid 系以外的激活函数$+$距离损失函数（MSE）<br>MSE 可谓是一个万金油，它不会出太大问题、同时也基本不能很好地解决问题。这里特地指出不能使用 Sigmoid 系激活函数（目前我们提到过的 Sigmoid 系函数只有 Sigmoid 函数本身和 Tanh 函数），是因为 Sigmoid 系激活函数在图像两端都非常平缓（可以结合之前的图来理解）、从而会引起梯度消失的现象。MSE 这个损失函数无法处理这种梯度消失、所以一般来说不会用 Sigmoid 系激活函数<script type="math/tex">+</script>MSE 这个组合。具体而言，由于对 MSE 来说：  <script type="math/tex; mode=display">
L\left( y,v^{\left( m \right)} \right) = \left\| y - v^{\left( m \right)} \right\|^{2}</script>所以  <script type="math/tex; mode=display">
\frac{\partial L\left( y,v^{\left( m \right)} \right)}{\partial v^{\left( m \right)}} = - 2\left\lbrack y - v^{\left( m \right)} \right\rbrack</script>结合 Sigmoid 的函数图像不难得知：若模型的输出<script type="math/tex">v^{\left( m \right)} \rightarrow \mathbf{0} = \left( 0,\ldots,0 \right)^{T}</script>但真值<script type="math/tex">y = \mathbf{1} = \left( 1,\ldots,1 \right)^{T}</script>；此时虽然预测值和真值之间的误差几乎达到了极大值、不过由于  <script type="math/tex; mode=display">
\frac{\partial L\left( y,v^{\left( m \right)} \right)}{\partial v^{\left( m \right)}} = - 2\left\lbrack y - v^{\left( m \right)} \right\rbrack \rightarrow - 2 \cdot \mathbf{1}</script><script type="math/tex; mode=display">
\phi_{m}^{'}\left( u^{\left( m \right)} \right) \rightarrow \mathbf{0}</script>从而  <script type="math/tex; mode=display">
\delta^{\left( m \right)} = \frac{\partial L\left( y,v^{\left( m \right)} \right)}{\partial v^{\left( m \right)}}*\phi_{m}^{'}\left( u^{\left( m \right)} \right) \rightarrow \mathbf{0}</script>亦即第一步算的局部梯度就趋近于 0 向量了；可以想象在此场景下模型参数的更新将会非常困难、收敛速度因为会变得很慢。前文提到若干次的梯度消失、正是这种由于激活函数在接近饱和时变化过于缓慢所引发的现象</li>
<li>Sigmoid<script type="math/tex">+</script>Cross Entropy<br>Sigmoid 激活函数之所以有梯度消失的现象是因为它的导函数形式为  <script type="math/tex; mode=display">
\phi^{'}\left( x \right) = \phi\left( x \right)\left\lbrack 1 - \phi\left( x \right) \right\rbrack</script>想要解决梯度消失的话，比较自然的想法是定义一个损失函数、使得它导函数的分母上有<script type="math/tex">\phi\left( x \right)\left\lbrack 1 - \phi\left( x \right) \right\rbrack</script>这一项。而前文说过的 Cross Entropy 这个损失函数恰恰满足该条件、因为其导函数形式为  <script type="math/tex; mode=display">
\frac{\partial L\left( y,v^{\left( m \right)} \right)}{\partial v^{\left( m \right)}} = - \frac{y}{v^{\left( m \right)}} + \frac{1 - y}{1 - v^{\left( m \right)}} = - \frac{y - v^{\left( m \right)}}{v^{\left( m \right)}\left( 1 - v^{\left( m \right)} \right)}</script>且<script type="math/tex">v^{\left( m \right)} = \phi_{m}\left( u^{\left( m \right)} \right)</script>，从而有  <script type="math/tex; mode=display">
\delta^{\left( m \right)} = \frac{\partial L\left( y,v^{\left( m \right)} \right)}{\partial v^{\left( m \right)}}*\phi_{m}^{'}\left( u^{\left( m \right)} \right)\phi_{m}\left( u^{\left( m \right)} \right) - y</script>这就相当完美地解决了梯度消失问题</li>
<li>Softmax<script type="math/tex">+</script>Cross Entropy / log-likelihood<br>这两个组合的核心都在于前面额外用了一个 Softmax。Softmax 比起一个激活函数来说更像是一个（针对向量的）变换，它具有相当好的直观：能把模型的输出向量通过指数函数归一化成一个概率向量。比如若输出是<script type="math/tex">\left( 1,\ 1,\ 1,\ 1 \right)^{T}</script>，经过 Softmax 之后就是<script type="math/tex">\left( 0.25,\ 0.25,\ 0.25,\ 0.25 \right)^{T}</script>。它的严格定义式也比较简洁（以<script type="math/tex">\varphi</script>代指 Softmax）：  <script type="math/tex; mode=display">
v^{\left( m \right)} = \varphi\left( u^{\left( m \right)} \right) = \left( \varphi_{1},\ldots,\varphi_{K} \right)^{T}</script>其中  <script type="math/tex; mode=display">
u^{\left( m \right)} = \left( u_{1}^{\left( m \right)},\ldots,u_{K}^{\left( m \right)} \right)^{T}</script><script type="math/tex; mode=display">
\varphi_{i} = \frac{e^{u_{i}^{\left( m \right)}}}{\sum_{j = 1}^{K}e^{u_{j}^{\left( m \right)}}}</script>从而  <script type="math/tex; mode=display">
\varphi_{i}^{'}\left( u_{i}^{\left( m \right)} \right) = \frac{e^{u_{i}^{\left( m \right)}} \cdot \sum_{j = 1}^{K}e^{v_{j}^{\left( m \right)}} - \left( e^{u_{i}^{\left( m \right)}} \right)^{2}}{\left( \sum_{j = 1}^{K}e^{u_{j}^{\left( m \right)}} \right)^{2}} = \varphi_{i} - \varphi_{i}^{2} = \varphi_{i}\left( 1 - \varphi_{i} \right)</script>亦即  <script type="math/tex; mode=display">
\varphi^{'}\left( u^{\left( m \right)} \right) = \varphi\left( u^{\left( m \right)} \right)\left\lbrack 1 - \varphi\left( u^{\left( m \right)} \right) \right\rbrack</script>这和 Sigmoid 函数的导函数形式一模一样<br>之所以要进行这一步变换，其实是因为 Cross Entropy 用概率向量来定义损失（要比用随便一个各位都在内的向量）更好、且 log-likelihood 更是只能使用概率向量来定义损失。由于 Sigmoid<script type="math/tex">+</script>Cross Entropy 的求导已经介绍过且 Softmax 导函数与 Sigmoid 导函数一致、这里就只需给出 Softmax<script type="math/tex">+</script>log-likelihood 的求导公式：  <script type="math/tex; mode=display">
\frac{\partial L^{*}(x)}{\partial w_{\text{pq}}^{\left( m - 1 \right)}} = \left\{ \begin{matrix}
\left( \varphi_{p} - 1 \right)v_{q}^{\left( m - 1 \right)},\ \ & p = k \\
0,\ \ & p \neq k \\
\end{matrix} \right.\</script>亦即  <script type="math/tex; mode=display">
\delta_{p}^{\left( m \right)} = \left\{ \begin{matrix}
\varphi_{p} - 1,\ \ & p = k \\
0,\ \ & p \neq k \\
\end{matrix} \right.\</script>其中  <script type="math/tex; mode=display">
L^{*}(x) \triangleq L\left( y,v^{\left( m \right)} \right)</script>且  <script type="math/tex; mode=display">
y\in c_k</script>将该式写成向量化的形式并不容易、但从实现的角度来说却也不算困难（以上公式的推导过程会放在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>中）。不过需要注意的是，像这样算出来的局部梯度会是一个非常稀疏的矩阵（亦即大部分元素都是 0）、从而很容易导致训练根本无法收敛，这也正是为何前文说 log-likelihood 的原始形式不尽合理。改进的方法很简单、只需将损失函数变为：  <script type="math/tex; mode=display">
L\left( y,G\left( x \right) \right) = \left\{ \begin{matrix}
- \ln v_{p},\ \ & p = k \\
- \ln{(1 - v_{p})},\ \ & p \neq k \\
\end{matrix} \right.\</script>即可。不难发现这个改进后的损失函数和 Cross Entropy 从本质上来说是一样的、所以我们在后文不会实现 log-likelihood 对应的算法</li>
</ul>
<p>以上我们对如何获取局部梯度作了比较充分的介绍，对于如何利用局部梯度更新参数的详细讲解会放在第5节、这里仅介绍一种最简单的做法：直接应用上一章说过的随机梯度下降（SGD）。由于可以推出（推导过程同样可参见<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>）：</p>
<script type="math/tex; mode=display">
\frac{\partial L^{*}\left( x \right)}{\partial w_{\text{pq}}^{\left( i - 1 \right)}} = \delta_{q}^{\left( i \right)}v_{p}^{\left( i - 1 \right)}</script><p>从而只需</p>
<script type="math/tex; mode=display">
w_{\text{pq}}^{\left( i - 1 \right)} \leftarrow w_{\text{pq}}^{\left( i - 1 \right)} - \eta\delta_{q}^{\left( i \right)}v_{p}^{\left( i - 1 \right)}</script><p>即可完成一步训练</p>
<h1 id="相关实现"><a href="#相关实现" class="headerlink" title="相关实现"></a>相关实现</h1><p>至此、神经网络中的 Layer 结构所需完成的所有工作就都已经介绍完毕，接下来就是归纳总结并着手实现的环节了。不难发现，每个 Layer 除了前向传导和反向传播算法核心以外，其余结构、功能等都完全一致；再加上这两大算法的核心只随激活函数的不同而不同、所以只需把激活函数留给具体的子类定义即可，其余的部分则都应该抽象成一个基类。由简入繁、我们可以先进行一个朴素的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Layer</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self.shape：记录着上个Layer和该Layer所含神经元的个数，具体而言：</div><div class="line">            self.shape[0] = 上个Layer所含神经元的个数</div><div class="line">            self.shape[1] = 该Layer所含神经元的个数</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, shape)</span>:</span></div><div class="line">        self.shape = shape</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.__class__.__name__</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> str(self)</div><div class="line"></div><div class="line"><span class="meta">    @property</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">name</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> str(self)</div></pre></td></tr></table></figure>
<p>以上是对结构的抽象。由于我们实现的是一个比较朴素的版本、所以这个框架里也没有太多东西；如果要考虑上特殊的结构（比如后文会介绍的 Dropout、Normalize 等“附加层”）的话、就需要再往这个框架中添加若干属性</p>
<p>接下来就是对两大算法（前向传导、反向传播）的抽象（不妨设当前 Layer 为<script type="math/tex">L_{i}</script>）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="comment"># 将激活函数的导函数的定义留给子类定义</span></div><div class="line"><span class="comment"># 需要特别指出的是、这里的参数y其实是</span></div><div class="line"><span class="comment"># 这样设置参数y的原因会马上在后文叙述、这里暂时按下不表</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">derivative</span><span class="params">(self, y)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"><span class="comment"># 前向传导算法的封装</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">activate</span><span class="params">(self, x, w, bias)</span>:</span></div><div class="line">    <span class="keyword">return</span> self._activate(x.dot(w) + bias)</div><div class="line"></div><div class="line"><span class="comment"># 反向传播算法的封装，主要是利用上面定义的导函数derivative来完成局部梯度的计算</span></div><div class="line"><span class="comment"># 其中：、、prev_delta；</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bp</span><span class="params">(self, y, w, prev_delta)</span>:</span></div><div class="line">    <span class="keyword">return</span> prev_delta.dot(w.T) * self.derivative(y)</div></pre></td></tr></table></figure>
<p>出于优化的考虑、我们在上述实现的<code>bp</code>方法中留了一些“余地”。具体而言，考虑到神经网络最后两层通常都是前文提到的 4 种组合之一、所以针对它们进行算法的优化是合理的；而为了具有针对性、CostLayer 的 BP 算法就无法包含在这个相对而言抽象程度比较高的方法里面。具体细节会在后文进行介绍、这里只说一下 CostLayer 自带的 BP 算法的大致思路：它会根据需要将相应的额外变换（比如 Softmax 变换）和损失函数整合在一起并算出一个整合后的梯度</p>
<p>以上便完成了 Layer 结构基类的定义，接下来就说明一下为何在定义<code>derivative</code>这个计算激活函数导函数的方法时、传进去的参数是该 Layer 的输出值<script type="math/tex">v^{\left( i \right)} = \phi_{i}\left( u^{\left( i \right)} \right)</script>。其实理由相当平凡：很多常用的激活函数的导函数使用函数值来定义会比使用自变量来定义要更好（所谓更好是指形式上更简单、从而计算开销会更小）。接下来就罗列一下上文提到过的、6 种激活函数的导函数的形式：</p>
<ul>
<li>逻辑函数（Sigmoid）  <script type="math/tex; mode=display">
\phi\left( x \right) = \frac{1}{1 + e^{- x}}</script><script type="math/tex; mode=display">
\Rightarrow \phi^{'}\left( x \right) = \frac{e^{- x}}{\left( 1 + e^{- x} \right)^{2}} = \phi\left( x \right)\left\lbrack 1 - \phi\left( x \right) \right\rbrack</script></li>
<li>正切函数（Tanh）  <script type="math/tex; mode=display">
\phi\left( x \right) = \tanh(x) = \frac{1 - e^{- 2x}}{1 + e^{- 2x}}</script><script type="math/tex; mode=display">
\Rightarrow \phi^{'}\left( x \right) = \frac{4e^{-2x}}{\left( 1 + e^{-2x} \right)^{2}} = 1 - \phi\left( x \right)^{2}</script></li>
<li>线性整流函数（Rectified Linear Unit，常简称为 ReLU）  <script type="math/tex; mode=display">
\phi\left( x \right) = \max\left( 0,x \right)</script><script type="math/tex; mode=display">
\Rightarrow \phi^{'}\left( x \right) = \left\{ \begin{matrix}
0,\ \ & x \leq 0 \\
1,\ \ & x > 0 \\
\end{matrix} \right.\  = \left\{ \begin{matrix}
0,\ \ &\phi(x) = 0 \\
1,\ \ &\phi(x) \neq 0 \\
\end{matrix} \right.\</script></li>
<li>ELU 函数（Exponential Linear Unit）  <script type="math/tex; mode=display">
\phi\left( \alpha,x \right) = \left\{ \begin{matrix}
\alpha\left( e^{x} - 1 \right),\ \ & x < 0 \\
x,\ \ & x \geq 0 \\
\end{matrix} \right.\</script><script type="math/tex; mode=display">
\Rightarrow \phi^{'}\left( \alpha,x \right) = \left\{ \begin{matrix}
\alpha\left( e^{x} - 1 \right),\ \ & x < 0 \\
1,\ \ & x \geq 0 \\
\end{matrix} \right.\  = \left\{ \begin{matrix}
\phi\left( x \right) + \alpha,\ \ & x < 0 \\
1,\ \ & x \geq 0 \\
\end{matrix} \right.\</script></li>
<li>Softplus 函数  <script type="math/tex; mode=display">
\phi\left( x \right) = \ln{(1 + e^{x})}</script><script type="math/tex; mode=display">
\Rightarrow \phi^{'}\left( x \right) = \frac{e^{x}}{1 + e^{x}} = 1 - \frac{1}{e^{\phi\left( x \right)}}</script></li>
<li>恒同映射（Identity）  <script type="math/tex; mode=display">
\phi\left( x \right) = x</script><script type="math/tex; mode=display">
\Rightarrow \phi^{'}\left( x \right) = 1</script></li>
</ul>
<p>可以看出，用<script type="math/tex">\phi(x)</script>来表示<script type="math/tex">\phi'(x)</script>确实基本都比用<script type="math/tex">x</script>来表示<script type="math/tex">\phi'(x)</script>要简单、高效不少，所以在传参时将激活函数值传给计算导函数值的方法是合理的</p>
<p>接下来就是实现具体要用在神经网络中的 Layer 了；由前文讨论可知、它们只需定义相应的激活函数及（用激活函数值表示的）导函数即可。以经典的 Sigmoid 激活函数所对应的 Layer 为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sigmoid</span><span class="params">(Layer)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">derivative</span><span class="params">(self, y)</span>:</span></div><div class="line">        <span class="keyword">return</span> y * (<span class="number">1</span> - y)</div></pre></td></tr></table></figure>
<p>其余 5 个激活函数对应 Layer 的实现是类似的、观众老爷们可以尝试对照着公式进行实现，我个人实现的版本则可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/f_NN/Layers.py" target="_blank" rel="external">这里</a></p>
<p>最后我们要实现的就是那有些特殊的 CostLayer 了。总结一下前文所说的诸多内容、可知实现 CostLayer 时需要注意如下两点：</p>
<ul>
<li>没有激活函数、但可能会有特殊的变换函数（比如说 Softmax），同时还需要定义某个损失函数</li>
<li>定义导函数时，需要考虑到自身特殊的变换函数并计算相应的、整合后的梯度</li>
</ul>
<p>具体的代码也是非常直观的，先来看看其基本架构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CostLayer</span><span class="params">(Layer)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._available_cost_functions：记录所有损失函数的字典</div><div class="line">        self._available_transform_functions：记录所有特殊变换函数的字典</div><div class="line">        self._cost_function、self._cost_function_name：记录损失函数及其名字的两个属性</div><div class="line">        self._transform_function 、self._transform：记录特殊变换函数及其名字的两个属性</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, shape, cost_function=<span class="string">"MSE"</span>)</span>:</span></div><div class="line">        super(CostLayer, self).__init__(shape)</div><div class="line">        self._available_cost_functions = &#123;</div><div class="line">            <span class="string">"MSE"</span>: CostLayer._mse,</div><div class="line">            <span class="string">"SVM"</span>: CostLayer._svm,</div><div class="line">            <span class="string">"CrossEntropy"</span>: CostLayer._cross_entropy</div><div class="line">        &#125;</div><div class="line">        self._available_transform_functions = &#123;</div><div class="line">            <span class="string">"Softmax"</span>: CostLayer._softmax,</div><div class="line">            <span class="string">"Sigmoid"</span>: CostLayer._sigmoid</div><div class="line">        &#125;</div><div class="line">        self._cost_function_name = cost_function</div><div class="line">        self._cost_function = self._available_cost_functions[cost_function]</div><div class="line">        <span class="keyword">if</span> transform <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">and</span> cost_function == <span class="string">"CrossEntropy"</span>:</div><div class="line">            self._transform = <span class="string">"Softmax"</span></div><div class="line">            self._transform_function = CostLayer._softmax</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self._transform = transform</div><div class="line">            self._transform_function = self._available_transform_functions.get(</div><div class="line">                transform, <span class="keyword">None</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> self._cost_function_name</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_activate</span><span class="params">(self, x, predict)</span>:</span></div><div class="line">        <span class="comment"># 如果不使用特殊的变换函数的话、直接返回输入值即可</span></div><div class="line">        <span class="keyword">if</span> self._transform_function <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">return</span> x</div><div class="line">        <span class="comment"># 否则、调用相应的变换函数以获得结果</span></div><div class="line">        <span class="keyword">return</span> self._transform_function(x)</div><div class="line"></div><div class="line">    <span class="comment"># 由于CostLayer有自己特殊的BP算法，所以这个方法不会被调用、自然也无需定义</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_derivative</span><span class="params">(self, y, delta=None)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>接下来就要定义相应的变换函数了。由前文对四种损失函数组合的讨论及上述代码都可以看出、我们需要定义 Softmax 和 Sigmoid 这两种变换函数及相应导函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@staticmethod</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">safe_exp</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.exp(x - np.max(x, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>))</div><div class="line"></div><div class="line"><span class="meta">@staticmethod</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_softmax</span><span class="params">(y, diff=False)</span>:</span></div><div class="line">    <span class="keyword">if</span> diff:</div><div class="line">        <span class="keyword">return</span> y * (<span class="number">1</span> - y)</div><div class="line">    exp_y = CostLayer.safe_exp(y)</div><div class="line">    <span class="keyword">return</span> exp_y / np.sum(exp_y, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="meta">@staticmethod</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_sigmoid</span><span class="params">(y, diff=False)</span>:</span></div><div class="line">    <span class="keyword">if</span> diff:</div><div class="line">        <span class="keyword">return</span> y * (<span class="number">1</span> - y)</div><div class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-y))</div></pre></td></tr></table></figure>
<p>其中前三行代码实现的<code>safe_exp</code>方法主要利用了如下恒等式：</p>
<script type="math/tex; mode=display">
\frac{e^{v_{i}^{\left( m \right)}}}{\sum_{j = 1}^{K}e^{v_{j}^{\left( m \right)}}} = \frac{e^{v_{i}^{\left( m \right)} - c}}{\sum_{j = 1}^{K}e^{v_{j}^{\left( m \right)} - c}}</script><p>其中<script type="math/tex">c</script>是任意一个常数；如果此时我们取</p>
<script type="math/tex; mode=display">
c = \max{\{ v_{1}^{\left( m \right)},\ldots,v_{K}^{\left( m \right)}\}}</script><p>这样的话分母、分子中所有幂次都不大于 0，从而不会出现由于某个<script type="math/tex">v_{i}^{\left( m \right)}</script>很大而导致对应的<script type="math/tex">e^{v_{i}^{\left( m \right)}}</script>很大、并因而导致数据溢出的情况，从而在一定程度上保证了数值稳定性</p>
<p>接下来要实现的就是各种损失函数以及能够根据损失函数计算整合梯度的方法了；考虑到可拓展性，我们不仅要优化特定的组合对应的整合算法、同时也要考虑一般性的情况。因此在实现损失函数的同时、实现损失函数的导函数是有必要的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算整合梯度的方法，注意这里返回的是负梯度</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bp_first</span><span class="params">(self, y, y_pred)</span>:</span></div><div class="line">    <span class="comment"># 如果是Sigmoid / Softmax和Cross Entropy的组合、就用进行优化</span></div><div class="line">    <span class="comment"># 注意返回时需要返回负梯度，下同</span></div><div class="line">    <span class="keyword">if</span> self._cost_function_name == <span class="string">"CrossEntropy"</span> <span class="keyword">and</span> (</div><div class="line">            self._transform == <span class="string">"Softmax"</span> <span class="keyword">or</span> self._transform == <span class="string">"Sigmoid"</span>):</div><div class="line">        <span class="keyword">return</span> y - y_pred</div><div class="line">    <span class="comment"># 否则、就只能用普适性公式进行计算：</span></div><div class="line">    <span class="comment">#            （没有特殊变换函数）</span></div><div class="line">    <span class="comment">#  （有特殊变换函数）</span></div><div class="line">    dy = -self._cost_function(y, y_pred)</div><div class="line">    <span class="keyword">if</span> self._transform_function <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> dy</div><div class="line">    <span class="keyword">return</span> dy * self._transform_function(y_pred, diff=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment"># 定义计算损失的方法</span></div><div class="line"><span class="meta">@property</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> y, y_pred: self._cost_function(y, y_pred, <span class="keyword">False</span>)</div><div class="line"></div><div class="line"><span class="comment"># 定义距离损失函数及其导函数</span></div><div class="line"><span class="meta">@staticmethod</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_mse</span><span class="params">(y, y_pred, diff=True)</span>:</span></div><div class="line">    <span class="keyword">if</span> diff:</div><div class="line">        <span class="keyword">return</span> -y + y_pred</div><div class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * np.average((y - y_pred) ** <span class="number">2</span>)</div><div class="line"></div><div class="line"><span class="comment"># 定义Cross Entropy损失函数及其导函数</span></div><div class="line"><span class="meta">@staticmethod</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cross_entropy</span><span class="params">(y, y_pred, diff=True, eps=<span class="number">1e-8</span>)</span>:</span></div><div class="line">    <span class="keyword">if</span> diff:</div><div class="line">        <span class="keyword">return</span> -y / (y_pred + eps) + (<span class="number">1</span> - y) / (<span class="number">1</span> - y_pred + eps)</div><div class="line">    <span class="keyword">return</span> np.average(-y * np.log(y_pred + eps) - (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - y_pred + eps))</div></pre></td></tr></table></figure>
<p>至此、我们打算实现的朴素神经网络模型中的所有 Layer 结构就都实现完毕了。下一节我们会介绍一些特殊的 Layer 结构，它们不会整合在我们的朴素神经网络结构中；但是如果想在实际任务中应用神经网络的话、了解它们是有必要的</p>
]]></content>
      
        <categories>
            
            <category> 神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> 算法 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[前向传导算法]]></title>
      <url>/posts/2a8cdd6/</url>
      <content type="html"><![CDATA[<p>时至今日，在各个编程语言的世界里、神经网络的成熟的库都可谓不在少数；这可能就导致有许多人虽然能够熟练应用神经网络、但对于其内部机制却不甚了解。事实上就笔者所展开的简单调查来看，有不少平时经常用到神经网络的程序员其实对神经网络的数学部分有一种“望而生畏”的感觉、其中各种梯度的计算更是让他们发出“眼花缭乱”的感叹</p>
<p>虽然很想说一些令人鼓舞的话，但是如果从繁复性来说、神经网络算法确实是我们目前为止介绍过的算法中推导步骤最多的；不过可以保证的是，如果把算法的逻辑理清，那么静下心来好好演算一下的话、就会觉得它比想象中的简单</p>
<a id="more"></a>
<h1 id="算法概述"><a href="#算法概述" class="headerlink" title="算法概述"></a>算法概述</h1><p>如果把前文所说过的内容提炼、总结一下的话，就会发现我们其实已经把前向传导算法的过程都叙述了一遍。以一个简单的神经网络结构为例：</p>
<img src="/posts/2a8cdd6/p1.png" alt="一个简单的三层（单隐层）神经网络" title="一个简单的三层（单隐层）神经网络">
<p><strong><em>注意：虽然上图将一个个的“节点”画了出来，但是本篇文章及今后的所有讨论中、我们都应该时刻记住：神经网络的基本组成单元是层（Layer）而不是节点，之所以用节点来说明问题也仅仅是为了简化问题、在实现中是需要将节点上的算法“整合”成层的算法的</em></strong></p>
<p>在展开叙述前、做一些符号约定是有必要的：</p>
<ul>
<li>记上图中的神经网络从左到右对应的 Layer 为<script type="math/tex">L_{1},L_{2},L_{3}</script>、记<script type="math/tex">L_{i}</script>中从上往下数的第 j 个神经元为<script type="math/tex">u_{ij}</script></li>
<li>记<script type="math/tex">L_{i}</script>对应的：<ul>
<li>神经元个数为<script type="math/tex">n_{i}</script>（从而<script type="math/tex">n_{1} = 3</script>、<script type="math/tex">n_{2} = 5</script>、<script type="math/tex">n_{3} = 2</script>）</li>
<li>激活函数、偏置量分别为<script type="math/tex">\phi_{i}</script>、<script type="math/tex">b^{\left( i \right)}</script>（注意<script type="math/tex">b^{\left( 3 \right)}</script>其实不会被用到）</li>
</ul>
</li>
<li>记<script type="math/tex">L_i,L_{i+1}</script>之间的权值矩阵为<script type="math/tex">w^{(i)}</script>、神经元<script type="math/tex">u_{ij},u_{i+1,k}</script>之间的权值为<script type="math/tex">w_{jk}^{(i)}</script>，可知：  <script type="math/tex; mode=display">
w^{\left( i \right)} = \begin{bmatrix}
w_{11}^{\left( i \right)} & \cdots & w_{1,n_{i + 1}}^{\left( i \right)} \\
\vdots & \ddots & \vdots \\
w_{n_{i}1}^{\left( i \right)} & \cdots & w_{n_{i},n_{i + 1}}^{\left( i \right)} \\
\end{bmatrix}_{n_{i} \times n_{i + 1}},\ \ i = 1,2</script></li>
<li>记<script type="math/tex">L_{i}</script>对应的输入、输出为<script type="math/tex">u^{\left( i \right)},v^{\left( i \right)}</script></li>
<li>记模型的输入、输出集为<script type="math/tex">X</script>、<script type="math/tex">Y</script>，样本数为 N，损失函数为<script type="math/tex">L</script>；一般我们会要求<script type="math/tex">L</script>是一个二元对称函数，亦即对于<script type="math/tex">L</script>的输入空间中的任意两个向量（矩阵）<script type="math/tex">p</script>、<script type="math/tex">q</script>都有：  <script type="math/tex; mode=display">
L\left( p,q \right) = L(q,p)</script></li>
</ul>
<p>那么上述神经网络的前向传导算法的所有步骤即为（运算符“<script type="math/tex">\times</script>”代表矩阵乘法、后同）：</p>
<ul>
<li><script type="math/tex">u^{\left( 1 \right)} = X</script>、<script type="math/tex">v^{\left( 1 \right)} = \phi_{1}(u^{\left( 1 \right)})</script>，注意<script type="math/tex">u^{\left( 1 \right)}</script>、<script type="math/tex">v^{\left( 1 \right)}</script>都是<script type="math/tex">N \times 3</script>维矩阵</li>
<li><script type="math/tex">u^{\left( 2 \right)} = v^{\left( 1 \right)} \times w^{\left( 1 \right)} + b^{\left( 1 \right)}</script>、<script type="math/tex">v^{\left( 2 \right)} = \phi_{2}(u^{\left( 2 \right)})</script>，注意<script type="math/tex">w^{\left( 1 \right)}</script>是<script type="math/tex">3 \times 5</script>的矩阵、所以<script type="math/tex">u^{\left( 2 \right)}</script>、<script type="math/tex">v^{\left( 2 \right)}</script>都是<script type="math/tex">N \times 5</script>维矩阵</li>
<li><script type="math/tex">u^{\left( 3 \right)} = v^{\left( 2 \right)} \times w^{\left( 2 \right)} + b^{\left( 2 \right)}</script>、<script type="math/tex">v^{\left( 3 \right)} = \phi_{3}(u^{\left( 3 \right)})</script>，注意<script type="math/tex">w^{\left( 2 \right)}</script>是<script type="math/tex">5 \times 2</script>的矩阵、所以<script type="math/tex">u^{\left( 3 \right)}</script>、<script type="math/tex">v^{\left( 3 \right)}</script>都是<script type="math/tex">N \times 2</script>维矩阵</li>
</ul>
<p>其中<script type="math/tex">v^{\left( 3 \right)}</script>即为模型的输出、<script type="math/tex">L\left( v^{\left( 3 \right)},Y \right) = L\left( Y,v^{\left( 3 \right)} \right)</script>即为模型在<script type="math/tex">(X,Y)</script>上的损失。可以看到这个过程确实相当平凡、但是里面蕴含的数学思想却是有趣而深刻的，接下来我们就分析一下其中的一些细节</p>
<p><strong><em>注意：以上这个例子中的神经网络模型其实是一个二分类模型（<script type="math/tex">n_{3} = 2</script>），如果想用神经网络解决多分类问题（比如 K 分类问题）的话、只需自然地将输出层的神经元个数设为类别个数（<script type="math/tex">n_{3} \leftarrow K</script>）即可。此外，简便起见，如果我们没有特别指出的话、那么下文中所讨论的情况都是<script type="math/tex">N = 1</script>、亦即样本集里只有单样本的情形</em></strong></p>
<h1 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h1><p>首先说说前文不断在提却又没有细说的激活函数<script type="math/tex">\phi</script>。直观来讲，所谓激活函数、正是整个结构中非线性扭曲力。这里介绍几个常见的激活函数：</p>
<h2 id="逻辑函数（Sigmoid）"><a href="#逻辑函数（Sigmoid）" class="headerlink" title="逻辑函数（Sigmoid）"></a>逻辑函数（Sigmoid）</h2><script type="math/tex; mode=display">
\phi\left( x \right) = \frac{1}{1 + e^{- x}}</script><p>其函数图像如下图所示：</p>
<img src="/posts/2a8cdd6/p2.png" alt="p2.png" title="">
<h2 id="正切函数（Tanh）"><a href="#正切函数（Tanh）" class="headerlink" title="正切函数（Tanh）"></a>正切函数（Tanh）</h2><script type="math/tex; mode=display">
\phi\left( x \right) = \tanh\left( x \right) = \frac{1 - e^{- 2x}}{1 + e^{- 2x}}</script><p>其函数图像如下图所示：</p>
<img src="/posts/2a8cdd6/p3.png" alt="p3.png" title="">
<h2 id="线性整流函数（ReLU）"><a href="#线性整流函数（ReLU）" class="headerlink" title="线性整流函数（ReLU）"></a>线性整流函数（ReLU）</h2><p>ReLU 的全称是 Rectified Linear Unit，定义式很简洁：</p>
<script type="math/tex; mode=display">
\phi\left( x \right) = \max\left( 0,x \right)</script><p>其函数图像如下图所示（注意纵轴范围与上述两个激活函数不同）：</p>
<img src="/posts/2a8cdd6/p4.png" alt="p4.png" title="">
<h2 id="ELU-函数（Exponential-Linear-Unit）"><a href="#ELU-函数（Exponential-Linear-Unit）" class="headerlink" title="ELU 函数（Exponential Linear Unit）"></a>ELU 函数（Exponential Linear Unit）</h2><script type="math/tex; mode=display">
\phi\left( \alpha,x \right) = \left\{ \begin{matrix}
\alpha\left( e^{x} - 1 \right),\ \ & x < 0 \\
x,\ \ & x \geq 0 \\
\end{matrix} \right.\</script><p>我们在实现时会取<script type="math/tex">\alpha=1</script>、其函数图像如下图所示：</p>
<img src="/posts/2a8cdd6/p5.png" alt="p5.png" title="">
<h2 id="Softplus"><a href="#Softplus" class="headerlink" title="Softplus"></a>Softplus</h2><script type="math/tex; mode=display">
\phi\left( x \right) = \ln{(1 + e^{x})}</script><p>其函数图像如下图所示：</p>
<img src="/posts/2a8cdd6/p6.png" alt="p6.png" title="">
<h2 id="恒同映射（Identify）"><a href="#恒同映射（Identify）" class="headerlink" title="恒同映射（Identify）"></a>恒同映射（Identify）</h2><script type="math/tex; mode=display">
\phi\left( x \right) = x</script><p>其函数图像从略</p>
<p>囿于篇幅、这些激活函数的由来及背后相关的错综复杂的数学理论研究就不展开叙述了；我们只需知道，神经网络之所以为非线性模型的关键、其实就在于激活函数</p>
<p>然后来看看层与层之间的权值矩阵<script type="math/tex">w</script>以及偏置量<script type="math/tex">b</script>、它们的意义也都有比较好的解释：</p>
<ul>
<li><script type="math/tex">w</script>能把从激活函数得到的函数值线性映射到另一个维度的空间上</li>
<li><script type="math/tex">b</script>能在此基础上再进行一步平移的操作</li>
</ul>
<p>其中<script type="math/tex">w</script>的重要性似乎无需过多说明也能让人明白，但<script type="math/tex">b</script>的重要性相对而言可能就没那么明显。为了直观体会偏置量<script type="math/tex">b</script>的重要性、可以设想这么一个场景（取之前的三层网络结构来说明问题）：</p>
<ul>
<li>激活函数全是中心对称的函数（比如常见的 tanh 函数）、亦即：  <script type="math/tex; mode=display">
\phi_{i}\left( x \right) + \phi_{i}\left( - x \right) = 0,\ \ i = 1,2,3</script></li>
<li>训练样本集为：  <script type="math/tex; mode=display">
D = \{\left( x_{1},y_{1} \right),(x_{2},y_{2})\}</script></li>
<li>权值矩阵<script type="math/tex">w^{\left( 1 \right)}</script>、<script type="math/tex">w^{\left( 2 \right)}</script>可变但偏置量恒为 0</li>
</ul>
<p>在此场景下不难想象，无论我们怎样进行训练、模型<script type="math/tex">G</script>在训练集<script type="math/tex">D</script>上的准确率都不可能达到 100%。这是因为我们有：</p>
<script type="math/tex; mode=display">
G(x) = \phi_{3}\left( \phi_{2}\left( \phi_{1}\left( x \right) \cdot w^{\left( 1 \right)} \right) \cdot w^{\left( 2 \right)} \right)</script><p>从而由激活函数为中心对称函数可知：</p>
<script type="math/tex; mode=display">
\begin{align}
G\left( - x \right) &= \phi_{3}\left( \phi_{2}\left( \phi_{1}\left( - x \right) \cdot w^{\left( 1 \right)} \right) \cdot w^{\left( 2 \right)} \right) \\
&= \phi_{3}\left( \phi_{2}\left( - \phi_{1}\left( x \right) \cdot w^{\left( 1 \right)} \right) \cdot w^{\left( 2 \right)} \right) \\
&= \phi_{3}\left( - \phi_{2}\left( \phi_{1}\left( x \right) \cdot w^{\left( 1 \right)} \right) \cdot w^{\left( 2 \right)} \right) \\
&= - \phi_{3}\left( \phi_{2}\left( \phi_{1}\left( x \right) \cdot w^{\left( 1 \right)} \right) \cdot w^{\left( 2 \right)} \right) \\
&= - G(x)
\end{align}</script><p>亦即</p>
<script type="math/tex; mode=display">
G\left( x_{1} \right) = - G(x_{2})</script><p>但我们有<script type="math/tex">y_{1} = y_{2} = - 1</script>、所以模型<script type="math/tex">G</script>不可能同时预测对<script type="math/tex">(x_{1},y_{1})</script>和<script type="math/tex">(x_{2},y_{2})</script>。事实上由上述讨论可知、此时模型<script type="math/tex">G</script>所做的预测必定是关于输入空间“中心对称”的，这当然不是一个良好的结果。而如果我们引入偏置量的话、上述的对称性就会被打破，这就是偏置量重要性的其中一个比较浅显、直观的方面</p>
<h1 id="损失函数（Cost-Function）"><a href="#损失函数（Cost-Function）" class="headerlink" title="损失函数（Cost Function）"></a>损失函数（Cost Function）</h1><p>注意到前向传导算法的最后一步是将模型的输出与真值相比较、并通过损失函数的作用来得到一个损失。损失函数有时也写作 Loss Function、我们之前已经提及它许多次。损失函数的直观意义是明确的：它是模型对数据拟合程度的反映；拟合得越差、损失函数的值就应该越大。如果同时考虑到梯度下降法的应用、我们自然还应该期望，当损失函数在函数值比较大（亦即模型的表现越差）时、它对应的梯度也要比较大（亦即更新参数的幅度也要比较大）</p>
<p>由于我们此前没有对梯度下降法进行过深刻的应用（上个系列中的随机梯度下降只是一个相当粗浅的应用）、所以至今为止我们涉及到的损失函数基本只满足了“模型越差函数值越大”这一点，对于“函数值越大则梯度越大”这一点则没怎么考虑到。而对于神经网络而言、梯度下降可谓就是训练的全部，时至今日也没能出现能够与之抗衡的其余算法、最多也只是不断地研究出各式各样的梯度下降法的变体而已；所以对于神经网络来说，定义一个足够合适的损失函数是有必要的。接下来就介绍其中最常用的几个，为此需要先做符号约定：</p>
<ul>
<li>假设样本为<script type="math/tex">(x,y)</script></li>
<li>假设共有 K 类：<script type="math/tex">\{ c_{1},\ldots,c_{K}\}</script></li>
<li>假设讨论的模型为<script type="math/tex">G</script>、其输出（向量）为<script type="math/tex">G(x)</script></li>
</ul>
<p>其中<script type="math/tex">x \in \mathbb{R}^{n}</script>、<script type="math/tex">y \in \mathbb{R}^{K}</script>，且<script type="math/tex">y</script>是除了一位为 1、其余位都是 0 的向量。换句话说，若<script type="math/tex">y \in c_{k}</script>、那么<script type="math/tex">y</script>除了第 k 位为 1、其余位都是 0</p>
<p><strong><em>注意：这种<script type="math/tex">y</script>的表示方法通常叫做 one-hot representation</em></strong></p>
<p>在神经网络的训练算法中、损失函数通常需要结合输出层的激活函数来讨论；不过如果只考虑前向传导算法、只叙述损失函数的基本形式就可以：</p>
<ul>
<li>距离损失函数  <script type="math/tex; mode=display">
L\left( y,G\left( x \right) \right) = \left\| y - G\left( x \right) \right\|^{2} = \left\lbrack y - G\left( x \right) \right\rbrack^{2}</script>该损失函数对应着最小平方误差准则（Minimum Squared Error，常简称为 MSE），它的直观意义是明确的：模型预测和真值的（欧氏）距离越大、损失就越大，反之就越小</li>
<li>交叉熵损失函数（要求<script type="math/tex">G(x)</script>每一位的取值都在<script type="math/tex">(0,1)</script>中）  <script type="math/tex; mode=display">
L\left( y,G\left( x \right) \right) = - \left\lbrack y\ln{G\left( x \right)} + \left( 1 - y \right)\ln\left( 1 - G\left( x \right) \right) \right\rbrack</script>其中交叉熵（Cross Entropy）是信息论中的一个概念、其本身是有一定内涵的，感兴趣的观众老爷可以参见<a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank" rel="external">维基百科</a>来了解背后的那一套数学理论。囿于篇幅、我们无法展开叙述这一部分，但是从交叉熵的名字就可以看出、它和决策树里面提到过的熵有千丝万缕的关系；考虑到熵是定义在概率分布上的、所以进一步要求是一个概率向量（亦即进一步要求<script type="math/tex">\sum_{k=1}^Kv_k=1</script>）是一个非常合理的做法</li>
<li>log-likelihood 损失函数（要求<script type="math/tex">G(x)</script>是一个概率向量、以及不妨假设<script type="math/tex">y\in c_k</script>）  <script type="math/tex; mode=display">
L\left( y,G\left( x \right) \right) = - \ln v_{k}</script>换句话说，log-likelihood 即为模型预测的、真值 y 对应的类（<script type="math/tex">c_k</script>）的概率的负对数。需要指出的是、log-likelihood 这种原始的定义方式在神经网络里面不尽合理，我们会在下一节进行相关的讨论</li>
</ul>
<p>以上、我们就比较完整地叙述了一遍前向传导算法。可以看出在前向传导算法中、神经网络的各个 Layer 结构在很多地方的表现都一致、所以把 Layer 的共性抽象出来是有必要的。事实上再通过后面对神经网络的训练算法（反向传播算法）的说明我们就可以看出，每个变换层除了所对应的激活函数有所不同以外、其余部分的表现都几乎一样；而 CostLayer 虽然表现会有点不同（比如需要额外考虑损失函数、从而导致反向传播的形式会有些许改变）、其总体结构仍与变换层大致相同</p>
]]></content>
      
        <categories>
            
            <category> 神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[从感知机到多层感知机]]></title>
      <url>/posts/3fa9a563/</url>
      <content type="html"><![CDATA[<p>“神经网络”这个概念本身其实是一个庞大的交叉学科，而在机器学习领域里、“神经网络”则是“人工神经网络（Artificial Neural Network）”的简称。顾名思义，本系列所将介绍的神经网络模型多少借鉴了神经生理学关于神经网络的研究、并尝试通过数学建模来描述机器智能，这也正是为何许多机器学习相关书籍对神经网络的介绍都会从“真正的”神经网络开始（比如介绍细胞体、树突、轴突和突触之类的）。然而个人认为，直到目前为止、我们一般应用的神经网络结构其实和真正的神经网络之结构之间的差距还是相当大的；如果按照生物学意义上的神经网络来理解人工神经网络的话，虽说从直观上来说可能更加易懂、但在逻辑和原理的层面上反而会造成混淆</p>
<a id="more"></a>
<p>为此我们就跳过“老生常谈”般的、介绍生物学意义上的神经网络的部分并直接把数学建模后的结果进行说明：近现代最常用的NN模型其实脱胎于 1943 年由 W. S. McCulloch 和 W. H. Pitts 提出的 McCulloch-Pitts 神经元模型（常简称为 M-P 神经元模型），它针对单个的神经元进行了数学建模。具体而言、M-P 模型是具有如下三个功能的模型：</p>
<ul>
<li>能够接收 n 个 M-P 模型传递过来的信号</li>
<li>能够在信号的传递过程中为信号分配权重</li>
<li>能够将得到的信号进行汇总、变换并输出</li>
</ul>
<p>可以通过下图来直观认知 M-P 模型的结构：</p>
<img src="/posts/3fa9a563/p1.png" alt="p1.png" title="">
<p>图中的<script type="math/tex">x_{1},\ldots,x_{n}</script>即为 n 个 M-P 模型的输出信号、<script type="math/tex">w_{1},\ldots,w_{n}</script>即为这 n 个信号对应的权值；<script type="math/tex">\phi</script>即为所示神经元对输入信号的变换函数、y 即为模型的输出。一般而言我们可以把 y 写成：</p>
<script type="math/tex; mode=display">
y = \phi\left( \sum_{i = 1}^{n}{w_{i}x_{i}} + b \right)</script><p>其中 b 为神经元对输入信号的“平移”。我们通常会称<script type="math/tex">\phi</script>为激活函数而称 b 为偏置量，有关它们的详细讨论会在<a href="/posts/2a8cdd6/" title="前向传导算法">前向传导算法</a>中进行、这里就暂时先按下不表</p>
<p>有了 M-P 神经元模型的话、基于它来定义神经网络似乎就不是一件困难的事了；事实上、只需要把许多 M-P 神经元按照一定的层次结构进行连接即可。一个非常自然的想法就是构建一个有向无环图（DAG 图），其输入节点和输出节点视具体问题而定。比如若想通过三维的输入来得到二维的输出、我们可以简单地以 M-P 模型为有向无环图中的节点来构造一个如下图所示的有向无环图：</p>
<img src="/posts/3fa9a563/p2.png" alt="p2.png" title="">
<p>如果人工神经网络模型真的能够对任意 DAG 图都能进行高效训练的话、那么说它和真正的神经网络能够互相类比可能也不算夸张；然而遗憾的是，由于现在我们对矩阵运算的依赖程度很大（因为矩阵运算是被高度优化了的），所以目前主流的神经网络模型结构基本都是一类及其特殊的 DAG 图。具体而言、主流人工神经网络模型是以“层（Layer）”（而不是以“节点”）为基本单位的，其结构大致如下图所示：</p>
<img src="/posts/3fa9a563/p3.png" alt="以“层”为基本单位" title="以“层”为基本单位">
<p>其中，输入（层）、变换层和输出（层）都可以想象为是若干 M-P 神经元“排列在一起”而组成的“神经层”、从而整张神经网络即为由若干神经层“堆叠而成”的一个结构。不难想象在这种情况下、同一层中的所有 M-P 神经元会共享激活函数<script type="math/tex">\phi</script>和偏置量 b，所以通常我们会针对层结构定义<script type="math/tex">\phi</script>和 b 而不是针对单个的神经元定义<script type="math/tex">\phi</script>和 b</p>
<p>如果确实想以“节点”为基本单位、那么上图所示结构可以化为如下图所示的模型：</p>
<img src="/posts/3fa9a563/p4.png" alt="以“节点”为基本单位" title="以“节点”为基本单位">
<p>其中除了输出层外、当前层的每个节点都会出来一个箭头指向下一层中的每个节点，这也正是当前层将信号传输给下一层的方式。容易想象当没有变换层时、人工神经网络就会“退化”成我们上个系列中讲过的感知机。事实上可以将第一张图所示的神经元看作是只有一个神经元的输出层并令<script type="math/tex">\phi</script>为恒同映射、亦即：</p>
<script type="math/tex; mode=display">
\phi(x) = x,\ \ \forall x\mathbb{\in R}</script><p>那么就有</p>
<script type="math/tex; mode=display">
y = \sum_{i = 1}^{n}{w_{i}x_{i}} + b = w \cdot x + b</script><p>其中</p>
<script type="math/tex; mode=display">
w = \left( w_{1},\ldots,w_{n} \right)^{T},\ \ x = \left( x_{1},\ldots,x_{n} \right)^{T}</script><p>可以看出上式即为感知机的决策公式。由此可见、这种主流人工神经网络结构其实可以称为多层感知机模型（Multi-Layer Perceptron，常简称为 MLP），本章所说的神经网络所代指的也正是 MLP 模型。它的工作原理是直观的：</p>
<ul>
<li>输入层和输出层即为整个模型的入口和出口</li>
<li>变换层则会把上一层的输出当成输入、经过一番内部处理后把输出传给下一层</li>
</ul>
<p>所以问题的关键就在于层结构（Layer）的搭建上。不过在着手实现它之前、了解它具体需要做哪些工作是有必要的。如果往简单去说、神经网络算法其实只包含如下三个部分：</p>
<ul>
<li>通过将输入进行一层一层的变换来得到输出</li>
<li>通过输出与真值的比较得到损失函数的梯度</li>
<li>利用得到的这个梯度来更新模型的各个参数</li>
</ul>
<p>其中前两个部分相关的内容会在下两节进行简单的说明、第三个部分相关内容的简要叙述则会放在第四节。注意到第二个部分中提到了“损失函数”的概念；在我们将要实现的神经网络模型中、我们会将损失作为一个单独的层结构跟在输出层后面。换句话说、一个完整的神经网络模型将如下图所示：</p>
<img src="/posts/3fa9a563/p5.png" alt="p5.png" title="">
<p><strong><em>注意：今后章节中出现的各个数学算式中的元素如果不带下标的话、一般而言都代指向量或者矩阵而不是标量；为使文章结构连贯，我们不会一一说明哪些是标量、哪些是向量而哪些是矩阵，但是通过上下文和具体的算法、相关叙述应该是不会引起歧义的</em></strong></p>
<p><strong><em>此外需要指出的是，由于损失层 CostLayer 只是为了实现的便利性而存在的结构、从数学的角度来讲它是不必抽出来作为一个独立个体的。因此我们有时会在叙述数学相关问题时会隐去 CostLayer</em></strong></p>
]]></content>
      
        <categories>
            
            <category> 神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[神经网络综述]]></title>
      <url>/posts/d94622d/</url>
      <content type="html"><![CDATA[<p>之前所介绍的算法可算是比较“经典”、“传统”的算法；它们其实都属于统计学习方法、有着相当深厚的统计学理论作为支撑。而本章所讲的神经网络（Neural Network，常简称为 NN）则是近代比较火热的算法。尽管该算法的提出已经颇有些年头，相应的数学理论亦提出了不少，而且也有不少人认为它归于统计学习方法，但相当多的人还是认为，该算法更像一门“手艺”</p>
<p>以下是目录：</p>
<ul>
<li><a href="/posts/3fa9a563/" title="从感知机到多层感知机">从感知机到多层感知机</a></li>
<li><a href="/posts/2a8cdd6/" title="前向传导算法">前向传导算法</a></li>
<li><a href="/posts/437097cd/" title="反向传播算法">反向传播算法</a></li>
<li><a href="/posts/a33ff165/" title="特殊的层结构">特殊的层结构</a></li>
<li><a href="/posts/55a23cf0/" title="参数的更新">参数的更新</a></li>
<li><a href="/posts/3bb962a6/" title="朴素的网络结构">朴素的网络结构</a></li>
<li><a href="/posts/65c8a24f/" title="“大数据”下的网络结构">“大数据”下的网络结构</a></li>
<li><a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a></li>
<li><a href="/posts/66bacb27/" title="“神经网络”小结">“神经网络”小结</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> 神经网络 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
            <tag> 目录 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[“支持向量机”小结]]></title>
      <url>/posts/5b3e9c59/</url>
      <content type="html"><![CDATA[<ul>
<li>感知机利用 SGD 能保证对线性可分数据集正确分类（无论学习速率为多少）、但它没怎么考虑泛化能力的问题</li>
<li>线性 SVM 通过引入间隔（硬、软）最大化的概念来增强模型的泛化能力</li>
<li>核技巧能够将线性算法“升级”为非线性算法，通过将原始问题转化为对偶问题能够非常自然地对核技巧进行应用</li>
<li>对于一个二分类模型，有许多方法能够直接将它拓展为多分类问题</li>
<li>SVM 的思想能用于做回归（SVR）；具体而言、SVR 容许模型输出和真值之间存在<script type="math/tex">\epsilon</script>的差距以期望提高泛化能力</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 支持向量机 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 小结 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[相关数学理论]]></title>
      <url>/posts/613bbb2f/</url>
      <content type="html"><![CDATA[<p>这篇文章会叙述之前没有解决的纯数学问题，会涉及到相当庞杂的数学概念与思想，其中一些推导的难度相对而言可能会比较大</p>
<a id="more"></a>
<h1 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h1><p>前文已经相当充分地说明了梯度下降的直观，本节则打算用较严谨的数学语言来重新叙述一遍这个方法</p>
<p>首先说明其地位：梯度下降法（又称最速下降法）是求解无约束最优化问题的最常用的手段之一，同时由于现有的深度学习框架（比如 Tensorflow）基本都会含有自动求导并更新参数的功能、所以梯度下降法的实现往往会简单且高效</p>
<p>其次说明一下梯度下降法的大致步骤。正如前文所说、梯度下降法的核心在于在于函数的“求导”，而由于一般来说样本都是高维的样本（亦即<script type="math/tex">x \in \mathbb{R}^{n}</script>、<script type="math/tex">n \geq 2</script>）、所以此时我们要求的其实是函数的梯度。由于梯度是微积分里面的基础知识、这里就不“追本溯源”般地讲解梯度的定义之类的了，如果确实不甚了解且不满足于前文给出的直观解释的话、可以参见维基百科中的详细定义（<a href="https://zh.wikipedia.org/wiki/梯度" target="_blank" rel="external">中文版</a>和<a href="https://en.wikipedia.org/wiki/Gradient" target="_blank" rel="external">英文版</a>都有，个人建议尽量看英文版）</p>
<p>不管怎么说、函数梯度的这一点性质需要谨记：它是使函数值上升最快的方向，这就同时意味着负梯度是使函数值下降最快的“更新方向”。利用该性质，梯度下降法认为在每一步迭代中、都应该以梯度为更新方向“迈进”一步；在机器学习中、我们通常把这时迈进的“步长”称作“学习速率”：</p>
<ol>
<li><strong>输入</strong>：想要最小化的目标函数<script type="math/tex">f(x)</script>、迭代次数 M、学习速率<script type="math/tex">\eta</script>、计算精度<script type="math/tex">\epsilon</script>，其中<script type="math/tex">x \in \mathbb{R}^{n}</script></li>
<li><strong>过程</strong>：<ol>
<li>求出<script type="math/tex">f(x)</script>的梯度函数：  <script type="math/tex; mode=display">
g(x) \triangleq \nabla f(x)</script></li>
<li>取一个初始估计值<script type="math/tex">x^{\left( 0 \right)} \in \mathbb{R}^{n}</script></li>
<li>对<script type="math/tex">j = 1,\ldots,M</script>：<ol>
<li>计算负梯度——<script type="math/tex">g_{j} = - g(x^{\left( j \right)})</script>，若<script type="math/tex">\left\| g_{j} \right\| < \epsilon</script>则退出循环、令最终解<script type="math/tex">x^{*} = x^{\left( j \right)}</script></li>
<li>否则、向更新方向<script type="math/tex">g_{j}</script>迈进步长为<script type="math/tex">\eta</script>的一步：  <script type="math/tex; mode=display">
x^{\left( j + 1 \right)} = x^{\left( j \right)} + \eta g_{j}</script></li>
<li>若<script type="math/tex">\left\| f\left( x^{\left( j + 1 \right)} \right) - f(x^{\left( j \right)}) \right\| < \epsilon</script>或<script type="math/tex">\left\| x^{\left( j + 1 \right)} - x^{\left( j \right)} \right\| < \epsilon</script>则退出循环、令最终解<script type="math/tex">x^{*} = x^{\left( j + 1 \right)}</script></li>
</ol>
</li>
</ol>
</li>
<li><strong>输出</strong>：最终解<script type="math/tex">x^{*}</script></li>
</ol>
<p>上述算法是一个最为朴素的梯度下降法框架，通过在其基础上结合具体的模型进行改进、拓展能够衍生出一系列著名的算法。具体而言、这些拓展算法通常会针对如下两个部分进行改进：</p>
<ul>
<li>不是单纯地把梯度作为更新方向、而是利用更多的属性来定出更新方向</li>
<li>不把学习速率设成常量、而设法让其能够“适应算法”并根据具体情况进行调整</li>
</ul>
<p>有关梯度下降的拓展算法会在下一个系列的文章中进行比较详细的叙述，这里我们仅针对第二点来举一个非常直观的改进例子（仅写出与上述算法中不同的部分）：</p>
<ul>
<li><strong>算法 2.3.2 步</strong><br>对<script type="math/tex">j = 1,\ldots,M</script>：<ul>
<li>否则求出<script type="math/tex">\eta_{j}</script>、使得：  <script type="math/tex; mode=display">
f\left( x^{\left( j \right)} + {\eta_{j}g}_{j} \right) = \min_{\eta>0}{f(x^{\left( j \right)} + \eta g_{j})}</script>然后根据<script type="math/tex">\eta_{j}</script>来更新估计值：  <script type="math/tex; mode=display">
x^{\left( j + 1 \right)} = x^{\left( j \right)} + \eta_{j}g_{j}</script>这种算法又可以称作精确线性搜索准则。当优化问题为凸优化、亦即函数为凸函数时，可以证明若迭代次数 M 足够大、精确线性搜索必定能够收敛到全局最优解</li>
</ul>
</li>
</ul>
<p>考虑到对于具体的机器学习模型而言、其训练时一般会同时用到许多的样本，此时进行梯度下降法的话就不免会遇到一个问题：计算梯度时，是应该同时对多个样本进行求解然后将结果整合、还是对样本逐个进行求解？对该问题的不同解答对应着不同的算法、前文也已经有所提及。具体而言：</p>
<ul>
<li>对于随机梯度下降（SGD）、其求梯度的公式为：  <script type="math/tex; mode=display">
g_{j} = - \nabla f\left( x_{i} \right)</script>其中 i 是一个合适的下标。SGD 的优缺点都比较直观：虽然（在同样的迭代次数下）它的训练速度很快、但它搜索解空间的过程会显得比较盲目（就有种东走一下西走一下的感觉），这直接导致其收敛速度反而可能会更慢。同时如果考虑实际应用的话，由于 SGD 难以并行实现、所以其效率往往会比较低</li>
<li>对于小批量梯度下降（MBGD）、其求梯度的公式为：  <script type="math/tex; mode=display">
g_{j} = - \frac{1}{m}\left( \nabla f\left( x_{S_{1}} \right) + \ldots + \nabla f\left( x_{S_{m}} \right) \right)</script>其中 m 是一个合适的、小于总样本数 N 的数，<script type="math/tex">S_1,...,S_m</script>则是 m 个合适的下标；通常我们会称<script type="math/tex">\left\{ x_{S_1},...,x_{S_m}\right\}</script>为一个 batch。MBGD 可谓是应用得最广泛的梯度下降法，它在单步迭代中会比 SGD 慢、但它对解空间的搜索会显得“可控”很多、从而收敛速度一般反而会比 SGD 要快</li>
<li>对于批量梯度下降（BGD）、其求梯度的公式为：  <script type="math/tex; mode=display">
g_{j} = - \frac{1}{N}\sum_{i = 1}^{N}{\nabla f\left( x_{i} \right)}</script>BGD 会有一种“过犹不及”的感觉，由于它单步迭代中会用到所有样本，所以当训练集很大的时候、无论是时间开销还是空间开销都会变得难以忍受</li>
</ul>
<p>以上我们就大概综述了一遍梯度下降法的框架，更为细致的具体算法则会在下一个系列中介绍神经网络时进行部分说明</p>
<h1 id="拉格朗日对偶性"><a href="#拉格朗日对偶性" class="headerlink" title="拉格朗日对偶性"></a>拉格朗日对偶性</h1><p>如果按照最一般性的定义来讲的话，拉格朗日对偶性会显得太过“纯粹”、或说可以算是数学家的游戏。因此本小节拟打算通过推导如何将软间隔最大化 SVM 的原始最优化问题转化为对偶问题、来间接说明拉格朗日对偶性的一般性步骤</p>
<p>注意到原始问题为：</p>
<script type="math/tex; mode=display">
\min_{w,b}{L\left( w,b,x,y \right) =}\frac{1}{2}\left\| w \right\|^{2} + C\sum_{i = 1}^{N}\xi_{i}</script><p>使得：</p>
<script type="math/tex; mode=display">
y_{i}\left( w \cdot x_{i} + b \right) \geq 1 - \xi_{i}\ (i = 1,\ldots,N)</script><p>其中</p>
<script type="math/tex; mode=display">
\xi_{i} \geq 0\ (i = 1,\ldots,N)</script><p>那么原始问题的拉格朗日函数即为：</p>
<script type="math/tex; mode=display">
L\left( w,b,\xi,\alpha,\beta \right) = \frac{1}{2}\left\| w \right\|^{2} + C\sum_{i = 1}^{N}\xi_{i} - \sum_{i = 1}^{N}{\alpha_{i}\left\lbrack y_{i}\left( w \cdot x_{i} + b \right) - 1 + \xi_{i} \right\rbrack} - \sum_{i = 1}^{N}{\beta_{i}\xi_{i}}</script><p>为求解<script type="math/tex">L</script>的极小、我们需要对<script type="math/tex">w</script>、<script type="math/tex">b</script>和<script type="math/tex">\xi</script>求偏导并令偏导为 0。易知：</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla_{w}L &= w - \sum_{i = 1}^{N}{\alpha_{i}y_{i}x_{i}} = 0 \\
\nabla_{b}L &= \sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0 \\
\nabla_{\xi_{i}}L &= C - \alpha_{i} - \beta_{i} = 0
\end{align}</script><p>解得</p>
<script type="math/tex; mode=display">
w = \sum_{i = 1}^{N}{\alpha_{i}y_{i}x_{i}}</script><script type="math/tex; mode=display">
\sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0</script><p>以及对<script type="math/tex">i = 1,\ldots,N</script>、都有</p>
<script type="math/tex; mode=display">
\alpha_{i} + \beta_{i} = C</script><p>将它们带入<script type="math/tex">L\left( w,b,\xi,\alpha,\beta \right)</script>、得</p>
<script type="math/tex; mode=display">
L\left( w,b,\xi,\alpha,\beta \right) = - \frac{1}{2}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\alpha_{i}\alpha_{j}y_{i}y_{j}\left( x_{i} \cdot x_{j} \right)}} + \sum_{i = 1}^{N}\alpha_{i}</script><p>从而原始问题的对偶问题即为求上式的极大值、亦即</p>
<script type="math/tex; mode=display">
\max_{\alpha}{- \frac{1}{2}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\alpha_{i}\alpha_{j}y_{i}y_{j}\left( x_{i} \cdot x_{j} \right)}} + \sum_{i = 1}^{N}\alpha_{i}}</script><p>其中约束条件为：</p>
<script type="math/tex; mode=display">
\sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0</script><p>以及对<script type="math/tex">i = 1,\ldots,N</script>、都有</p>
<script type="math/tex; mode=display">
\alpha_{i} \geq 0,\ \ \beta_{i} \geq 0</script><script type="math/tex; mode=display">
\alpha_{i} + \beta_{i} = C\ (i = 1,\ldots,N)</script><p>易知上述约束可以简化为对<script type="math/tex">i = 1,\ldots,N</script>、都有</p>
<script type="math/tex; mode=display">
0 \leq \alpha_{i} \leq C</script><p>综上所述即得前文叙述过的软间隔最大化的对偶形式。注意到原始问题是凸二次规划、从而对偶形式的解<script type="math/tex">w^{*}</script>、<script type="math/tex">b^{*}</script>、<script type="math/tex">\xi^{*}</script>、<script type="math/tex">\alpha^{*}</script>和<script type="math/tex">\beta^{*}</script>满足 KKT 条件，亦即：</p>
<script type="math/tex; mode=display">
\nabla_{w}L(w^{*},b^{*},\xi^{*},\alpha^{*},\beta^{*}) = \nabla_{b}L(w^{*},b^{*},\xi^{*},\alpha^{*},\beta^{*}) = \nabla_{\xi}L(w^{*},b^{*},\xi^{*},\alpha^{*},\beta^{*}) = 0</script><p>以及对<script type="math/tex">i = 1,\ldots,N</script>、都有</p>
<script type="math/tex; mode=display">
\alpha_{i}^{*}\left\lbrack y_{i}\left( w^{*} \cdot x_{i} + b^{*} \right) - 1 + \xi^{*} \right\rbrack = 0</script><script type="math/tex; mode=display">
y_{i}\left( w^{*} \cdot x_{i} + b^{*} \right) - 1 + \xi^{*} \geq 0</script><script type="math/tex; mode=display">
\alpha^{*} \geq 0,\beta^{*} \geq 0,\xi^{*} \geq 0</script><script type="math/tex; mode=display">
\xi^{*}\beta^{*} = 0</script><p>由它们就可以推出前文说明过的、<script type="math/tex">w^{*}</script>和<script type="math/tex">b^{*}</script>关于<script type="math/tex">\alpha^{*}</script>的表达式了</p>
]]></content>
      
        <categories>
            
            <category> 支持向量机 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[多分类与支持向量回归]]></title>
      <url>/posts/1dc4445a/</url>
      <content type="html"><![CDATA[<p>本篇文章将简要说明几种将二分类模型拓展为多分类模型的普适性方法，它们不仅能对前三篇文章叙述的感知机和 SVM 进行应用、同时还能应用于在上一个系列中进行了说明的 AdaBoost 二分类模型；在本篇的第四节（也是最后一节）、我们则会简要地说明一下如何将支持向量机的思想应用在回归问题上</p>
<a id="more"></a>
<h1 id="一对多方法（One-vs-Rest）"><a href="#一对多方法（One-vs-Rest）" class="headerlink" title="一对多方法（One-vs-Rest）"></a>一对多方法（One-vs-Rest）</h1><p>一对多方法常简称为 OvR、是一种比较比较“豪放”的方法：对于一个 K 类问题、OvR 将训练 K 个二分类模型<script type="math/tex">\{ G_{1},\ldots,G_{K}\}</script>，每个模型将训练集中的某一类的样本作为正样本、其余类的样本作为负样本。模型的输出空间为实数空间、它反映了模型对决策的“信心”</p>
<p>具体而言、模型<script type="math/tex">G_{i}</script>会把第<script type="math/tex">i</script>类看成一类、把其余类看成另一类并尝试通过训练来区分开第<script type="math/tex">i</script>类和剩余类别；若<script type="math/tex">G_{i}</script>有比较大的自信来判定输入样本<em>x</em>是（或不是）第<script type="math/tex">i</script>类、那么<script type="math/tex">G_{i}(x)</script>将会是一个比较大的正（负）数，否则、<script type="math/tex">G_{i}(x)</script>将会是一个比较小的正（负）数</p>
<p>训练好 K 个模型后、直接将输出最大的模型所对应的类别作为决策即可、亦即：</p>
<script type="math/tex; mode=display">
y_{\text{pred}} = \arg{\max_{i}{G_{i}(x)}}</script><p>之所以称这种方法比较“豪放”、主要是因为对每个模型的训练都存在比较严重的偏差：正样本集和负样本集的样本数之比在原始训练集均匀的情况下将会是<script type="math/tex">\frac{1}{K - 1}</script>。针对该缺陷、一种比较常见的做法是只抽取负样本集中的一部分来进行训练（比如抽取其中的三分之一）</p>
<h1 id="一对一方法（One-vs-One）"><a href="#一对一方法（One-vs-One）" class="headerlink" title="一对一方法（One-vs-One）"></a>一对一方法（One-vs-One）</h1><p>一对一方法常简称为 OvO、可谓是一种很直观的方法：对于一个 K 类问题、OvO 将直接训练出<script type="math/tex">\frac{K\left( K - 1 \right)}{2}</script>个二分类模型<script type="math/tex">\{ G_{12},\ldots,G_{1K},G_{23},\ldots,G_{2K},\ldots,G_{K - 1,K}\}</script>，每个模型都只从训练集中接受两个类的样本来进行训练。模型的输出空间为二值空间<script type="math/tex">\{ - 1, + 1\}</script>、亦即模型只需要具有投票的能力即可</p>
<p>具体而言、模型<script type="math/tex">G_{\text{ij}}(i < j)</script>将接受且仅接受所有第<script type="math/tex">i</script>类和第<script type="math/tex">j</script>类的样本并尝试通过训练来区分开第<script type="math/tex">i</script>类和第<script type="math/tex">j</script>类；同时，假设<script type="math/tex">c_{i}</script>代表第<script type="math/tex">i</script>类的样本空间、那么就有：</p>
<script type="math/tex; mode=display">
G_{ij}(x) = \left\{ \begin{matrix}
 - 1,\ \ & x \in c_{j} \\
 + 1,\ \ & x \in c_{i} \\
\end{matrix} \right.\</script><p>训练好<script type="math/tex">\frac{K\left( K - 1 \right)}{2}</script>个模型后，OvO 将通过投票表决来进行决策、在<script type="math/tex">\frac{K\left( K - 1 \right)}{2}</script>次投票中得票最多的类即为模型所预测的结果。具体而言，如果考察<script type="math/tex">G_{ij}</script>、那么若<script type="math/tex">G_{\text{ij}}</script>输出<script type="math/tex">- 1</script>则第<script type="math/tex">j</script>类得一票、若<script type="math/tex">G_{ij}</script>输出<script type="math/tex">+ 1</script>则第<script type="math/tex">i</script>类得一票。如果只有两个类别（比如第<script type="math/tex">i</script>类和第<script type="math/tex">j</script>类）得票一致、那么直接看针对这两个类别的模型（亦即<script type="math/tex">G_{ij}</script>）的结果即可；如果多于两个类别的得票一致、则需要具体情况具体分析</p>
<p>OvO 是一个相当不错的方法、没有类似于 OvR 中“有偏”的问题。然而它也是有一个显而易见的缺点的——由于模型的量级是<script type="math/tex">K^{2}</script>、所以它的时间开销会相当大</p>
<h1 id="有向无环图方法（Directed-Acyclic-Graph-Method）"><a href="#有向无环图方法（Directed-Acyclic-Graph-Method）" class="headerlink" title="有向无环图方法（Directed Acyclic Graph Method）"></a>有向无环图方法（Directed Acyclic Graph Method）</h1><p>有向无环图方法常简称为 DAG，它的训练过程和 OvO 的训练过程完全一致、区别只在于最后的决策过程。具体而言、DAG 会将<script type="math/tex">\frac{K\left( K - 1 \right)}{2}</script>个模型作为一个有向无环图中的<script type="math/tex">\frac{K\left( K - 1 \right)}{2}</script>节点并逐步进行决策。其工作原理可以用下图进行说明（假设<script type="math/tex">K = 4</script>）：</p>
<img src="/posts/1dc4445a/p1.png" alt="p1.png" title="">
<h1 id="支持向量回归（Support-Vector-Regression）"><a href="#支持向量回归（Support-Vector-Regression）" class="headerlink" title="支持向量回归（Support Vector Regression）"></a>支持向量回归（Support Vector Regression）</h1><p>支持向量回归常简称为 SVR，它的基本思想与“软”间隔的思想类似——传统的回归模型通常只有在模型预测值<script type="math/tex">f(x)</script>和真值<script type="math/tex">y</script>完全一致时损失函数的值才为 0（最经典的就是当损失函数为<script type="math/tex">\left\| f\left( x \right) - y \right\|^{2}</script>的情形），而 SVR 则允许<script type="math/tex">f(x)</script>和<script type="math/tex">y</script>之间有一个<script type="math/tex">\epsilon</script>的误差、亦即仅当：</p>
<script type="math/tex; mode=display">
\left| f\left( x \right) - y \right| > \epsilon</script><p>时、我们才认为模型在<script type="math/tex">(x,y)</script>点处有损失。这与支持向量机做分类时有种“恰好相反”的感觉：对于分类问题、只有当样本点离分界面足够远时才不计损失；对于回归问题、则只有当真值离预测值足够远时才计损失。但是仔细思考的话、就不难想通它们的思想和目的是完全一致的：都是为了提高模型的泛化能力</p>
<p>类比于之前讲过的 SVM 算法、可以很自然地写出 SVR 所对应的无约束优化问题：</p>
<script type="math/tex; mode=display">
\min_{w,b,x,y}{\frac{1}{2}\left\| w \right\|^{2} + C\sum_{i = 1}^{N}{l_{\epsilon}(w,b,x_{i},y_{i})}}</script><p>其中</p>
<script type="math/tex; mode=display">
l_{\epsilon}(w,b,x_{i},y_{i}) = \left\{ \begin{matrix}
0,\ \ &|f\left( x_{i} \right) - y_{i}| \leq \epsilon \\
\left| f\left( x_{i} \right) - y_{i} \right| - \epsilon,\ \ &|f\left( x_{i} \right) - y_{i}| > \epsilon \\
\end{matrix} \right.\</script><p>于是可以利用梯度下降法等进行求解。同样类比于 SVM 的对偶问题、我们可以提出 SVR 的对偶问题，细节就不展开叙述了</p>
]]></content>
      
        <categories>
            
            <category> 支持向量机 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[核模型的实现与评估]]></title>
      <url>/posts/917ccef9/</url>
      <content type="html"><![CDATA[<p>有了上一篇文章的诸多准备、我们就能以之为基础实现核感知机和 SVM 了。不过需要指出的是，由于我们实现的 SVM 是一个朴素的版本、所以如果是要在实际任务中应用 SVM 的话，还是应该使用由前人开发、维护并经过长年考验的成熟的库（比如 LibSVM 等）；这些库能够处理更大的数据和更多的边值情况、运行的速度也会快上很多，这是因为它们通常都使用了底层语言来实现核心算法、且在算法上也做了许多数值稳定性和数值优化的处理</p>
<a id="more"></a>
<h1 id="核感知机的实现"><a href="#核感知机的实现" class="headerlink" title="核感知机的实现"></a>核感知机的实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> Util.Bases <span class="keyword">import</span> KernelBase</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KernelPerceptron</span><span class="params">(KernelBase)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        KernelBase.__init__(self)</div><div class="line">        <span class="comment"># 对于核感知机而言、循环体中所需的额外参数是学习速率（默认为1）</span></div><div class="line">        self._fit_args, self._fit_args_names = [<span class="number">1</span>], [<span class="string">"lr"</span>]</div><div class="line"></div><div class="line">    <span class="comment"># 更新dw</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_dw_cache</span><span class="params">(self, idx, lr, sample_weight)</span>:</span></div><div class="line">        self._dw_cache = lr * self._y[idx] * sample_weight[idx]</div><div class="line"></div><div class="line">    <span class="comment"># 更新db</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_db_cache</span><span class="params">(self, idx, lr, sample_weight)</span>:</span></div><div class="line">        self._db_cache = self._dw_cache</div><div class="line"></div><div class="line">    <span class="comment"># 利用和训练样本中的类别向量y来更新w和b</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_params</span><span class="params">(self)</span>:</span></div><div class="line">        self._w = self._alpha * self._y</div><div class="line">        self._b = np.sum(self._w)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_fit</span><span class="params">(self, sample_weight, lr)</span>:</span></div><div class="line">        <span class="comment"># 获取加权误差向量</span></div><div class="line">        _err = (np.sign(self._prediction_cache) != self._y) * sample_weight</div><div class="line">        <span class="comment"># 引入随机性以进行随机梯度下降</span></div><div class="line">        _indices = np.random.permutation(len(self._y))</div><div class="line">        <span class="comment"># 获取“错得最严重”的样本所对应的下标</span></div><div class="line">        _idx = _indices[np.argmax(_err[_indices])]</div><div class="line">        <span class="comment"># 若该样本被正确分类、则所有样本都已正确分类，此时返回真值、退出训练循环体</span></div><div class="line">        <span class="keyword">if</span> self._prediction_cache[_idx] == self._y[_idx]:</div><div class="line">            <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">        <span class="comment"># 否则、进行随机梯度下降</span></div><div class="line">        self._alpha[_idx] += lr</div><div class="line">        self._update_dw_cache(_idx, lr, sample_weight)</div><div class="line">        self._update_db_cache(_idx, lr, sample_weight)</div><div class="line">        self._update_pred_cache(_idx)</div></pre></td></tr></table></figure>
<p>可以看到代码清晰简洁，这主要得益于核感知机算法本身比较直白。我们可以先通过螺旋线数据集来大致看看它的分类能力、结果如下图所示：</p>
<img src="/posts/917ccef9/p1.png" alt="p1.png" title="">
<p>左图为 RBF 核感知机（<script type="math/tex">\gamma = 0.5</script>）、准确率为 90.0%；右图为多项式核感知机（<script type="math/tex">p = 12</script>）、准确率为 98.75%（迭代次数都是<script type="math/tex">10^{5}</script>）。虽说效果貌似还不错，但是由它们的训练曲线可以看出、训练过程其实是相当“不稳定”的：</p>
<img src="/posts/917ccef9/p2.png" alt="p2.png" title="">
<p>左、右图分别对应着 RBF 核感知机和多项式核感知机的训练曲线。之所以有这么大的波动、是因为我们采取的随机梯度下降每次只会进行非常局部的更新，而螺旋线数据集本身又具有比较特殊的结构，从而在直观上也能想象、模型的参数在训练的过程中很容易来回震荡。这一点在 SVM 上也会有体现、因为我们打算实现的 SMO 算法同样也是针对局部（两个变量）进行更新的</p>
<h1 id="核-SVM-的实现"><a href="#核-SVM-的实现" class="headerlink" title="核 SVM 的实现"></a>核 SVM 的实现</h1><p>接下来就看看核 SVM 的实现，虽说有些繁复、但其实只是一步一步地将之前说过的算法翻译出来而已，如果能理顺算法的逻辑的话、实现本身其实并不困难：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="keyword">from</span> Util.Bases <span class="keyword">import</span> KernelBase, KernelConfig</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span><span class="params">(KernelBase, metaclass=SubClassChangeNamesMeta)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        KernelBase.__init__(self)</div><div class="line">        <span class="comment"># 对于核SVM而言、循环体中所需的额外参数是容许误差（默认为）</span></div><div class="line">        self._fit_args, self._fit_args_names = [<span class="number">1e-3</span>], [<span class="string">"tol"</span>]</div><div class="line">        self._c = <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="comment"># 实现SMO算法中、挑选出第一个变量的方法</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_pick_first</span><span class="params">(self, tol)</span>:</span></div><div class="line">        con1 = self._alpha &gt; <span class="number">0</span></div><div class="line">        con2 = self._alpha &lt; self._c</div><div class="line">        <span class="comment"># 算出损失向量并拷贝成3份</span></div><div class="line">        err1 = self._y * self._prediction_cache - <span class="number">1</span></div><div class="line">        err2 = err1.copy()</div><div class="line">        err3 = err1.copy()</div><div class="line">        <span class="comment"># 将相应的数位置为0</span></div><div class="line">        err1[con1 | (err1 &gt;= <span class="number">0</span>)] = <span class="number">0</span></div><div class="line">        err2[(~con1 | ~con2) | (err2 == <span class="number">0</span>)] = <span class="number">0</span></div><div class="line">        err3[con2 | (err3 &lt;= <span class="number">0</span>)] = <span class="number">0</span></div><div class="line">        <span class="comment"># 算出总的损失向量并取出最大的一项</span></div><div class="line">        err = err1 ** <span class="number">2</span> + err2 ** <span class="number">2</span> + err3 ** <span class="number">2</span></div><div class="line">        idx = np.argmax(err)</div><div class="line">        <span class="comment"># 若该项的损失小于则返回返回空值</span></div><div class="line">        <span class="keyword">if</span> err[idx] &lt; tol:</div><div class="line">            <span class="keyword">return</span></div><div class="line">        <span class="comment"># 否则、返回对应的下标</span></div><div class="line">        <span class="keyword">return</span> idx</div><div class="line"></div><div class="line">    <span class="comment"># 实现SMO算法中、挑选出第二个变量的方法（事实上是随机挑选）</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_pick_second</span><span class="params">(self, idx1)</span>:</span></div><div class="line">        idx = np.random.randint(len(self._y))</div><div class="line">        <span class="keyword">while</span> idx == idx1:</div><div class="line">            idx = np.random.randint(len(self._y))</div><div class="line">        <span class="keyword">return</span> idx</div><div class="line"></div><div class="line">    <span class="comment"># 获取新的的下界</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_lower_bound</span><span class="params">(self, idx1, idx2)</span>:</span></div><div class="line">        <span class="keyword">if</span> self._y[idx1] != self._y[idx2]:</div><div class="line">            <span class="keyword">return</span> max(<span class="number">0.</span>, self._alpha[idx2] - self._alpha[idx1])</div><div class="line">        <span class="keyword">return</span> max(<span class="number">0.</span>, self._alpha[idx2] + self._alpha[idx1] - self._c)</div><div class="line"></div><div class="line">    <span class="comment"># 获取新的的上界</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_upper_bound</span><span class="params">(self, idx1, idx2)</span>:</span></div><div class="line">        <span class="keyword">if</span> self._y[idx1] != self._y[idx2]:</div><div class="line">            <span class="keyword">return</span> min(self._c, self._c + self._alpha[idx2] - self._alpha[idx1])</div><div class="line">        <span class="keyword">return</span> min(self._c, self._alpha[idx2] + self._alpha[idx1])</div><div class="line"></div><div class="line">    <span class="comment"># 更新dw</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_dw_cache</span><span class="params">(self, idx1, idx2, da1, da2, y1, y2)</span>:</span></div><div class="line">        self._dw_cache = np.array([da1 * y1, da2 * y2])</div><div class="line"></div><div class="line">    <span class="comment"># 更新db</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_db_cache</span><span class="params">(self, idx1, idx2, da1, da2, y1, y2, e1, e2)</span>:</span></div><div class="line">        gram_12 = self._gram[idx1][idx2]</div><div class="line">        b1 = -e1 - y1 * self._gram[idx1][idx1] * da1 - y2 * gram_12 * da2</div><div class="line">        b2 = -e2 - y1 * gram_12 * da1 - y2 * self._gram[idx2][idx2] * da2</div><div class="line">        self._db_cache = (b1 + b2) * <span class="number">0.5</span></div><div class="line"></div><div class="line">    <span class="comment"># 利用和训练样本中的类别向量y来更新w和b</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_params</span><span class="params">(self)</span>:</span></div><div class="line">        self._w = self._alpha * self._y</div><div class="line">        _idx = np.argmax((self._alpha != <span class="number">0</span>) &amp; (self._alpha != self._c))</div><div class="line">        self._b = self._y[_idx] – np.sum(self._alpha * self._y * self._gram[_idx])</div></pre></td></tr></table></figure>
<p>以上就是 SMO 算法中的核心步骤，接下来只需要将它们整合进一个大框架中即可（需要指出的是，随机选取第二个变量虽说效果也不错、但效率终究还是会差上一点；不过考虑到实现的复杂度、我们还是用随机选取的方法来进行实现）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义局部更新参数的方法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update_alpha</span><span class="params">(self, idx1, idx2)</span>:</span></div><div class="line">    l, h = self._get_lower_bound(idx1, idx2), self._get_upper_bound(idx1, idx2)</div><div class="line">    y1, y2 = self._y[idx1], self._y[idx2]</div><div class="line">    e1 = self._prediction_cache[idx1] - self._y[idx1]</div><div class="line">    e2 = self._prediction_cache[idx2] - self._y[idx2]</div><div class="line">    eta = self._gram[idx1][idx1] + self._gram[idx2][idx2] - <span class="number">2</span> * self._gram[idx1][idx2]</div><div class="line">    a2_new = self._alpha[idx2] + (y2 * (e1 - e2)) / eta</div><div class="line">    <span class="keyword">if</span> a2_new &gt; h:</div><div class="line">        a2_new = h</div><div class="line">    <span class="keyword">elif</span> a2_new &lt; l:</div><div class="line">        a2_new = l</div><div class="line">    a1_old, a2_old = self._alpha[idx1], self._alpha[idx2]</div><div class="line">    da2 = a2_new - a2_old</div><div class="line">    da1 = -y1 * y2 * da2</div><div class="line">    self._alpha[idx1] += da1</div><div class="line">    self._alpha[idx2] = a2_new</div><div class="line">    <span class="comment"># 根据、来更新dw和db并局部更新</span></div><div class="line">    self._update_dw_cache(idx1, idx2, da1, da2, y1, y2)</div><div class="line">    self._update_db_cache(idx1, idx2, da1, da2, y1, y2, e1, e2)</div><div class="line">    self._update_pred_cache(idx1, idx2)</div><div class="line"></div><div class="line"><span class="comment"># 初始化惩罚因子C</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_prepare</span><span class="params">(self, **kwargs)</span>:</span></div><div class="line">    self._c = kwargs.get(<span class="string">"c"</span>, KernelConfig.default_c)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_fit</span><span class="params">(self, sample_weight, tol)</span>:</span></div><div class="line">    idx1 = self._pick_first(tol)</div><div class="line">    <span class="comment"># 若没能选出第一个变量、则所有样本的误差都，此时返回真值、退出训练循环体</span></div><div class="line">    <span class="keyword">if</span> idx1 <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    idx2 = self._pick_second(idx1)</div><div class="line">    self._update_alpha(idx1, idx2)</div></pre></td></tr></table></figure>
<p>可以看到大部分代码确实只是算法的直译。同样可以先通过螺旋线数据集来大致看看核 SVM 的分类能力、结果如下图所示（图中用黑圈标注的样本点即是支持向量）：</p>
<img src="/posts/917ccef9/p3.png" alt="p3.png" title="">
<p>左图为 RBF 核 SVM（<script type="math/tex">\gamma = 0.5</script>）、迭代了 729 次即达到了停机条件（所有样本的误差都<script type="math/tex">< \epsilon</script>）、最终准确率为 51.25%；右图为多项式核 SVM（<script type="math/tex">p = 12</script>）、迭代了 6727 次即达到了停机条件、准确率为 97.5%。它们的训练曲线如下图所示：</p>
<img src="/posts/917ccef9/p4.png" alt="p4.png" title="">
<p>左、右图分别对应着 RBF 核 SVM 和多项式核 SVM 的训练曲线。虽说看上去似乎比核感知机的表现还要差、但这毕竟只是一个特殊的情形；事实上、即使是成熟的 SVM 库也并不是万能的。比如如果直接使用螺旋线数据集来训练 sklearn 中的、基于 LibSVM 进行实现的 SVM 模型的话、会得到如下图所示的结果：</p>
<img src="/posts/917ccef9/p5.png" alt="p5.png" title="">
<p>左图为 RBF 核 SVM（<script type="math/tex">\gamma = 0.5</script>）、最终准确率为 50.0%；右图为多项式核 SVM（<script type="math/tex">p = 12</script>）、准确率为 65.0%。造成这种差异的原因在于我们实现的多项式核函数和 sklearn 中的 SVM 所使用的多项式核函数不一样，如果将我们的核函数传进去、是可以得到相似结果的</p>
<p>作为本篇文章的收尾，我们可以通过画出两种核模型在蘑菇数据集上的训练曲线来简单地评估一下模型在真实数据下的表现。为了说明模型的泛化能力，我们只取 100 个样本作为训练样本、并用剩余 8000 多个样本作为测试样本来检验</p>
<p>首先来看一下核感知机的表现：</p>
<img src="/posts/917ccef9/p6.png" alt="p6.png" title="">
<p>左图为 RBF 核感知机（<script type="math/tex">\gamma \approx 0.04546</script>）的训练曲线、最终在测试集上的准确率为 92.53%；右图为多项式核感知机（<script type="math/tex">p = 3</script>）的训练曲线、最终在测试集上的准确率为 91.59%（迭代次数都是<script type="math/tex">10^{4}</script>）。由于只采用了 100 个样本训练、每次训练后的模型表现会波动得比较厉害；不过总体而言、RBF 核感知机会比多项式核感知机波动得更厉害一点</p>
<p>接下来看一下核 SVM 的表现：</p>
<img src="/posts/917ccef9/p7.png" alt="p7.png" title="">
<p>左图为 RBF 核 SVM（<script type="math/tex">\gamma \approx 0.04546</script>）、迭代了 462 次即达到了停机条件、最终在测试集上的准确率为 94.29%；右图为多项式核 SVM（<script type="math/tex">p = 3</script>）、迭代 1609 次即达到了停机条件、最终在测试集上的准确率为 92.96%</p>
]]></content>
      
        <categories>
            
            <category> 支持向量机 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[从线性到非线性]]></title>
      <url>/posts/924abfe1/</url>
      <content type="html"><![CDATA[<p>前文已经提过，由于对偶形式中的样本点仅以内积的形式出现、所以利用核技巧能将线性算法“升级”为非线性算法。有一个与核技巧（Kernel Trick）类似的概念叫核方法（Kernel Method），这两者的区别可以简单地从字面意思去认知：当我们提及核方法（Method）时、我们比较注重它背后的原理；当我们提及核技巧（Trick）时、我们更注重它实际的应用。考虑到本书的主旨、我们还是选择了核技巧这一说法</p>
<p><strong><em>注意：以上关于核技巧和核方法这两个名词的区分不是一种共识、而是我个人为了简化问题而作的一种形象的说明，所以切忌将其作为严谨的叙述</em></strong></p>
<a id="more"></a>
<h1 id="核技巧简述"><a href="#核技巧简述" class="headerlink" title="核技巧简述"></a>核技巧简述</h1><p>虽说重视应用、但一些基本的概念还是需要稍微了解的。核方法本身要深究的话会牵扯到诸如正定核、内积空间、希尔伯特空间乃至于再生核希尔伯特空间（Reproducing Kernel Hilbert Space，常简称为 RKHS）、这些东西又会牵扯到泛函的相关理论，可谓是一个可以单独拿来出书的知识点。幸运的是，单就核技巧而言、我们仅需要知道其中的三个定理即可，这三个定理分别说明了核技巧的合理性、普适性和高效性。不过在叙述这三个定理之前，我们可以先来看看核技巧的直观解释</p>
<p>核技巧往简单地说，就是将一个低维的线性不可分的数据映射到一个高维的空间、并期望映射后的数据在高维空间里是线性可分的。我们以异或数据集为例：在二维空间中、异或数据集是线性不可分的；但是通过将其映射到三维空间、我们可以非常简单地让其在三维空间中变得线性可分。比如定义映射：</p>
<script type="math/tex; mode=display">
\phi\left( x,y \right) = \left\{ \begin{matrix}
\left( x,y,1 \right),\ \ & xy > 0 \\
\left( x,y,0 \right),\ \ & xy \leq 0 \\
\end{matrix} \right.\</script><p>该映射的效果如下图所示：</p>
<img src="/posts/924abfe1/p1.png" alt="p1.png" title="">
<p>可以看到，虽然左图的数据集线性不可分、但显然右图的数据集是线性可分的，这就是核技巧工作原理的一个不太严谨但仍然合理的解释</p>
<p><strong><em>注意：这里我们暂时采用了“从低维到高维的映射”这一说法、但该说法并不完全严谨，原因会在后文说明、这里只需留一个心眼即可</em></strong></p>
<p>从直观上来说，确实容易想象、同一份数据在越高维的空间中越有可能线性可分，但从理论上是否确实如此呢？1965 年提出的 Cover 定理解决了这个问题，它的具体叙述如下：若设 d 维空间中 N 个点线性可分的概率为<script type="math/tex">p(d,N)</script>，那么就有：</p>
<script type="math/tex; mode=display">
p\left( d,N \right) = \frac{2\sum_{i = 0}^{m}C_{N - 1}^{i}}{2^{N}} = \left\{ \begin{matrix}
\frac{\sum_{i = 1}^{d}C_{N - 1}^{i}}{2^{N - 1}},\ \ & N > d + 1 \\
1,\ \ & N \leq d + 1 \\
\end{matrix} \right.\</script><p>其中</p>
<script type="math/tex; mode=display">
m = \min{(d,\ N - 1)}</script><p>定理的证明细节从略，我们只需要知道它证明了当空间的维数 d 越大时、其中的 N 个点线性可分的概率就越大，这构成了核技巧的理论基础之一</p>
<p>至此，似乎问题就转化为了如何寻找合适的映射<script type="math/tex">\phi</script>、使得数据集在被它映射到高维空间后变得线性可分。不过可以想象的是，现实任务中的数据集要比上文我们拿来举例的异或数据集要复杂得多、直接构造一个恰当的<script type="math/tex">\phi</script>的难度甚至可能高于解决问题本身。而核技巧的巧妙之处就在于，它能将构造映射这个过程再次进行转化、从而使得问题变得简易：它通过核函数来避免显式定义映射<script type="math/tex">\phi</script>。往简单里说、核技巧会通过用核函数</p>
<script type="math/tex; mode=display">
K\left( x_{i},x_{j} \right) = \phi\left( x_{i} \right) \cdot \phi(x_{j})</script><p>替换各式算法中出现的内积</p>
<script type="math/tex; mode=display">
x_{i} \cdot x_{j}</script><p>来完成将数据从低维映射到高维的过程。换句话说、核技巧的思想如下：</p>
<ul>
<li>将算法表述成样本点内积的组合（这经常能通过算法的对偶形式实现）</li>
<li>设法找到核函数<script type="math/tex">K\left( x_i,x_j\right)</script>，它能返回样本点<script type="math/tex">x_i</script>、<script type="math/tex">x_j</script>被<script type="math/tex">\phi</script>作用后的内积</li>
<li>用<script type="math/tex">K\left( x_i,x_j\right)</script>替换<script type="math/tex">x_i\cdot x_j</script>、完成低维到高维的映射（同时也完成了从线性算法到非线性算法的转换）</li>
</ul>
<p>而核技巧事实上能够应用的场景更为宽泛——在 2002 年由 Sch<script type="math/tex">\ddot{o}</script>lkopf 和 Smola 证明的表示定理告诉我们：设<script type="math/tex">\mathcal{H}</script>为核函数<script type="math/tex">K</script>对应的映射后的空间（RKHS），<script type="math/tex">\left\| h \right\|_{\mathcal{H}}</script>表示<script type="math/tex">\mathcal{H}</script>中<script type="math/tex">h</script>的范数，那么对于任意单调递增的函数<script type="math/tex">C</script>和任意非负损失函数<script type="math/tex">L</script>、优化问题</p>
<script type="math/tex; mode=display">
\min_{h\in\mathcal{H}}{L\left( h\left( x_{1} \right),\ldots,h\left( x_{N} \right) \right) + C(\left\| h \right\|_{\mathcal{H}})}</script><p>的解总可以表述为核函数<script type="math/tex">K</script>的线性组合</p>
<script type="math/tex; mode=display">
h^{*}\left( x \right) = \sum_{i = 1}^{N}{\alpha_{i}K(x,x_{i})}</script><p>这意味着对于任意一个损失函数和一个单调递增的正则化项组成的优化问题、我们都能够对其应用核技巧。所以至此、大多数的问题就转化为如何找到能够表示成高维空间中内积的核函数了。幸运的是、1909 年提出的 Mercer 定理解决了这个问题，它的具体叙述如下：<script type="math/tex">K\left( x_{i},x_{j} \right)</script>若满足</p>
<script type="math/tex; mode=display">
K\left( x_{i},x_{j} \right) = K\left( x_{j},x_{i} \right)</script><p>亦即如果<script type="math/tex">K\left( x_{i},x_{j} \right)</script>是对称函数的话、那么它具有 Hilbert 空间中内积形式的充要条件有以下两个：</p>
<ul>
<li>对任何平方可积<script type="math/tex">g</script>、满足  <script type="math/tex; mode=display">
\int K\left( x_{i},x_{j} \right)g\left( x_{i} \right)g\left( x_{j} \right)dx_{i}dx_{j} \geq 0</script></li>
<li>对含任意 N 个样本的数据集<script type="math/tex">D=\left\{ x_1,...,x_N\right\}</script>、核矩阵：  <script type="math/tex; mode=display">
\mathbf{K} = \begin{bmatrix}
K\left( x_{1},x_{1} \right) & \ldots & K\left( x_{1},x_{N} \right) \\
\vdots & \ddots & \vdots \\
K\left( x_{N},x_{1} \right) & \ldots & K\left( x_{N},x_{N} \right) \\
\end{bmatrix}_{N \times N} = \left\lbrack K_{ij} \right\rbrack_{N \times N}</script>是半正定矩阵</li>
</ul>
<p><strong><em>注意：通常我们会称满足这两个充要条件之一的函数为 Mercer 核函数而把核函数定义得更宽泛。由于本书不打算在理论上深入太多、所以一律将 Mercer 核函数简称为核函数。此外，虽说 Mercer 核函数确实具有 Hilbert 空间中的内积形式、但此时的 Hilbert 空间并不一定具有“维度”这么好的概念（或说、可以认为此时 Hilbert 空间的维度为无穷大，比如下面马上就要讲到的 RBF 核、它映射后的空间就是无穷维的）。这也正是为何前文说“从低维到高维的映射”不完全严谨</em></strong></p>
<p>Mercer 定理为寻找核函数带来了极大的便利。可以证明如下两族函数都是核函数：</p>
<ul>
<li>多项式核  <script type="math/tex; mode=display">
K\left( x_{i},x_{j} \right) = \left( x_{i} \cdot x_{j} + 1 \right)^{p}</script></li>
<li>径向基（Radial Basis Function，常简称为RBF）核  <script type="math/tex; mode=display">
K\left( x_{i},x_{j} \right) = \exp\left( - \gamma\left\| x_{i} - x_{j} \right\|^{2} \right)</script></li>
</ul>
<h1 id="核技巧的应用"><a href="#核技巧的应用" class="headerlink" title="核技巧的应用"></a>核技巧的应用</h1><p>我们接下来会实现的也正是这两族核函数对应的、应用了核技巧的算法，具体而言、我们会利用核技巧来将感知机和支持向量机算法从原始的线性版本“升级”为非线性版本</p>
<p>由简入繁、先从核感知机讲起；由于感知机对偶算法十分简单、对其应用核技巧相应的也非常平凡——直接用核函数替换掉相应内积即可。不过需要注意的是，由于我们采用的是随机梯度下降、所以算法中也应尽量只更新局部参数以避免进行无用的计算：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,\left( x_{N},y_{N} \right)\}</script>、迭代次数 M、学习速率<script type="math/tex">\eta</script>，其中：  <script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script></li>
<li><p><strong>过程</strong>：</p>
<ol>
<li><p>初始化参数：  </p>
<script type="math/tex; mode=display">
\alpha = \left( \alpha_{1},\ldots,\alpha_{N} \right)^{T} = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{N}</script><script type="math/tex; mode=display">
\hat{y} = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{N}</script><p>同时计算核矩阵：  </p>
<script type="math/tex; mode=display">
\mathbf{K} = \left\lbrack K\left( x_{i},x_{j} \right) \right\rbrack_{N \times N}</script></li>
<li><p>对<script type="math/tex">j = 1,\ldots,M</script>：  </p>
<script type="math/tex; mode=display">
E = \left\{ \left( x_{i},y_{i} \right) \middle| {\hat{y}}_{i} \leq 0 \right\}</script><ol>
<li>若<script type="math/tex">E = \varnothing</script>（亦即没有误分类的样本点）则退出循环体</li>
<li>否则，任取<script type="math/tex">E</script>中的一个样本点<script type="math/tex">(x_{i},y_{i})</script>并利用其下标更新局部参数：  <script type="math/tex; mode=display">
\alpha_{i} \leftarrow \alpha_{i} + \eta</script><script type="math/tex; mode=display">
dw = db = \eta y_{i}</script></li>
<li>利用<script type="math/tex">dw</script>和<script type="math/tex">db</script>更新预测向量<script type="math/tex">\hat{y}</script>：  <script type="math/tex; mode=display">
\hat{y} \leftarrow \hat{y} + dw\mathbf{K}_{i\mathbf{\cdot}} + db\mathbf{1}</script>其中、<script type="math/tex">\mathbf{K}_{i\mathbf{\cdot}}</script>表示<script type="math/tex">\mathbf{K}</script>的第 i 行、<script type="math/tex">\mathbf{1}</script>表示全为 1 的向量</li>
</ol>
</li>
</ol>
</li>
<li><strong>输出</strong>：感知机模型<script type="math/tex">g\left( x \right) = \text{sign}\left( f\left( x \right) \right) = sign\left( \sum_{i = 1}^{N}{\alpha_{i}y_{i}\left( K\left( x_{i},x \right) + 1 \right)} \right)</script></li>
</ol>
<p>再来看如何对 SVM 应用核技巧。虽说在对偶算法上应用核技巧是非常自然、直观的，但是直接在原始算法上应用核技巧也无不可</p>
<p>注意原始问题可以表述为：</p>
<script type="math/tex; mode=display">
\min_{w,b}\hat{L}\left( w,b,x,y \right) = \min_{w,b}{\frac{1}{2}\left\| w \right\|^{2} + \sum_{i = 1}^{N}{\max(0,1 - y_{i}(w \cdot x_{i} + b)}}</script><p>若令<script type="math/tex">w = \sum_{i = 1}^{N}{u_{i}\phi(x_{i})} = u \cdot \phi(x)</script>、其中：</p>
<script type="math/tex; mode=display">
\begin{align}
u &= (u_{1},\ldots,u_{N}) \\
\phi\left( x \right) &= \left( \phi\left( x_{1} \right),\ldots,\phi\left( x_{N} \right) \right)^{T}
\end{align}</script><p>则可知上述问题能够通过<script type="math/tex">\phi</script>映射到高维空间上：</p>
<script type="math/tex; mode=display">
\min_{w,b}{\frac{1}{2}u^{T}\mathbf{K}u + \sum_{i = 1}^{N}{\max(0,1 - y_{i}\left( \sum_{j = 1}^{N}{u_{i}\phi\left( x_{j} \right) \cdot \phi\left( x_{i} \right)} \right))}}</script><p>亦即</p>
<script type="math/tex; mode=display">
\min_{w,b}{\frac{1}{2}u^{T}\mathbf{K}u + \sum_{i = 1}^{N}{max(0,1 - y_{i}u \cdot \mathbf{K}_{i\mathbf{\cdot}})}}</script><p>利用一定的技巧是可以直接利用梯度下降法直接对这个无约束最优化问题求解的，不过相关的数学理论基础都相当繁复、实现起来也有些麻烦；尽管如此、还是有许多优秀的算法是基于上述思想的</p>
<p>直观起见、我们还是将重点放在如何对 SMO 应用核技巧的讨论上。由于前文已经说明了 SMO 的大致步骤，所以我们先补充说明当时没有讲到的、选出两个变量后应该如何继续求解，然后再来看具体的算法应该如何叙述</p>
<script type="math/tex; mode=display">
\max_{\alpha}{- \frac{1}{2}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\alpha_{i}\alpha_{j}y_{i}y_{j}K_{ij}}} + \sum_{i = 1}^{N}\alpha_{i}}</script><p>使得对<script type="math/tex">i = 1,\ldots,N</script>、都有</p>
<script type="math/tex; mode=display">
\sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0</script><script type="math/tex; mode=display">
0 \leq \alpha_{i} \leq C</script><p>不妨设<script type="math/tex">i=1</script>、<script type="math/tex">j=2</script>，那么在针对<script type="math/tex">\alpha_1</script>、<script type="math/tex">\alpha_2</script>的情况下，<script type="math/tex">\alpha_3,...,\alpha_N</script>是固定的、且上述最优化问题可以转化为：</p>
<script type="math/tex; mode=display">\max_{\alpha_1,\alpha_2}{- \frac{1}{2}\left( K_{11}\alpha_{1}^{2} + y_{1}y_{2}K_{12}\alpha_{1}\alpha_{2} + K_{22}\alpha_{2}^{2} \right) - \left( y_{1}\alpha_{1}\sum_{i = 3}^{N}{y_{i}\alpha_{i}K_{i1}} + y_{2}\alpha_{2}\sum_{i = 3}^{N}K_{i2} \right) + {(\alpha}_{1} + \alpha_{2})}</script><p>使得对<script type="math/tex">i = 1</script>和<script type="math/tex">i = 2</script>、有</p>
<script type="math/tex; mode=display">
y_{1}\alpha_{1} + y_{2}\alpha_{2} = - \sum_{i = 3}^{N}{y_{i}\alpha_{i} = const}</script><script type="math/tex; mode=display">
0 \leq \alpha_{i} \leq C</script><p>其中<script type="math/tex">const</script>为常数。可以看出此时问题确实转化为了一个带约束的二次函数求极值问题、从而能够比较简单地求出其解析解。推导过程从略、以下就直接在算法中写出结果：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,\left( x_{N},y_{N} \right)\}</script>、迭代次数 M、容许误差<script type="math/tex">\epsilon</script>，其中：  <script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script></li>
<li><p><strong>过程</strong>：</p>
<ol>
<li>初始化参数：  <script type="math/tex; mode=display">
\alpha = \left( \alpha_{1},\ldots,\alpha_{N} \right)^{T} = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{N}</script><script type="math/tex; mode=display">
\hat{y} = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{N}</script>同时计算核矩阵：  <script type="math/tex; mode=display">
\mathbf{K} = \left\lbrack K\left( x_{i},x_{j} \right) \right\rbrack_{N \times N}</script></li>
<li><p>对<script type="math/tex">j = 1,\ldots,M</script>：</p>
<ol>
<li>选出违反 KKT 条件最严重的样本点<script type="math/tex">(x_{i},y_{i})</script>，若其违反程度小于<script type="math/tex">\epsilon</script>、则退出循环体</li>
<li><p>否则、选出异于i的任一个下标 j，针对<script type="math/tex">\alpha_{i}</script>和<script type="math/tex">\alpha_{j}</script>构造一个新的只有两个变量二次规划问题并求出解析解。具体而言，首先要更新的是<script type="math/tex">\alpha_{2}</script>、它由以下几个参数定出：  </p>
<script type="math/tex; mode=display">
\begin{align}
e_{i} &= {\hat{y}}_{i} - y_{i}\ (i = 1,2) \\
dK &= K_{11} + K_{22} - 2K_{12} \\
\alpha_{2}^{new,raw} &= \alpha_{2} + \frac{y_{2}\left( e_{1} - e_{2} \right)}{dK}
\end{align}</script><p>考虑到约束条件、我们需要定出新的<script type="math/tex">\alpha_{2}</script>下上界：  </p>
<script type="math/tex; mode=display">
\begin{align}
l &= \left\{ \begin{matrix}
max(0,\alpha_{2} - \alpha_{1}),\ \ & y_{1} \neq y_{2} \\
max(0,\alpha_{2} + \alpha_{1} - C),\ \ & y_{1} = y_{2} \\
\end{matrix} \right.\ \\

h &= \left\{ \begin{matrix}
min(C,C + \alpha_{2} - \alpha_{1}),\ \ & y_{1} \neq y_{2} \\
max(C,\alpha_{2} + \alpha_{1}),\ \ & y_{1} = y_{2} \\
\end{matrix} \right.\
\end{align}</script><p>继而根据<script type="math/tex">l</script>和<script type="math/tex">h</script>对<script type="math/tex">\alpha_{2}^{new,raw}</script>进行“裁剪”即可：  </p>
<script type="math/tex; mode=display">
\alpha_{2} \leftarrow \left\{ \begin{matrix}
l,\ \ &\alpha_{2}^{new,raw} < l \\
\alpha_{2}^{new,raw},\ \ &{l \leq \alpha}_{2}^{new,raw} \\
h,\ \ &\alpha_{2}^{new,raw} > h \\
\end{matrix} \right.\  \leq h</script><p>这里要注意记录<script type="math/tex">\alpha_{2}</script>的增量：  </p>
<script type="math/tex; mode=display">
\Delta\alpha_{2} = \alpha_{2}^{\text{new}} - \alpha_{2}^{\text{old}}</script></li>
<li>利用<script type="math/tex">\Delta\alpha_{2}</script>更新<script type="math/tex">\alpha_{1}</script>、同时注意记录<script type="math/tex">\alpha_{1}</script>的增量：  <script type="math/tex; mode=display">
\begin{align}
\alpha_{1} &\leftarrow \alpha_{1} - y_{1}y_{2}\Delta\alpha_{2} \\
\Delta\alpha_{1} &= \alpha_{1}^{\text{new}} - \alpha_{1}^{\text{old}}
\end{align}</script></li>
<li>利用<script type="math/tex">\Delta\alpha_{1}</script>、<script type="math/tex">\Delta\alpha_{2}</script>进行局部更新：  <script type="math/tex; mode=display">
\begin{align}
dw &= (y_{1}\Delta\alpha_{1},y_{2}\Delta\alpha_{2}) \\
db &= \frac{b_{1} + b_{2}}{2}
\end{align}</script>其中  <script type="math/tex; mode=display">
\begin{align}
b_{1} &= - e_{1} - y_{1}K_{11}\Delta\alpha_{1} - y_{2}K_{12}\Delta\alpha_{2} \\
b_{2} &= - e_{2} - y_{1}K_{12}\Delta\alpha_{1} - y_{2}K_{22}\Delta\alpha_{2}
\end{align}</script></li>
<li>利用<script type="math/tex">dw</script>和<script type="math/tex">db</script>更新预测向量<script type="math/tex">\hat{y}</script>：  <script type="math/tex; mode=display">
\hat{y} \leftarrow \hat{y} + dw_{1}\mathbf{K}_{1\mathbf{\cdot}} + dw_{2}\mathbf{K}_{2\mathbf{\cdot}} + db\mathbf{1}</script>其中、`$\mathbf{K}_{\mathbf{i}\mathbf{\cdot}}$表示$\mathbf{K}$的第i行、$\mathbf{1}$表示全为1的向量</li>
</ol>
</li>
</ol>
</li>
<li><strong>输出</strong>：感知机模型<script type="math/tex">g\left( x \right) = \text{sign}\left( f\left( x \right) \right) = sign\left( \sum_{i = 1}^{N}{\alpha_{i}y_{i}K\left( x_{i},x \right)} + b \right)</script>、其中：</li>
</ol>
<script type="math/tex; mode=display">
b = y_{k} - \sum_{i = 1}^{N}{y_{i}\alpha_{i}K_{ik}}</script><p>这里的下标 k 满足</p>
<script type="math/tex; mode=display">
0 < \alpha_{k} < C</script><p>可以用反证法证明这样的下标 k 必存在、具体步骤从略</p>
<p>从这两种算法应用核技巧的方式可以看出，虽然它们应用的训练算法完全不同（一个是随机梯度下降、一个是序列最小最优化）、但它们每一次迭代中做的事情却有相当多是一致的；为了合理重复利用代码、我们可以先把对应的实现都抽象出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KernelBase</span><span class="params">(ClassifierBase)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._fit_args, self._args_names：记录循环体中所需额外参数的信息的属性</div><div class="line">        self._x, self._y, self._gram：记录数据集和Gram矩阵的属性</div><div class="line">            self._w, self._b, self._alpha：记录各种参数的属性</div><div class="line">        self._kernel, self._kernel_name, self._kernel_param：记录核函数相关信息的属性</div><div class="line">            self._prediction_cache, self._dw_cache, self._db_cache：记录 、dw、db的属性</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(KernelBase, self).__init__()</div><div class="line">        self._fit_args, self._fit_args_names = <span class="keyword">None</span>, []</div><div class="line">        self._x = self._y = self._gram = <span class="keyword">None</span></div><div class="line">        self._w = self._b = self._alpha = <span class="keyword">None</span></div><div class="line">        self._kernel = self._kernel_name = self._kernel_param = <span class="keyword">None</span></div><div class="line">        self._prediction_cache = self._dw_cache = self._db_cache = <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="comment"># 定义计算多项式核矩阵的函数</span></div><div class="line"><span class="meta">    @staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_poly</span><span class="params">(x, y, p)</span>:</span></div><div class="line">        <span class="keyword">return</span> (x.dot(y.T) + <span class="number">1</span>) ** p</div><div class="line"></div><div class="line">    <span class="comment"># 定义计算RBF核矩阵的函数</span></div><div class="line"><span class="meta">    @staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_rbf</span><span class="params">(x, y, gamma)</span>:</span></div><div class="line">        <span class="keyword">return</span> np.exp(-gamma * np.sum((x[..., <span class="keyword">None</span>, :] - y) ** <span class="number">2</span>, axis=<span class="number">2</span>))</div></pre></td></tr></table></figure>
<p>其中定义 RBF 核函数时用到了升维的操作、这算是 Numpy 的高级使用技巧之一；具体的思想和机制会在后续的文章中进行简要说明、这里就暂时按下不表</p>
<p>以上我们就搭好了基本的框架、接下来要做的就是继续把具有普适性的训练过程进行抽象和实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 默认使用RBF核、默认迭代次数epoch为一万次</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, kernel=<span class="string">"rbf"</span>, epoch=<span class="number">10</span> ** <span class="number">4</span>, **kwargs)</span>:</span></div><div class="line">    self._x, self._y = np.atleast_2d(x), np.array(y)</div><div class="line">    <span class="keyword">if</span> kernel == <span class="string">"poly"</span>:</div><div class="line">        <span class="comment"># 对于多项式核、默认使用KernelConfig中的default_p作为p的取值</span></div><div class="line">        _p = kwargs.get(<span class="string">"p"</span>, KernelConfig.default_p)</div><div class="line">        self._kernel_name = <span class="string">"Polynomial"</span></div><div class="line">        self._kernel_param = <span class="string">"degree = &#123;&#125;"</span>.format(_p)</div><div class="line">        self._kernel = <span class="keyword">lambda</span> _x, _y: KernelBase._poly(_x, _y, _p)</div><div class="line">    <span class="keyword">elif</span> kernel == <span class="string">"rbf"</span>:</div><div class="line">         <span class="comment"># 对于RBF核、默认使用样本x的维数n的倒数1/n作为的取值</span></div><div class="line">        _gamma = kwargs.get(<span class="string">"gamma"</span>, <span class="number">1</span> / self._x.shape[<span class="number">1</span>])</div><div class="line">        self._kernel_name = <span class="string">"RBF"</span></div><div class="line">        self._kernel_param = <span class="string">"gamma = &#123;&#125;"</span>.format(_gamma)</div><div class="line">        self._kernel = <span class="keyword">lambda</span> _x, _y: KernelBase._rbf(_x, _y, _gamma)</div><div class="line">    <span class="comment"># 初始化参数</span></div><div class="line">    self._alpha, self._w, self._prediction_cache = (</div><div class="line">        np.zeros(len(x)), np.zeros(len(x)), np.zeros(len(x)))</div><div class="line">    self._gram = self._kernel(self._x, self._x)</div><div class="line">    self._b = <span class="number">0</span></div><div class="line">    <span class="comment"># 调用 _prepare方法进行特殊参数的初始化（比如SVM中的惩罚因子C）</span></div><div class="line">    self._prepare(**kwargs)</div><div class="line">    <span class="comment"># 获取在循环体中会用到的参数</span></div><div class="line">    _fit_args = []</div><div class="line">    <span class="keyword">for</span> _name, _arg <span class="keyword">in</span> zip(self._fit_args_names, self._fit_args):</div><div class="line">        <span class="keyword">if</span> _name <span class="keyword">in</span> kwargs:</div><div class="line">            _arg = kwargs[_name]</div><div class="line">        _fit_args.append(_arg)</div><div class="line">    <span class="comment"># 迭代、直至达到迭代次数epoch或 _fit核心方法返回真值</span></div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</div><div class="line">        <span class="keyword">if</span> self._fit(sample_weight, *_fit_args):</div><div class="line">            <span class="keyword">break</span></div><div class="line">    <span class="comment"># 利用和训练样本来更新w和b</span></div><div class="line">    self._update_params()</div></pre></td></tr></table></figure>
<p>注意到我们调用了一个叫<code>KernelConfig</code>的类、它的定义很简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KernelConfig</span>:</span></div><div class="line">    default_c = <span class="number">1</span></div><div class="line">    default_p = <span class="number">3</span></div></pre></td></tr></table></figure>
<p>亦即默认惩罚因子<script type="math/tex">C</script>为 1、多项式核的次数<script type="math/tex">p</script>为 3。同时需要注意的是，我们在循环体里面调用了<code>_fit</code>核心方法、在最后调用了<code>_update_params</code>方法，这两个方法都是留给子类定义的；不过比较巧妙的是，无论是记录<script type="math/tex">\hat{y}</script>的<code>_prediction_cache</code>的更新还是预测函数<code>predict</code>的定义、都可以写成同一种形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义更新预测向量 _prediction_cache的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update_pred_cache</span><span class="params">(self, *args)</span>:</span></div><div class="line">    self._prediction_cache += self._db_cache</div><div class="line">    <span class="keyword">if</span> len(args) == <span class="number">1</span>:</div><div class="line">        self._prediction_cache += self._dw_cache * self._gram[args[<span class="number">0</span>]]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        self._prediction_cache += self._dw_cache.dot(self._gram[args, ...])</div><div class="line"></div><div class="line"><span class="comment"># 定义预测函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x, get_raw_results=False)</span>:</span></div><div class="line">    <span class="comment"># 计算测试集和训练集之间的核矩阵并利用它来做决策</span></div><div class="line">    x = self._kernel(np.atleast_2d(x), self._x)</div><div class="line">    y_pred = x.dot(self._w) + self._b</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> get_raw_results:</div><div class="line">        <span class="keyword">return</span> np.sign(y_pred)</div><div class="line">    <span class="keyword">return</span> y_pred</div></pre></td></tr></table></figure>]]></content>
      
        <categories>
            
            <category> 支持向量机 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> 算法 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[从感知机到支持向量机]]></title>
      <url>/posts/d455305a/</url>
      <content type="html"><![CDATA[<p>感知机确实能够解决线性可分数据集的分类问题，但从它的解法容易看出、感知机的解是有无穷多个的。这主要是因为它对自己的要求太低：只需对训练集中所有样本点都能正确分类即可。换句话说、感知机基本没有考虑模型的泛化能力，这就导致感知机有时会训练出如下图所示的结果：</p>
<img src="/posts/d455305a/p1.png" alt="p1.png" title="">
<p>可以看出它们是不尽合理的。支持向量机（SVM）针对这一点提出了一种改进方法，本篇文章主要叙述的就是该改进的思想和具体内容</p>
<a id="more"></a>
<h1 id="间隔最大化与线性-SVM"><a href="#间隔最大化与线性-SVM" class="headerlink" title="间隔最大化与线性 SVM"></a>间隔最大化与线性 SVM</h1><p>上图的结果之所以显得不合理、主要是因为分离超平面离正负样本点集都显得“太近”了。因此一个自然的想法就是：在训练过程中考虑上超平面到点集的距离、并努力让这个距离最大化</p>
<p>然而直接从集合出发定义集合到平面的距离是相对困难的、所以通常会将它转化为点到平面的距离。前文已经说过，对于样本点<script type="math/tex">(x_{i},y_{i})</script>而言、它到超平面<script type="math/tex">\Pi:w \cdot x + b = 0</script>的相对距离即为：</p>
<script type="math/tex; mode=display">
d^{*}\left( x_{i},\Pi \right) = |w \cdot x_{i} + b|</script><p>这里的相对距离<script type="math/tex">d^{*}</script>有一个更学术一点称谓——函数间隔（Functional Margin）。函数间隔有一个比较明显的缺陷就是、当<script type="math/tex">w</script>和<script type="math/tex">b</script>等比例变大或变小时，虽然超平面不会改变、但是<script type="math/tex">d^{*}</script>却会随之等比例变大或变小。为解决这个问题、我们可以比较自然地定义出所谓的几何间隔（Geometric Distance）：</p>
<script type="math/tex; mode=display">
d\left( x_{i},\Pi \right) = \frac{1}{\left\| w \right\|} \cdot d^{*}\left( x_{i},\Pi \right) = \frac{1}{\left\| w \right\|} \cdot |w \cdot x_{i} + b|</script><p>这里的<script type="math/tex">\left\| w \right\|</script>是<script type="math/tex">w</script>的欧式范数。顾名思义、几何间隔描述的就是向量<script type="math/tex">x_{i}</script>到超平面<script type="math/tex">\Pi</script>的几何距离（欧氏距离），它不会随<script type="math/tex">w</script>和<script type="math/tex">b</script>的等比例变化而变化、是相对稳定且直观意义优良的距离的定义方法。SVM 在训练过程中所引入的也正是各个样本点到当前分离超平面的几何距离，结合前文所说的“努力让超平面到点集的距离最大化”、SVM 算法就可以比较自然地叙述为：最大化（几何间隔）<script type="math/tex">d</script>、使得：</p>
<script type="math/tex; mode=display">
\frac{1}{\left\| w \right\|} \cdot \left\lbrack y_{i}\left( w \cdot x_{i} + b \right) \right\rbrack \geq d\ (i = 1,\ldots,N)</script><p>考虑到几何间隔和函数间隔之间的转换关系、该问题可以等价为：最大化<script type="math/tex">\frac{d^{*}}{\left\| w \right\|}</script>、使得：</p>
<script type="math/tex; mode=display">
y_{i}\left( w \cdot x_{i} + b \right) \geq d^{*}\ (i = 1,\ldots,N)</script><p>可以发现函数间隔<script type="math/tex">d^{*}</script>的取值其实对该优化问题的解没有影响。这是因为当<script type="math/tex">d^{*}</script>变成<script type="math/tex">\lambda d^{*}</script>时、<script type="math/tex">w</script>和<script type="math/tex">b</script>也会相应地变成<script type="math/tex">\lambda w</script>和<script type="math/tex">\lambda b</script>（在超平面不变的情况下），此时<script type="math/tex">\frac{d^{*}}{\left\| w \right\|}</script>和不等式约束都没有变、所以对优化问题确实没有影响。这样的话我们就能不妨设<script type="math/tex">d^{*} = 1</script>、从而优化问题就可以转换为：最大化<script type="math/tex">\frac{1}{\left\| w \right\|}</script>、使得：</p>
<script type="math/tex; mode=display">
y_{i}\left( w \cdot x_{i} + b \right) \geq 1\ (i = 1,\ldots,N)</script><p>易知该优化问题又能转化为：最小化<script type="math/tex">\frac{1}{2}\left\| w \right\|^{2}</script>、使得：</p>
<script type="math/tex; mode=display">
y_{i}\left( w \cdot x_{i} + b \right) - 1 \geq 0\ (i = 1,\ldots,N)</script><p>这就是 SVM 算法的最原始的形式。可以证明，只要训练集<script type="math/tex">D</script>线性可分、那么 SVM 算法对应的这个优化问题的解就存在且唯一；其中存在性的证明相对直观、唯一性的证明需要用到反证法和一些数学上的技巧，细节从略</p>
<p>假设该优化问题的解为<script type="math/tex">w^{*}</script>和<script type="math/tex">b^{*}</script>，我们通常称超平面：</p>
<script type="math/tex; mode=display">
\Pi^{*}:w^{*} \cdot x + b^{*} = 0</script><p>为<script type="math/tex">D</script>的最大硬间隔分离超平面。之所以称它为“硬间隔”的理由会在后文叙述，这里暂时按下不表。需要指出的是，考虑到优化问题中的不等式约束、易知在超平面</p>
<script type="math/tex; mode=display">
\Pi_{1}^{*}:w^{*} \cdot x + b = - 1</script><p>和超平面</p>
<script type="math/tex; mode=display">
\Pi_{2}^{*}:w^{*} \cdot x + b = + 1</script><p>之间、是没有任何<script type="math/tex">D</script>中的样本点的。不过在<script type="math/tex">\Pi_{1}^{*}</script>和<script type="math/tex">\Pi_{2}^{*}</script>上、确实有可能有样本点。我们通常称<script type="math/tex">\Pi_{1}^{*}</script>和<script type="math/tex">\Pi_{2}^{*}</script>为间隔边界、称其上的某些点为支持向量</p>
<p><strong><em>注意：也有间隔边界上的样本点全是支持向量的说法，本书采用的支持向量的定义将更“苛刻”一些，具体细节会在 SVM 算法的对偶形式的叙述中讲到</em></strong></p>
<p>以上的叙述比较完整地说明了 SVM 如何应用于线性可分的数据集，接下来我们就看看如何将这种思想拓展到线性不可分数据集的分类之上。事实上，由于单用超平面的话、甚至连对线性不可分数据集正确分类都做不到，更不用提在此之上的将（硬）间隔最大化的问题了；但是考虑到间隔最大化的思想、我们可以做一定的“妥协”：将“硬”间隔转化为更加普适的“软”间隔。从数学的角度来说，这等价于将不等式约束放宽：</p>
<script type="math/tex; mode=display">
y_{i}\left( w \cdot x_{i} + b \right) \geq 1 \rightarrow y_{i}\left( w \cdot x_{i} + b \right) \geq 1 - \xi_{i}</script><p>其中的<script type="math/tex">\xi_{i}</script>通常被称为“松弛变量”，它需要满足<script type="math/tex">\xi_{i} \geq 0</script>。当然、这个约束的放宽并不是没有代价的，我们要在需要最小化的<script type="math/tex">{\frac{1}{2}\left\| w \right\|}^{2}</script>上加进一个“惩罚项”来“惩罚”<script type="math/tex">\xi_{i}</script>。换句话说，我们需要最小化的项将变为：</p>
<script type="math/tex; mode=display">
L\left( w,b,x,y \right) = \frac{1}{2}\left\| w \right\|^{2} + C\sum_{i = 1}^{N}\xi_{i}</script><p>式中<script type="math/tex">L(w,b,x,y)</script>即为损失函数、损失函数中的<script type="math/tex">C</script>（<script type="math/tex">> 0</script>）通常被称为“惩罚因子”，它描述了对松弛变量<script type="math/tex">\xi_{i}</script>的“惩罚力度”：<script type="math/tex">C</script>越大意味着最终的 SVM 模型越不能容忍误分类的点，越小则反之</p>
<p>综上所述、SVM 算法对应的优化问题可以拓展为：最小化<script type="math/tex">L(w,b,x,y)</script>、使得：</p>
<script type="math/tex; mode=display">
y_{i}\left( w \cdot x_{i} + b \right) \geq 1 - \xi_{i}\ (i = 1,\ldots,N)</script><p>其中</p>
<script type="math/tex; mode=display">
\xi_{i} \geq 0\ (i = 1,\ldots,N)</script><p>可以证明该优化问题的解存在、且<script type="math/tex">w</script>的解唯一但<script type="math/tex">b</script>的解不唯一，证明细节从略。同时参照感知机算法、自然希望能够写出使用随机梯度下降来训练软间隔最大化 SVM 的算法；但是注意到<script type="math/tex">L</script>表达式中的<script type="math/tex">\xi_{i}</script>是有约束的（需要不小于 0）、所以直接对其进行随机梯度下降存在一定的困难。为了将问题近似转化为无约束最优化问题、我们可以引入 Hinge 损失，其定义很简单：</p>
<script type="math/tex; mode=display">
l\left( w,b,x,y \right) = \max(0,1 - y\left( w \cdot x + b \right))</script><p>其中<script type="math/tex">y \in \left\{ - 1, + 1 \right\}</script>。换句话说，只有在模型作出足够肯定的正确的预测时、Hinge 损失才为 0；否则即使模型作出了正确的预测、Hinge 损失还是有可能给予模型一个惩罚</p>
<p>利用 Hinge 损失、我们可以把损失函数<script type="math/tex">L</script>写成：</p>
<script type="math/tex; mode=display">
\hat{L}\left( w,b,x,y \right) = \frac{1}{2}\left\| w \right\|^{2} + C\sum_{i = 1}^{N}{l(w,b,x_{i},y_{i})}</script><p>并通过最小化<script type="math/tex">\hat{L}</script>来求解上述 SVM 算法对应的最优化问题</p>
<p><strong><em>注意：最小化<script type="math/tex">\hat{L}</script>和上文最优化问题的等价性可能并不太显然，但是通过对比损失函数及逐条比对约束条件、完成等价性证明不算太困难（比如直接令<script type="math/tex">\xi_{i} = l(w,b,x_{i},y_{i})</script>）</em></strong></p>
<p>由于我们想要写出随机梯度下降的算法、所以求出<script type="math/tex">\hat{L}</script>在单一样本<script type="math/tex">(x_{i},y_{i})</script>上对<script type="math/tex">w</script>和<script type="math/tex">b</script>的偏导数是有必要的：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial\hat{L}(w,b,x_{i},y_{i})}{\partial w} &= w + \left\{ \begin{matrix}
0,\ \ & y_{i}\left( w \cdot x_{i} + b \right) \geq 1 \\
 - Cy_{i}x_{i},\ \ & y_{i}\left( w \cdot x_{i} + b \right) < 1 \\
\end{matrix} \right.\ \\

\frac{\partial\hat{L}(w,b,x_{i},y_{i})}{\partial b} &= \left\{ \begin{matrix}
0,\ \ & y_{i}\left( w \cdot x_{i} + b \right) \geq 1 \\
 - Cy_{i},\ \ & y_{i}\left( w \cdot x_{i} + b \right) < 1 \\
\end{matrix} \right.\
\end{align}</script><p>有了这两个偏导数之后，模仿感知机算法、我们就可以比较轻松地写出软间隔最大化 SVM 的随机梯度下降训练算法：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,\left( x_{N},y_{N} \right)\}</script>、迭代次数 M、惩罚因子<script type="math/tex">C</script>、学习速率<script type="math/tex">\eta</script>，其中：  <script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script></li>
<li><strong>过程</strong>：<ol>
<li>初始化参数：  <script type="math/tex; mode=display">
w = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{n},b = 0</script></li>
<li>对<script type="math/tex">j = 1,\ldots,M</script>：<ol>
<li>算出误差向量<script type="math/tex">e = \left( e_{1},\ldots,e_{N} \right)^{T}</script>、其中：  <script type="math/tex; mode=display">
e_{i} = 1 - y_{i}(w \cdot x_{i} + b)</script></li>
<li>取出误差最大的一项：  <script type="math/tex; mode=display">
i = \arg{\min_{i}e_{i}}</script></li>
<li>若<script type="math/tex">e_{i} \leq 0</script>则直接退出循环体、否则取对应的样本来进行随机梯度下降  <script type="math/tex; mode=display">
\begin{align}
w &\leftarrow (1 - \eta)w + \eta Cy_{i}x_{i} \\
b &\leftarrow b + \eta Cy_{i}
\end{align}</script></li>
</ol>
</li>
</ol>
</li>
<li><strong>输出</strong>：线性 SVM 模型<script type="math/tex">g\left( x \right) = \text{sign}\left( f\left( x \right) \right) = sign\left( w \cdot x + b \right)</script></li>
</ol>
<p>需要指出的是，虽然算法看上去差不多、内核也都是随机梯度下降，但其实在感知机模型对学习速率不敏感的同时、线性 SVM 对学习速率是相当敏感的。由前文提到过的 Novikoff 定理和凸优化相关理论可以从理论上解释这个现象，囿于篇幅、这里就不展开叙述了</p>
<p>由于上述线性 SVM 算法的实现和感知机算法的实现几乎一致、所以我们就略去对线性 SVM 实现的详细说明；观众老爷们可以参照感知机的代码来尝试进行实现、我个人实现的版本则可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/e_SVM/LinearSVM.py" target="_blank" rel="external">这里</a></p>
<p>可以通过二维线性可分数据集来简单直观地感受一下感知机和线性 SVM 的区别、结果如下图所示：</p>
<img src="/posts/d455305a/p2.png" alt="p2.png" title="">
<p>其中左、右图分别为感知机和线性 SVM 的表现，可以看出线性 SVM 要更合理</p>
<h1 id="SVM-算法的对偶形式"><a href="#SVM-算法的对偶形式" class="headerlink" title="SVM 算法的对偶形式"></a>SVM 算法的对偶形式</h1><p>与感知机类似、SVM 算法也是存在着对偶形式，不过这个转化的过程会比感知机那里的转化过程复杂不少。具体的推导步骤会放在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>中、这里我们就直接看结果：</p>
<ul>
<li>硬间隔最大化的对偶形式  <script type="math/tex; mode=display">
\max_{\alpha}{- \frac{1}{2}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\alpha_{i}\alpha_{j}y_{i}y_{j}\left( x_{i} \cdot x_{j} \right)}} + \sum_{i = 1}^{N}\alpha_{i}}</script>使得对<script type="math/tex">i = 1,\ldots,N</script>、都有：  <script type="math/tex; mode=display">
\sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0</script><script type="math/tex; mode=display">
\alpha_{i} \geq 0</script></li>
<li>软间隔最大化的对偶形式  <script type="math/tex; mode=display">
\max_{\alpha}{- \frac{1}{2}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\alpha_{i}\alpha_{j}y_{i}y_{j}\left( x_{i} \cdot x_{j} \right)}} + \sum_{i = 1}^{N}\alpha_{i}}</script>使得对<script type="math/tex">i = 1,\ldots,N</script>、都有：  <script type="math/tex; mode=display">
\sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0</script><script type="math/tex; mode=display">
0 \leq \alpha_{i} \leq C</script></li>
</ul>
<p>可以看到它们彼此之间相似度非常高、且转化的过程和感知机的转化过程也多少有些相似。同样的，由于对偶形式中样本点仅以内积的形式出现、我们通常会先把 Gram 矩阵算出来。现我们假设对偶形式的解为<script type="math/tex">\alpha^{*} = \left( \alpha_{1},\ldots,\alpha_{N} \right)^{T}</script>、那么就有：</p>
<script type="math/tex; mode=display">
\begin{align}
w^{*} &= \sum_{i = 1}^{N}{\alpha_{i}^{*}y_{i} \cdot x_{i}} \\
b^{*} &= y_{j} - \sum_{i = 1}^{N}{y_{i}\alpha_{i}^{*}\left( x_{i} \cdot x_{j} \right)}
\end{align}</script><p>其中<script type="math/tex">w^{*}</script>的表达式和感知机中<script type="math/tex">w^{*}</script>的表达式一致、<script type="math/tex">b^{*}</script>表达式中出现的 j 是满足<script type="math/tex">0 < \alpha_{j} < C</script>的下标（用反证法可以证明这种 j 必然存在，细节从略）</p>
<p>在有了对偶形式之后、我们就可以叙述支持向量的一个比较“苛刻”的定义了：假设支持向量的集合为<script type="math/tex">SV</script>、那么</p>
<ul>
<li>在硬间隔最大化 SVM 中：  <script type="math/tex; mode=display">
x_{i} \in SV \Leftrightarrow \alpha_{i}^{*} > 0</script></li>
<li>在软间隔最大化 SVM 中：  <script type="math/tex; mode=display">
x_{i} \in SV \Leftrightarrow 0 < \alpha_{i}^{*} \leq C</script></li>
</ul>
<p>其中在软间隔最大化 SVM 里，由于<script type="math/tex">\alpha_{i}^{*} \leq C</script>本身其实是由约束条件规定的、所以可以把上述两式统一写成：</p>
<script type="math/tex; mode=display">
x_{i} \in SV \Leftrightarrow \alpha_{i}^{*} > 0</script><p>我们可以通过下图来直观认知何谓支持向量：</p>
<img src="/posts/d455305a/p3.png" alt="p3.png" title="">
<p>图中的线段即为决策边界、被一个黑色圆圈给圈住的样本点即为支持向量，左图为线性可分数据集上的情况、右图为线性不可分数据集上的情况</p>
<p>此外，说明<script type="math/tex">\alpha_{i}^{*}</script>和<script type="math/tex">\xi_{i}^{*}</script>是如何定出各个样本点和间隔边界、分离超平面之间的位置关系是有必要的，它能加深我们对对偶形式求解过程中涉及到的 KKT 条件的理解与记忆（KKT 条件的相关定义会在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>中讲到）。具体而言：</p>
<ul>
<li>若<script type="math/tex">\alpha_{i}^{*} = 0</script>，那么<script type="math/tex">x_{i}</script>被正确分类且不在间隔边界上、又或被正确分类且在间隔边界上但不是支持向量</li>
<li>若<script type="math/tex">0 < \alpha_{i}^{*} < C</script>，那么就有<script type="math/tex">\xi_{i} = 0</script>、亦即<script type="math/tex">x_{i}</script>落在间隔边界上且为支持向量</li>
<li>若<script type="math/tex">\alpha_{i}^{*} = C</script>，那么：<ul>
<li>若<script type="math/tex">\xi_{i} = 0</script>、则<script type="math/tex">x_{i}</script>落在间隔边界上且为支持向量</li>
<li>若<script type="math/tex">0 < \xi_{i} < 1</script>、则<script type="math/tex">x_{i}</script>被正确分类且落在间隔边界和分离超平面之间</li>
<li>若<script type="math/tex">\xi_{i} = 1</script>、则<script type="math/tex">x_{i}</script>落在分离超平面上</li>
<li>若<script type="math/tex">\xi_{i} > 1</script>、则<script type="math/tex">x_{i}</script>被错误分类<br>由此可知、<script type="math/tex">\xi_{i}</script>其实刻画了<script type="math/tex">x_{i}</script>到相应间隔边界的函数间隔。换句话说、<script type="math/tex">\frac{\xi_{i}}{\left\| w \right\|}</script>即是<script type="math/tex">x_{i}</script>到间隔边间的距离（几何间隔）</li>
</ul>
</li>
</ul>
<h1 id="SVM-的训练"><a href="#SVM-的训练" class="headerlink" title="SVM 的训练"></a>SVM 的训练</h1><p>前文曾经提过、原始算法的对偶形式通常能将问题简化；虽然这点在感知机算法上没有太多体现，但是对于 SVM 来说，由于它的应用场景更为广泛、在许多问题的提法下转化成对偶形式的意义将非常重大。目前已经有许多针对 SVM 的成熟算法，本书拟介绍的是其中由 Platt 在 1998 年提出的、针对对偶问题求解的序列最小最优化算法（SMO）。本篇文章主要介绍 SMO 的思路和大概步骤，详细的叙述会在下一篇文章介绍完核技巧后进行</p>
<p>SMO 是一种启发式算法，其主要手段是在每次迭代中专注于只有两个变量的优问题以期望在可以接受的时间内得到一个较优解。具体而言、SMO 要解决的是软间隔最大化 SVM 的对偶问题：</p>
<script type="math/tex; mode=display">
\max_{\alpha}{- \frac{1}{2}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\alpha_{i}\alpha_{j}y_{i}y_{j}\left( x_{i} \cdot x_{j} \right)}} + \sum_{i = 1}^{N}\alpha_{i}}</script><p>使得对<script type="math/tex">i = 1,\ldots,N</script>、都有：</p>
<script type="math/tex; mode=display">
\sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0</script><script type="math/tex; mode=display">
0 \leq \alpha_{i} \leq C</script><p>解决方案是在循环体中不断针对两个变量构造二次规划、并通过求出其解析解来优化原始的对偶问题。大致步骤如下：</p>
<ul>
<li>考察所有变量（<script type="math/tex">\alpha_{1},\ldots,\alpha_{N}</script>）及对应的样本点（<script type="math/tex">\left( x_{1},y_{1} \right),\ldots,(x_{N},y_{N})</script>）满足 KKT 条件的情况</li>
<li>若所有变量及对应样本在容许误差内都满足 KKT 条件，则退出循环体、完成训练</li>
<li>否则、通过如下步骤选出两个变量来构造新的规划问题：<ul>
<li>选出违反 KKT 条件最严重的样本点、以其对应的变量作为第一个变量</li>
<li>第二个变量的选取有一种比较繁复且高效的方法，但对于一个朴素的实现而言、第二个变量即使随机选取也无不可</li>
</ul>
</li>
<li>将上述步骤选出的变量以外的变量固定、仅针对这两个变量进行最优化。易知此时问题转化为了求二次函数的极大值、从而能简单地得到解析解</li>
</ul>
<p>这里仅简要说明一下 SVM 对偶算法中的 KKT 条件，详细的陈列则会放在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>中。具体而言、<script type="math/tex">\alpha_{i}</script>及其对应样本<script type="math/tex">(x_{i},y_{i})</script>的 KKT 条件为：</p>
<script type="math/tex; mode=display">
\begin{align}
\alpha_{i} = 0 &\Leftrightarrow y_{i}g\left( x_{i} \right) \geq 1 \\
0 < \alpha_{i} < C &\Leftrightarrow y_{i}g\left( x_{i} \right) = 1 \\
\alpha_{i} = C &\Leftrightarrow y_{i}g\left( x_{i} \right) \leq 1
\end{align}</script><p>所谓违反 KKT 条件最严重的样本点的定义也有许多种、其中一种简单有效的定义为：</p>
<script type="math/tex; mode=display">
c_{i} = \left\lbrack y_{i}g\left( x_{i} \right) - 1 \right\rbrack^{2}</script><p>所谓违反 KKT 条件最严重的样本点的定义也有许多种、其中一种简单有效的定义为：</p>
<ul>
<li>计算“损失向量”<script type="math/tex">c = \left( c_{1},\ldots,c_{N} \right)^{T}</script>、其中：  <script type="math/tex; mode=display">
c_{i} = \left\lbrack y_{i}g\left( x_{i} \right) - 1 \right\rbrack^{2}</script></li>
<li>将损失向量<script type="math/tex">c</script>复制三份（<script type="math/tex">c^{\left( 1 \right)}</script>、<script type="math/tex">c^{\left( 2 \right)}</script>、<script type="math/tex">c^{\left( 3 \right)}</script>）并分情况将相应位置的损失置为 0。具体而言：<ul>
<li>将<script type="math/tex">\alpha_{i} > 0</script>或<script type="math/tex">y_{i}g\left( x_{i} \right) \geq 1</script>对应的<script type="math/tex">c_{i}^{\left( 1 \right)}</script>置为 0</li>
<li>将<script type="math/tex">\alpha_{i} = 0</script>或<script type="math/tex">\alpha_{i} = C</script>或<script type="math/tex">y_{i}g\left( x_{i} \right) = 1</script>对应的<script type="math/tex">c_{i}^{\left( 2 \right)}</script>置为 0</li>
<li>将<script type="math/tex">\alpha_{i} < C</script>或<script type="math/tex">y_{i}g\left( x_{i} \right) \leq 1</script>对应的<script type="math/tex">c_{i}^{\left( 3 \right)}</script>置为 0</li>
</ul>
</li>
<li>将三份损失向量相加并取损失最大的样本对应<script type="math/tex">\alpha_i</script>的作为SMO的第一个变量、亦即：  <script type="math/tex; mode=display">
i = \arg{\max_{i}{\left\{ c_{i}^{\left( 1 \right)} + c_{i}^{\left( 2 \right)} + c_{i}^{\left( 3 \right)}|i = 1,\ldots,N\right\}}}</script></li>
</ul>
<p>在后面 SVM 的朴素实现中、我们打算采用的正是这种定义</p>
]]></content>
      
        <categories>
            
            <category> 支持向量机 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[感知机模型]]></title>
      <url>/posts/93db8ec2/</url>
      <content type="html"><![CDATA[<p>本篇文章所叙述的感知机模型是我们第一次应用到梯度下降法的模型，它的算法相当简单、但其框架却相当具有代表性。虽然说感知机模型只能处理非常特殊的问题（线性可分的数据集的分类问题）、但它的思想却是值得琢磨的</p>
<a id="more"></a>
<h1 id="线性可分性与感知机策略"><a href="#线性可分性与感知机策略" class="headerlink" title="线性可分性与感知机策略"></a>线性可分性与感知机策略</h1><p>在详细叙述感知机模型的原始算法之前，了解感知机适用范围和基本思想是必要的。正如前文所说，感知机只能用于给线性可分的数据集分类。其中，所谓的“线性可分性”的定义其实相当直观：对于一个数据集</p>
<script type="math/tex; mode=display">
D = \{\left( x_{1},y_{1} \right),\ldots,(x_{N},y_{N})\}</script><p>其中</p>
<script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script><p>如果存在一个超平面<script type="math/tex">\Pi</script>能够将<script type="math/tex">D</script>中的正负样本点精确地划分到<script type="math/tex">S</script>的两侧、亦即：</p>
<script type="math/tex; mode=display">
\exists\Pi:\ w \cdot x + b = 0</script><p>使得</p>
<script type="math/tex; mode=display">
\begin{align}
w \cdot x_{i} + b &< 0\ \left( \forall y_{i} = - 1 \right) \\
w \cdot x_{i} + b &> 0\ \left( \forall y_{i} = + 1 \right)
\end{align}</script><p>那么就称数据集<script type="math/tex">D</script>是线性可分的（Linearly Separable）；否则、就称<script type="math/tex">D</script>线性不可分</p>
<p>当维数<script type="math/tex">n = 2</script>时、数据集线性可分等价于正负样本点能在二维平面上被一条直线精确划分；当<script type="math/tex">n = 3</script>时则等价于能在三维空间中被一个平面精确划分。下给出一组线性可分和一组线性不可分的例子：</p>
<img src="/posts/93db8ec2/p1.png" alt="线性可分的数据集" title="线性可分的数据集">
<img src="/posts/93db8ec2/p2.png" alt="线性不可分的数据集" title="线性不可分的数据集">
<p>从数学的角度来说，线性可分性还有一个比较直观的等价定义：正负样本点集的凸包彼此不交。所谓凸包的定义如下：若集合<script type="math/tex">S \subset \mathbb{R}^{n}</script>由<script type="math/tex">N</script>个点组成：</p>
<script type="math/tex; mode=display">
S = \left\{ x_{1},\ldots,x_{N} \right\}\ (x_{i} \in \mathbb{R}^{n},\forall i = 1,\ldots,N)</script><p>那么<script type="math/tex">S</script>的凸包<script type="math/tex">\text{conv}(S)</script>即为：</p>
<script type="math/tex; mode=display">
\text{conv}\left( S \right) = \left\{ x = \sum_{i = 1}^{N}{\lambda_{i}x_{i}}|\sum_{i = 1}^{N}\lambda_{i} = 1,\lambda_{i} \geq 0(i = 1,\ldots,N)\right\}</script><p>比如，上述两个二维数据集的凸包将如下面两张图所示：</p>
<img src="/posts/93db8ec2/p3.png" alt="p3.png" title="">
<img src="/posts/93db8ec2/p4.png" alt="p4.png" title="">
<p>第一张图中，正负样本点集的凸包不交、所以数据集线性可分；第二张图中的橙色区域即为正负样本点集凸包的相交处、所以数据集线性不可分</p>
<p>该等价性的证明可以用反证法得出；由于过程不算困难且结论相当直观、所以具体的推导步骤从略</p>
<p>知道了线性可分性的定义之后、感知机模型的目的也就容易想到了——无非就是为了找到上文提到过的、能将线性可分数据集中的正负样本点精确划分到两侧的超平面<script type="math/tex">\Pi</script>。考虑到机器学习的共性，我们希望能将找超平面的过程转化为最小化一个损失函数的过程；感知机策略的具体表现、就在于如何定义这个损失函数上。考虑到<script type="math/tex">\Pi</script>的性质，损失函数的定义其实是很自然的：</p>
<script type="math/tex; mode=display">
L\left( w,b,x,y \right) = - \sum_{x_{i} \in E}^{}{y_{i}(w \cdot x_{i} + b)}</script><p>其中<script type="math/tex">E</script>是被当前感知机误分类的点集，亦即对<script type="math/tex">\forall x_{i} \in E</script>、有：</p>
<script type="math/tex; mode=display">
\begin{align}
w \cdot x_{i} + b &\geq 0\ \left( if\ y_{i} = - 1 \right) \\
w \cdot x_{i} + b &\leq 0\ \left( if\ y_{i} = + 1 \right)
\end{align}</script><p>换句话说、损失函数还可以表示为：</p>
<script type="math/tex; mode=display">
L\left( w,b,x,y \right) = \sum_{x_{i} \in E}^{}\left| w \cdot x_{i} + b \right|</script><p>注意到对于样本点<script type="math/tex">(x_{i},y_{i})</script>而言、<script type="math/tex">\left| w \cdot x_{i} + b \right|</script>能够相对地表示向量<script type="math/tex">x_{i}</script>到分离超平面<script type="math/tex">w \cdot x + b = 0</script>的距离，所以损失函数的几何解释即为：损失函数值<script type="math/tex">=</script>所有被误分类的样本点到当前分离超平面的相对距离的总和。如果感知机能将所有样本点正确分类的话、<script type="math/tex">E</script>就是空集、此时损失函数<script type="math/tex">L\left( w,b \right) = 0</script>；同时，若误分类的样本点越少或误分类的样本点离当前分离超平面越近、损失函数<script type="math/tex">L\left( w,b \right)</script>的值就越小。是故寻找最终正确的分离超平面<script type="math/tex">\Pi</script>的过程、确实可以转化为最小化损失函数<script type="math/tex">L\left( w,b \right)</script>的过程，而这也正是感知机所采用的训练策略</p>
<p><strong><em>注意：需要强调的是，<script type="math/tex">\left| w \cdot x_{i} + b \right|</script>所描述的“相对距离”和我们直观上的“欧氏距离”或说“几何距离”是不一样的（事实上它们之间相差了一个<script type="math/tex">\left\| w \right\|</script>）；相对严谨的叙述会放在<a href="/posts/d455305a/" title="从感知机到支持向量机">从感知机到支持向量机</a>中、这里就先按下不表</em></strong></p>
<h1 id="感知机算法"><a href="#感知机算法" class="headerlink" title="感知机算法"></a>感知机算法</h1><p>最小化损失函数这个过程在决策树的训练中也出现过，彼时我们采用的是一种启发式的算法——选取当前能使损失（信息的不确定性）减少最多的特征作为划分标准来划分数据。感知机算法采用的则是一种应用场景更广的方法——梯度下降法。梯度下降法的一般性定义和相关说明会放在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>中、这里我们只说一个直观：在许多情况下，损失函数是足够好的函数、从而它在每个点都能进行“求导”。求导之后我们就能得到一个损失函数增长最快的“方向”，此时我们沿反方向前进的话、就能期望损失函数以“最快的速度”减少（这也正是为何梯度下降法又叫最速下降法）</p>
<p>注意到在求得“方向”后、沿方向“走多远”也是需要考虑的参数。一般我们称该参数为“学习速率”或“步长”，通过调整和优化该参数的表现、常常能导出原理一致但表现迥异的算法。不过即使如此，梯度下降法的关键还是在于损失函数的“求导”。需要指出的是，梯度下降法本身还可以大致分为三种具体的算法——随机梯度下降法（Stochastic Gradient Descent，常简称为 SGD）、小批量梯度下降法（Mini-batch Gradient Descent，常简称为 MBGD）和批量梯度下降法（Batch Gradient Descent，常简称为 BGD）。这三种梯度下降法的说明、对比同样会放在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>中进行，这里只需要知道它们行为上的差别：随机梯度下降法在每个迭代中只使用一个样本来进行参数的更新、小批量梯度下降法则会同时选用多个样本来更新参数、批量梯度下降法则更是会同时选用所有样本来更新参数</p>
<p><strong><em>注意：也有认为 SGD 即为 MBGD 的说法、所以具体的含义需要结合具体情况分析</em></strong></p>
<p>简单起见、我们采用随机梯度下降来进行训练，此时我们需要求出损失函数在单一样本上对<script type="math/tex">w</script>和<script type="math/tex">b</script>的偏导数：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial L(w,b,x_{i},y_{i})}{\partial w} &= \left\{ \begin{matrix}
0,\ \ &(x_{i},y_{i}) \notin E \\
 - y_{i}x_{i},\ \ &{(x}_{i},y_{i}) \in E \\
\end{matrix} \right.\ \\

\frac{\partial L(w,b,x_{i},y_{i})}{\partial b} &= \left\{ \begin{matrix}
0,\ \ &(x_{i},y_{i}) \notin E \\
 - y_{i},\ \ &{(x}_{i},y_{i}) \in E \\
\end{matrix} \right.\
\end{align}</script><p>利用它们可以自然地写出感知机模型的随机梯度下降训练算法：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,\left( x_{N},y_{N} \right)\}</script>、迭代次数 M、学习速率<script type="math/tex">\eta</script>，其中：  <script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script></li>
<li><strong>过程</strong>：<ol>
<li>初始化参数：  <script type="math/tex; mode=display">
w = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{n},b = 0</script></li>
<li>对<script type="math/tex">j = 1,\ldots,M</script>：  <script type="math/tex; mode=display">
E = \left\{ \left( x_{i},y_{i} \right) \middle| y_{i}\left( w \cdot x_{i} + b \right) \leq 0 \right\}</script><ol>
<li>若<script type="math/tex">E = \varnothing</script>（亦即没有误分类的样本点）则退出循环体</li>
<li>否则，任取<script type="math/tex">E</script>中的一个样本点<script type="math/tex">(x_{i},y_{i})</script>并利用它更新参数：  <script type="math/tex; mode=display">
\begin{align}
w &\leftarrow w + \eta y_{i}x_{i} \\
b &\leftarrow b + \eta y_{i}
\end{align}</script></li>
</ol>
</li>
</ol>
</li>
<li><strong>输出</strong>：感知机模型<script type="math/tex">g\left( x \right) = \text{sign}\left( f\left( x \right) \right) = \text{sign}\left( w \cdot x + b \right)</script></li>
</ol>
<p>其中最后一步用到的 sign 是符号函数。由于感知机算法中更新一次参数的时间开销非常小、所以通常会把迭代次数 M 设置成一个比较大的数（比如<script type="math/tex">10^{4}</script>）</p>
<p>此外需要指出的是，虽说感知机只适用于线性可分数据集的分类、但它有个优点就是：无论学习速率<script type="math/tex">\eta</script>是多少，只要数据集线性可分，那么上述感知机算法在 M 足够大的情况下、必然能够训练出一个使得<script type="math/tex">E = \varnothing</script>的分离超平面（这其实就是著名的 Novikoff 定理，证明会用到比较纯粹的数学技巧、所以从略）</p>
<p>虽说所蕴含的梯度下降的思想并不平凡，但感知机算法单就复杂度而言、可以说是目前为止遇到过的最简单的算法了，其实现相对的也非常简单直观：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> Util.Bases <span class="keyword">import</span> ClassifierBase</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(ClassifierBase)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(Perceptron, self).__init__()</div><div class="line">        self._w = self._b = <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight=None, lr=<span class="number">0.01</span>, epoch=<span class="number">10</span> ** <span class="number">6</span>)</span>:</span></div><div class="line">        x, y = np.atleast_2d(x), np.array(y)</div><div class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            sample_weight = np.ones(len(y))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            sample_weight = np.array(sample_weight) * len(y)</div><div class="line">        <span class="comment"># 初始化参数</span></div><div class="line">        self._w = np.zeros(x.shape[<span class="number">1</span>])</div><div class="line">        self._b = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</div><div class="line">            y_pred = self.predict(x)</div><div class="line">            <span class="comment"># 获取加权误差向量</span></div><div class="line">            _err = (y_pred != y) * sample_weight</div><div class="line">            <span class="comment"># 引入随机性以进行随机梯度下降</span></div><div class="line">            _indices = np.random.permutation(len(y))</div><div class="line">            _idx = _indices[np.argmax(_err[_indices])]</div><div class="line">            <span class="comment"># 若没有被误分类的样本点则完成了训练</span></div><div class="line">            <span class="keyword">if</span> y_pred[_idx] == y[_idx]:</div><div class="line">                <span class="keyword">return</span></div><div class="line">            <span class="comment"># 否则，根据选出的样本点更新参数</span></div><div class="line">            _delta = lr * y[_idx] * sample_weight[_idx]</div><div class="line">            self._w += _delta * x[_idx]</div><div class="line">            self._b += _delta</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x, get_raw_results=False)</span>:</span></div><div class="line">        rs = np.sum(self._w * x, axis=<span class="number">1</span>) + self._b</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> get_raw_results:</div><div class="line">            <span class="keyword">return</span> np.sign(rs)</div><div class="line">        <span class="keyword">return</span> rs</div></pre></td></tr></table></figure>
<p>可以用之前提到过的两个二维数据集来简单评估一下我们实现的感知机模型，结果如下图所示：</p>
<img src="/posts/93db8ec2/p5.png" alt="p5.png" title="">
<p>其中在线性可分的数据集上、感知机只用了一次迭代便得到了上面左图中的效果；在线性不可分的数据集上、感知机在迭代 1000 次后可以得到右图中的效果，此时正确率为 97.5%</p>
<h1 id="感知机算法的对偶形式"><a href="#感知机算法的对偶形式" class="headerlink" title="感知机算法的对偶形式"></a>感知机算法的对偶形式</h1><p>本节将会简要介绍一个非常重要的概念：拉格朗日对偶性（Lagrange Duality）；在有约束的最优化问题中，为了便于求解、我们常常会利用它来将比较原始问题转化为更好解决的对偶问题。对于特定的问题，原始算法的对偶形式也常常会有一些共性存在。比如对于感知机和后文会介绍的支持向量机来说，它们的对偶算法都会将模型的参数表示为样本点的某种线性组合、并把问题转化为求解线性组合中的各个系数</p>
<p>虽说感知机算法的原始形式已经非常简单，但是通过将它转化为对偶形式、我们可以比较清晰地感受到转化的过程，这有助于理解和记忆后文介绍的、较为复杂的支持向量机的对偶形式</p>
<p>考虑到原始算法的核心步骤为：</p>
<script type="math/tex; mode=display">
\begin{align}
w &\leftarrow w + \eta y_{i}x_{i} \\
b &\leftarrow b + \eta y_{i}
\end{align}</script><p>其中<script type="math/tex">\left( x_{i},y_{i} \right) \in E</script>、<script type="math/tex">E</script>是当前被误分类的样本点的集合；可以看见、参数的更新是完全基于样本点的。考虑到我们要将参数<script type="math/tex">w</script>和<script type="math/tex">b</script>表示为样本点的线性组合，一个自然的想法就是记录下在核心步骤中、各个样本点分别被利用了多少次、然后利用这个次数来将<script type="math/tex">w</script>和<script type="math/tex">b</script>表示出来。比如说，若设样本点<script type="math/tex">\left( x_{i},y_{i} \right)</script>一共在上述核心步骤中被利用了<script type="math/tex">n_{i}</script>次、那么就有（假设初始化参数时<script type="math/tex">w = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{n},b = 0</script>）：</p>
<script type="math/tex; mode=display">
\begin{align}
w &= \eta\sum_{i = 1}^{N}{n_{i}y_{i}x_{i}} \\
b &= \eta\sum_{i = 1}^{N}{n_{i}y_{i}}
\end{align}</script><p>如果进一步设<script type="math/tex">\alpha_{i} = \eta n_{i}</script>、则有：</p>
<script type="math/tex; mode=display">
\begin{align}
w &= \sum_{i = 1}^{N}{\alpha_{i}y_{i}x_{i}} \\
b &= \sum_{i = 1}^{N}{\alpha_{i}y_{i}}
\end{align}</script><p>在此基础上，感知机算法的对偶形式就能很自然地写出来了：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,\left( x_{N},y_{N} \right)\}</script>、迭代次数 M、学习速率<script type="math/tex">\eta</script>，其中：  <script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script></li>
<li><strong>过程</strong>：<ol>
<li>初始化参数：  <script type="math/tex; mode=display">
\alpha = \left( \alpha_{1},\ldots,\alpha_{N} \right)^{T} = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{N}</script></li>
<li>对<script type="math/tex">j = 1,\ldots,M</script>：  <script type="math/tex; mode=display">
E = \left\{ \left( x_{i},y_{i} \right) \middle| y_{i}\left( \sum_{k = 1}^{N}{\alpha_{k}y_{k}\left( x_{k} \cdot x_{i} + 1 \right)} \right) \leq 0 \right\}</script><ol>
<li>若<script type="math/tex">E = \varnothing</script>（亦即没有误分类的样本点）则退出循环体</li>
<li>否则，任取<script type="math/tex">E</script>中的一个样本点<script type="math/tex">(x_{i},y_{i})</script>并利用其下标更新参数：  <script type="math/tex; mode=display">
\alpha_{i} \leftarrow \alpha_{i} + \eta</script></li>
</ol>
</li>
</ol>
</li>
<li><strong>输出</strong>：感知机模型<script type="math/tex">g\left( x \right) = \text{sign}\left( f\left( x \right) \right) = sign\left( \sum_{k = 1}^{N}{\alpha_{k}y_{k}\left( x_{k} \cdot x_{i} + 1 \right)} \right)</script></li>
</ol>
<p>需要指出的是，在对偶形式中、样本点里面的<script type="math/tex">x</script>仅以内积的形式（<script type="math/tex">x_{k} \cdot x_{i}</script>）出现；这是一个非常重要且深刻的性质，利用它和后文将进行介绍核技巧、能够将许多算法从线性算法“升级”成为非线性算法</p>
<p>注意到对偶形式的训练过程常常会重复用到大量的、样本点之间的内积，我们通常会提前将样本点两两之间的内积计算出来并存储在一个矩阵中；这个矩阵就是著名的 Gram 矩阵、其数学定义即为：</p>
<script type="math/tex; mode=display">
G = \left( x_{i} \cdot x_{j} \right)_{N \times N}</script><p>这样的话，在训练过程中如果要用到相应的内积、只需从 Gram 矩阵中提取即可，这样在大多数情况下都能大大提高效率</p>
]]></content>
      
        <categories>
            
            <category> 支持向量机 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> 算法 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[支持向量机综述]]></title>
      <url>/posts/487ba3a6/</url>
      <content type="html"><![CDATA[<p>目前为止讲过的模型中，朴素贝叶斯模型属于生成模型：它的训练过程其实很难称之为“训练”——毕竟它只是对输入的训练数据集进行了若干“计数”的操作；决策树模型的训练过程虽然确实有一些训练的意思在里面，但其本质——各种信息不确定性的度量仍然脱不出“计数”的范畴。换句话说，朴素贝叶斯和决策树的核心步骤似乎都只是“计数”而已。随机森林和 AdaBoost 自不用提，它们都只是将已有的模型进行集成、其本身的训练过程可谓不是主体</p>
<p>然而我们都知道、机器学习当然不只是“计数”那么简单的一回事。因此我们拟在本系列及以后的系列中介绍另一大类训练方法——梯度下降法（Gradient Decent；有时我们也称之为最速下降法（Steepest Descent））。就本系列而言，我们会先介绍一个比较简易的、应用到了梯度下降法的模型——感知机（Perceptron），然后我们会介绍一个思想和感知机类似、但是应用场景更加广泛的模型——支持向量机（Support Vector Machine，常简称为SVM）</p>
<p>以下是目录：</p>
<ul>
<li><a href="/posts/93db8ec2/" title="感知机模型">感知机模型</a></li>
<li><a href="/posts/d455305a/" title="从感知机到支持向量机">从感知机到支持向量机</a></li>
<li><a href="/posts/924abfe1/" title="从线性到非线性">从线性到非线性</a></li>
<li><a href="/posts/917ccef9/" title="核模型的实现与评估">核模型的实现与评估</a></li>
<li><a href="/posts/1dc4445a/" title="多分类与支持向量回归">多分类与支持向量回归</a></li>
<li><a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a></li>
<li><a href="/posts/5b3e9c59/" title="“支持向量机”小结">“支持向量机”小结</a>
</li>
</ul>
<p>需要指出的是、本系列的前三篇文章讨论的都是二类分类问题，回归问题和把二类算法拓展成多类算法的手段会放在<a href="/posts/1dc4445a/" title="多分类与支持向量回归">多分类与支持向量回归</a>中进行简要介绍（注意虽然我们上一个系列叙述 AdaBoost 算法时同样也只针对二类分类问题进行了说明、但是应用<a href="/posts/1dc4445a/" title="多分类与支持向量回归">多分类与支持向量回归</a>中的内容是可以将上一个系列涉及到的的诸多 AdaBoost 二分类模型推广成多分类模型的）</p>
]]></content>
      
        <categories>
            
            <category> 支持向量机 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
            <tag> 目录 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[“集成学习”小结]]></title>
      <url>/posts/48a0211a/</url>
      <content type="html"><![CDATA[<ul>
<li>集成学习是将个体模型进行集成的方法，大致可分为 Bagging 和 Boosting 两类</li>
<li>随机森林是 Bagging 算法的一种常见拓展、性能优异；它不仅对样本的选取引入随机性、还对个体模型（决策树）的特征选取步骤引入随机性</li>
<li>AdaBoost 是 Boosting 族算法的代表，通过以下三步进行提升：<ul>
<li>根据样本权重训练弱分类器</li>
<li>根据该弱分类器的加权错误率为其分配“话语权”</li>
<li>根据该弱分类器的表现更新样本权重</li>
</ul>
</li>
<li>集成模型具有相当不错的正则化能力、但该正则化能力并不是必然存在的</li>
<li>AdaBoost 可以用前向分步算法和加法模型来解释</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 集成学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 小结 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[相关数学理论]]></title>
      <url>/posts/613bbb2f/</url>
      <content type="html"><![CDATA[<p>这篇文章会叙述之前没有解决的纯数学问题，同样会涉及到概率论的一些基础概念和思想，可能会有一定的难度</p>
<a id="more"></a>
<h1 id="经验分布函数"><a href="#经验分布函数" class="headerlink" title="经验分布函数"></a>经验分布函数</h1><p>正如前文所说，经验分布函数的数学表达式为：</p>
<script type="math/tex; mode=display">
F_{N}\left( x \right) = \frac{1}{N}\sum_{i = 1}^{N}{I_{\left( - \infty,x \right\rbrack}(x_{i})}</script><p>如果将<script type="math/tex">x_{1},\ldots,x_{N}</script>按从小到大的顺序排成<script type="math/tex">x_{(1)},\ldots,x_{\left( N \right)}</script>，我们通常称其中的<script type="math/tex">x_{\left( i \right)}</script>为第 i 个次序统计量。易知可以利用次序统计量将<script type="math/tex">F_{N}(x)</script>表示成更直观的形式：</p>
<script type="math/tex; mode=display">
F_{N}\left( x \right) = \left\{ \begin{matrix}
0,\ \ &x < x_{\left( 1 \right)} \\
\frac{i}{N},\ \ &x \in \left\lbrack x_{\left( i \right)},x_{\left( i + 1 \right)} \right)\ (i = 1,\ldots,N - 1) \\
1,\ \ &x \geq x_{\left( N \right)} \\
\end{matrix} \right.\</script><p>关于其优良性，前文所说的“频率估计概率”的严谨叙述其实就是强大数律：</p>
<script type="math/tex; mode=display">
p\left( \lim_{N}{F_{N}\left( x \right) - F\left( x \right) = 0} \right) = 1\ (\forall x)</script><p>亦即</p>
<script type="math/tex; mode=display">
F_{N}(x) \xrightarrow{a.s.} F(x)</script><p>同时还有一个更强的结论（Glivenko-Cantelli 定理）：</p>
<script type="math/tex; mode=display">
p\left( \lim_{N}{\sup_{x}\left| F_{N}\left( x \right) - F\left( x \right) \right| = 0} \right) = 1</script><p>亦即</p>
<script type="math/tex; mode=display">
\left\| F_{N}\left( x \right) - F\left( x \right) \right\|_{\infty} \equiv \sup_{x}\left| F_{N}\left( x \right) - F\left( x \right) \right|\xrightarrow{a.s.}0</script><p>其中，<script type="math/tex">\sup_{x}\left| F_{N}\left( x \right) - F\left( x \right) \right|</script>就是著名的柯尔莫诺夫-斯米尔诺夫检验（Kolmogorov-Smirnov Statistic）。值得一提的是，用其它范数来代替这里的无穷范数有时也是合理的。比如说用二范数来代替时、对应的就是 Cramér-von Mises Criterion</p>
<p>此外，我们还可以利用中心极限定理等来研究经验分布函数（比如与正态分布扯上关系等等），这里就不详细展开了。总之，经验分布函数的优良性是相当有保证的，与其本质类似的 Bootstrap 的优良性也因而有了保证。当然、Bootstrap 自己是有一套成熟理论的，不过如果就这点展开来叙述的话、多多少少会偏离了本系列文章的主旨，所以这里就仅通过讨论经验分布函数来间接地感受 Bootstrap 的优良性</p>
<h1 id="AdaBoost-与前向分步加法模型"><a href="#AdaBoost-与前向分步加法模型" class="headerlink" title="AdaBoost 与前向分步加法模型"></a>AdaBoost 与前向分步加法模型</h1><p>本节主要用于推导如下定理：AdaBoost 分类模型可以等价为损失函数为指数函数的前向分步加法模型</p>
<p>假设经过<script type="math/tex">k</script>轮迭代后、前项分布算法已经得到了加法模型<script type="math/tex">f_{k}(x)</script>，亦即：</p>
<script type="math/tex; mode=display">
\begin{align}
f_{k}\left( x \right) &= f_{k - 1}\left( x \right) + \alpha_{k}g_{k}\left( x \right) = f_{k - 2}\left( x \right) + \alpha_{k - 1}g_{k - 1}\left( x \right) + \alpha_{k}g_{k}(x) \\

&= \ldots = \sum_{i = 1}^{k}{\alpha_{i}g_{i}(x)}
\end{align}</script><p>可知、第<script type="math/tex">k + 1</script>轮的模型<script type="math/tex">f_{k + 1}</script>能表示为：</p>
<script type="math/tex; mode=display">
f_{k + 1}\left( x \right) = f_{k}\left( x \right) + \alpha_{k + 1}g_{k + 1}(x)</script><p>我们关心的问题就是，如何在<script type="math/tex">f_{k}(x)</script>确定下来的情况下、训练出第<script type="math/tex">k + 1</script>轮的个体分类器<script type="math/tex">g_{k + 1}(x)</script>及其权重<script type="math/tex">\alpha_{k + 1}</script>。注意到我们的损失函数是指数函数，亦即：</p>
<script type="math/tex; mode=display">
\begin{align}
L &= \sum_{i = 1}^{N}{\exp\left\lbrack - y_{i}f_{k + 1}\left( x_{i} \right) \right\rbrack} \\

&= \sum_{i = 1}^{N}w_{ki}\exp\lbrack - y_{i}\alpha_{k + 1}g_{k + 1}(x_{i})\rbrack
\end{align}</script><p>其中</p>
<script type="math/tex; mode=display">
w_{ki} = \exp\lbrack - y_{i}f_{k}(x_{i})\rbrack</script><p>在<script type="math/tex">f_{k}(x)</script>确定下来的情况下是常数。由于我们的最终目的是最小化损失函数、所以<script type="math/tex">\alpha_{k + 1}</script>和<script type="math/tex">g_{k + 1}(x)</script>就可以表示为：</p>
<script type="math/tex; mode=display">
\begin{align}
\left( \alpha_{k + 1},g_{k + 1}\left( x \right) \right) &= \arg{\min_{\alpha,g}{\sum_{i = 1}^{N}{w_{ki}\exp\left\lbrack - y_{i}\alpha g\left( x_{i} \right) \right\rbrack}}} \\

&= \arg{\min_{\alpha,g}{\sum_{y_{i} = g\left( x_{i} \right)}^{}{w_{ki}e^{- \alpha}} + \sum_{y_{i} \neq g\left( x_{i} \right)}^{}{w_{ki}e^{\alpha}}}} \\

&= \arg{\min_{\alpha,g}{\left( e^{\alpha} - e^{- \alpha} \right)\sum_{i = 1}^{N}{w_{ki}I\left( y_{i} \neq g\left( x_{i} \right) \right)} + e^{- \alpha}\sum_{i = 1}^{N}w_{ki}}} \\

&= \arg{\min_{\alpha,g}{\left( e^{\alpha} - e^{- \alpha} \right)\sum_{i = 1}^{N}{w_{ki}I\left( y_{i} \neq g\left( x_{i} \right) \right)} + e^{- \alpha}}}
\end{align}</script><p>上式可以分两步求解。先看当<script type="math/tex">\alpha</script>确定下来后应该如何定出<script type="math/tex">g_{k + 1}(x)</script>，易知：</p>
<script type="math/tex; mode=display">
g_{k + 1}\left( x \right) = \arg{\min_{g}{\sum_{i = 1}^{N}{w_{ki}I\left( y_{i} \neq g\left( x_{i} \right) \right)}}}</script><p>亦即第<script type="math/tex">k + 1</script>步的个体分类器应该使训练集上的加权错误率最小。不妨设解出的<script type="math/tex">g_{k + 1}(x)</script>在训练集上的加权错误率为<script type="math/tex">e_{k + 1}</script>、亦即：</p>
<script type="math/tex; mode=display">
\sum_{i = 1}^{N}{w_{ki}I\left( y_{i} \neq g_{k + 1}\left( x_{i} \right) \right)} \triangleq e_{k + 1}</script><p>我们需要利用它来定出<script type="math/tex">\alpha_{k + 1}</script>。注意到对目标函数求偏导后易知：</p>
<script type="math/tex; mode=display">
\begin{align}
\alpha_{k + 1} &= \arg{\min_{\alpha}{\left( e^{\alpha} - e^{- \alpha} \right)e_{k + 1} + e^{- \alpha}}} \\

&\Leftrightarrow \left( e^{\alpha_{k + 1}} + e^{- \alpha_{k + 1}} \right)e_{k + 1} - e^{- \alpha_{k + 1}} = 0 \\

&\Leftrightarrow \alpha_{k + 1} = \frac{1}{2}\ln\frac{1 - e_{k + 1}}{e_{k + 1}}
\end{align}</script><p>这和 AdaBoost 中确定个体分类器权值的式子一模一样。接下来只需要证明样本权重更新的式子也彼此一致即可得证定理，而事实上、由于：</p>
<script type="math/tex; mode=display">
f_{k + 1}\left( x \right) = f_{k}\left( x \right) + \alpha_{k + 1}g_{k + 1}\left( x \right)</script><p>从而</p>
<script type="math/tex; mode=display">
\begin{align}
w_{k + 1,i} &= \exp\left\lbrack - y_{i}f_{k + 1}\left( x_{i} \right) \right\rbrack \\

&= \exp\left\lbrack - y_{i}f_{k}\left( x_{i} \right) \right\rbrack \cdot \exp\left\lbrack - y_{i}\alpha_{k + 1}g_{k + 1}\left( x_{i} \right) \right\rbrack \\

&= w_{ki} \cdot \exp\left\lbrack - y_{i}\alpha_{k + 1}g_{k + 1}\left( x_{i} \right) \right\rbrack
\end{align}</script><p>注意到我们要将样本权重归一化，所以须有：</p>
<script type="math/tex; mode=display">
w_{k + 1,i} \leftarrow \frac{w_{k + 1,i}}{Z_{k}}</script><p>其中</p>
<script type="math/tex; mode=display">
Z_{k} = \sum_{i = 1}^{N}w_{k + 1,i} = \sum_{i = 1}^{N}{w_{ki} \cdot \exp\left\lbrack - \alpha_{k + 1}y_{i}g_{k + 1}\left( x_{i} \right) \right\rbrack}</script><p>是故</p>
<script type="math/tex; mode=display">
w_{k + 1,i} = \frac{w_{ki}}{Z_{k}} \cdot \exp\left\lbrack - y_{i}\alpha_{k + 1}g_{k + 1}\left( x_{i} \right) \right\rbrack</script><p>这和 AdaBoost 中更新样本权重的式子也一模一样。综上所述、定理得证</p>
]]></content>
      
        <categories>
            
            <category> 集成学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AdaBoost 算法的解释]]></title>
      <url>/posts/707464b/</url>
      <content type="html"><![CDATA[<p>我们前面提到过 Bagging 的数学基础是 Bootstrap 理论、但还没有讲 Boosting 的数学基础。本篇文章拟打算直观地阐述 Boosting 族的代表算法——AdaBoost 算法的解释，由于具体的推导相当繁琐，相关的细节我们会放在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>里面说明</p>
<a id="more"></a>
<p>首先将结论给出：AdaBoost 算法是前向分步算法的特例，AdaBoost 模型等价于损失函数为指数函数的加法模型</p>
<p>其中，加法模型的定义是直观且熟悉的：</p>
<script type="math/tex; mode=display">
f\left( x \right) = \sum_{k = 1}^{M}{\alpha_{k}g(x;\Theta_{k})}</script><p>这里的<script type="math/tex">g(x;\Theta_{k})</script>为基函数，<script type="math/tex">\alpha_{k}</script>是基函数的权重，<script type="math/tex">\Theta_{k}</script>是基函数的参数。显然的是，我们的 AdaBoost 算法的最后一步生成的模型正是这么一个加法模型</p>
<p>而所谓的前向分步算法，就是从前向后、一步一步地学习加法模型中的每一个基函数及其权重而非将<script type="math/tex">f(x)</script>作为一个整体来训练，这也正是 AdaBoost 的思想</p>
<p>如果此时需要最小化的损失函数是指数损失函数<script type="math/tex">L\left( y,f\left( x \right) \right) = \exp\left\lbrack - yf\left( x \right) \right\rbrack</script>的话，通过一系列的数学推导后可以证明、此时的加法模型确实等价于 AdaBoost 模型</p>
<p>可能大家会觉得这里面有一些别扭：为什么一个实现起来非常简便的模型，它背后的数学原理却如此复杂？事实上有趣的是，AdaBoost 是为数不多的、先有算法后有解释的模型。也就是说，是先有了 AdaBoost 这个东西，然后数学家们看到它的表现非常好之后、才开始绞尽脑汁并想出了一套适用于 AdaBoost 的数学理论。更有意思的是，该数学理论并非毫无意义：在 AdaBoost 的回归问题中，就可以用前向分步算法的理论、将每一步的训练转化为了拟合当前模型的残差、从而简化了训练步骤。我们可以简单地叙述一下其原理：</p>
<p>加法模型的等价叙述为</p>
<script type="math/tex; mode=display">
f_{k + 1}\left( x \right) = f_{k}\left( x \right) + g_{k + 1}(x;\Theta_{k + 1})</script><p>其中<script type="math/tex">g_{k + 1}</script>为第<script type="math/tex">k + 1</script>步的基函数（亦即 AdaBoost 中的弱分类器），<script type="math/tex">\Theta_{k + 1}</script>为其参数。当采用平方误差损失函数<script type="math/tex">L\left( y,f\left( x \right) \right) = {\lbrack y - f\left( x \right)\rbrack}^{2}</script>时，可知第<script type="math/tex">k + 1</script>步的损失变为：</p>
<script type="math/tex; mode=display">
L = {\left\lbrack y - f_{k + 1}\left( x \right) \right\rbrack^{2} = \left\lbrack y - f_{k}\left( x \right) - g_{k + 1} \right\rbrack}^{2} = \left\lbrack r_{k}(x) - g_{k + 1}(x) \right\rbrack^{2}</script><p>其中<script type="math/tex">r_{k}(x) = y - f_{k}(x)</script>是第<script type="math/tex">k</script>步模型的残差。</p>
<p>从上式可以看出在第<script type="math/tex">k + 1</script>步时，为了最小化损失<script type="math/tex">L</script>，只需让当前的基函数<script type="math/tex">g_{k + 1}</script>拟合当前模型的残差<script type="math/tex">r_{k}</script>即可，这就完成了 AdaBoost 回归问题的转化。比较具有代表性的是回归问题的提升树算法，它正是利用了以上叙述的转化技巧来进行模型训练的</p>
]]></content>
      
        <categories>
            
            <category> 集成学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[集成模型的性能分析]]></title>
      <url>/posts/fb0d2f02/</url>
      <content type="html"><![CDATA[<p>正如前文所说，在实现完 AdaBoost 框架后，我们需要先用 sklearn 中的分类器进行检验、然后再用我们前两章实现的模型进行对比实验。检验的步骤就不在这里详述（毕竟只是一些调试的活），我们在此仅展示在随机森林模型和经过检验的 AdaBoost 模型上进行的一系列的分析</p>
<p>直观起见，我们先采用二维的数据进行实验、并通过可视化来加深对随机森林和 AdaBoost 的理解，然后再用蘑菇数据集做比较贴近现实的实验。为讨论方便，我们一律采用决策树作为 AdaBoost 的弱分类器（亦即采用提升树模型进行讨论）、其强度可以通过调整其最深层数来控制。我们可以利用<code>DataUtil</code>类来生成或获取原始数据集，其完整代码可参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py" target="_blank" rel="external">这里</a>、生成数据集的代码则会在前三节分别放出</p>
<p>对于二维数据，我们拟打算使用三种数据集来进行评估：</p>
<ul>
<li>随机数据集。该数据集主要用于直观地感受模型的分类能力</li>
<li>异或数据集。该数据集主要用于直观地理解：<ul>
<li>集成模型正则化的能力</li>
<li>为何说 AdaBoost 不要选用分类能力太强的弱分类器</li>
</ul>
</li>
<li>螺旋线数据集，主要用于直观认知随机森林和提升树的不足</li>
</ul>
<a id="more"></a>
<h1 id="随机数据集上的表现"><a href="#随机数据集上的表现" class="headerlink" title="随机数据集上的表现"></a>随机数据集上的表现</h1><p>生成随机数据集的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_random</span><span class="params">(size=<span class="number">100</span>)</span>:</span></div><div class="line">    xy = np.random.rand(size, <span class="number">2</span>)</div><div class="line">    z = np.random.randint(<span class="number">2</span>, size=size)</div><div class="line">    <span class="comment"># 注意：我们的AdaBoost框架要求类别空间为(-1,+1)</span></div><div class="line">    z[z == <span class="number">0</span>] = <span class="number">-1</span></div><div class="line">    <span class="keyword">return</span> xy, z</div></pre></td></tr></table></figure>
<p>随机森林在随机数据集上的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p1.png" alt="p1.png" title="">
<p>左图为包含 1 棵 CART 树的随机森林，准确率为 78.0%；右图则为包含 10 棵 CART 树的随机森林，准确率为 93.0%。如果将树的数量继续往上抬、达到 100%准确率并非难事。比如，包含 5 0棵 CART 树的随机森林的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p2.png" alt="随机数据集上准确率为 100%的随机森林" title="随机数据集上准确率为 100%的随机森林">
<p>提升树（弱模型为决策树的 AdaBoost）在随机数据集上的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p3.png" alt="p3.png" title="">
<p>左图为包含 1 棵 CART 树的 AdaBoost，准确率为 93.0%；右图则为包含 10 棵 CART 树的 AdaBoost，准确率为 99.0%</p>
<h1 id="异或数据集上的表现"><a href="#异或数据集上的表现" class="headerlink" title="异或数据集上的表现"></a>异或数据集上的表现</h1><p>这里主要是想说明随机森林和提升树正则化的效果。从直观上来说，由于随机森林的理论基础是 Bootstrap、所以自然是包含越多树越好；至于 AdaBoost，可以想象它会对难以分类的数据特别在意、从而导致如下两种可能的结果：</p>
<ul>
<li>太过注重噪声，导致过拟合</li>
<li>专注于类似于下一个系列要讲的 SVM 中的“支持向量”，从而达到正则化</li>
</ul>
<p>事实上正如之前提到过的，即使 AdaBoost 在某一步迭代时、所得的模型在训练集上的加权错误率已经达到了 0，继续进行训练仍然可以使模型进一步提升（因为单个模型的正确率没有那么高、从而能使模型继续专注于“支持向量”。所谓支持向量、可以暂时直观地理解为“非常重要的”样本）。为说明这一点，我们可以比较同一数据集上、同样使用最深层数为 3 层的决策树作为弱分类器时、两种不同训练策略在异或数据集上的表现。为了比较准确地衡量正则化能力，我们需要进行交叉验证。、</p>
<p>生成异或数据集的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_xor</span><span class="params">(size=<span class="number">100</span>)</span>:</span></div><div class="line">    x = np.random.randn(size)</div><div class="line">    y = np.random.randn(size)</div><div class="line">    z = np.ones(size)</div><div class="line">    z[x * y &lt; <span class="number">0</span>] = <span class="number">-1</span></div><div class="line">    <span class="keyword">return</span> np.c_[x, y].astype(np.float32), z</div></pre></td></tr></table></figure>
<p>随机森林在异或数据集上的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p4.png" alt="p4.png" title="">
<p><strong><em>注意：该异或数据集和上一章用到的异或数据集是同一个数据集，感兴趣的读者可以进行一些对比</em></strong></p>
<p>左图为包含 1 棵 CART 树的随机森林，准确率为 93.0%；右图则为包含 10 棵 CART 树的随机森林，准确率为 98.0%。虽说右图中随机森林的表现已经足够好，由前文讨论可知、我们应该尝试训练一个更复杂的随机森林来看看其正则化能力。比如，包含 1000 棵 CART 树的随机森林的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p5.png" alt="p5.png" title="">
<p>仔细观察决策边界，可以发现它会倾向于画在使得样本和边界“间隔较大”的地方。关于“间隔”的详细讨论会放在下一个系列，这里只需直观地感受一下即可</p>
<p>对于提升树，首先看一下不提前停止训练时的表现。为更好地说明问题，这里我们换了一个异或数据集来进行分析：</p>
<img src="/posts/fb0d2f02/p6.png" alt="p6.png" title="">
<p>此时在测试数据集上的正确率为 97.0%。然后看当模型在训练集上错误率足够小就马上停止训练时的表现：</p>
<img src="/posts/fb0d2f02/p7.png" alt="p7.png" title="">
<p>此时在测试数据集上的正确率为 94.0%</p>
<p>当然，正如前面所说，事实上确实有论文（G. Ratsch et al. ML, 2001）给出了 AdaBoost 会很快就过拟合的例子。但总体而言，笔者认为 AdaBoost 在正则化这一方面的表现还是相当优异的</p>
<p>由前面的诸多讨论可以得知，AdaBoost 的正则化能力是来源于各个弱分类器的“分而治之”，那么如果使用分类能力强的弱分类器会有什么结果呢？下面就放出当选用不限制层数的决策树作为弱模型的、异或数据集上的表现，相信会带来很好的直观：</p>
<img src="/posts/fb0d2f02/p8.png" alt="p8.png" title="">
<p>此时在测试数据集上的正确率为 90.0%。值得一提的是，用单独的决策树做出来的效果和上图的效果几乎完全一致。换句话说、此时使用 AdaBoost 没有太大的意义</p>
<h1 id="螺旋数据集上的表现"><a href="#螺旋数据集上的表现" class="headerlink" title="螺旋数据集上的表现"></a>螺旋数据集上的表现</h1><p>随机森林和提升树虽然确实都相当强大、但它同样具有其基本组成单元——决策树所具有的某些缺点。比如说，它们在处理连续性比较强的数据时可能会有些吃力、因为它们的决策边界一般而言都是“不太光滑”的。下面我们就统一使用个体决策树不做层数限制的随机森林和提升树以及螺旋线数据集作为样例来进行说明</p>
<p>生成螺旋数据集的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_spin</span><span class="params">(size=<span class="number">30</span>)</span>:</span></div><div class="line">    xs = np.zeros((size * <span class="number">4</span>, <span class="number">2</span>), dtype=np.float32)</div><div class="line">    ys = np.zeros(size * <span class="number">4</span>, dtype=np.int8)</div><div class="line">    <span class="comment"># 根据螺旋线在极坐标中的公式、生成四条螺旋线</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</div><div class="line">        ix = range(size * i, size * (i + <span class="number">1</span>))</div><div class="line">        <span class="comment"># 去掉原点以避免出现原点同时从属于两类的不合理情况</span></div><div class="line">        r = np.linspace(<span class="number">0.0</span>, <span class="number">1</span>, size + <span class="number">1</span>)[<span class="number">1</span>:]</div><div class="line">        t = np.linspace(<span class="number">2</span> * i * pi / <span class="number">4</span>, <span class="number">2</span> * (i + <span class="number">4</span>) * pi / <span class="number">4</span>, size) + np.random.random(</div><div class="line">            size=size) * <span class="number">0.1</span></div><div class="line">        xs[ix] = np.c_[r * np.sin(t), r * np.cos(t)]</div><div class="line">        ys[ix] = <span class="number">2</span> * (i % <span class="number">2</span>) - <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> xs, ys</div></pre></td></tr></table></figure>
<p>随机森林和提升树在其上的表现分别如下两张图所示：</p>
<img src="/posts/fb0d2f02/p9.png" alt="p9.png" title="">
<img src="/posts/fb0d2f02/p10.png" alt="p10.png" title="">
<p>上面两组图的左边都是包含 10 棵 CART 树的模型、右边都是包含 1000 棵 CART 树的模型，准确率则都是为 100.0%。可以看到，虽然它们都确实能够将大致的趋势给描述出来、但是决策边界相对而言都是“直来直去”的，这一点要比支持向量机、神经网络等模型的训练出来的结果要差不少。总之，决策树那使用二类问题的解决方案来处理连续型特征的做法、导致了随机森林和提升树在处理连续特征上的一些不足</p>
<h1 id="蘑菇数据集上的表现"><a href="#蘑菇数据集上的表现" class="headerlink" title="蘑菇数据集上的表现"></a>蘑菇数据集上的表现</h1><p>目前为止我们对二维数据上的测试做了比较详尽的说明，接下来我们不妨拿蘑菇数据集来测试一下我们的模型在真实数据下的表现；鉴于该数据集比较简单、我们只使用 100 个样本进行训练并用剩余的 8000 多个样本进行测试。为了直观感受模型的分类能力，我们可以画出当个体模型为 CART 决策树桩时、两种集成模型在测试集上的准确率随训练迭代次数变化而变化的曲线：</p>
<img src="/posts/fb0d2f02/p11.png" alt="p11.png" title="">
<p>其中蓝线是随机森林的训练曲线、绿线是提升树的训练曲线。这个结果是符合直观的，毕竟从个体模型来讲，引入了随机性的、随机森林中的决策树桩要比提升树中正常的决策树桩要弱，所以提升树的收敛速度理应比随机森林的要快；此外，由于随机森林和提升树相比、受个体模型分类能力的影响更大、我们采用的又是 CART 决策树桩这种相当弱的个体模型，所以随机森林收敛后的表现也要比提升树收敛后的表现要差</p>
<p>不过需要指出的是，当我们取消个体 CART 决策树的层数限制时，虽然随机森林的收敛速度仍会比提升树的收敛速度慢、但是收敛后的表现却很有可能比提升树收敛后的表现要好。这是因为取消了层数限制的决策树是相当强力的模型，而且：</p>
<ul>
<li>一方面正如刚刚所说的，随机森林受个体模型的分类能力影响较大、所以取消个体树的层数限制后、随机森林的分类能力自然大大增强</li>
<li>另一方面则如之前所讨论的，具有较强分类能力的个体模型与 AdaBoost 的原理可能不太兼容，这就使得 AdaBoost 本身的优势被抑制了</li>
</ul>
<p>取消层数限制后重复上述实验，此时两种集成模型的训练曲线如下图所示：</p>
<img src="/posts/fb0d2f02/p12.png" alt="p12.png" title="">
<p>可能观众老爷们已经发现，取消层数限制后的提升树似乎还没有取消限制之前的提升树的表现好；事实上，由于我们只用了 100 个样本来进行训练，所以容易想象、取消限制后的提升树将会产生比较严重的过拟合。可以把取消层数限制前后的训练曲线放在一起来进行直观对比，结果如下图所示：</p>
<img src="/posts/fb0d2f02/p13.png" alt="p13.png" title="">
<p>其中蓝线是个体模型为 CART 决策树桩时的训练曲线、绿线是个体模型为正常 CART 决策树时的训练曲线</p>
]]></content>
      
        <categories>
            
            <category> 集成学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[AdaBoost 算法]]></title>
      <url>/posts/f5f50863/</url>
      <content type="html"><![CDATA[<p>由前文的讨论可知，问题的关键主要在如下两点：</p>
<ul>
<li>如何根据弱模型的表现更新训练集的权重</li>
<li>如何根据弱模型的表现决定弱模型的话语权</li>
</ul>
<p>我们接下来就看看 AdaBoost 算法是怎样解决上述两个问题的。事实上，能够将这两个问题的解决方案有机地糅合在一起、正是 AdaBoost 的巧妙之处之一</p>
<a id="more"></a>
<h1 id="AdaBoost-算法陈述"><a href="#AdaBoost-算法陈述" class="headerlink" title="AdaBoost 算法陈述"></a>AdaBoost 算法陈述</h1><p>不失一般性、我们以二类分类问题来进行讨论，易知此时我们的弱模型、强模型和最终模型为弱分类器、强分类器和最终分类器。再不妨假设我们现在有的是一个二类分类的训练数据集：</p>
<script type="math/tex; mode=display">
D = \{\left( x_{1},y_{1} \right),\left( x_{2},y_{2} \right),\ldots,(x_{n},\ y_{n})\}</script><p>其中，每个样本点都是由实例<script type="math/tex">x_{i}</script>和类别<script type="math/tex">y_{i}</script>组成、且：</p>
<script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script><p>这里的<script type="math/tex">X</script>是样本空间、<script type="math/tex">Y</script>是类别空间。AdaBoost 会利用如下的步骤、从训练数据中训练出一系列的弱分类器、然后把这些弱分类器集成为一个强分类器：</p>
<ol>
<li><strong>输入</strong>：训练数据集（包含 N 个数据）、弱学习算法及对应的弱分类器、迭代次数 M</li>
<li><strong>过程</strong>：<ol>
<li>初始化训练数据的权值分布  <script type="math/tex; mode=display">
W_{0} = (w_{01},\ldots,w_{0N})</script></li>
<li>对<script type="math/tex">k = 0,1,\ldots,\ M - 1</script>：<ol>
<li>使用权值分布为<script type="math/tex">W_{k}</script>的训练数据集训练弱分类器  <script type="math/tex; mode=display">g_{k + 1}(x)$$：$$X \rightarrow \{ - 1,\  + 1\}</script></li>
<li>计算<script type="math/tex">g_{k + 1}(x)</script>在训练数据集上的加权错误率  <script type="math/tex; mode=display">
e_{k + 1} = \sum_{i = 1}^{N}{w_{\text{ki}}I(g_{k + 1}\left( x_{i} \right) \neq y_{i})}</script></li>
<li>根据加权错误率计算<script type="math/tex">g_{k + 1}(x)</script>的“话语权”  <script type="math/tex; mode=display">
\alpha_{k + 1} = \frac{1}{2}\ln\frac{1 - e_{k + 1}}{e_{k + 1}}</script></li>
<li>根据<script type="math/tex">g_{k + 1}(x)</script>的表现更新训练数据集的权值分布：被<script type="math/tex">g_{k + 1}\left( x \right)</script>误分的样本（<script type="math/tex">y_{i}g_{k + 1}\left( x_{i} \right) < 0</script>的样本）要相对地（以<script type="math/tex">e^{\alpha_{k + 1}}</script>为比例地）增大其权重，反之则要（以<script type="math/tex">e^{- \alpha_{k + 1}}</script>为比例地）减少其权重  <script type="math/tex; mode=display">
w_{k + 1,i} = \frac{w_{\text{ki}}}{Z_{k}} \cdot exp( - \alpha_{k + 1}y_{i}g_{k + 1}(x_{i}))</script><script type="math/tex; mode=display">
W_{k + 1} = (w_{k + 1,1},\ldots,w_{k + 1,N})</script>这里的<script type="math/tex">Z_{k}</script>是规范化因子  <script type="math/tex; mode=display">
Z_{k} = \sum_{i = 1}^{N}{w_{\text{ki}} \cdot exp( - \alpha_{k + 1}y_{i}g_{k + 1}(x_{i}))}</script>它的作用是将<script type="math/tex">W_{k + 1}</script>归一化成为一个概率分布</li>
</ol>
</li>
<li>加权集成弱分类器  <script type="math/tex; mode=display">
f\left( x \right) = \sum_{k = 1}^{M}{\alpha_{k}g_{k}(x)}</script></li>
</ol>
</li>
<li><strong>输出</strong>：最终分类器<script type="math/tex">g(x)</script>  <script type="math/tex; mode=display">
g\left( x \right) = sign\left( f\left( x \right) \right) = \text{sign}\left( \sum_{k = 1}^{M}{\alpha_{k}g_{k}\left( x \right)} \right)</script></li>
</ol>
<p><strong><em>注意：2.2.2 步骤得到的加权错误率如果足够小的话，可以考虑提前停止训练，但这样做往往不是最合理的选择（这点会在后文进行模型性能分析时进行较详细的说明）</em></strong></p>
<p>我们在分配弱分类器的话语权时用到了一个公式：<script type="math/tex">\alpha_{k + 1} = \frac{1}{2}\ln\frac{1 - e_{k + 1}}{e_{k + 1}}</script>。在该公式中，话语权<script type="math/tex">\alpha_{k + 1}</script>会随着加权错误率<script type="math/tex">e_{k + 1} \in \lbrack 0,\ 1\rbrack</script>的增大而减小。它们之间的函数关系如下图所示：</p>
<img src="/posts/f5f50863/p1.png" alt="p1.png" title="">
<p>大多数情况我们训练出来的弱分类器的<script type="math/tex">e_{k} < 0.5</script>、对应着的是上图左半边的部分；不过即使我们的弱分类器非常差、以至于<script type="math/tex">e_{k} > 0.5</script>，由于此时<script type="math/tex">\alpha_{k} < 0</script>、亦即我们知道该分类器的表决应该反着来看、所以也不会出问题（有一种做法是如果训练到<script type="math/tex">e_{k} > 0.5</script>的话就停止训练，个人感觉也有道理）</p>
<h1 id="弱模型的选择"><a href="#弱模型的选择" class="headerlink" title="弱模型的选择"></a>弱模型的选择</h1><p>看到这里，观众老爷们可能会产生这么一个疑问：如果我们不拘泥于对弱模型进行提升、转而对强模型或比较强的弱模型进行提升的话，会不会提升出更好的模型呢？从 Boosting 的思想来看、需要指出的是：用 Boosting 进行提升的弱模型的学习能力不宜太强，否则使用 Boosting 就没有太大的意义、甚至从原理上不太兼容。直观地说，Boosting 是为了让各个弱模型专注于“某一方面”、最后加权表决，如果使用了较强的弱模型，可能一个弱模型就包揽了好几方面，最后可能反而会模棱两可、起不到“提升”的效果。而且从迭代的角度来说，可以想象：如果使用较强弱模型的话，可能第一次产生的模型就已经达到“最优”、从而使得模型没有“提升空间”</p>
<p><strong><em>注意：虽然笔者认为在 Boosting 中的弱模型就应该选择足够弱的模型，但确实亦有对强模型（如核 SVM）应用 Boosting 也很好的说法。详细而严谨的讨论会牵扯大量的数学理论、这里就不详细展开了</em></strong></p>
<p>可能观众老爷们此时又会产生一个新的疑问：如果说 Boosting 中的弱模型不宜太强的话，是不是说 Bagging 中的个体模型也不宜太强呢？需要指出的是，虽然从理论上来说使用弱模型进行集成就已足以获得一个相当不错的最终模型，但使用较强的模型来进行集成从原理上是不太矛盾的。考虑到不同的场合，有时确实可以选用较强的模型来作为个体模型</p>
<p>那么所谓的不太强的弱模型大概是个什么东西呢？一个比较直观的例子就是限制层数的决策树。极端的情况就是限定它只能有一层、亦即上一章我们提到过的“决策树桩”，对应的进行了提升后的模型就是相当有名的提升树（Boosting Tree），它被认为是统计学习中性能最好的方法之一、既可以用来做分类也可以拿来做回归，是个相当强力的模型</p>
<h1 id="AdaBoost-的实现"><a href="#AdaBoost-的实现" class="headerlink" title="AdaBoost 的实现"></a>AdaBoost 的实现</h1><p>从之前的算法讲解其实可以看出，虽然 AdaBoost 算法本身很不平凡，但它给出的步骤都是相当便于实现的，基本上一个步骤就对应着 Python 里面的一行代码。在实现的过程中，困难之处可能主要在于如何让实现出来的 AdaBoost 框架易于扩展并具有方便调用的接口，而不在于实现算法本身。同时，为了能够更好地理解 AdaBoost 算法，我们需要对其性能作一系列的分析</p>
<p>由于 AdaBoost 是一个用于提升弱模型的算法，所以我们整体的实现思路大致是（不失一般性、我们先讨论二类分类问题）：</p>
<ul>
<li>搭建 AdaBoost 框架</li>
<li>使用 sklearn 中的分类器对框架的正确性进行检验</li>
<li>使用前两章实现的分类器进行对比实验</li>
</ul>
<p>所以我们要先把 AdaBoost 框架实现出来。为此，先来看 AdaBoost 框架的初始化步骤（其中<a href="https://github.com/carefree0910/MachineLearning/tree/master/_SKlearn" target="_blank" rel="external">_SKlearn</a>是我对 sklearn 中的模型做了一定程度的拓展后的模型包）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</div><div class="line"><span class="comment"># 导入我们之前实现的朴素贝叶斯模型和决策树模型</span></div><div class="line"><span class="keyword">from</span> b_NaiveBayes.Vectorized.MultinomialNB <span class="keyword">import</span> MultinomialNB</div><div class="line"><span class="keyword">from</span> b_NaiveBayes.Vectorized.GaussianNB <span class="keyword">import</span> GaussianNB</div><div class="line"><span class="keyword">from</span> c_CvDTree.Tree <span class="keyword">import</span> *</div><div class="line"><span class="keyword">from</span> _SKlearn.NaiveBayes <span class="keyword">import</span> *</div><div class="line"><span class="keyword">from</span> _SKlearn.Tree <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdaBoost</span>:</span></div><div class="line">    <span class="comment"># 弱分类器字典，如果想要测试新的弱分类器的话、只需将其加入该字典即可</span></div><div class="line">    _weak_clf = &#123;</div><div class="line">        <span class="string">"SKMNB"</span>: SKMultinomialNB,</div><div class="line">        <span class="string">"SKGNB"</span>: SKGaussianNB,</div><div class="line">        <span class="string">"SKTree"</span>: SKTree,</div><div class="line"></div><div class="line">        <span class="string">"MNB"</span>: MultinomialNB,</div><div class="line">        <span class="string">"GNB"</span>: GaussianNB,</div><div class="line">        <span class="string">"ID3"</span>: ID3Tree,</div><div class="line">        <span class="string">"C45"</span>: C45Tree,</div><div class="line">        <span class="string">"Cart"</span>: CartTree</div><div class="line">    &#125;</div><div class="line">    <span class="string">"""</span></div><div class="line">        AdaBoost框架的朴素实现</div><div class="line">        使用的弱分类器需要有如下两个方法：</div><div class="line">            1) 'fit'      方法，它需要支持输入样本权重</div><div class="line">            2) 'predict'  方法, 它用于返回预测的类别向量</div><div class="line">        初始化结构</div><div class="line">        self._clf：记录弱分类器名称的变量</div><div class="line">        self._clfs：记录弱分类器的列表</div><div class="line">        self._clfs_weights：记录弱分类器“话语权”的列表</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self._clf, self._clfs, self._clfs_weights = <span class="string">""</span>, [], []</div></pre></td></tr></table></figure>
<p>接下来就是训练和预测部分的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight=None, clf=None, epoch=<span class="number">10</span>, eps=<span class="number">1e-12</span>, **kwargs)</span>:</span></div><div class="line">    <span class="comment"># 默认使用10个CART决策树桩作为弱分类器</span></div><div class="line">    <span class="keyword">if</span> clf <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> AdaBoost._weak_clf[clf] <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        clf = <span class="string">"Cart"</span></div><div class="line">        kwargs = &#123;<span class="string">"max_depth"</span>: <span class="number">1</span>&#125;</div><div class="line">    self._clf = clf</div><div class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        sample_weight = np.ones(len(y)) / len(y)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        sample_weight = np.array(sample_weight)</div><div class="line">    <span class="comment"># AdaBoost算法的主循环，epoch为迭代次数</span></div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</div><div class="line">        <span class="comment"># 根据样本权重训练弱分类器</span></div><div class="line">        tmp_clf = AdaBoost._weak_clf[clf](**kwargs)</div><div class="line">        tmp_clf.fit(x, y, sample_weight)</div><div class="line">        <span class="comment"># 调用弱分类器的predict方法进行预测</span></div><div class="line">        y_pred = tmp_clf.predict(x)</div><div class="line">        <span class="comment"># 计算加权错误率；考虑到数值稳定性，在边值情况加了一个小的常数</span></div><div class="line">        em = min(max((y_pred != y).dot(self._sample_weight[:, <span class="keyword">None</span>])[<span class="number">0</span>], eps), <span class="number">1</span> - eps)</div><div class="line">        <span class="comment"># 计算该弱分类器的“话语权”</span></div><div class="line">        am = <span class="number">0.5</span> * log(<span class="number">1</span> / em - <span class="number">1</span>)</div><div class="line">        <span class="comment"># 更新样本权重并利用deepcopy将该弱分类器记录在列表中</span></div><div class="line">        sample_weight *= np.exp(-am * y * y_pred)</div><div class="line">        sample_weight /= np.sum(sample_weight)</div><div class="line">        self._clfs.append(deepcopy(tmp_clf))</div><div class="line">        self._clfs_weights.append(am)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></div><div class="line">    x = np.atleast_2d(x)</div><div class="line">    rs = np.zeros(len(x))</div><div class="line">    <span class="comment"># 根据各个弱分类器的“话语权”进行决策</span></div><div class="line">    <span class="keyword">for</span> clf, am <span class="keyword">in</span> zip(self._clfs, self._clfs_weights):</div><div class="line">        rs += am * clf.predict(x)</div><div class="line">    <span class="comment"># 将预测值大于0的判为类别1，小于0的判为类别-1</span></div><div class="line">    <span class="keyword">return</span> np.sign(rs)</div></pre></td></tr></table></figure>]]></content>
      
        <categories>
            
            <category> 集成学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> 算法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[随机森林算法]]></title>
      <url>/posts/c0a9c025/</url>
      <content type="html"><![CDATA[<p>由前文讨论可知，我们在实现 RF 算法之前，需要先在决策树模型的生成过程中加一个参数、使得我们能够对特征选取加入随机性。这个过程相当平凡，下给出代码片段以进行粗略的说明。首先在<code>CvDBase</code>的<code>fit</code>方法中加入一个参数<code>feature_bound</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight=None, alpha=None, eps=<span class="number">1e-8</span>,</span></span></div><div class="line">    cv_rate=<span class="number">0.2</span>, train_only=False, feature_bound=None):</div></pre></td></tr></table></figure>
<p>然后在同一个方法里面、把这个参数传给<code>CvDNode</code>的<code>fit</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.root.fit(x_train, y_train, _train_weights, feature_bound, eps)</div></pre></td></tr></table></figure>
<p>在<code>CvDNode</code>的<code>fit</code>方法中，原始代码中有一个对可选特征空间的遍历：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> self.feats:</div></pre></td></tr></table></figure>
<p>根据参数<code>feature_bound</code>对它加入随机性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">feat_len = len(self.feats)</div><div class="line"><span class="comment"># 默认没有随机性</span></div><div class="line"><span class="keyword">if</span> feature_bound <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">    indices = range(<span class="number">0</span>, feat_len)</div><div class="line"><span class="keyword">elif</span> feature_bound == <span class="string">"log"</span>:</div><div class="line">    <span class="comment"># np.random.permutation(n)：将数组打乱后返回</span></div><div class="line">    indices = np.random.permutation(feat_len)[:max(<span class="number">1</span>, int(log2(feat_len)))]</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    indices = np.random.permutation(feat_len)[:feature_bound]</div><div class="line">tmp_feats = [self.feats[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices]</div><div class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> tmp_feats:</div></pre></td></tr></table></figure>
<p>然后要在同一个方法里面、把<code>feature_bound</code>传给<code>_gen_children</code>方法，而在<code>_gen_children</code>中、再把<code>feature_bound</code>传给子节点的<code>fit</code>方法即可</p>
<p>以上所有实现细节可参见<a href="https://github.com/carefree0910/MachineLearning/tree/master/c_CvDTree" target="_blank" rel="external">这里</a>中的 Tree.py 和 Node.py</p>
<p>有了这些准备，我们就可以来看看 RF 的算法陈述了（以分类问题为例）：</p>
<a id="more"></a>
<ol>
<li><strong>输入</strong>：训练数据集（包含 N 个数据）、决策树模型、迭代次数 M</li>
<li><strong>过程</strong>：<ol>
<li>对<script type="math/tex">j=1,2,...,M</script>：<ol>
<li>通过 Bootstrap 生成包含 N 个数据的数据集<script type="math/tex">D_k</script></li>
<li>利用<script type="math/tex">D_j</script>和输入的决策树模型进行训练，注意不用对训练好的决策树模型<script type="math/tex">g_j</script>进行剪枝。同时需要注意的是，在训练决策树的过程中、每一步的生成都要对特征的选取加入随机性</li>
</ol>
</li>
<li>对个体决策树进行简单组合。不妨用符号<script type="math/tex">\text{freq}(c_{k})</script>表示类别<script type="math/tex">c_{k}</script>在 M 个决策树模型的决策中出现的频率，那么：  <script type="math/tex; mode=display">
g\left( x \right) = \arg{\max_{c_k}{\text{freq}(c_{k})}}</script></li>
</ol>
</li>
<li><strong>输出</strong>：最终分类器<script type="math/tex">g(x)</script></li>
</ol>
<p>从算法即可看出随机森林算法的实现（在实现好决策树模型后）是相当平凡的，需要额外做的工作只有定义一个能够计算上述算法第 2.2 步中<script type="math/tex">\arg{\max_{c_k}{freq(c_{k})}}</script>的函数而已：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入我们自己实现的决策树模型</span></div><div class="line"><span class="keyword">from</span> c_CvDTree.Tree <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomForest</span><span class="params">(ClassifierBase)</span>:</span></div><div class="line">    <span class="comment"># 建立一个决策树字典，以便调用</span></div><div class="line">    _cvd_trees = &#123;</div><div class="line">        <span class="string">"id3"</span>: ID3Tree,</div><div class="line">        <span class="string">"c45"</span>: C45Tree,</div><div class="line">        <span class="string">"cart"</span>: CartTree</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(RandomForest, self).__init__()</div><div class="line">        self._trees = []</div><div class="line"></div><div class="line">    <span class="comment"># 实现计算的函数</span></div><div class="line"><span class="meta">    @staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">most_appearance</span><span class="params">(arr)</span>:</span></div><div class="line">        u, c = np.unique(arr, return_counts=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">return</span> u[np.argmax(c)]</div><div class="line"></div><div class="line">    <span class="comment"># 默认使用 10 棵 CART 树、默认 k = log(d)</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight=None, tree=<span class="string">"cart"</span>, epoch=<span class="number">10</span>, feature_bound=<span class="string">"log"</span>,</span></span></div><div class="line">            *args, **kwargs):</div><div class="line">        x, y = np.atleast_2d(x), np.array(y)</div><div class="line">        n_sample = len(y)</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</div><div class="line">            tmp_tree = RandomForest._cvd_trees[tree](*args, **kwargs)</div><div class="line">            _indices = np.random.randint(n_sample, size=n_sample)</div><div class="line">            <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">                _local_weight = <span class="keyword">None</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                _local_weight = sample_weight[_indices]</div><div class="line">                _local_weight /= _local_weight.sum()</div><div class="line">            tmp_tree.fit(x[_indices], y[_indices],</div><div class="line">                sample_weight=_local_weight, feature_bound=feature_bound)</div><div class="line">            self._trees.append(deepcopy(tmp_tree))</div><div class="line"></div><div class="line">    <span class="comment"># 对个体决策树进行简单组合</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></div><div class="line">        _matrix = np.array([_tree.predict(x) <span class="keyword">for</span> _tree <span class="keyword">in</span> self._trees]).T</div><div class="line">        <span class="keyword">return</span> np.array([RandomForest.most_appearance(rs) <span class="keyword">for</span> rs <span class="keyword">in</span> _matrix])</div></pre></td></tr></table></figure>
<p>需要指出的是，<code>most_appearance</code>函数用到了 Numpy 中的<code>unique</code>方法、它和标准库<code>collections</code>中的<code>Counter</code>具有差不多的用法。举个小栗子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = np.array([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">"dcbabcd"</span>])</div><div class="line">np.unique(x, return_counts=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p>这两行代码会返回：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">(</div><div class="line">    array([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>], dtype=<span class="string">'&lt;U1'</span>),</div><div class="line">    array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], dtype=int64)</div><div class="line">)</div></pre></td></tr></table></figure>
<p>换句话说，<code>unique</code>方法能够提取出一个 Numpy 数组中出现过的元素并对它们计数、同时输出的 Numpy 数组是经过排序的</p>
<p>以上就完成了一个简易可行的随机森林模型的实现，我们可以把对随机森林模型的评估与对 AdaBoost 的评估放在一起进行以便于对比、这里就先按下不表</p>
]]></content>
      
        <categories>
            
            <category> 集成学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> 算法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[“集成”的思想]]></title>
      <url>/posts/7081b0ee/</url>
      <content type="html"><![CDATA[<p>本文首先会介绍何谓“集成”、然后会介绍两种常见的集成学习方法：Bagging、AdaBoost 的基本定义。这些概念的背后有着深刻的数学理论，但是它们同时也拥有着很好的直观。获得对它们的直观有助于加深对各种模型的分类性能的理解、同时也有助于根据具体的数据集来挑选相应的、合适的模型来进行学习</p>
<a id="more"></a>
<h1 id="众擎易举"><a href="#众擎易举" class="headerlink" title="众擎易举"></a>众擎易举</h1><p>集成学习基于这样的思想：对于比较复杂的任务，综合许多人的意见来进行决策会比“一家独大”要更好。换句话说、就是通过适当的方式集成许多“个体模型”所得到的最终模型要比单独的“个体模型”的性能更优。我们可以通过下图来直观感知这个过程：</p>
<img src="/posts/7081b0ee/p1.png" alt="p1.png" title="">
<p>所以问题的关键转化为了两点：如何选择、生成弱分类器和如何对它们进行提升（集成）。在此基础上，通常有三种不同的思路：</p>
<ul>
<li>将不同类型的弱分类器进行提升</li>
<li>将相同类型但参数不同的弱分类器进行提升</li>
<li>将相同类型但训练集不同的弱分类器进行提升</li>
</ul>
<p>其中第一种思路的应用相对来说可能不太广泛，而第二、第三种思路则指导着两种常见的做法，这两种做法的区别主要体现在基本组成单元——弱分类器的生成方式：</p>
<p>第一种做法期望各个弱分类器之间依赖性不强、可以同时进行生成。这种做法又称并行方法，其代表为 Bagging，而 Bagging 一个著名的拓展应用便是本系列的主题之一——随机森林（Random Forest，常简称为 RF）。</p>
<p>第二种做法中弱分类器之间具有强依赖性、只能序列生成。这种做法又称串行方法，其代表为 Boosting，而 Boosting 族算法中的代表即是本系列的另一主题——AdaBoost</p>
<h1 id="Bagging-与随机森林"><a href="#Bagging-与随机森林" class="headerlink" title="Bagging 与随机森林"></a>Bagging 与随机森林</h1><p>Bagging 是 1996 年由 Breiman 提出的，它的思想根源是数理统计中非常重要的 Bootstrap 理论。Bootstrap 可以翻译成“自举”，它通过模拟的方法来逼近样本的概率分布函数。可以想象这样一个场景：现在有一个包含 N 个样本的数据集<script type="math/tex">X = \{ x_{1},\ldots,x_{N}\}</script>，这 N 个样本是由随机变量<script type="math/tex">x</script>独立生成的。我们想要研究<script type="math/tex">x</script>的均值估计<script type="math/tex">\bar{x} = \frac{1}{N}\sum_{i = 1}^{N}x_{i}</script>的统计特性（误差、方差等等），但由于研究统计特性是需要大量样本的、而数据集<script type="math/tex">X</script>只能给我们提供一个<script type="math/tex">\bar{x}</script>的样本，从而导致无法进行研究</p>
<p>在这种场景下，容易想到的一种解决方案是：通过<script type="math/tex">x</script>的分布生成出更多的数据集<script type="math/tex">X_{1},\ldots X_{M}</script>、每个数据集都包含 N 个样本。这 M 个数据集都能产生一个均值估计、从而就有了 M 个均值估计的样本。那么只要 M 足够大、我们就能研究<script type="math/tex">\bar{x}</script>的统计特性了</p>
<p>当然这种解决方案的一个最大的困难就是：我们并不知道<script type="math/tex">x</script>的真实分布。Bootstrap 就是针对这个困难提出了一个解决办法：通过不断地“自采样”来模拟随机变量真实分布生成的数据集。具体而言，Bootstrap 的做法是：</p>
<ul>
<li>从<script type="math/tex">X</script>中随机抽出一个样本（亦即抽出<script type="math/tex">x_1,...,x_N</script>的概率相同）</li>
<li>将该样本的拷贝放入数据集<script type="math/tex">X_j</script></li>
<li>将该样本放回<script type="math/tex">X</script>中</li>
</ul>
<p>以上三个步骤将重复 N 次、从而使得<script type="math/tex">X_{j}</script>中有 N 个样本。这个过程将对<script type="math/tex">j = 1,\ldots,M</script>都进行一遍、从而我们最终能得到 M 个含有 N 个样本的数据集<script type="math/tex">X_{1},\ldots X_{M}</script></p>
<p>简单来说的话、Bootstrap 其实就是一个有放回的随机抽样过程，所以原始数据<script type="math/tex">\{ x_{1},\ldots,x_{N}\}</script>中可能会在<script type="math/tex">X_{1},\ldots X_{M}</script>中重复出现、也有可能不出现在<script type="math/tex">X_{1},\ldots X_{M}</script>中。事实上，由于<script type="math/tex">X</script>中一个样本在 N 次采样中始终不被采到的概率为<script type="math/tex">\left( 1 - \frac{1}{N} \right)^{N}</script>、且：</p>
<script type="math/tex; mode=display">
\lim_{N\rightarrow\infty}\left( 1 - \frac{1}{N} \right)^{N} \rightarrow \frac{1}{e} \approx 0.368</script><p>所以在统计意义上可以认为、<script type="math/tex">X_{j}</script>中含有<script type="math/tex">X</script>中 63.2%的样本<script type="math/tex">\left( \forall j = 1,\ldots,M \right)</script></p>
<p>这种模拟的方法在理论上是具有最优性的。事实上、这种模拟的本质和经验分布函数对真实分布函数的模拟几乎一致：</p>
<ul>
<li>Bootstrap 以<script type="math/tex">\frac 1N</script>的概率、有放回地从<script type="math/tex">X</script>中抽取 N 个样本作为数据集、并以之估计真实分布生成的具有 N 个样本的数据集</li>
<li>经验分布函数则是在 N 个样本点上以每点的概率为<script type="math/tex">\frac 1N</script>作为概率密度函数、然后进行积分的函数</li>
</ul>
<p>经验分布函数的数学表达式为：</p>
<script type="math/tex; mode=display">
F_{N}\left( x \right) = \frac{1}{N}\sum_{i = 1}^{N}{I_{\left( - \infty,x \right\rbrack}(x_{i})}</script><p>可以看出、经验分布函数用到了频率估计概率的思想。用它来模拟真实分布函数是具有很好的优良性的，详细的讨论可参见<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a></p>
<p>知道 Bootstrap 是什么之后、我们就可以来看 Bagging 的具体定义了。Bagging 的全称是 Bootstrap Aggregating，其思想非常简单：</p>
<ul>
<li>用 Bootstrap 生成出 M 个数据集</li>
<li>用这 M 个数据集训练出 M 个弱分类器</li>
<li>最终模型即为这M个弱分类器的简单组合</li>
</ul>
<p>所谓简单组合就是：</p>
<ul>
<li>对于分类问题使用简单的投票表决</li>
<li>对于回归问题则进行简单的取平均</li>
</ul>
<p>简单组合虽说简单、其背后仍然是有数学理论支撑的。考虑二分类问题：</p>
<script type="math/tex; mode=display">
y \in \{ - 1, + 1\}</script><p>假设样本空间到类别空间的真实映射为<script type="math/tex">f</script>、我们得到的 M 个弱分类器模型<script type="math/tex">G_{1},\ldots,G_{M}</script>所对应的映射为<script type="math/tex">g_{1},\ldots,g_{M}</script>，那么简单组合下的最终模型对应的映射即为：</p>
<script type="math/tex; mode=display">
g\left( x \right) = \text{sign}\left( \sum_{j = 1}^{M}{g_{j}\left( x \right)} \right)</script><p>这里的 sign 是符号函数、满足：</p>
<script type="math/tex; mode=display">
\text{sign}\left( x \right) = \left\{ \begin{matrix}
 - 1,\ \ if\ x < 0 \\
 + 1,\ \ if\ x > 0 \\
\end{matrix} \right.\</script><p>其中，<script type="math/tex">\text{sign}(0)</script>则可为<script type="math/tex">-1</script>也可为<script type="math/tex">+1</script>，令其有 50%的概率输出<script type="math/tex">-</script>、<script type="math/tex">+1</script>也是可行的</p>
<p>如果我们此时假设每个弱分类器的错误率为<script type="math/tex">\epsilon</script>：</p>
<script type="math/tex; mode=display">
p\left( g_{i}\left( x \right) \neq f\left( x \right) \right) = \epsilon</script><p>如果我们假设弱分类器的错误率相互独立，那么由霍夫丁不等式（Hoeffding’s Inequality）可以得知：</p>
<script type="math/tex; mode=display">
p\left( G\left( x \right) \neq f\left( x \right) \right) = \sum_{j = 0}^{\left\lfloor \frac{M}{2} \right\rfloor}\left( \frac{M}{j} \right)\left( 1 - \epsilon \right)^{j}\epsilon^{M - j} \leq \exp\left( - \frac{1}{2}M\left( 1 - 2\epsilon \right)^{2} \right)</script><p>亦即最终模型的错误率随弱分类器的个数 M 的增加、将会以指数级下降并最终趋于 0</p>
<p>虽说这个结果看上去很振奋人心，但需要注意的是、我们做了一个非常强的关键假设：假设弱分类器的错误率相互独立。这可以说是不可能做到的，因为这些弱分类器想要解决的都是同一个问题、且使用的训练集也都源自于同一份数据集</p>
<p>但不管怎么说，以上的分析给了我们这样一个重要信息：弱分类器之间的“差异”似乎应该尽可能的大。基于此，结合 Bagging 的特点、我们可以得出这样一个结论：对于“不稳定”（或说对训练集敏感：若训练样本稍有改变，得到的从样本空间到类别空间的映射 g 就会产生较大的变化）的分类器，Bagging 能够显著地对其进行提升。这也是被大量实验结果所证实了的</p>
<p>正如前文提过的，Bagging 有一个著名的拓展应用叫“随机森林”，从名字就容易想到、它是当个体模型为决策树时的 Bagging 算法。不过需要指出的是，随机森林算法不仅对样本进行 Bootstrap 采样，对每个 Node 调用生成算法时都会随机挑选出一个可选特征空间的子空间作为该决策树的可选特征空间；同时，生成好个体决策树后不进行剪枝、而是保持原始的形式。换句话说、随机森林算法流程大致如下：</p>
<ul>
<li>用 Bootstrap 生成出 M 个数据集</li>
<li>用这 M 个数据集训练出 M 颗不进行后剪枝决策树，且在每颗决策树的生成过程中，每次对 Node 进行划分时、都从可选特征（比如说有 d 个）中随机挑选出 k 个（<script type="math/tex">k\le d</script>）特征，然后依信息增益的定义从这 k 个特征中选出信息增益最大的特征作为划分标准</li>
<li>最终模型即为这 M 个弱分类器的简单组合</li>
</ul>
<p><strong><em>注意：有一种说法是随机森林中的个体决策树模型只能使用 CART 树</em></strong></p>
<p>也就是说，除了和一般 Bagging 算法那样对样本进行随机采样以外、随机森林还对特征进行了某种意义上的随机采样。这样做的意义是直观的：通过对特征引入随机扰动，可以使个体模型之间的差异进一步增加、从而提升最终模型的泛化能力。而这个特征选取的随机性，恰恰被上述算法第二步中的参数k所控制：</p>
<ul>
<li>若<script type="math/tex">k=d</script>，那么训练出来的决策树和一般意义下的决策树别无二致、亦即特征选取这一部分不具有随机性</li>
<li>若<script type="math/tex">k=1</script>，那么生成决策树的每一步都是在随机选择属性、亦即特征选取的随机性达到最大</li>
</ul>
<p>Breiman 在提出随机森林算法的同时指出，一般情况下、推荐取<script type="math/tex">k=\log_2{d}</script></p>
<h1 id="PAC-框架与-Boosting"><a href="#PAC-框架与-Boosting" class="headerlink" title="PAC 框架与 Boosting"></a>PAC 框架与 Boosting</h1><p>虽然同属集成学习方法，但 Boosting 和 Bagging 的数学理论根基不尽相同：Boosting 产生于计算学习理论（Computational Learning Theory）[Valiant, 1984]。一般而言，如果只是应用机器学习的话、我们无需对它进行太多的了解（甚至可以说对它一知半解反而有害），所以本节只打算对其最基本的概率近似正确（PAC）学习理论中的“可学习性（PAC Learnability）”进行简要的介绍</p>
<p>PAC 学习整体来说是一个比较纯粹的数学理论。有一种说法是、PAC 学习是统计学家研究机器学习的方式，它关心模型的可解释性、然而机器学习专家通常更关心模型的预测能力。这也正是为何说无需太过了解它，因为我们的目的终究不是成为统计的专家、而是更希望成为一个能够应用机器学习的人。不过幸运的是，虽然为了叙述 Boosting、PAC 学习中“可学习性”的概念难以避开，但其本身却是具有很直观的解释的。下面我们就来看看这个直观解释：</p>
<p>PAC 提出的一个主要的假设、就是它要求数据是从某个稳定的概率分布中产生。直观地说，就是样本在样本空间中的分布状况不能随时间的变化而变化、否则就失去了学习的意义（因为学习到的永远只是“某个时间”的分布，如果未知数据所处时间的分布状况和该时间数据的分布状况不同的话、模型就直接失效了）。然后所谓的PAC可学习性，就是看学习的算法是否能够在合理的时间（多项式时间）内、以足够高的概率输出一个错误率足够低的模型。由此，所谓的“强可学习”和“弱可学习”的概念就很直观了：</p>
<ul>
<li>若存在一个多项式算法可以学习出准确率很高的模型，则称为强可学习</li>
<li>若在在一个多项式算法可以学习但准确率仅仅略高于随机猜测，则称为弱可学习</li>
</ul>
<p><strong><em>注意：由于进行机器学习时、我们只能针对训练数据集进行学习、所以和真实情况相比肯定是有偏差的。这正是需要提出 PAC 可学习这个概念的原因之一</em></strong></p>
<p>虽然我们区分定义了这两个概念，不过神奇之处在于，这两个概念在PAC学习框架下是完全等价的[Schapire, 1990]。这意味着对于一个学习问题，只要我们找到了一个“弱学习算法”，就可以把它变成一个“强学习算法”。这当然是意义深刻的，因为往往比较粗糙的“弱学习算法”比较好找、而相对精确的“强学习算法”却难得一求</p>
<p>那么具体而言应该怎么做呢？这里就需要用到所谓的 Boosting（提升方法）了。提升方法可以定义为用于将由“弱学习算法”生成的“弱模型”、提升成和“强学习算法”所生成的“强模型”性能差不多的模型的方法，它的基本组成单元是许许多多的“弱模型”、然后通过某种手段把它们集成为最终模型。虽然该过程听上去和之前介绍的 Bagging 差不多、但它们的思想和背后的数学理论却有较大区别，加以辨析是有必要的</p>
<p>需要指出的是，Boosting 事实上是一族算法、该族算法有一个类似的框架：</p>
<ul>
<li>根据当前的数据训练出一个弱模型</li>
<li>根据该弱模型的表现调整数据的样本权重。具体而言：<ul>
<li>让该弱模型做错的样本在后续训练中获得更多的关注</li>
<li>让该弱模型做对的样本在后续训练中获得较少的关注</li>
</ul>
</li>
<li>最后再根据该弱模型的表现决定该弱模型的“话语权”、亦即投票表决时的“可信度”。自然、表现越好就越有话语权</li>
</ul>
<p>可以证明，当训练样本有无穷多时、Boosting 能让弱模型集成出一个对训练样本集的准确率任意高的模型。然而实际任务中训练样本当然不可能有无穷多，所以问题就转为了如何在固定的训练集上应用 Boosting 方法。而在 1996 年，Freund 和 Schapire 所提出的 AdaBoost（Adaptive Boosting）正是一个相当不错的解决方案，在理论和实验上均有优异的表现。虽然 AdaBoost 背后的理论深究起来可能会有些繁复、但它的思想并没有脱离 Boosting 族算法的那一套框架。值得一提的是、Boosting 还有一套比较有意思的解释方法；我们会在后文详细讨论其中的代表性算法——AdaBoost 的解释、这里就先按下不表</p>
]]></content>
      
        <categories>
            
            <category> 集成学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[集成学习综述]]></title>
      <url>/posts/d8e97c87/</url>
      <content type="html"><![CDATA[<p>目前为止我们已经讲过了若干的分类器了。从它们的复杂程度可以感受到，它们有些是比较“强”的、有些是比较“弱”的。这一章我们将会阐述所谓的“强”与“弱”的定义、它们之间的联系以及阐述如何将一个“弱分类器”通过集成学习来集成出一个“强分类器”。而由于集成学习有许多种具体的方法，我们会挑选出其中的随机森林和 AdaBoost 来作比较详细的说明</p>
<p>以下是目录：</p>
<ul>
<li><a href="/posts/7081b0ee/" title="“集成”的思想">“集成”的思想</a></li>
<li><a href="/posts/c0a9c025/" title="随机森林算法">随机森林算法</a></li>
<li><a href="/posts/f5f50863/" title="AdaBoost 算法">AdaBoost 算法</a></li>
<li><a href="/posts/fb0d2f02/" title="集成模型的性能分析">集成模型的性能分析</a></li>
<li><a href="/posts/707464b/" title="AdaBoost 算法的解释">AdaBoost 算法的解释</a></li>
<li><a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a></li>
<li><a href="/posts/48a0211a/" title="“集成学习”小结">“集成学习”小结</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> 集成学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
            <tag> 目录 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[“决策树”小结]]></title>
      <url>/posts/88953f51/</url>
      <content type="html"><![CDATA[<ul>
<li>决策树是从直观上很好理解的模型，可以把它理解为一个划分规则的序列</li>
<li>决策树常用的生成算法包括：<ul>
<li>ID3 算法，它使用互信息作为信息增益的度量</li>
<li>C4.5 算法，它使用信息增益比作为信息增益的度量</li>
<li>CART 算法，它规定生成出来的决策树为二叉树、且一般使用基尼增益作为信息增益的度量</li>
</ul>
</li>
<li>决策树常用的剪枝算法有两种，它们都是为了适当地降低模型复杂度、从而期望模型在未知数据上的表现更好</li>
<li>决策树的代码实现从始到终都贯彻着递归的思想，可以说是递归的一个经典应用</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 决策树 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 小结 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[相关数学理论]]></title>
      <url>/posts/613bbb2f/</url>
      <content type="html"><![CDATA[<p>本篇文章会叙述琴生不等式（Jensen’s Inequality）及其一个简单的应用，为此我们需要知道凸函数的概念：</p>
<ul>
<li>若函数<script type="math/tex">f(x)</script>对<script type="math/tex">\forall p\in [0,1]</script>、都满足：  <script type="math/tex; mode=display">
pf\left( x_{1} \right) + \left( 1 - p \right)f\left( x_{2} \right) \leq f\left( px_{1} + \left( 1 - p \right)x_{2} \right)</script>则称<script type="math/tex">f(x)</script>为凸函数（有时又叫上凸函数）</li>
</ul>
<p>琴生不等式是针对凸函数提出的，下面就具体说一下其内容与证明</p>
<a id="more"></a>
<h1 id="定理内容"><a href="#定理内容" class="headerlink" title="定理内容"></a>定理内容</h1><p>对于<script type="math/tex">\lbrack a,b\rbrack</script>上的凸函数，若</p>
<script type="math/tex; mode=display">
p_{1},...,p_{K} \in \left\lbrack 0,1 \right\rbrack,\ \ p_{1} + p_{2} + \ldots + p_{K} = 1</script><p>则有</p>
<script type="math/tex; mode=display">
\sum_{k = 1}^{K}{p_{i}f(x_{i})} \leq f(\sum_{k = 1}^{K}{p_{i}x_{i}})</script><p>接下来我们会利用它来证明等概率分布具有最大熵。注意到可以证明函数</p>
<script type="math/tex; mode=display">
\hat{H}\left( p \right) = - p\log p</script><p>是一个凸函数，于是熵的定义式可以写成</p>
<script type="math/tex; mode=display">
H\left( y \right) = - \sum_{k = 1}^{K}{p_{k}\log p_{k}} = \sum_{k = 1}^{K}{\hat{H}\left( p_{k} \right)}</script><p>从而</p>
<script type="math/tex; mode=display">
\frac{1}{K}H\left( y \right) = \frac{1}{K}\sum_{k = 1}^{K}{\hat{H}(p_{k})} \leq \hat{H}\left( \sum_{k = 1}^{K}{\frac{1}{K}p_{k}} \right) = \hat{H}\left( \frac{1}{K} \right) = - \frac{1}{K}\log\frac{1}{K} = \frac{1}{K}\log K</script><p>亦即</p>
<script type="math/tex; mode=display">
H\left( y \right) \leq \log K</script><p>等式当且仅当</p>
<script type="math/tex; mode=display">
p_{1} = p_{2} = \ldots = p_{K} = \frac{1}{K}</script><p>时取得</p>
<h1 id="定理证明"><a href="#定理证明" class="headerlink" title="定理证明"></a>定理证明</h1><p>应用数学归纳法可以比较简单地完成证明：</p>
<ul>
<li>当时<script type="math/tex">K=2</script>、由凸函数定义直接证毕，此为奠基</li>
<li>假设<script type="math/tex">K = n</script>时成立、考虑<script type="math/tex">K = n + 1</script>的情况，令  <script type="math/tex; mode=display">
s_{n} = \sum_{k = 1}^{n}p_{k}</script>则  <script type="math/tex; mode=display">
\sum_{k = 1}^{n + 1}{p_{k}f\left( x_{k} \right)} = s_{n}\sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} + p_{n + 1}f\left( x_{n + 1} \right)</script>注意到  <script type="math/tex; mode=display">
\sum_{k = 1}^{n}\frac{p_{k}}{s_{n}} = 1</script>从而由<script type="math/tex">K = n</script>时的琴生不等式可知  <script type="math/tex; mode=display">
\sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} \leq f\left( \sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} \right)</script>注意到  <script type="math/tex; mode=display">
s_{n} + p_{n + 1} = 1</script>从而由凸函数定义知  <script type="math/tex; mode=display">
s_{n}f\left( \sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} \right) + p_{n + 1}f\left( x_{n + 1} \right) \leq f\left( s_{n}\sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} + p_{n + 1}x_{n + 1} \right)</script>综上所述、即得  <script type="math/tex; mode=display">
\sum_{k = 1}^{n + 1}{p_{k}f(x_{k})} \leq f\left( \sum_{k = 1}^{n + 1}{p_{k}f\left( x_{k} \right)} \right)</script></li>
</ul>
<p>琴生不等式的应用非常广泛，证明等概率分布具有最大熵只是其中一个小应用。在许许多多涉及到凸问题的算法中、琴生不等式都显示出了强大的威力</p>
]]></content>
      
        <categories>
            
            <category> 决策树 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[评估与可视化]]></title>
      <url>/posts/c12a819/</url>
      <content type="html"><![CDATA[<p>之前我们实现了一个 Node 基类<code>CvDNode</code>和一个 Tree 基类<code>CvDBase</code>；为了评估决策树模型的表现、我们需要先在这两个基类的基础上根据不同的算法实现出各种具体的决策树。由于我们在基类里面已经完成了绝大部分工作、所以在其上进行扩展是平凡的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ID3Node</span><span class="params">(CvDNode)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></div><div class="line">        CvDNode.__init__(self, *args, **kwargs)</div><div class="line">        self.criterion = <span class="string">"ent"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">C45Node</span><span class="params">(CvDNode)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></div><div class="line">        CvDNode.__init__(self, *args, **kwargs)</div><div class="line">        self.criterion = <span class="string">"ratio"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CartNode</span><span class="params">(CvDNode)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></div><div class="line">        CvDNode.__init__(self, *args, **kwargs)</div><div class="line">        self.criterion = <span class="string">"gini"</span></div><div class="line">        self.is_cart = <span class="keyword">True</span></div></pre></td></tr></table></figure>
<p>在<code>CvDBase</code>的基础上定义三种算法对应的 Tree 结构的方法是类似的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ID3Tree</span><span class="params">(CvDBase, ID3Node, metaclass=CvDMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">C45Tree</span><span class="params">(CvDBase, C45Node, metaclass=CvDMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CartTree</span><span class="params">(CvDBase, CartNode, metaclass=CvDMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>其中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CvDMeta</span><span class="params">(type)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(mcs, *args, **kwargs)</span>:</span></div><div class="line">        name, bases, attr = args[:<span class="number">3</span>]</div><div class="line">        _, _node = bases</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, whether_continuous=None, max_depth=None, node=None, **_kwargs)</span>:</span></div><div class="line">            tmp_node = node <span class="keyword">if</span> isinstance(node, CvDNode) <span class="keyword">else</span> _node</div><div class="line">            CvDBase.__init__(self, whether_continuous, max_depth, tmp_node(**_kwargs))</div><div class="line">            self._name = name</div><div class="line"></div><div class="line">        attr[<span class="string">"__init__"</span>] = __init__</div><div class="line">        <span class="keyword">return</span> type(name, bases, attr)</div></pre></td></tr></table></figure>
<p>接下来就是具体的评估与相应的可视化</p>
<a id="more"></a>
<p>我们同样可以使用蘑菇数据集来评估决策树模型的表现，结果如下所示：</p>
<img src="/posts/c12a819/p1.png" alt="蘑菇数据集上 ID3 算法的表现" title="蘑菇数据集上 ID3 算法的表现">
<img src="/posts/c12a819/p2.png" alt="蘑菇数据集上 C4.5 算法的表现" title="蘑菇数据集上 C4.5 算法的表现">
<img src="/posts/c12a819/p3.png" alt="蘑菇数据集上 CART 算法的表现" title="蘑菇数据集上 CART 算法的表现">
<p>可以看到 CART 算法的表现相对来说要差不少，可能的原因有如下三条：</p>
<ul>
<li>CART 算法在选择划分标准时是从所有二分标准里面进行选择的，这里就会比 ID3 和 C4.5 算法多出不少倍的运算量</li>
<li>由于我们在实现 CART 剪枝算法时为了追求简洁、直接调用了标准库 copy 中的 deepcopy 方法对整颗决策树进行了深拷贝。这一步可能会连不必要的东西也进行了拷贝、从而导致了一些不必要的开销</li>
<li>CART 算法生成的是二叉决策树，所以可能生成出来的树会更深、各叶节点中的样本数可能也会分布得比较均匀、从而无论是建模过程还是预测过程都会要慢一些</li>
</ul>
<p>当然，如果结合蘑菇数据集来说的话、笔者认为最大的问题在于：CART 算法不适合应用于蘑菇数据集。一方面是因为蘑菇数据集全是离散型特征且各特征取值都挺多，另一方面是因为蘑菇数据集相对简单、有一些特征非常具有代表性（我们在说明朴素贝叶斯时也有所提及），仅仅用二分标准划分数据的话、会显得比较没有效率</p>
<p>为了更客观地评估我们模型的表现，我们可以对成熟第三方库 sklearn 中的决策树模型进行恰当的封装并看看它在蘑菇数据集上的表现：</p>
<img src="/posts/c12a819/p4.png" alt="蘑菇数据集上 sklearn 决策树的表现（criterion=entropy）" title="蘑菇数据集上 sklearn 决策树的表现（criterion=entropy）">
<img src="/posts/c12a819/p5.png" alt="蘑菇数据集上 sklearn 决策树的表现（criterion=gini）" title="蘑菇数据集上 sklearn 决策树的表现（criterion=gini）">
<p>不得不承认、成熟第三方库的效率确实要高很多（比我们的要快 5 倍左右）；这是因为虽然算法思想可能大致相同，但 sklearn 的核心实现都经过了高度优化、且（如不出意料的话）应该都是用 C 或者其它底层语言直接写的。不过正如第一章说过的，要想应用 sklearn 中的决策树、就必须先将数据数值化（即使是离散型数据）；而我们实现的决策树在处理离散型数据时却无需这一步数据预处理、可以直接应用在原始数据上（但处理混合型数据时还是要先进行数值化处理、而且将离散型数据数值化也能显著提升模型的运行速度）</p>
<p>我们在本系列的综述里面曾说过、决策树可能是从直观上最好理解的模型；事实上，我们之前画过的一些决策树示意图也确实非常直观易懂、于是我们可能自然就会希望程序能将生成类似的东西。虽然不能做到那么漂亮、不过我们确实是能在之前实现的决策树模型的基础上做出类似效果的：</p>
<img src="/posts/c12a819/p6.png" alt="蘑菇数据集上 ID3 决策树的可视化" title="蘑菇数据集上 ID3 决策树的可视化">
<img src="/posts/c12a819/p7.png" alt="蘑菇数据集上 C4.5 决策树的可视化" title="蘑菇数据集上 C4.5 决策树的可视化">
<img src="/posts/c12a819/p8.png" alt="蘑菇数据集上 CART 决策树的可视化" title="蘑菇数据集上 CART 决策树的可视化">
<p>其中，红色数字代表该 Node 作为划分标准的特征所属的维度，位于各条连线中央的字母代表着该维度特征的各个取值、加号“+”代表着“其它”，绿色字母代表类别标记。以上三张图在一定程度上验证了我们之前的很多说法，比如说 ID3 会倾向选择取值比较多的特征、C4.5 可能会倾向选择取值比较少的特征且倾向于在每个二叉分枝处留下一个小 Node 作为叶节点、CART 各个叶节点上的样本分布较均匀且生成出的决策树会比较深……等等</p>
<p>我们在说明朴素贝叶斯时曾经提过，即使只根据第 5 维的取值来进行类别的判定、最后的准确率也一定会非常高。验证这一命题的方法很简单——只需将决策树的最大深度设为 1 即可，结果如下图所示：</p>
<img src="/posts/c12a819/p9.png" alt="p9.png" title="">
<p>此时模型的表现如下图所示：</p>
<img src="/posts/c12a819/p10.png" alt="p10.png" title="">
<p>可以看到其表现确实不错。值得一提的是，单层决策树又可称为“决策树桩（Decision Stump）”、它是有特殊应用场景的（比如我们在下个系列中讲 AdaBoost 时就会用到它）</p>
<p>至今为止我们用到的数据集都是离散型数据集，为了更全面地进行评估、使用连续型混合型数据集进行评估是有必要的；同时为了增强直观、我们可以用异或数据集来进行评估。原始数据集如下图所示：</p>
<img src="/posts/c12a819/p11.png" alt="p11.png" title="">
<p>生成异或数据集（及其它二维数据集）的代码定义在之前提过 DataUtil 类中（可参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py" target="_blank" rel="external">这里</a>），读者也可以在下一章中找到相应的讲解。为使评估更具有直观性、我们可以把四种决策树（ID3、C4.5、CART 决策树和 sklearn 的决策树）在异或数据集上的表现直接画出来：</p>
<img src="/posts/c12a819/p12.png" alt="异或数据集上 ID3、CART 和 sklearn 决策树的表现" title="异或数据集上 ID3、CART 和 sklearn 决策树的表现">
<img src="/posts/c12a819/p13.png" alt="异或数据集上 C4.5 决策树的表现" title="异或数据集上 C4.5 决策树的表现">
<p>可以看到 C4.5 决策树的过拟合现象比较严重。正如我们之前所分析的一般、这很有可能是因为 C4.5 在二叉分枝时会倾向于进行“不均匀的二分”（从上图也可以大概看出）</p>
]]></content>
      
        <categories>
            
            <category> 决策树 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[剪枝算法的实现]]></title>
      <url>/posts/602f7125/</url>
      <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py" target="_blank" rel="external">这里</a>和<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Tree.py" target="_blank" rel="external">这里</a>）</p>
<p>决策树剪枝算法的实现相对而言比较平凡，只需要把算法依次翻译成程序语言即可</p>
<a id="more"></a>
<h1 id="ID3、C4-5-剪枝算法的实现"><a href="#ID3、C4-5-剪枝算法的实现" class="headerlink" title="ID3、C4.5 剪枝算法的实现"></a>ID3、C4.5 剪枝算法的实现</h1><p>回忆算法本身，可以知道我们需要获取“从下往上”这个顺序，为此我们需要先在<code>CvDNode</code>中利用递归定义一个函数来更新 Tree 的<code>self.layers</code>属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_layers</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 根据该Node的深度、在self.layers对应位置的列表中记录自己</span></div><div class="line">    self.tree.layers[self._depth].append(self)</div><div class="line">    <span class="comment"># 遍历所有子节点、完成递归</span></div><div class="line">    <span class="keyword">for</span> _node <span class="keyword">in</span> sorted(self.children):</div><div class="line">        _node = self.children[_node]</div><div class="line">        <span class="keyword">if</span> _node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            _node.update_layers()</div></pre></td></tr></table></figure>
<p>然后、在<code>CvDBase</code>中定义一个对应的函数进行封装：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update_layers</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 根据整颗决策树的高度、在self.layers里面放相应数量的列表</span></div><div class="line">    self.layers = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.root.height)]</div><div class="line">    self.root.update_layers()</div></pre></td></tr></table></figure>
<p>同时，为了做到合理的代码重用、我们可以先在<code>CvDNode</code>中定义一个计算损失的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(self, pruned=False)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pruned:</div><div class="line">        <span class="keyword">return</span> sum([leaf[<span class="string">"chaos"</span>] * len(leaf[<span class="string">"y"</span>]) <span class="keyword">for</span> leaf <span class="keyword">in</span> self.leafs.values()])</div><div class="line">    <span class="keyword">return</span> self.chaos * len(self._y)</div></pre></td></tr></table></figure>
<p>有了以上两个函数，算法本身的实现就很直观了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_prune</span><span class="params">(self)</span>:</span></div><div class="line">    self._update_layers()</div><div class="line">    _tmp_nodes = []</div><div class="line">    <span class="comment"># 更新完决策树每一“层”的Node之后，从后往前地向 _tmp_nodes中加Node</span></div><div class="line">    <span class="keyword">for</span> _node_lst <span class="keyword">in</span> self.layers[::<span class="number">-1</span>]:</div><div class="line">        <span class="keyword">for</span> _node <span class="keyword">in</span> _node_lst[::<span class="number">-1</span>]:</div><div class="line">            <span class="keyword">if</span> _node.category <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">                _tmp_nodes.append(_node)</div><div class="line">    _old = np.array([node.cost() + self.prune_alpha * len(node.leafs)</div><div class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> _tmp_nodes])</div><div class="line">    _new = np.array([node.cost(pruned=<span class="keyword">True</span>) + self.prune_alpha</div><div class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> _tmp_nodes])</div><div class="line">    <span class="comment"># 使用 _mask变量存储 _old和 _new对应位置的大小关系</span></div><div class="line">    _mask = _old &gt;= _new</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># 若只剩根节点就退出循环体</span></div><div class="line">        <span class="keyword">if</span> self.root.height == <span class="number">1</span>:</div><div class="line">            <span class="keyword">return</span></div><div class="line">        p = np.argmax(_mask)</div><div class="line">        <span class="comment"># 如果 _new中有比 _old中对应损失小的损失、则进行局部剪枝</span></div><div class="line">        <span class="keyword">if</span> _mask[p]:</div><div class="line">            _tmp_nodes[p].prune()</div><div class="line">            <span class="comment"># 根据被影响了的Node、更新 _old、_mask对应位置的值</span></div><div class="line">            <span class="keyword">for</span> i, node <span class="keyword">in</span> enumerate(_tmp_nodes):</div><div class="line">                <span class="keyword">if</span> node.affected:</div><div class="line">                    _old[i] = node.cost() + self.prune_alpha * len(node.leafs)</div><div class="line">                    _mask[i] = _old[i] &gt;= _new[i]</div><div class="line">                    node.affected = <span class="keyword">False</span></div><div class="line">            <span class="comment"># 根据被剪掉的Node、将各个变量对应的位置除去（注意从后往前遍历）</span></div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(_tmp_nodes) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</div><div class="line">                <span class="keyword">if</span> _tmp_nodes[i].pruned:</div><div class="line">                    _tmp_nodes.pop(i)</div><div class="line">                    _old = np.delete(_old, i)</div><div class="line">                    _new = np.delete(_new, i)</div><div class="line">                    _mask = np.delete(_mask, i)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">    self.reduce_nodes()</div></pre></td></tr></table></figure>
<p>上述代码的第 25 行和第 28 行出现了 Node 的<code>affected</code>属性，这是我们之前没有进行定义的（因为若在彼时定义会显得很突兀）；不过由剪枝算法可知，这个属性的用处与其名字一致——标记一个 Node 是否是“被影响到的”Node。事实上，在一个 Node 进行了局部剪枝后，会有两类 Node “被影响到”：</p>
<ul>
<li>该 Node 的子节点、子节点的子节点……等等，它们属于被剪掉的 Node、应该要将它们在<code>_old</code>、<code>_tmp_nodes</code>中对应的位置从这些列表中除去</li>
<li>该 Node 的父节点、父节点的父节点……等等，它们存储叶节点的列表会因局部剪枝而发生改变、所以要更新<code>_old</code>和<code>_mask</code>列表中对应位置的值</li>
</ul>
<p>其中，我们之前定义的 Node 中是用<code>pruned</code>属性来标记该 Node 是否已被剪掉、且介绍了如何通过递归来更新<code>pruned</code>属性；<code>affected</code>属性和<code>pruned</code>属性的本质几乎没什么区别，所以我们同样可以通过递归来更新<code>affected</code>属性。具体而言，我们只需：</p>
<ul>
<li>在初始化时令<code>self.affected = False</code></li>
<li>在局部剪枝函数内部插入<code>_parent.affected = True</code></li>
</ul>
<p>即可，其余部分可以保持不变。</p>
<h1 id="CART-剪枝算法的实现"><a href="#CART-剪枝算法的实现" class="headerlink" title="CART 剪枝算法的实现"></a>CART 剪枝算法的实现</h1><p>同样的，为了做到合理的代码重用、我们先利用之前实现的<code>cost</code>函数、在<code>CvDNode</code>里面定义一个获取 Node 阈值的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_threshold</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> (self.cost(pruned=<span class="keyword">True</span>) - self.cost()) / (len(self.leafs) - <span class="number">1</span>)</div></pre></td></tr></table></figure>
<p>由于算法本身的实现的思想以及用到的工具都和第一种剪枝算法大同小异、所以代码写起来也差不多：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cart_prune</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 暂时将所有节点记录所属Tree的属性置为None</span></div><div class="line">    <span class="comment"># 这样做的必要性会在后文进行说明</span></div><div class="line">    self.root.cut_tree()</div><div class="line">    _tmp_nodes = [node <span class="keyword">for</span> node <span class="keyword">in</span> self.nodes <span class="keyword">if</span> node.category <span class="keyword">is</span> <span class="keyword">None</span>]</div><div class="line">    _thresholds = np.array([node.get_threshold() <span class="keyword">for</span> node <span class="keyword">in</span> _tmp_nodes])</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># 利用deepcopy对当前根节点进行深拷贝、存入self.roots列表</span></div><div class="line">        <span class="comment"># 如果前面没有把记录Tree的属性置为None，那么这里就也会对整个Tree做</span></div><div class="line">        <span class="comment"># 深拷贝。可以想象、这样会引发严重的内存问题，速度也会被拖慢非常多</span></div><div class="line">        root_copy = deepcopy(self.root)</div><div class="line">        self.roots.append(root_copy)</div><div class="line">        <span class="keyword">if</span> self.root.height == <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        p = np.argmin(_thresholds)</div><div class="line">        _tmp_nodes[p].prune()</div><div class="line">        <span class="keyword">for</span> i, node <span class="keyword">in</span> enumerate(_tmp_nodes):</div><div class="line">            <span class="comment"># 更新被影响到的Node的阈值</span></div><div class="line">            <span class="keyword">if</span> node.affected:</div><div class="line">                _thresholds[i] = node.get_threshold()</div><div class="line">                node.affected = <span class="keyword">False</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(_tmp_nodes) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</div><div class="line">            <span class="comment"># 去除掉各列表相应位置的元素</span></div><div class="line">            <span class="keyword">if</span> _tmp_nodes[i].pruned:</div><div class="line">                _tmp_nodes.pop(i)</div><div class="line">                _thresholds = np.delete(_thresholds, i)</div><div class="line">    self.reduce_nodes()</div></pre></td></tr></table></figure>
<p>代码第 4 行对根节点调用的<code>cut_tree</code>方法同样是利用递归实现的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_tree</span><span class="params">(self)</span>:</span></div><div class="line">    self.tree = <span class="keyword">None</span></div><div class="line">    <span class="keyword">for</span> child <span class="keyword">in</span> self.children.values():</div><div class="line">        <span class="keyword">if</span> child <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            child.cut_tree()</div></pre></td></tr></table></figure>
<p>然后就是最后一步、通过交叉验证选出最优树了。注意到之前我们封装生成算法时、最后一行调用了剪枝算法的封装——<code>self.prune</code>方法。由于该方法是第一个接收了交叉验证集<code>x_cv</code>和<code>y_cv</code>的方法、所以我们应该让该方法来做交叉验证。简洁起见，我们直接选用加权正确率作为交叉验证的标准：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算加权正确率的函数</span></div><div class="line"><span class="meta">@staticmethod</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">acc</span><span class="params">(y, y_pred, weights)</span>:</span></div><div class="line">    <span class="keyword">if</span> weights <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> np.sum((np.array(y) == np.array(y_pred)) * weights) / len(y)</div><div class="line">    <span class="keyword">return</span> np.sum(np.array(y) == np.array(y_pred)) / len(y)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prune</span><span class="params">(self, x_cv, y_cv, weights)</span>:</span></div><div class="line">    <span class="keyword">if</span> self.root.is_cart:</div><div class="line">        <span class="comment"># 如果该Node使用CART剪枝，那么只有在确实传入了交叉验证集的情况下</span></div><div class="line">        <span class="comment"># 才能调用相关函数、否则没有意义</span></div><div class="line">        <span class="keyword">if</span> x_cv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> y_cv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            self._cart_prune()</div><div class="line">            _arg = np.argmax([CvDBase.acc(</div><div class="line">                y_cv, tree.predict(x_cv), weights) <span class="keyword">for</span> tree <span class="keyword">in</span> self.roots])</div><div class="line">            _tar_root = self.roots[_arg]</div><div class="line">            <span class="comment"># 由于Node的feed_tree方法会递归地更新nodes属性、所以要先重置</span></div><div class="line">            self.nodes = []</div><div class="line">            _tar_root.feed_tree(self)</div><div class="line">            self.root = _tar_root</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        self._prune()</div></pre></td></tr></table></figure>]]></content>
      
        <categories>
            
            <category> 决策树 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[决策树的剪枝算法]]></title>
      <url>/posts/1a7aa546/</url>
      <content type="html"><![CDATA[<p>在知道怎么得到一颗决策树后、我们当然就会想知道：这样建立起来的决策树的表现究竟如何？从直观上来说，只要决策树足够深、划分标准足够细，它在训练集上的表现就能接近完美；但同时也容易想象，由于它可能把训练集的一些“特性”当做所有数据的“共性”来看待，它在未知的测试数据上的表现可能就会比较一般、亦即会出现过拟合的问题。我们知道，模型出现过拟合问题一般是因为模型太过复杂，所以决策树解决过拟合的方法是采取适当的“剪枝”、我们在上一篇文章中也已经大量接触了这一概念。剪枝通常分为两类：“预剪枝（Pre-Pruning）”和“后剪枝（Post-Pruning）”，其中“预剪枝”的概念在生成算法中已有定义，彼时我们采取的说法是“停止条件”；而一般提起剪枝时指的都是“后剪枝”，它是指在决策树生成完毕后再对其进行修剪、把多余的节点剪掉。换句话说，后剪枝是从全局出发、通过某种标准对一些 Node 进行局部剪枝，这样就能减少决策树中 Node 的数目、从而有效地降低模型复杂度</p>
<a id="more"></a>
<p>是故问题的关键在于如何定出局部剪枝的标准。通常来说我们有两种做法：</p>
<ul>
<li>应用交叉验证的思想，若局部剪枝能够使得模型在测试集上的错误率降低、则进行局部剪枝（预剪枝中也应用了类似的思想）</li>
<li>应用正则化的思想、综合考虑不确定性和模型复杂度来定出一个新的损失（此前我们的损失只考虑了不确定性），用该损失作为一个 Node 是否进行局部剪枝的标准</li>
</ul>
<p>第二种做法又涉及到另一个关键问题：如何定量分析决策树中一个 Node 的复杂度？一个直观且合理的方法是：直接使用该 Node 下属叶节点的个数作为复杂度。基于此、第二种做法的数学描述就是：</p>
<ul>
<li>定义新损失（<script type="math/tex">T</script>代表一个 Node）  <script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = C\left( T \right) + \alpha\left| T \right|</script>其中，<script type="math/tex">C\left( T \right)</script>即是该 Node 和不确定性相关的损失、<script type="math/tex">|T|</script>则是该 Node 下属叶节点的个数。不妨设第 t 个叶节点含有<script type="math/tex">N_{t}</script>个样本且这<script type="math/tex">N_{t}</script>个样本的不确定性为<script type="math/tex">H_{t}(T)</script>，那么新损失一般可以直接定义为加权不确定性：  <script type="math/tex; mode=display">
C\left( T \right) = \sum_{t = 1}^{\left| T \right|}{N_{t}H_{t}(T)}</script></li>
</ul>
<p>我们会采取这种做法来进行实现。需要指出的是，在这种做法下、仍然可以分支出两种不同的算法：</p>
<ul>
<li>直接比较一个 Node 局部剪枝前的损失<script type="math/tex">C_{\alpha}(T)</script>和局部剪枝后的损失<script type="math/tex">C_{\alpha}(t)</script>的大小，若：  <script type="math/tex; mode=display">
C_{\alpha}\left( T \right) \geq C_{\alpha}(t)</script></li>
<li>获取一系列的剪枝阈值：<script type="math/tex">0 = \alpha_{0} < \alpha_{1} < \ldots < \alpha_{p} < + \infty</script>，在每个剪枝阈值<script type="math/tex">\alpha_{i}</script>上对相应的 Node 进行局部剪枝并将局部剪枝后得到的决策树<script type="math/tex">T_{i}</script>储存在一个列表中。在<script type="math/tex">\alpha_{p}</script>上我们会对根节点进行局部剪枝，此时剩下来的决策树<script type="math/tex">T_{p}</script>就只包含根节点这一个 Node。最后，通过交叉验证选出<script type="math/tex">T_{0},\ldots,T_{p}</script>中最好的决策树作为最终生成的决策树（注意其中的<script type="math/tex">T_{0}</script>即是没有剪过枝的原始树）</li>
</ul>
<p>第一种算法清晰易懂，第二种算法则稍显复杂；一般我们会在 ID3 和 C4.5 中应用第一种剪枝算法、在 CART 中应用第二种剪枝算法。上述这个第二种算法的说明可能有些过于简略、让人摸不着头脑；由于详细的算法叙述会在后文再次进行，所以这里只要有一个大概的直观感受即可，细节可以暂时按下、不必太过纠结</p>
<h1 id="ID3、C4-5-的剪枝算法"><a href="#ID3、C4-5-的剪枝算法" class="headerlink" title="ID3、C4.5 的剪枝算法"></a>ID3、C4.5 的剪枝算法</h1><p>首先我们来看看第一种算法的详细叙述。虽说算法本身的思想很简单，但由于其中涉及到许多中间变量、所以我们采取类似于伪代码的形式来进行叙述：</p>
<ol>
<li><strong>输入</strong>：生成算法产生的原始决策树<script type="math/tex">T</script>，惩罚因子<script type="math/tex">\alpha</script></li>
<li><strong>过程</strong>：<ol>
<li>从下往上地获取<script type="math/tex">T</script>中所有 Node，存入列表<code>_tmp_nodes</code></li>
<li>对<code>_tmp_nodes</code>中的所有 Node 计算损失，存入列表<code>_old</code></li>
<li>计算<code>_tmp_nodes</code>中所有 Node 进行局部剪枝后的损失，存入列表<code>_new</code></li>
<li>进入循环体：<ol>
<li>若<code>_new</code>中所有损失都大于<code>_old</code>中对应的损失、则退出循环体</li>
<li>否则，设 p 满足：  <script type="math/tex; mode=display">
p = \arg{\min_{p}{\text{_new}\lbrack p\rbrack \leq \text{_old}\lbrack p\rbrack}}</script>则对<code>_tmp_nodes[p]</code>进行局部剪枝</li>
<li>在完成局部剪枝后，更新<code>_old</code>、<code>_new</code>、<code>_tmp_nodes</code>等变量。具体而言，我们无需重新计算它们、只需更新“被影响到的” Node 所对应的位置的值即可</li>
</ol>
</li>
<li>最后调用<code>self.reduce_nodes</code>方法、将被剪掉的 Node 从<code>nodes</code>中除去</li>
</ol>
</li>
<li><strong>输出</strong>：修剪过后的决策树<script type="math/tex">T_{\alpha}</script></li>
</ol>
<p>我们可以在我们之前用气球数据集 1.0 根据 ID3 算法生成的决策树上过一遍剪枝算法以加深理解。由于算法顺序是从下往上、所以我们先考察最右下方的 Node（该 Node 的划分标准是“测试人员”），该 Node 所包含的数据集如下表所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>测试人员</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>不爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>从而：</p>
<ul>
<li>局部剪枝前、该 Node 的损失为：  <script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = C\left( T \right) + \alpha\left| T \right| = 0 + 2\alpha = 2\alpha</script></li>
<li>局部剪枝后、该 Node 的损失为：  <script type="math/tex; mode=display">
C_{\alpha}\left( t \right) = C\left( t \right) + \alpha\left| t \right| = C\left( t \right) + \alpha</script>其中  <script type="math/tex; mode=display">
C\left( t \right) = N_{t}H_{t} = 2 \times \left( - \frac{1}{2}\log\frac{1}{2} - \frac{1}{2}\log\frac{1}{2} \right) = 2</script>故  <script type="math/tex; mode=display">
C_{\alpha}\left( t \right) = 2 + \alpha</script></li>
</ul>
<p>回忆生成算法的实现，我们彼时将<script type="math/tex">\alpha</script>定义为了<script type="math/tex">\alpha = \frac{特征个数}{2}</script>（注意：这只是<script type="math/tex">\alpha</script>的一种朴素的定义方法，很难说它有什么合理性、只能说它从直观上有一定道理；如果想让模型表现更好、需要结合具体的问题来分析<script type="math/tex">\alpha</script>应该取何值）。由于气球数据集 1.0 一共有四个特征、所以此时<script type="math/tex">\alpha = 2</script>；结合各个公式、我们发现：</p>
<script type="math/tex; mode=display">
C_{\alpha}\left( t \right) = 2\alpha = 4 = 2 + \alpha = C_{\alpha}(t)</script><p>所以我们应该对该 Node 进行局部剪枝。局部剪枝后的决策树如下图所示：</p>
<img src="/posts/1a7aa546/p1.png" alt="p1.png" title="">
<p><strong><em>注意：进行局部剪枝后，由于该 Node 中样本只有两个、且一个样本类别为“不爆炸”一个为“爆炸”，所以给该 Node 标注为“不爆炸”、“爆炸”甚至以 50%的概率标注为“不爆炸”等做法都是合理的。为简洁，我们如上图中所做的一般、将其标注为“爆炸”</em></strong></p>
<p>然后我们需要考察最左下方的 Node（该 Node 的划分标准也是“测试人员”），易知计算过程和上述的没有区别。对其进行局部剪枝后的决策树如下图所示：</p>
<img src="/posts/1a7aa546/p2.png" alt="p2.png" title="">
<p>然后我们需要考察右下方的 Node（该 Node 的划分标准是“动作”），该 Node 所包含的数据集如下表所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用手打</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>从而：</p>
<ul>
<li><p>局部剪枝前、该 Node 的损失为：  </p>
<script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = C\left( T \right) + \alpha\left| T \right| = C\left( T \right) + 2\alpha</script><p>其中  </p>
<script type="math/tex; mode=display">
\begin{align}
C\left( T \right) = N_{}H_{} + N_{}H_{} \\

= 2 \times \left( - \frac{1}{2}\log\frac{1}{2} - \frac{1}{2}\log\frac{1}{2} \right) + 4 \times 0 = 2
\end{align}</script><p>故  </p>
<script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = 2 + 2\alpha</script></li>
<li>局部剪枝后、该 Node 的损失为：  <script type="math/tex; mode=display">
C_{\alpha}\left( t \right) = C\left( t \right) + \alpha\left| t \right| = C\left( t \right) + \alpha</script>其中  <script type="math/tex; mode=display">
C\left( t \right) = N_{t}H_{t} = 6 \times \left( - \frac{1}{6}\log\frac{1}{6} - \frac{5}{6}\log\frac{5}{6} \right) \approx 3.9</script>故  <script type="math/tex; mode=display">
C_{\alpha}\left( t \right) \approx 3.9 + \alpha</script></li>
</ul>
<p>将<script type="math/tex">\alpha = 2</script>代入、知：</p>
<script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = 2 + 2\alpha = 6 > 5.9 = 3.9 + \alpha \approx C_{\alpha}(t)</script><p>故应该对该 Node 进行局部剪枝。局部剪枝后的决策树如下图所示：</p>
<img src="/posts/1a7aa546/p3.png" alt="p3.png" title="">
<p>然后我们需要考察左下方的 Node（该 Node 的划分标准也是“动作”），易知计算过程和上述的没有区别。对其进行局部剪枝后的决策树如下图所示：</p>
<img src="/posts/1a7aa546/p4.png" alt="p4.png" title="">
<p>通过计算易知不应对根节点进行局部剪枝、所以上图所示的决策树即是当<script type="math/tex">\alpha = 2</script>时最终修剪出来的决策树</p>
<h1 id="CART-剪枝"><a href="#CART-剪枝" class="headerlink" title="CART 剪枝"></a>CART 剪枝</h1><p>第二种剪枝算法（CART 剪枝）中的许多定义可能还不是很清晰，所以我们先对相关概念进行详细一点的直观说明：</p>
<p>首先需要指出的是：关于第二种算法中出现的一系列的阈值，它们的含义其实和第一种算法中的<script type="math/tex">\alpha</script>一样、都是模型复杂度的“惩罚因子”；不同的是，第一种算法的<script type="math/tex">\alpha</script>是人为给定的、第二种算法中一系列的阈值则是算法生成出来的。其中，<script type="math/tex">\alpha_{0} = 0</script>意味着算法初始不对模型复杂度进行惩罚、此时最优树即是原始树<script type="math/tex">T_{0}</script>。然后我们设想<script type="math/tex">\alpha</script>缓慢增大、亦即缓慢增大对模型复杂度的惩罚，那么到某个阈值<script type="math/tex">\alpha_{1}</script>时，对决策树中某个 Node 进行局部剪枝就是一个更好的选择。我们将该 Node 进行局部剪枝后的决策树<script type="math/tex">T_{1}</script>存进一个列表中、然后继续缓慢增加惩罚因子<script type="math/tex">\alpha</script>，继而到某个阈值<script type="math/tex">\alpha_{2}</script>后、对某个 Node 进行局部剪枝就又会是一个更好的选择……以此类推，直到<script type="math/tex">\alpha</script>变成一个充分大的数<script type="math/tex">\alpha_{p}</script>后、只保留根节点这一个 Node 会是最好的选择，此时就终止算法并通过交叉验证从<script type="math/tex">T_{0},\ldots,T_{p}</script>中选出最好的<script type="math/tex">T_{p^{*}}</script>作为修剪后的决策树。</p>
<p>那么这个相对比较复杂的算法有什么优异之处呢？可以证明：在 CART 剪枝里得到的决策树<script type="math/tex">T_{0},\ldots,T_{p}</script>中，对<script type="math/tex">\forall i = 0,\ldots,p</script>、<script type="math/tex">T_{i}</script>都是当惩罚因子<script type="math/tex">\alpha \in \lbrack\alpha_{i},\alpha_{i + 1})</script>时的最优决策树。这条性质保证了 CART 算法最终通过交叉验证选出来的决策树<script type="math/tex">T_{p^{*}}</script>具有一定的优良性。</p>
<p>该算法的详细叙述则如下：</p>
<ol>
<li><strong>输入</strong>：在训练集上调用生成算法所产生的原始决策树<script type="math/tex">T</script>，交叉验证集</li>
<li>过程：<ol>
<li>从下往上地获取<script type="math/tex">T</script>中所有 Node，存入列表<code>_tmp_nodes</code></li>
<li>对<code>_tmp_nodes</code>中的所有 Node 计算阈值，存入列表<code>_thresholds</code>；其中，第 t 个 Node 的阈值<script type="math/tex">\alpha_{t}</script>应满足：  <script type="math/tex; mode=display">
C\left( T_{t} \right) + \alpha_{t}\left| T_{t} \right| = C_{\alpha_{t}}\left( T_{t} \right) = C_{\alpha_{t}}\left( t \right) = C\left( t \right) + \alpha_{t}</script>其中<script type="math/tex">C(t)</script>即是第 t 个 Node 自身数据的不确定性；换言之，<script type="math/tex">C_{\alpha_{t}}(T_{t})</script>代表着第 t 个 Node 进行局部剪枝前的新损失、<script type="math/tex">C_{\alpha_{t}}(t)</script>代表着局部剪枝后的新损失。由上式可求出：  <script type="math/tex; mode=display">
\alpha_{t} = \frac{C\left( t \right) - C\left( T_{t} \right)}{\left| T_{t} \right| - 1}</script>此即阈值的计算公式</li>
<li>进入循环体：<ol>
<li>将当前决策树存入列表<code>self.roots</code></li>
<li>若当前决策树中只剩根节点、则退出循环体</li>
<li>否则，取 p 满足：  <script type="math/tex; mode=display">
p = \arg{\min_{p}{\_\text{thresholds}}}</script>然后对<code>_tmp_nodes[p]</code>进行局部剪枝</li>
<li>在完成局部剪枝后，更新<code>_thresholds</code>、<code>_tmp_nodes</code>等变量。具体而言，我们无需重新计算它们、只需更新“被影响到的” Node 所对应的位置的值即可</li>
</ol>
</li>
<li>然后调用<code>self.reduce_nodes</code>方法、将被剪掉的 Node 从<code>nodes</code>中除去</li>
<li>最后利用交叉验证、从<code>self.roots</code>中选出表现最好的决策树<script type="math/tex">T_{p^{*}}</script></li>
</ol>
</li>
<li><strong>输出</strong>：修剪过后的决策树<script type="math/tex">T_{p^{*}}</script></li>
</ol>
]]></content>
      
        <categories>
            
            <category> 决策树 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[树结构的实现]]></title>
      <url>/posts/b07c81ec/</url>
      <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Tree.py" target="_blank" rel="external">这里</a>）</p>
<p>由前文的诸多讨论可知，Tree 结构需要做到如下几点：</p>
<ul>
<li>定义好需要在各个 Node 上调用的“全局变量”</li>
<li>做好数据预处理的工作、保证传给 Node 的数据是合乎要求的</li>
<li>对各个 Node 进行合适的封装，做到：<ul>
<li>生成决策树时能够正确地调用它们的生成算法</li>
<li>进行后剪枝时能够正确地调用它们的局部剪枝函数</li>
</ul>
</li>
<li>定义预测函数和评估函数以供用户调用</li>
</ul>
<p>既然 Node 我们可以抽象出一个基类<code>CvDNode</code>，我们自然也能相应地对 Tree 结构抽象出一个基类<code>CvDBase</code></p>
<a id="more"></a>
<p>下面先来看看如何搭建该基类的基本框架：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</div><div class="line"><span class="comment"># 导入Node结构以进行封装</span></div><div class="line"><span class="keyword">from</span> c_CvDTree.Node <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="comment"># 定义一个足够抽象的Tree结构的基类以适应我们Node结构的基类</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CvDBase</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self.nodes：记录所有Node的列表</div><div class="line">        self.roots：主要用于CART剪枝的属性，可先按下不表</div><div class="line">（用于存储算法过程中产生的各个决策树）</div><div class="line">            self.max_depth：记录决策树最大深度的属性</div><div class="line">        self.root, self.feature_sets：根节点和记录可选特征维度的列表</div><div class="line">            self.label_dic：和朴素贝叶斯里面相应的属性意义一致、是类别的转换字典</div><div class="line">        self.prune_alpha, self.layers：主要用于ID3和C4.5剪枝的两个属性，可先按下不表</div><div class="line">（self.prune_alpha是“惩罚因子”，self.layers则记录着每一“层”的Node）</div><div class="line">        self.whether_continuous：记录着各个维度的特征是否连续的列表</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, max_depth=None, node=None)</span>:</span></div><div class="line">        self.nodes, self.layers, self.roots = [], [], []</div><div class="line">        self.max_depth = max_depth</div><div class="line">        self.root = node</div><div class="line">        self.feature_sets = []</div><div class="line">        self.label_dic = &#123;&#125;</div><div class="line">        self.prune_alpha = <span class="number">1</span></div><div class="line">        self.whether_continuous = <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> <span class="string">"CvDTree (&#123;&#125;)"</span>.format(self.root.height)</div><div class="line"></div><div class="line">    __repr__ = __str__</div></pre></td></tr></table></figure>
<p>回忆朴素贝叶斯的实现，可以知道在第二章的混合型朴素贝叶斯中、我们要求用户告诉程序哪些维度的特征是连续的；这里我们介绍一种非常简易却有一定合理性的做法、从而可以让程序在进行数据预处理时自动识别出连续特征对应的维度：</p>
<ul>
<li>将训练集中每个维度特征的所有可能的取值算出来，这一步可以用 Python 内置的数据结构<code>set</code>来完成</li>
<li>如果第 i 维可能的取值个数<script type="math/tex">S_i</script>比上训练集总样本数 N 大于某个阈值，亦即若：  <script type="math/tex; mode=display">
S_i\ge\beta N</script>那么就认为第 i 维的特征是连续型随机变量<br><script type="math/tex">\beta</script>的具体取值需要视情况而定。一般来说在样本数 N 足够大时、可以取得比较小（比如取就是个不错的选择）；但是样本数 N 比较小时，我们可能需要将取得大一些（比如取）。具体应该取什么值还是要看具体的任务和数据，毕竟这种自动识别的方法还是过于朴素了</li>
</ul>
<p>以上所叙述的数据预处理的实现如下（注：我们在朴素贝叶斯的实现里用到过的<code>quantize_data</code>方法中整合了如下代码中的核心部分）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_data</span><span class="params">(self, x, continuous_rate=<span class="number">0.2</span>)</span>:</span></div><div class="line">    <span class="comment"># 利用set获取各个维度特征的所有可能取值</span></div><div class="line">    self.feature_sets = [set(dimension) <span class="keyword">for</span> dimension <span class="keyword">in</span> x.T]</div><div class="line">    data_len, data_dim = x.shape</div><div class="line">    <span class="comment"># 判断是否连续</span></div><div class="line">    self.whether_continuous = np.array(</div><div class="line">        [len(feat) &gt;= continuous_rate * data_len <span class="keyword">for</span> feat <span class="keyword">in</span> self.feature_sets])</div><div class="line">    self.root.feats = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>])]</div><div class="line">    self.root.feed_tree(self)</div></pre></td></tr></table></figure>
<p>最后一行我们对根节点调用了<code>feed_tree</code>方法，该方法会做以下三件事：</p>
<ul>
<li>让决策树中所有的 Node 记录一下它们所属的 Tree 结构</li>
<li>将自己记录在 Tree 中记录所有 Node 的列表<code>nodes</code>里</li>
<li>根据 Tree 的相应属性更新记录连续特征的列表</li>
</ul>
<p>实现的时候同样利用上了递归：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_tree</span><span class="params">(self, tree)</span>:</span></div><div class="line">    self.tree = tree</div><div class="line">    self.tree.nodes.append(self)</div><div class="line">    self.wc = tree.whether_continuous</div><div class="line">    <span class="keyword">for</span> child <span class="keyword">in</span> self.children.values():</div><div class="line">        <span class="keyword">if</span> child <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            child.feed_tree(tree)</div></pre></td></tr></table></figure>
<p><strong><em>注意：以上代码应定义在<code>CvDNode</code>里面</em></strong></p>
<p>接下来就是对生成算法的封装了。考虑到第二节会讲到的剪枝算法、我们需要做的是：</p>
<ul>
<li>将类别向量数值化（和朴素贝叶斯里面的数值化类别向量的方法一样）</li>
<li>将数据集切分成训练集和交叉验证集、同时处理好样本权重</li>
<li>对根节点调用决策树的生成算法</li>
<li>调用自己的剪枝算法</li>
</ul>
<p>具体的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 参数alpha和剪枝有关、可按下不表</span></div><div class="line"><span class="comment"># cv_rate用于控制交叉验证集的大小，train_only则控制程序是否进行数据集的切分</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, alpha=None, sample_weight=None, eps=<span class="number">1e-8</span>,</span></span></div><div class="line">    cv_rate=<span class="number">0.2</span>, train_only=False):</div><div class="line">    <span class="comment"># 数值化类别向量</span></div><div class="line">    _dic = &#123;c: i <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(set(y))&#125;</div><div class="line">    y = np.array([_dic[yy] <span class="keyword">for</span> yy <span class="keyword">in</span> y])</div><div class="line">    self.label_dic = &#123;value: key <span class="keyword">for</span> key, value <span class="keyword">in</span> _dic.items()&#125;</div><div class="line">    x = np.array(x)</div><div class="line">    <span class="comment"># 根据特征个数定出alpha</span></div><div class="line">    self.prune_alpha = alpha <span class="keyword">if</span> alpha <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">else</span> x.shape[<span class="number">1</span>] / <span class="number">2</span></div><div class="line">    <span class="comment"># 如果需要划分数据集的话</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> train_only <span class="keyword">and</span> self.root.is_cart:</div><div class="line">        <span class="comment"># 根据cv_rate将数据集随机分成训练集和交叉验证集</span></div><div class="line">        <span class="comment"># 实现的核心思想是利用下标来进行各种切分</span></div><div class="line">        _train_num = int(len(x) * (<span class="number">1</span>-cv_rate))</div><div class="line">        _indices = np.random.permutation(np.arange(len(x)))</div><div class="line">        _train_indices = _indices[:_train_num]</div><div class="line">        _test_indices = _indices[_train_num:]</div><div class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            <span class="comment"># 注意对切分后的样本权重做归一化处理</span></div><div class="line">            _train_weights = sample_weight[_train_indices]</div><div class="line">            _test_weights = sample_weight[_test_indices]</div><div class="line">            _train_weights /= np.sum(_train_weights)</div><div class="line">            _test_weights /= np.sum(_test_weights)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            _train_weights = _test_weights = <span class="keyword">None</span></div><div class="line">        x_train, y_train = x[_train_indices], y[_train_indices]</div><div class="line">        x_cv, y_cv = x[_test_indices], y[_test_indices]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        x_train, y_train, _train_weights = x, y, sample_weight</div><div class="line">        x_cv = y_cv = _test_weights = <span class="keyword">None</span></div><div class="line">    self.feed_data(x_train)</div><div class="line">    <span class="comment"># 调用根节点的生成算法</span></div><div class="line">    self.root.fit(x_train, y_train, _train_weights, eps)</div><div class="line">    <span class="comment"># 调用对Node剪枝算法的封装</span></div><div class="line">    self.prune(x_cv, y_cv, _test_weights)</div></pre></td></tr></table></figure>
<p>这里我们用到了<code>np.random.permutation</code>方法，它其实可以看成两行代码的缩写、亦即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">_indices = np.random.permutation(np.arange(n))</div></pre></td></tr></table></figure>
<p>从效果上来说等价于</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">_indices = np.arange(n)</div><div class="line">np.random.shuffle(_indices)</div></pre></td></tr></table></figure>
<p>不过前者不仅写起来更便利、而且运行速度也要稍微快一点，是故我们选择了前一种方法来进行实现</p>
<p>除了<code>fit</code>这个函数以外，回忆 Node 中生成算法的实现过程、可知彼时我们调用了 Tree 的<code>reduce_nodes</code>方法来将被剪掉的 Node 从<code>nodes</code>中除去。该方法的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_nodes</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.nodes)<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</div><div class="line">        <span class="keyword">if</span> self.nodes[i].pruned:</div><div class="line">            self.nodes.pop(i)</div></pre></td></tr></table></figure>
<p><strong><em>注意：虽然该实现相当简单直观、不过其中却蕴含了一个具有普适意义的编程思想：如果要在遍历列表的同时进行当前列表元素的删除操作、就一定要从后往前遍历</em></strong></p>
]]></content>
      
        <categories>
            
            <category> 决策树 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[节点结构的实现]]></title>
      <url>/posts/41abb98b/</url>
      <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py" target="_blank" rel="external">这里</a>）</p>
<p>在实现完计算各种信息量的函数之后，我们就可以着手实现决策树本身了。由前文的讨论可知、组成决策树主体的是一个个的 Node，所以我们接下来首先要实现的就是 Node 这个结构。而且由于我们所关心的 ID3、C4.5 和 CART 分类树的 Node 在大多数情况下表现一致、只有少数几个地方有所不同，因此我们可以写一个统一的 Node 结构的基类<code>CvDNode</code>来囊括我们所有关心的决策树生成算法，该基类需要实现如下功能：</p>
<ul>
<li>根据离散型特征划分数据（ID3、C4.5、CART）</li>
<li>根据连续型特征划分数据（C4.5、CART）</li>
<li>根据当前的数据判定所属的类别</li>
</ul>
<p>虽然说起来显得轻巧，但这之中的抽象还是比较繁琐的</p>
<a id="more"></a>
<p>我们先看看这个基类的基本框架该如何搭建：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment"># 导入之前定义的Cluster类以计算各种信息量</span></div><div class="line"><span class="keyword">from</span> c_Tree.Cluster <span class="keyword">import</span> Cluster</div><div class="line"></div><div class="line"><span class="comment"># 定义一个足够抽象的基类以囊括所有我们关心的算法</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CvDNode</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._x, self._y：记录数据集的变量</div><div class="line">        self.base, self.chaos：记录对数的底和当前的不确定性</div><div class="line">        self.criterion, self.category：记录该Node计算信息增益的方法和所属的类别</div><div class="line">        self.left_child, self.right_child：针对连续型特征和CART、记录该Node的左右子节点</div><div class="line">        self._children, self.leafs：记录该Node的所有子节点和所有下属的叶节点</div><div class="line">        self.sample_weight：记录样本权重</div><div class="line">        self.wc：记录着各个维度的特征是否连续的列表（whether continuous的缩写）</div><div class="line">        self.tree：记录着该Node所属的Tree</div><div class="line">        self.feature_dim, self.tar, self.feats：记录该Node划分标准的相关信息。具体而言：</div><div class="line">            self.feature_dim：记录着作为划分标准的特征所对应的维度</div><div class="line">            self.tar：针对连续型特征和CART、记录二分标准</div><div class="line">            self.feats：记录该Node能进行选择的、作为划分标准的特征的维度</div><div class="line">        self.parent, self.is_root：记录该Node的父节点以及该Node是否为根节点</div><div class="line">        self._depth, self.prev_feat：记录Node的深度和其父节点的划分标准</div><div class="line">        self.is_cart：记录该Node是否使用了CART算法</div><div class="line">        self.is_continuous：记录该Node选择的划分标准对应的特征是否连续</div><div class="line">        self.pruned：记录该Node是否已被剪掉，后面实现局部剪枝算法时会用到</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tree=None, base=<span class="number">2</span>, chaos=None,</span></span></div><div class="line">               depth=<span class="number">0</span>, parent=None, is_root=True, prev_feat=<span class="string">"Root"</span>):</div><div class="line">        self._x = self._y = <span class="keyword">None</span></div><div class="line">        self.base, self.chaos = base, chaos</div><div class="line">        self.criterion = self.category = <span class="keyword">None</span></div><div class="line">        self.left_child = self.right_child = <span class="keyword">None</span></div><div class="line">        self._children, self.leafs = &#123;&#125;, &#123;&#125;</div><div class="line">        self.sample_weight = <span class="keyword">None</span></div><div class="line">        self.wc = <span class="keyword">None</span></div><div class="line">        self.tree = tree</div><div class="line">        <span class="comment"># 如果传入了Tree的话就进行相应的初始化</span></div><div class="line">        <span class="keyword">if</span> tree <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            <span class="comment"># 由于数据预处理是由Tree完成的</span></div><div class="line">            <span class="comment"># 所以各个维度的特征是否是连续型随机变量也是由Tree记录的</span></div><div class="line">            self.wc = tree.whether_continuous</div><div class="line">            <span class="comment"># 这里的nodes变量是Tree中记录所有Node的列表</span></div><div class="line">            tree.nodes.append(self)</div><div class="line">        self.feature_dim, self.tar, self.feats = <span class="keyword">None</span>, <span class="keyword">None</span>, []</div><div class="line">        self.parent, self.is_root = parent, is_root</div><div class="line">        self._depth, self.prev_feat = depth, prev_feat</div><div class="line">        self.is_cart = self.is_continuous = self.pruned = <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, item)</span>:</span></div><div class="line">        <span class="keyword">if</span> isinstance(item, str):</div><div class="line">            <span class="keyword">return</span> getattr(self, <span class="string">"_"</span> + item)</div><div class="line"></div><div class="line">    <span class="comment"># 重载 __lt__ 方法，使得Node之间可以比较谁更小、进而方便调试和可视化</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, other)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.prev_feat &lt; other.prev_feat</div><div class="line">    </div><div class="line">    <span class="comment"># 重载 __str__ 和 __repr__ 方法，同样是为了方便调试和可视化</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">if</span> self.category <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">return</span> <span class="string">"CvDNode (&#123;&#125;) (&#123;&#125; -&gt; &#123;&#125;)"</span>.format(</div><div class="line">                self._depth, self.prev_feat, self.feature_dim)</div><div class="line">        <span class="keyword">return</span> <span class="string">"CvDNode (&#123;&#125;) (&#123;&#125; -&gt; class: &#123;&#125;)"</span>.format(</div><div class="line">            self._depth, self.prev_feat, self.tree.label_dic[self.category])</div><div class="line">    __repr__ = __str__</div></pre></td></tr></table></figure>
<p>可以看到，除了重载 <strong>getitem</strong> 方法以外、我们还重载 <strong>lt</strong>、<strong>str</strong> 和 <strong>repr</strong> 方法。这是因为决策树模型的结构比起朴素贝叶斯模型而言要复杂一些，为了开发过程中的调试和最后的可视化更加便利、通常来说最好让我们模型的表现更贴近内置类型的表现</p>
<p>然后我们需要定义几个 property 以使开发过程变得便利：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义children属性，主要是区分开连续+CART的情况和其余情况</span></div><div class="line"><span class="comment"># 有了该属性后，想要获得所有子节点时就不用分情况讨论了</span></div><div class="line"><span class="meta">@property</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">children</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> &#123;</div><div class="line">        <span class="string">"left"</span>: self.left_child, <span class="string">"right"</span>: self.right_child</div><div class="line">    &#125; <span class="keyword">if</span> (self.is_cart <span class="keyword">or</span> self.is_continuous) <span class="keyword">else</span> self._children</div><div class="line"></div><div class="line"><span class="comment"># 递归定义height（高度）属性：</span></div><div class="line"><span class="comment"># 叶节点高度都定义为1、其余节点的高度定义为最高的子节点的高度+1</span></div><div class="line"><span class="meta">@property</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">height</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">if</span> self.category <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span> + max([_child.height <span class="keyword">if</span> _child <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">else</span> <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> _child <span class="keyword">in</span> self.children.values()])</div><div class="line"></div><div class="line"><span class="comment"># 定义info_dic（信息字典）属性，它记录了该Node的主要信息</span></div><div class="line"><span class="comment"># 在更新各个Node的叶节点时，被记录进各个self.leafs属性的就是该字典</span></div><div class="line"><span class="meta">@property</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">info_dic</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> &#123;<span class="string">"chaos"</span>: self.chaos, <span class="string">"y"</span>: self._y&#125;</div></pre></td></tr></table></figure>
<p>以上就是<code>CvDNode</code>的基本框架，接下来就可以在这个框架的基础上实现决策树的各种生成算法了。首先需要指出的是，由于 Node 结构是会被 Tree 结构封装的、所以我们应该将数据预处理操作交给 Tree 来做。其次，由于我们实现的是抽象程度比较高的基类，所以我们要做比较完备的分情况讨论</p>
<p><strong><em>注意：把 ID3、C4.5 和 CART 这三种算法分开实现是可行且高效的、此时各个部分的代码都会显得更加简洁可读一些；但这样做的话，整体的代码量就会不可避免地骤增。具体应当选择何种实现方案需要看具体的需求</em></strong></p>
<p>我们先来看看实现生成算法所需要做的一些准备工作，比如定义停止继续生成的准则、定义停止后该 Node 的行为等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义第一种停止准则：当特征维度为0或当前Node的数据的不确定性小于阈值时停止</span></div><div class="line"><span class="comment"># 同时，如果用户指定了决策树的最大深度，那么当该Node的深度太深时也停止</span></div><div class="line"><span class="comment"># 若满足了停止条件，该函数会返回True、否则会返回False</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop1</span><span class="params">(self, eps)</span>:</span></div><div class="line">    <span class="keyword">if</span> (</div><div class="line">        self._x.shape[<span class="number">1</span>] == <span class="number">0</span> <span class="keyword">or</span> (self.chaos <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self.chaos &lt;= eps)</div><div class="line">        <span class="keyword">or</span> (self.tree.max_depth <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self._depth &gt;= self.tree.max_depth)</div><div class="line">    ):</div><div class="line">        <span class="comment"># 调用处理停止情况的方法</span></div><div class="line">        self._handle_terminate()</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># 定义第二种停止准则，当最大信息增益仍然小于阈值时停止</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop2</span><span class="params">(self, max_gain, eps)</span>:</span></div><div class="line">    <span class="keyword">if</span> max_gain &lt;= eps:</div><div class="line">        self._handle_terminate()</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># 利用bincount方法定义根据数据生成该Node所属类别的方法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_category</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.argmax(np.bincount(self._y))</div><div class="line"></div><div class="line"><span class="comment"># 定义处理停止情况的方法，核心思想就是把该Node转化为一个叶节点</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_handle_terminate</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 首先要生成该Node所属的类别</span></div><div class="line">    self.category = self.get_category()</div><div class="line">    <span class="comment"># 然后一路回溯、更新父节点、父节点的父节点、……等等记录叶节点的属性leafs</span></div><div class="line">    _parent = self.parent</div><div class="line">    <span class="keyword">while</span> _parent <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        _parent.leafs[id(self)] = self.info_dic</div><div class="line">        _parent = _parent.parent</div></pre></td></tr></table></figure>
<p>接下来就要实现生成算法的核心了，我们可以将它分成三步以使逻辑清晰：</p>
<ul>
<li>定义一个方法使其能将一个有子节点的 Node 转化为叶节点（局部剪枝）</li>
<li>定义一个方法使其能挑选出最好的划分标准</li>
<li>定义一个方法使其能根据划分标准进行生成</li>
</ul>
<p>我们先来看看如何进行局部剪枝：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prune</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 调用相应方法进行计算该Node所属类别</span></div><div class="line">    self.category = self.get_category()</div><div class="line">    <span class="comment"># 记录由于该Node转化为叶节点而被剪去的、下属的叶节点</span></div><div class="line">    _pop_lst = [key <span class="keyword">for</span> key <span class="keyword">in</span> self.leafs]</div><div class="line">    <span class="comment"># 然后一路回溯、更新各个parent的属性leafs（使用id作为key以避免重复）</span></div><div class="line">    _parent = self.parent</div><div class="line">    <span class="keyword">while</span> _parent <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">for</span> _k <span class="keyword">in</span> _pop_lst:</div><div class="line">            <span class="comment"># 删去由于局部剪枝而被剪掉的叶节点</span></div><div class="line">            _parent.leafs.pop(_k)</div><div class="line">        _parent.leafs[id(self)] = self.info_dic</div><div class="line">        _parent = _parent.parent</div><div class="line">    <span class="comment"># 调用mark_pruned方法将自己所有子节点、子节点的子节点……</span></div><div class="line">    <span class="comment"># 的pruned属性置为True，因为它们都被“剪掉”了</span></div><div class="line">    self.mark_pruned()</div><div class="line">    <span class="comment"># 重置各个属性</span></div><div class="line">    self.feature_dim = <span class="keyword">None</span></div><div class="line">    self.left_child = self.right_child = <span class="keyword">None</span></div><div class="line">    self._children = &#123;&#125;</div><div class="line">    self.leafs = &#123;&#125;</div></pre></td></tr></table></figure>
<p>第 16 行的 mark_pruned 方法用于给各个被局部剪枝剪掉的 Node 打上一个标记、从而今后 Tree 可以根据这些标记将被剪掉的 Node 从它记录所有 Node 的列表<code>nodes</code>中删去。该方法同样是通过递归实现的，代码十分简洁：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mark_pruned</span><span class="params">(self)</span>:</span></div><div class="line">    self.pruned = <span class="keyword">True</span></div><div class="line">    <span class="comment"># 遍历各个子节点</span></div><div class="line">    <span class="keyword">for</span> _child <span class="keyword">in</span> self.children.values():</div><div class="line">        <span class="comment"># 如果当前的子节点不是None的话、递归调用mark_pruned方法</span></div><div class="line">        <span class="comment">#（连续型特征和CART算法有可能导致children中出现None，</span></div><div class="line">        <span class="comment"># 因为此时children由left_child和right_child组成，它们有可能是None）</span></div><div class="line">        <span class="keyword">if</span> _child <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            _child.mark_pruned()</div></pre></td></tr></table></figure>
<p>有了能够进行局部剪枝的方法后，我们就能实现拿来挑选最佳划分标准的方法了。开发时需要时刻注意分清楚二分（连续 / CART）和多分（其它）的情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight, eps=<span class="number">1e-8</span>)</span>:</span></div><div class="line">    self._x, self._y = np.atleast_2d(x), np.array(y)</div><div class="line">    self.sample_weight = sample_weight</div><div class="line">    <span class="comment"># 若满足第一种停止准则、则退出函数体</span></div><div class="line">    <span class="keyword">if</span> self.stop1(eps):</div><div class="line">        <span class="keyword">return</span></div><div class="line">    <span class="comment"># 用该Node的数据实例化Cluster类以计算各种信息量</span></div><div class="line">    _cluster = Cluster(self._x, self._y, sample_weight, self.base)</div><div class="line">    <span class="comment"># 对于根节点、我们需要额外算一下其数据的不确定性</span></div><div class="line">    <span class="keyword">if</span> self.is_root:</div><div class="line">        <span class="keyword">if</span> self.criterion == <span class="string">"gini"</span>:</div><div class="line">            self.chaos = _cluster.gini()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.chaos = _cluster.ent()</div><div class="line">    _max_gain, _chaos_lst = <span class="number">0</span>, []</div><div class="line">    _max_feature = _max_tar = <span class="keyword">None</span></div><div class="line">    <span class="comment"># 遍历还能选择的特征</span></div><div class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> self.feats:</div><div class="line">        <span class="comment"># 如果是连续型特征或是CART算法，需要额外计算二分标准的取值集合</span></div><div class="line">        <span class="keyword">if</span> self.wc[feat]:</div><div class="line">            _samples = np.sort(self._x.T[feat])</div><div class="line">            _set = (_samples[:<span class="number">-1</span>] + _samples[<span class="number">1</span>:]) * <span class="number">0.5</span></div><div class="line">        <span class="keyword">elif</span> self.is_cart:</div><div class="line">            _set = self.tree.feature_sets[feat]</div><div class="line">        <span class="comment"># 然后遍历这些二分标准并调用二类问题相关的、计算信息量的方法</span></div><div class="line">        <span class="keyword">if</span> self.is_cart <span class="keyword">or</span> self.wc[feat]:</div><div class="line">            <span class="keyword">for</span> tar <span class="keyword">in</span> _set:</div><div class="line">                _tmp_gain, _tmp_chaos_lst = _cluster.bin_info_gain(</div><div class="line">                    feat, tar, criterion=self.criterion,</div><div class="line">                    get_chaos_lst=<span class="keyword">True</span>, continuous=self.wc[feat])</div><div class="line">                <span class="keyword">if</span> _tmp_gain &gt; _max_gain:</div><div class="line">                    (_max_gain, _chaos_lst), _max_feature, _max_tar = (</div><div class="line">                        _tmp_gain, _tmp_chaos_lst), feat, tar</div><div class="line">        <span class="comment"># 对于离散型特征的ID3和C4.5算法，调用普通的计算信息量的方法</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            _tmp_gain, _tmp_chaos_lst = _cluster.info_gain(</div><div class="line">                feat, self.criterion, <span class="keyword">True</span>, self.tree.feature_sets[feat])</div><div class="line">            <span class="keyword">if</span> _tmp_gain &gt; _max_gain:</div><div class="line">                (_max_gain, _chaos_lst), _max_feature = (</div><div class="line">                    _tmp_gain, _tmp_chaos_lst), feat</div><div class="line">    <span class="comment"># 若满足第二种停止准则、则退出函数体</span></div><div class="line">    <span class="keyword">if</span> self.stop2(_max_gain, eps):</div><div class="line">        <span class="keyword">return</span></div><div class="line">    <span class="comment"># 更新相关的属性</span></div><div class="line">    self.feature_dim = _max_feature</div><div class="line">    <span class="keyword">if</span> self.is_cart <span class="keyword">or</span> self.wc[_max_feature]:</div><div class="line">        self.tar = _max_tar</div><div class="line">        <span class="comment"># 调用根据划分标准进行生成的方法</span></div><div class="line">        self._gen_children(_chaos_lst)</div><div class="line">        <span class="comment"># 如果该Node的左子节点和右子节点都是叶节点且所属类别一样</span></div><div class="line">        <span class="comment"># 那么就将它们合并、亦即进行局部剪枝</span></div><div class="line">        <span class="keyword">if</span> (self.left_child.category <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span></div><div class="line">                self.left_child.category == self.right_child.category):</div><div class="line">            self.prune()</div><div class="line">            <span class="comment"># 调用Tree的相关方法，将被剪掉的、该Node的左右子节点</span></div><div class="line">            <span class="comment"># 从Tree的记录所有Node的列表nodes中除去</span></div><div class="line">            self.tree.reduce_nodes()</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># 调用根据划分标准进行生成的方法</span></div><div class="line">        self._gen_children(_chaos_lst)</div></pre></td></tr></table></figure>
<p>根据划分标准进行生成的方法相当冗长、因为需要进行相当多的分情况讨论。它的实现用到了递归的思想，真正写起来就会发现其实并不困难、只不过会有些繁琐。囿于篇幅、我们略去它的实现细节，其算法描述则如下：</p>
<ul>
<li>根据划分标准将数据划分成若干份</li>
<li>依次用这若干份数据实例化新 Node（新 Node 即是当前 Node 的子节点），同时将当前 Node 的相关信息传给新 Node。这里需要注意的是，如果划分标准是离散型特征的话：<ul>
<li>若算法是 ID3 或 C4.5，需将该特征对应的维度从新 Node 的<code>self.feats</code>属性中除去</li>
<li>若算法是 CART，需要将二分标准从新 Node 的二分标准取值集合中除去</li>
</ul>
</li>
<li>最后对新 Node 调用<code>fit</code>方法、完成递归</li>
</ul>
<p>我个人实现的版本可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py#L171" target="_blank" rel="external">这里</a></p>
<p>以上我们就实现了 Node 结构并在其上实现了决策树的生成算法，接下来我们要做的就是实现 Tree 结构来将各个 Node 封装起来</p>
]]></content>
      
        <categories>
            
            <category> 决策树 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[信息量计算的实现]]></title>
      <url>/posts/e0705aab/</url>
      <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Cluster.py" target="_blank" rel="external">这里</a>）</p>
<p>由于决策树的生成算法中会用到各种定义下的信息量的计算，所以我们应该先把这些计算信息量相关的算法实现出来。注意到这些算法同样是在不断地进行计数工作，所以我们同样需要尽量尝试利用好上一章讲述过的 bincount 方法。由于我们是在决策树模型中调用这些算法的，所以数据预处理应该交由决策树来做、这里就只需要专注于算法本身。值得一提的是，这一套算法不仅能够应用在决策树中，在遇到任何其它需要计算信息量的场合时都能够进行应用</p>
<a id="more"></a>
<p>首先实现其基本结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cluster</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._x, self._y：记录数据集的变量</div><div class="line">        self._counters：类别向量的计数器，记录第i类数据的个数</div><div class="line">            self._sample_weight：记录样本权重的属性</div><div class="line">        self._con_chaos_cache, self._ent_cache, self._gini_cache：记录中间结果的属性</div><div class="line">            self._base：记录对数的底的属性</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x, y, sample_weight=None, base=<span class="number">2</span>)</span>:</span></div><div class="line">        <span class="comment"># 这里我们要求输入的是Numpy向量（矩阵）</span></div><div class="line">        self._x, self._y = x.T, y</div><div class="line">        <span class="comment"># 利用样本权重对类别向量y进行计数</span></div><div class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            self._counters = np.bincount(self._y)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self._counters = np.bincount(self._y,</div><div class="line">                weights=sample_weight*len(sample_weight))</div><div class="line">        self._sample_weight = sample_weight</div><div class="line">        self._con_chaos_cache = self._ent_cache = self._gini_cache = <span class="keyword">None</span></div><div class="line">        self._base = base</div></pre></td></tr></table></figure>
<p>接下来就需要定义计算不确定性的两个函数。由于一个 Cluster 只接受一份数据，所以其实总的不确定性只用计算一次：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算信息熵的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ent</span><span class="params">(self, ent=None, eps=<span class="number">1e-12</span>)</span>:</span></div><div class="line">    <span class="comment"># 如果已经计算过且调用时没有额外给各类别样本的个数、就直接调用结果</span></div><div class="line">    <span class="keyword">if</span> self._ent_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> ent <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> self._ent_cache</div><div class="line">    _len = len(self._y)</div><div class="line">    <span class="comment"># 如果调用时没有给各类别样本的个数，就利用结构本身的计数器来获取相应个数</span></div><div class="line">    <span class="keyword">if</span> ent <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        ent = self._counters</div><div class="line">    <span class="comment"># 使用eps来让算法的数值稳定性更好</span></div><div class="line">    _ent_cache = max(eps, -sum(</div><div class="line">        [_c / _len * math.log(_c / _len, self._base) <span class="keyword">if</span> _c != <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> _c <span class="keyword">in</span> ent]))</div><div class="line">    <span class="comment"># 如果调用时没有给各类别样本的个数、就将计算好的信息熵储存下来</span></div><div class="line">    <span class="keyword">if</span> ent <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        self._ent_cache = _ent_cache</div><div class="line">    <span class="keyword">return</span> _ent_cache</div><div class="line"></div><div class="line"><span class="comment"># 定义计算基尼系数的函数，和计算信息熵的函数很类似、所以略去注释</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gini</span><span class="params">(self, p=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> self._gini_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> p <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> self._gini_cache</div><div class="line">    <span class="keyword">if</span> p <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        p = self._counters</div><div class="line">    _gini_cache = <span class="number">1</span> - np.sum((p / len(self._y)) ** <span class="number">2</span>)</div><div class="line">    <span class="keyword">if</span> p <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        self._gini_cache = _gini_cache</div><div class="line">    <span class="keyword">return</span> _gini_cache</div></pre></td></tr></table></figure>
<p>然后就需要定义计算<script type="math/tex">H(y|A)</script>和<script type="math/tex">\text{Gini}(y|A)</script>的函数。从算法公式可以看出它们具有形式一致性，所以我们可以把它们的实现整合在一起：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算和的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">con_chaos</span><span class="params">(self, idx, criterion=<span class="string">"ent"</span>, features=None)</span>:</span></div><div class="line">    <span class="comment"># 根据不同的准则、调用不同的方法</span></div><div class="line">    <span class="keyword">if</span> criterion == <span class="string">"ent"</span>:</div><div class="line">        _method = <span class="keyword">lambda</span> cluster: cluster.ent()</div><div class="line">    <span class="keyword">elif</span> criterion == <span class="string">"gini"</span>:</div><div class="line">        _method = <span class="keyword">lambda</span> cluster: cluster.gini()</div><div class="line">    <span class="comment"># 根据输入获取相应维度的向量</span></div><div class="line">    data = self._x[idx]</div><div class="line">    <span class="comment"># 如果调用时没有给该维度的取值空间features、就调用set方法获得该取值空间</span></div><div class="line">    <span class="comment"># 由于调用set方法比较耗时，在决策树实现时应努力将features传入</span></div><div class="line">    <span class="keyword">if</span> features <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        features = set(data)</div><div class="line">    <span class="comment"># 获得该维度特征各取值所对应的数据的下标</span></div><div class="line">    <span class="comment"># 用self._con_chaos_cache记录下相应结果以加速后面定义的相关函数</span></div><div class="line">    tmp_labels = [data == feature <span class="keyword">for</span> feature <span class="keyword">in</span> features]</div><div class="line">    self._con_chaos_cache = [np.sum(_label) <span class="keyword">for</span> _label <span class="keyword">in</span> tmp_labels]</div><div class="line">    <span class="comment"># 利用下标获取相应的类别向量</span></div><div class="line">    label_lst = [self._y[label] <span class="keyword">for</span> label <span class="keyword">in</span> tmp_labels]</div><div class="line">    rs, chaos_lst = <span class="number">0</span>, []</div><div class="line">    <span class="comment"># 遍历各下标和对应的类别向量</span></div><div class="line">    <span class="keyword">for</span> data_label, tar_label <span class="keyword">in</span> zip(tmp_labels, label_lst):</div><div class="line">        <span class="comment"># 获取相应的数据</span></div><div class="line">        tmp_data = self._x.T[data_label]</div><div class="line">        <span class="comment"># 根据相应数据、类别向量和样本权重计算出不确定性</span></div><div class="line">        <span class="keyword">if</span> self._sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            _chaos = _method(Cluster(tmp_data, tar_label, base=self._base))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            _new_weights = self._sample_weight[data_label]</div><div class="line">            _chaos = _method(Cluster(tmp_data, tar_label, _new_weights / np.sum(</div><div class="line">                _new_weights), base=self._base))</div><div class="line">        <span class="comment"># 依概率加权，同时把各个初始条件不确定性记录下来</span></div><div class="line">        rs += len(tmp_data) / len(data) * _chaos</div><div class="line">        chaos_lst.append(_chaos)</div><div class="line">    <span class="keyword">return</span> rs, chaos_lst</div></pre></td></tr></table></figure>
<p><strong><em>注意：如果仅仅是为了获得总的条件不确定性、是不用将划分后数据的各个部分的条件不确定记录下来的；之所以我们把它记录下来、是因为在决策树生成的过程里会用到这个中间变量。我们会在后面讲解决策树结构时进行相应的说明</em></strong></p>
<p>最后需要定义计算信息增益的函数。我们将会实现涉及过的三种定义方法，而且由于它们同样具有形式一致性、所以它们的实现同样可以整合在一起：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算信息增益的函数，参数get_chaos_lst用于控制输出</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">info_gain</span><span class="params">(self, idx, criterion=<span class="string">"ent"</span>, get_chaos_lst=False, features=None)</span>:</span></div><div class="line">    <span class="comment"># 根据不同的准则、获取相应的“条件不确定性”</span></div><div class="line">    <span class="keyword">if</span> criterion <span class="keyword">in</span> (<span class="string">"ent"</span>, <span class="string">"ratio"</span>):</div><div class="line">        _con_chaos, _chaos_lst = self.con_chaos(idx, <span class="string">"ent"</span>, features)</div><div class="line">        _gain = self.ent() - _con_chaos</div><div class="line">        <span class="keyword">if</span> criterion == <span class="string">"ratio"</span>:</div><div class="line">            _gain /= self.ent(self._con_chaos_cache)</div><div class="line">    <span class="keyword">elif</span> criterion == <span class="string">"gini"</span>:</div><div class="line">        _con_chaos, _chaos_lst = self.con_chaos(idx, <span class="string">"gini"</span>, features)</div><div class="line">        _gain = self.gini() - _con_chaos</div><div class="line">    <span class="keyword">return</span> (_gain, _chaos_lst) <span class="keyword">if</span> get_chaos_lst <span class="keyword">else</span> _gain</div></pre></td></tr></table></figure>
<p>考虑到二类问题的特殊性，我们需要定义专门处理二类问题的、计算信息增益相关的函数。它们大部分和以上定义的函数没有区别、代码也有大量重复，只是它会多传进一个代表二分标准的参数。为简洁，我们略去上文已经给出过的注释</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算二类问题条件不确定性的函数</span></div><div class="line"><span class="comment"># 参数tar即是二分标准，参数continuous则告诉我们该维度的特征是否连续</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bin_con_chaos</span><span class="params">(self, idx, tar, criterion=<span class="string">"gini"</span>, continuous=False)</span>:</span></div><div class="line">    <span class="keyword">if</span> criterion == <span class="string">"ent"</span>:</div><div class="line">        _method = <span class="keyword">lambda</span> cluster: cluster.ent()</div><div class="line">    <span class="keyword">elif</span> criterion == <span class="string">"gini"</span>:</div><div class="line">        _method = <span class="keyword">lambda</span> cluster: cluster.gini()</div><div class="line">    data = self._x[idx]</div><div class="line">    <span class="comment"># 根据二分标准划分数据，注意要分离散和连续两种情况讨论</span></div><div class="line">    tar = data == tar <span class="keyword">if</span> <span class="keyword">not</span> continuous <span class="keyword">else</span> data &lt; tar</div><div class="line">    tmp_labels = [tar, ~tar]</div><div class="line">    self._con_chaos_cache = [np.sum(_label) <span class="keyword">for</span> _label <span class="keyword">in</span> tmp_labels]</div><div class="line">    label_lst = [self._y[label] <span class="keyword">for</span> label <span class="keyword">in</span> tmp_labels]</div><div class="line">    rs, chaos_lst = <span class="number">0</span>, []</div><div class="line">    <span class="keyword">for</span> data_label, tar_label <span class="keyword">in</span> zip(tmp_labels, label_lst):</div><div class="line">        tmp_data = self._x.T[data_label]</div><div class="line">        <span class="keyword">if</span> self._sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            _chaos = _method(Cluster(tmp_data, tar_label, base=self._base))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            _new_weights = self._sample_weight[data_label]</div><div class="line">            _chaos = _method(Cluster(tmp_data, tar_label, _new_weights / np.sum(</div><div class="line">                _new_weights), base=self._base))</div><div class="line">        rs += len(tmp_data) / len(data) * _chaos</div><div class="line">        chaos_lst.append(_chaos)</div><div class="line">    <span class="keyword">return</span> rs, chaos_lst</div></pre></td></tr></table></figure>
<p>定义计算二类问题信息增益的函数时，只需将之前定义过的、计算信息增益的函数中计算条件不确定性的函数替换成计算二类问题条件不确定性的函数即可</p>
]]></content>
      
        <categories>
            
            <category> 决策树 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[决策树的生成算法]]></title>
      <url>/posts/c6faa205/</url>
      <content type="html"><![CDATA[<p>虽然我们之前已经用了许多文字来描述决策树、但可能还是显得过于抽象。为了能有直观的认知，在此援引维基百科上一张很好的图来进行说明：</p>
<img src="/posts/c6faa205/p1.png" alt="p1.png" title="">
<p>这张图基本蕴含了决策树中所有的关键结构，下面我们分开来分析它们</p>
<a id="more"></a>
<h1 id="决策树的相关术语"><a href="#决策树的相关术语" class="headerlink" title="决策树的相关术语"></a>决策树的相关术语</h1><p>首先是之前分析过的、“划分标准”这个概念。在上图中，被菱形橙色方框框起来的就是作为划分标准的特征，被长方形橙色方框框起来的就是对应特征的各个取值</p>
<p>然后是“节点”（Node）的概念。如果读者有学过数据结构，那么想必在提起“树”（Tree）的同时会自然而然地联想到 Node。考虑到可能有读者没有学过相关知识，这里就简要说一些相关的定义。决策树，顾名思义，确实是一个 Tree 模型。在上图中，我们可以直观地把整张图想象成一棵 Tree、把被黑色边框框起来的部分理解为一个 Node，这些 Node 是 Tree 的组成部分、Tree 本身可以协助 Node 之间的数据传输和参数调用。最上方的 Node 酷似整棵树的根，我们一般称其为“根节点”（Root）；用黑色方框（亦即四个角是直角而不是圆弧）框起来的 Node 是整棵树“生长的终点”、酷似树的叶子，我们一般称其为“叶节点”（Leaf）。比如，第三排的所有 Node 都是叶节点</p>
<p>通常来说，我们还会称一个非叶节点有一些“下属的叶节点”。比如，所有的叶节点都是根节点下属的叶节点，第三排左数第一、第二个叶节点是第二排左数第一个 Node 下属的叶节点</p>
<p>决策树中的叶节点还有一个有趣的特性：每个叶节点都对应着原样本空间的一个子空间，这些叶节点对应的子空间彼此不会相交、且并起来的话就会恰好构成完整的样本空间。换句话说、决策树的行为可以概括为如下两步：</p>
<ul>
<li>将样本空间划分为若干个互不相交的子空间</li>
<li>给每个子空间贴一个类别标签</li>
</ul>
<p>此外、我们在上图中还可以看到许多箭头，这些箭头代表着树的“生长方向”。我们一般习惯称箭头的起点是终点的“父节点”（parent）、终点是起点的“子节点”（child）；而当子节点只有两个时，通常把他们称作“左子节点”和“右子节点”。比如说，根节点是第二排所有 Node 的父节点，第二排所有 Node 都是根节点的子节点；第三排左数第一、第二个 Node 是第二排左数第一个 Node 的左、右子节点</p>
<p>对决策树有一个直观认知后，我们关心的就是怎样去生成这么一个结构了</p>
<h1 id="决策树的生成算法"><a href="#决策树的生成算法" class="headerlink" title="决策树的生成算法"></a>决策树的生成算法</h1><p>决策树的生成算法发展至今已经有许多变种，想要全面介绍它们不是短短一篇文章所能做到的。本文拟介绍其中三个上一节有所提及的、相对而言比较基本的算法：ID3、C4.5 和 CART。它们本身存在着某种递进关系：</p>
<ul>
<li>ID3 算法可说是“最朴素”的决策树算法，它给出了对离散型数据分类的解决方案</li>
<li>C4.5 算法在其上进一步发展、给出了对混合型数据分类的解决方案</li>
<li>CART 算法则更进一步、给出了对数据回归的解决方案</li>
</ul>
<p>虽说它们的功能越来越强大，但正如前文所言、它们的核心思想都是一致的：算法通过不断划分数据集来生成决策树，其中每一步的划分能够使当前的信息增益达到最大</p>
<p>值得一提的是，该核心思想的背后其实也有着机器学习的一些普适性的思想。我们可以这样来看待决策树：模型的损失就是数据集的不确定性，模型的算法就是最小化该不确定性；同时，和许多其它模型一样，想要从整个参数空间中选出模型的最优参数是个 NP 完全问题，所以我们（和许多其它算法一样）采用启发式的方法、近似求解这个最优化问题。具体而言，我们每次会选取一个局部最优解（每次选取一个特征对数据集进行划分使得信息增益最大化）、并把这些局部解合成最终解（合成一个划分规则的序列）</p>
<p>可以这样直观地去想一个决策树的生成过程：</p>
<ul>
<li>向根节点输入数据</li>
<li>根据信息增益的度量、选择数据的某个特征来把数据划分成（互不相交的）好几份并分别喂给一个新 Node</li>
<li>如果分完数据后发现：<ul>
<li>某份数据的不确定较小、亦即其中某一类别的样本已经占了大多数，此时就不再对这份数据继续进行划分、将对应的 Node 转化为叶节点</li>
<li>某份数据的不确定性仍然较大，那么这份数据就要继续分割下去（转第 2 步）</li>
</ul>
</li>
</ul>
<p><strong><em>注意：虽然划分的规则是根据数据定出的，但是划分本身其实是针对整个输入空间进行划分的</em></strong></p>
<p>从上述过程可知，决策树的生成过程就是根据某个度量从数据集中训练出一系列的划分规则、使得这些规则能够在数据集有好的表现。事实上，上文说的3种不同算法在分类问题上的区别亦仅表现在度量信息增益和划分数据的方法的不同上</p>
<h2 id="ID3（Interactive-Dichotomizer-3）"><a href="#ID3（Interactive-Dichotomizer-3）" class="headerlink" title="ID3（Interactive Dichotomizer-3）"></a>ID3（Interactive Dichotomizer-3）</h2><p>ID3 可以译为“交互式二分法”，虽说这个名字里面带了个“二分”，但该方法完全适用于“多分”的情况。它选择互信息作为信息增益的度量、针对离散型数据进行划分。其算法叙述如下：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,(x_{N},y_{N})\}</script></li>
<li><strong>过程</strong>：<ol>
<li>将数据集<script type="math/tex">D</script>喂给一个 Node</li>
<li>若<script type="math/tex">D</script>中的所有样本同属于类别<script type="math/tex">c_{k}</script>，则该 Node 不再继续生成、并将其类别标记为<script type="math/tex">c_{k}</script>类</li>
<li>若<script type="math/tex">x_{i}</script>已经是 0 维向量、亦即已没有可选特征，则将此时<script type="math/tex">D</script>中样本个数最多的类别<script type="math/tex">c_{k}</script>作为该 Node 的类别</li>
<li>否则，按照互信息定义的信息增益：  <script type="math/tex; mode=display">
g\left( y,x^{\left( j \right)} \right) = H\left( y \right) - H(y|x^{\left( j \right)})</script>来计算第 j 维特征的信息增益，然后选择使得信息增益最大的特征<script type="math/tex">x^{\left( j^{*} \right)}</script>作为划分标准，亦即：  <script type="math/tex; mode=display">
j^{*} = \arg{\max_{j}{g\left( y,x^{\left( j \right)} \right)}}</script></li>
<li>若<script type="math/tex">x^{\left( j^{*} \right)}</script>满足停止条件、则不再继续生成并则将此时<script type="math/tex">D</script>中样本个数最多的类别<script type="math/tex">c_{k}</script>作为类别标记</li>
<li>否则，依<script type="math/tex">x^{\left( j^{*} \right)}</script>的所有可能取值<script type="math/tex">\{ a_{1},\ldots,a_{m}\}</script>将数据集<script type="math/tex">D</script>划分为<script type="math/tex">{\{ D}_{1},\ldots,D_{m}\}</script>、使得：  <script type="math/tex; mode=display">
{(x}_{i},y_{i}) \in D_{j} \Leftrightarrow x_{i}^{\left( j^{*} \right)} = a_{j},\forall i = 1,\ldots,N</script>同时，将<script type="math/tex">x_{1},\ldots,x_{N}</script>的第<script type="math/tex">j^{*}</script>维去掉、使它们成为<script type="math/tex">n - 1</script>维的特征向量</li>
<li>对每个<script type="math/tex">D_{j}</script>从 2.1 开始调用算法</li>
</ol>
</li>
<li><strong>输出</strong>：原始数据对应的 Node（亦即根节点）</li>
</ol>
<p>其中算法第 2.5 步的“停止条件”（也可称为“预剪枝”；有关剪枝的讨论会放在<a href="/posts/1a7aa546/" title="决策树的剪枝算法">决策树的剪枝算法</a>）有许多种提法，常用的是如下两种：</p>
<ul>
<li>若选择<script type="math/tex">x^{\left( j^{*} \right)}</script>作为特征时信息增益<script type="math/tex">g(y,x^{\left( j^{*} \right)})</script>仍然很小（通常会传入一个参数<script type="math/tex">\epsilon</script>作为阈值）、则停止</li>
<li>事先把数据集分为训练集与测试集（交叉验证的思想），若由训练集得到的<script type="math/tex">x^{\left( j^{*} \right)}</script>并不能使得决策树在测试集上的错误率更小、则停止</li>
</ul>
<p>这两种停止条件的提法通用于 C4.5 和 CART，后文将不再赘述。同时，正如本章一开始有所提及的、决策树会在许多地方应用到递归的思想，上述算法中的第 2.6 步正是经典的递归</p>
<p>我们可以对气球数据集 1.0 过一遍 ID3 算法以加深理解。由算法可知，因为每个 Node 的信息熵是确定的、所以选择互信息最大的特征等价于选择条件熵最小的特征，是故我们只需要在每个 Node 上计算各个可选特征的条件熵。易知在根节点上：</p>
<script type="math/tex; mode=display">
\begin{align}
H\left( 不爆炸\middle| 颜色\right) &= - p_{11}\log p_{11} - p_{12}\log p_{12} = 1 \\

H\left( 不爆炸\middle| 大小\right) &= - p_{21}\log p_{21} - p_{22}\log p_{22} \approx 0.65 \\

H\left( 不爆炸\middle| 人员\right) &= - p_{31}\log p_{31} - p_{32}\log p_{32} \approx 0.92 \\

H\left( 不爆炸\middle| 动作\right) &= - p_{41}\log p_{41} - p_{42}\log p_{42} \approx 0.65
\end{align}</script><p>其中</p>
<script type="math/tex; mode=display">p_{11} \triangleq p\left( 不爆炸\middle| 黄色\right) = \frac{1}{2},\ \ p_{12} \triangleq p\left( 不爆炸\middle| 紫色\right) = \frac{1}{2}</script><script type="math/tex; mode=display">p_{21} \triangleq p\left( 不爆炸\middle| 小\right) = \frac{5}{6},\ \ p_{22} \triangleq p\left( 不爆炸\middle| 大\right) = \frac{1}{6}</script><script type="math/tex; mode=display">p_{31} \triangleq p\left( 不爆炸\middle| 成人\right) = \frac{1}{3},\ \ p_{32} \triangleq p\left( 不爆炸\middle| 小孩\right) = \frac{2}{3}</script><script type="math/tex; mode=display">p_{41} \triangleq p\left( 不爆炸\middle| 手打\right) = \frac{5}{6},\ \ p_{42} \triangleq p\left( 不爆炸\middle| 脚踩\right) = \frac{1}{6}</script><p>且易知</p>
<script type="math/tex; mode=display">H\left( 不爆炸\middle| A \right) = H\left( 爆炸\middle| A \right),\ \ \forall A \in \{ 颜色, 大小, 人员, 动作\}</script><p>从而可知应选测试动作或者气球大小作为根节点的划分标准。不妨选择气球大小作为划分标准，此时的决策树如下图所示（图片是使用 ProcessOn 在线绘制而成的）：</p>
<img src="/posts/c6faa205/p2.png" alt="p2.png" title="">
<p>图中 Node A 和 Node B 所对应的数据集分别如下面两张表所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用手打</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>注意此时气球大小已经不是可选特征了。我们接下来要分别对 Node A 和 Node B 调用 ID3 算法，计算过程和根节点上的计算过程大同小异。以此类推、最终我们可以得到如下图所示的决策树：</p>
<img src="/posts/c6faa205/p3.png" alt="p3.png" title="">
<p>可知该决策树在气球数据集 1.0 上的正确率为 100%、且它做的决策都很符合直观</p>
<h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><p>C4.5 使用信息增益比作为信息增益的度量，从而缓解了 ID3 算法会倾向于选择 m 比较大的特征<script type="math/tex">A</script>作为划分依据这个问题；也正因如此，C4.5 算法可以处理 ID3 算法比较难处理的混合型数据。我们先来看看它在离散型数据上的算法（仅展示和 ID3 算法中不同的部分）：</p>
<h3 id="算法-2-4-步"><a href="#算法-2-4-步" class="headerlink" title="算法 2.4 步"></a>算法 2.4 步</h3><p>否则，按照信息增益比定义的信息增益：</p>
<script type="math/tex; mode=display">
g_{R}\left( y,x^{\left( j \right)} \right) = \frac{g(y,x^{\left( j \right)})}{H_{x^{\left( j \right)}}(y)}</script><p>来计算第 j 维特征的信息增益，然后选择使得信息增益最大的特征<script type="math/tex">x^{\left( j^{*} \right)}</script>作为划分标准，亦即：</p>
<script type="math/tex; mode=display">
j^{*} = \arg{\max_{j}{g_{R}\left( y,x^{\left( j \right)} \right)}}</script><p><strong><em>注意：C4.5 算法虽然不会倾向于选择 m 比较大的特征、但有可能会倾向于选择 m 比较小的特征。针对这个问题，Quinlan 在 1993 年提出了这么一个启发式的方法：先选出互信息比平均互信息要高的特征、然后从这些特征中选出信息增益比最高的</em></strong></p>
<p>混合型数据的处理方法大同小异，本书拟介绍一种常用且符合直观的、同时亦是 C4.5 所采用的方法：使用二类问题的解决方案处理连续型特征。具体而言，当二类问题和决策树结合起来时，在连续的情况下、我们通常可以把它转述为：</p>
<script type="math/tex; mode=display">
Y_{1} = \left\{ y:y^{A} < a_{1} \right\},Y_{2} = \{ y:y^{A} \geq a_{1}\}</script><p>相对应的，我们同样可以用处理二类问题的思想来处理离散型特征，此时：</p>
<script type="math/tex; mode=display">
A \in \left\{ a_{1},a_{2} \right\};\ \ Y_{1} = \left\{ y:y^{A} = a_{1} \right\},\ Y_{2} = \{ y:y^{A} = a_{2}\}</script><p>更进一步、我们通常会将它表示为：</p>
<script type="math/tex; mode=display">
Y_{1} = \left\{ y:y^{A} = a_{1} \right\},Y_{2} = \{ y:y^{A} \neq a_{1}\}</script><p>我们通常称以上各式中的<script type="math/tex">a_{1}</script>为“二分标准”。一般而言，如何处理连续型特征这个问题会归结于如何选择“二分标准”这个问题。一个比较容易想到的做法是：</p>
<ul>
<li>若在当前数据集中有 m 个取值，不妨假设它们为<script type="math/tex">u_1,...,u_m</script>；不失一般性、再不妨假设它们满足<script type="math/tex">u_1<...<u_m</script>（若不然，进行一次排序操作即可），那么依次选择<script type="math/tex">v_1,...,v_p</script>作为二分标准并决出最好的一个，其中<script type="math/tex">v_1,...,v_p</script>构成等差数列、且:  <script type="math/tex; mode=display">
v_{1} = u_{1},v_{p} = u_{m}</script>p 的选取则视情况而定，一般而言会取 p 反比于“深度”。这意味着当数据越分越细时、对特征的划分会越分越粗，从直观上来说这有益于防止过拟合</li>
</ul>
<p>但这样可能会产生许多“冗余”的二分标准。试想如果这些取值满足：</p>
<script type="math/tex; mode=display">
u_{1} = 0,u_{2} = 100,u_{3} = 101,u_{4} = 102,\ldots</script><p>那么我们就会在<script type="math/tex">u_{1}</script>和<script type="math/tex">u_{2}</script>之间尝试大量的划分标准、但显然这些划分标准算出来的结果都是一样的。为了处理类似于这种不合理的情况，C4.5 采用如下做法：</p>
<ul>
<li>依次选择<script type="math/tex">v_{1} = \frac{u_{1} + u_{2}}{2},\ldots,v_{m - 1} = \frac{u_{m - 1} + u_{m}}{2}</script>作为二分标准、计算它们的信息增益比、从而决出最好的二分标准来划分数据</li>
</ul>
<p>在这之上还有另一种可行的做法：</p>
<ul>
<li>设<script type="math/tex">u_1,...,u_m</script>所对应的类别是<script type="math/tex">y_1,...,y_m</script>，那么在<script type="math/tex">v_1,...,v_{m-1}</script>中只选取使得：  <script type="math/tex; mode=display">
y_{i} \neq y_{i + 1},\ \ (i = 1,\ldots,m - 1)</script>的<script type="math/tex">v_i</script>作为二分标准、计算它们的信息增益比、从而决出最好的二分标准来划分数据</li>
</ul>
<p>这种做法在某些情况下会表现得更好、但在某些情况下也会显得不合理。鉴于此，本书会采用更稳定的上一种做法来进行实现</p>
<p><strong><em>注意：从以上讨论可知，我们完全可以把 ID3 算法推广成可以处理连续型特征的算法。只不过如果数据集是混合型数据集的话、ID3 就会倾向于选择离散型特征作为划分标准而已。如果数据集的所有特征都是连续型特征、那么 ID3 和 C4.5 之间孰优孰劣是难有定论的</em></strong></p>
<p>这里需要特别指出的是、C4.5 也是有一个比较糟糕的性质的：由信息增益比的定义可知，如果是二分的话、它会倾向于把数据集分成很不均匀的两份；因为此时<script type="math/tex">H_{A}(y)</script>将非常小、导致<script type="math/tex">g_{R}(y,A)</script>很大（即使<script type="math/tex">g(y,A)</script>比较小）。举个例子：如果当前划分标准为连续特征、那么 C4.5 可能会倾向于直接选择<script type="math/tex">v_{1},v_{2},v_{3}</script>等作为二分标准</p>
<p>之所以说该性质比较糟糕、是因为它将直接导致如下结果：当 C4.5 进行二叉分枝时、它可能总会直接分出一个比较小的 Node 作为叶节点、然后剩下一个大的 Node 继续进行生成。这种行为会导致决策树倾向于往深处发展、从而导致很容易产生过拟合现象，这并不是我们期望的结果</p>
<h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><p>CART 的全称是 Classification and Regression Tree（“分类与回归树”）。顾名思义，它既可做分类亦可做回归。囿于篇幅，本书会介绍分类问题的算法和实现、对于回归问题则只叙述算法，感兴趣的观众老爷们可以尝试触类旁通地实现它</p>
<p>CART 算法一般会使用基尼增益作为信息增益的度量（当然也可以使用互信息和信息增益比作为度量，需要视具体场合而定），其一大特色就是它假设了最终生成的决策树为二叉树、亦即它在处理离散型特征时也会通过决出二分标准来划分数据：</p>
<h3 id="算法-2-4-步-1"><a href="#算法-2-4-步-1" class="headerlink" title="算法 2.4 步"></a>算法 2.4 步</h3><p>否则，不妨设<script type="math/tex">x^{\left( j \right)}</script>在当前数据集中有<script type="math/tex">S_{j}</script>个取值<script type="math/tex">u_{1}^{\left( j \right)},\ldots,u_{S_{j}}^{\left( j \right)}</script>且它们满足<script type="math/tex">u_{1}^{\left( j \right)} < \ldots < u_{S_{j}}^{\left( j \right)}</script>，则：</p>
<ul>
<li>若<script type="math/tex">x^{\left( j \right)}</script>是离散型的，则依次选取<script type="math/tex">u_{1}^{\left( j \right)},\ldots,u_{S_{j}}^{\left( j \right)}</script>作为二分标准<script type="math/tex">a_{p}</script>，此时：  <script type="math/tex; mode=display">
A_{jp} = \{ x^{\left( j \right)} = a_{p},x^{\left( j \right)} \neq a_{p}\}</script></li>
<li>若<script type="math/tex">x^{\left( j \right)}</script>是连续型的，则依次选取<script type="math/tex">\frac{u_{1}^{\left( j \right)} + u_{2}^{\left( j \right)}}{2},\ldots,\frac{u_{S_{j} - 1}^{\left( j \right)} + u_{S_{j}}^{\left( j \right)}}{2}</script>作为二分标准<script type="math/tex">a_{p}</script>，此时：  <script type="math/tex; mode=display">
A_{jp} = \{ x^{\left( j \right)} < a_{p},x^{\left( j \right)} \geq a_{p}\}</script>按照基尼系数定义的信息增益：  <script type="math/tex; mode=display">
g_{\text{Gini}}\left( y,A_{jp} \right) = \text{Gini}\left( y \right) - \text{Gini}(y|A_{jp})</script>来计算第 j 维特征在这些二分标准下的信息增益，然后选择使得信息增益最大的特征<script type="math/tex">x^{\left( j^{*} \right)}</script>和相应的二分标准<script type="math/tex">u_{p^{*}}^{\left( j^{*} \right)}</script>作为划分标准，亦即：  <script type="math/tex; mode=display">
{(j}^{*},p^{*}) = \arg{\max_{j,p}{g_{\text{Gini}}\left( y,A_{jp} \right)}}</script></li>
</ul>
<p>从分类问题到回归问题不是一个不平凡的问题，它们的区别仅在于：回归问题除了特征是连续型的以外、“类别”也是连续型的，此时我们一般把“类别向量”改称为“输出向量”。正如前文所提及，决策树可以转化为最小化损失的问题。我们之前讨论的分类问题中的损失都是数据的不确定性，而在回归问题中、一种常见的做法就是将损失定义为平方损失：</p>
<script type="math/tex; mode=display">
L(D) = \sum_{i = 1}^{N}{I\left( y_{i} \neq f\left( x_{i} \right) \right)\left\lbrack y_{i} - f\left( x_{i} \right) \right\rbrack^{2}}</script><p>这里的<script type="math/tex">I</script>是示性函数，<script type="math/tex">f</script>是我们的模型、<script type="math/tex">f(x_{i})</script>是<script type="math/tex">x_{i}</script>在我们模型下的预测输出、<script type="math/tex">y_{i}</script>是真实输出。平方损失其实就是我们熟悉的“（欧式）距离”（预测向量和输出向量之间的距离），我们会在许多分类、回归问题中见到它的身影。在损失为平方损失时，一般称此时生成的回归决策树为最小二乘回归树</p>
<p>在分类问题中决策树是一个划分规则的序列、在回归问题中也差不多。具体而言，假设该序列一共会将输入空间划分为<script type="math/tex">R_{1},\ldots,R_{M}</script>（这 M 个子空间彼此不相交）、那么：</p>
<ul>
<li>对于分类问题，模型可表示为：  <script type="math/tex; mode=display">
f\left( x_{i} \right) = \sum_{m = 1}^{M}{y_{m}I(x_{i} \in R_{m})}</script></li>
<li>对于回归问题，模型可表示为：  <script type="math/tex; mode=display">
f\left( x_{i} \right) = \sum_{m = 1}^{M}{c_{m}I\left( x_{i} \in R_{m} \right)}</script>这里<script type="math/tex">c_{m} = \arg{\min_{c}{L_{m}\left( c \right)}} \triangleq \arg{\min_{c}{\sum_{\left( x_{i},y_{i} \right) \in R_{m}}^{}\left( y_{i} - c \right)^{2}}}</script>。那么由一阶条件：  <script type="math/tex; mode=display">
\frac{\partial L_{m}\left( c \right)}{\partial c} = 0 \Leftrightarrow - 2\sum_{\left( x_{i},y_{i} \right) \in R_{m}}^{}\left( y_{i} - c_{m} \right) = 0</script>可解得<script type="math/tex">c_{m} = \text{avg}\left( y_{i}|(x_{i},y_{i}) \in R_{m} \right) \triangleq \frac{1}{\left| R_{m} \right|}\sum_{\left( x_{i},y_{i} \right) \in R_{m}}^{}y_{i}</script></li>
</ul>
<p>最小二乘回归树的算法和 CART 做分类时的算法几乎完全一样，区别只在于：</p>
<ul>
<li>解决分类问题时，我们会在特征和二分标准选好后，通过求解：  <script type="math/tex; mode=display">
{(j}^{*},p^{*}) = \arg{\max_{j,p}{g_{\text{Gini}}\left( y,A_{jp} \right)}}</script>来选取划分标准</li>
<li>解决回归问题时，我们会在特征和二分标准选好后，通过求解：  <script type="math/tex; mode=display">
\left( j^{*},p^{*} \right) = \arg{\min_{j,p}{\lbrack\sum_{x_{i} < p}^{}{\left( y_{i} - c_{jp}^{\left( 1 \right)} \right)^{2} + \sum_{x_{i} \geq p}^{}\left( y_{i} - c_{jp}^{\left( 2 \right)} \right)^{2}}\rbrack}}</script>来选取划分标准，其中<script type="math/tex">c_{jp}^{\left( 1 \right)} = \text{avg}(y_{i}|x_{i} < p)</script>、<script type="math/tex">c_{jp}^{\left( 2 \right)} = \text{avg}(y_{i}|x_{i} \geq p)</script>，p（切分点）的选取则视情况而定（可以模仿分类问题中二分标准的选取方法）</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 决策树 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据的信息]]></title>
      <url>/posts/2ce87ace/</url>
      <content type="html"><![CDATA[<p>本文首先简要地说明一下决策树生成算法背后的数学基础和思想、然后再叙述具体的算法。往大了说、决策树的生成可以算是信息论的一个应用，但它其实只用到了信息论中一小部分的思想。不过，先对信息论有个概括性的认知还是有必要的、因为这样我们就可以有个更宽的视野</p>
<a id="more"></a>
<h1 id="信息论简介"><a href="#信息论简介" class="headerlink" title="信息论简介"></a>信息论简介</h1><p>（注：本节有许多内容节选、修改、总结自维基百科）<br>被誉为信息论创始人的是克劳德·艾尔伍德·香农（Claude Elwood Shannon，1916.4.30－2001.2.26），他是美国数学家、电子工程师和密码学家，是密歇根大学学士、麻省理工学院博士。他在 1948 年发表的划时代的论文——“通信的数学原理”奠定了现代信息论的基础</p>
<p>信息论（Information Theory）涉及的领域相当多，包括但不限于信息的量化、存储和通信、统计推断、自然语言处理、密码学等等。信息论的主要内容可以类比人类最广泛的交流手段——语言来阐述。一种简洁的语言（以英语为例）通常有如下两个重要特点：</p>
<ul>
<li>最常用的一些词汇（比如“a”、“the”、“I”）应该要比相对而言不太常用的词（比如“Python”、“Machine”、“Learning”）要短一些</li>
<li>如果句子的某一部分被漏听或者由于噪声干扰（比如身处闹市）而被误听，听者应该仍然可以抓住句子的大概意思</li>
</ul>
<p>其中第二点被称作为“鲁棒性（Robustness）”。如果把电子通信系统比作一种语言的话，这种鲁棒性（Robustness）不可或缺。信息论的基本研究课题是信源编码和信道编码（通俗一点来讲就是怎么发出信息和怎么传递信息），将鲁棒性引入通信正是通过其中的信道编码来完成的，由此可见信息论的重要性</p>
<p>注意这些内容同消息的重要性之间是毫不相干的。例如，像“你好；再见”这样的话语和像“救命”这样的紧急请求，在说起来或写起来所花的时间是差不多的，然而明显后者更重要也更有意义。信息论却不会考虑一段消息的重要性或内在意义，因为这些属于信息的质量的问题而不是信息量和可读性方面上的问题，后者只是由概率这一因素单独决定的</p>
<p>既然我们我们关注的是信息量，我们就需要有一个度量方法。决策树生成算法背后的思想正是利用该度量方法来衡量一种“数据划分”的优劣、从而生成一个“判定序列”。具体而言，它会不断地寻找数据的划分方法、使得在该划分下我们能够获得的信息量最大（更详细的叙述会在后文给出）</p>
<h1 id="不确定性"><a href="#不确定性" class="headerlink" title="不确定性"></a>不确定性</h1><p>在决策树的生成中，获得的信息量的度量方法是从反方向来定义的：若一种划分能使数据的“不确定性”减少得越多、就意味着该划分能获得越多信息。这是很符合直观的，关键问题就在于应该如何度量数据的不确定性（或说不纯度，Impurity）。常见的度量标准有两个：信息熵（Entropy）和基尼系数（Gini Index），接下来我们就说说它们的定义和性质</p>
<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>先来看看它的公式：</p>
<script type="math/tex; mode=display">
H\left( y \right) = - \sum_{k = 1}^{K}{p_{k}\log p_{k}}</script><p>对于具体的、随机变量<script type="math/tex">y</script>生成的数据集<script type="math/tex">D = \{ y_{1},\ldots,y_{N}\}</script>而言，在实际操作中通常会利用经验熵来估计真正的信息熵：</p>
<script type="math/tex; mode=display">
H\left( y \right) = H\left( D \right) = - \sum_{k = 1}^{K}{\frac{|C_{k}|}{|D|}\log\frac{|C_{k}|}{|D|}}</script><p>这里假设随机变量<script type="math/tex">y</script>的取值空间为<script type="math/tex">\{ c_{1},\ldots,c_{K}\}</script>，<script type="math/tex">p_{k}</script>表示<script type="math/tex">y</script>取<script type="math/tex">c_{k}</script>的概率：<script type="math/tex">p_{k} = p(y = c_{k})</script>；<script type="math/tex">|C_{k}|</script>代表由随机变量<script type="math/tex">y</script>中类别为<script type="math/tex">c_{k}</script>的样本的个数、<script type="math/tex">|D|</script>代表<script type="math/tex">D</script>的总样本个数（亦即<script type="math/tex">\left| D \right| = N</script>）。可以看到，经验公式背后的思想其实就是“频率估计概率”</p>
<p>通常来说，公式中对数的底会取为 2、此时信息熵<script type="math/tex">H(y)</script>的单位叫作比特（bit）；如果把底取为<script type="math/tex">e</script>（亦即取自然对数）的话，<script type="math/tex">H(y)</script>的单位就称作纳特（nat）</p>
<p>接下来说明为何上式能够度量数据的不确定性。可以证明（详细推导可参见<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>），当：</p>
<script type="math/tex; mode=display">
p_{1} = p_{2} = \ldots = p_{K} = \frac{1}{K}</script><p>时，<script type="math/tex">H(y)</script>达到最大值<script type="math/tex">- \log\frac{1}{K}</script>、亦即<script type="math/tex">\log K</script>。由于<script type="math/tex">p_{k} = p(y = c_{k})</script>，上式即意味着随机变量<script type="math/tex">y</script>取每一个类的概率都是一样的、亦即<script type="math/tex">y</script>完全没有规律可循，想要预测它的取值只能靠运气。换句话说，由<script type="math/tex">y</script>生成出来的数据<script type="math/tex">\{ y_{1},\ldots,y_{N}\}</script>的不确定性是在取值空间为<script type="math/tex">\{ c_{1},\ldots,c_{K}\}</script>、样本数为 N 的数据中最大的（想象预测 N 次正 K 面体骰子的结果）</p>
<p>我们的目的是想让<script type="math/tex">y</script>的不确定性减小、亦即想让<script type="math/tex">y</script>变得有规律以方便我们预测。稍微严谨地来说，就是<script type="math/tex">y</script>取某个类的概率特别大、取其它类的概率都特别小。极端的例子自然就是存在某个<script type="math/tex">k^{*}</script>、使得<script type="math/tex">p\left( {y = c}_{k^{*}} \right) = 1</script>、<script type="math/tex">p\left( y = c_{k} \right) = 0,\forall k \neq k^{*}</script>、亦即<em>y</em>生成的样本总属于<script type="math/tex">c_{k^{*}}</script>类。带入<script type="math/tex">H(y)</script>的定义式，可以发现此时<script type="math/tex">H\left( y \right) = 0</script>、亦即<script type="math/tex">y</script>生成的样本没有不确定性</p>
<p><strong><em>注意：由于<script type="math/tex">p\log p \rightarrow 0\ (p \rightarrow 0)</script>、所以认为<script type="math/tex">0\log 0 = 0</script></em></strong></p>
<p>特殊的情况就是二类问题、亦即<script type="math/tex">K = 2</script>的情况。先不妨设<script type="math/tex">y</script>只取 0、1 二值，再设：</p>
<script type="math/tex; mode=display">
p\left( y = 0 \right) = p,\ \ p\left( y = 1 \right) = 1 - p,\ \ 0 \leq p \leq 1</script><p>那么此时的信息熵<script type="math/tex">H(y)</script>即为：</p>
<script type="math/tex; mode=display">
H\left( y \right) = - \operatorname{plog}p - \left( 1 - p \right)\log{(1 - p)}</script><p>由此可得<script type="math/tex">H(y)</script>随<script type="math/tex">p</script>变化的函数曲线。底为 2 时函数图像如下图所示：</p>
<img src="/posts/2ce87ace/p1.png" alt="p1.png" title="">
<p>如前文所述，在<script type="math/tex">p = 0.5</script>时<script type="math/tex">H(y)</script>取得最大值 1。底为<script type="math/tex">e</script>时函数图像则如下图所示：</p>
<img src="/posts/2ce87ace/p2.png" alt="p2.png" title="">
<p>虽然最大值仍在<script type="math/tex">p = 0.5</script>时取得，但是此时<script type="math/tex">H(y)</script>仅有 0.693（<script type="math/tex">\ln 2</script>）左右</p>
<p>如果对上述二类问题稍作推广：<script type="math/tex">y \in \{ Y_{1},Y_{2}\}</script>、其中<script type="math/tex">Y_{1}</script>、<script type="math/tex">Y_{2}</script>都是一个集合，那么此时信息熵的定义式即为：</p>
<script type="math/tex; mode=display">
H\left( y \right) = - p\left( y \in Y_{1} \right)\log{p\left( y \in Y_{1} \right)} - p\left( y \in Y_{2} \right)\log{p(y \in Y_{2})}</script><p>且易知：</p>
<script type="math/tex; mode=display">
p\left( y \in Y_{1} \right) + p\left( y \in Y_{2} \right) = 1</script><p>如无特殊说明，今后谈及二类问题时讨论的范围都包括推广后的二类问题。</p>
<p>以上的叙述说明了，<script type="math/tex">y</script>越乱意味着<script type="math/tex">H(y)</script>越大、<script type="math/tex">y</script>越有规律意味着<script type="math/tex">H(y)</script>越小，亦即<script type="math/tex">H(y)</script>确实可以作为不确定性的度量标准</p>
<h2 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h2><p>基尼系数的定义会更简洁一些：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \right) = \sum_{k = 1}^{K}{p_{k}(1 - p_{k})} = 1 - \sum_{k = 1}^{K}p_{k}^{2}</script><p>同样可以利用经验基尼系数来进行估计：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \right) = \text{Gini}\left( D \right) = 1 - \sum_{k = 1}^{K}\left( \frac{\left| C_{k} \right|}{\left| D \right|} \right)^{2}</script><p>以及同样可以证明，当</p>
<script type="math/tex; mode=display">
p_{1} = p_{2} = \ldots = p_{K} = \frac{1}{K}</script><p>时，<script type="math/tex">\text{Gini}(y)</script>取得最大值<script type="math/tex">1 - \frac{1}{K}</script>；当存在<script type="math/tex">k^{*}</script>使得<script type="math/tex">p_{k^{*}} = 1</script>时、<script type="math/tex">\text{Gini}\left( y \right) = 0</script>。特别地、当<script type="math/tex">K = 2</script>时，可以导出：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \right) = 2p(1 - p)</script><p>此时<script type="math/tex">\text{Gini}(y)</script>的函数图像如下图所示：</p>
<img src="/posts/2ce87ace/p3.png" alt="p3.png" title="">
<p>虽然最大值仍在<script type="math/tex">p = 0.5</script>时取得，但是此时<script type="math/tex">\text{Gini}(y)</script>仅有 0.5。我们同样可以对二类问题进行推广、此时有：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \right) = 1 - p^{2}\left( y \in Y_{1} \right) - p^{2}(y \in Y_{2})</script><p>且</p>
<script type="math/tex; mode=display">
p\left( y \in Y_{1} \right) + p\left( y \in Y_{2} \right) = 1</script><p>以上的叙述说明了<script type="math/tex">\text{Gini}(y)</script>也可以用来度量不确定性</p>
<h1 id="信息的增益"><a href="#信息的增益" class="headerlink" title="信息的增益"></a>信息的增益</h1><p>在定义完不确定性的度量标准之后，我们就可以看看什么叫“获得信息”、亦即信息的增益了。从直观上来说，信息的增益是针对随机变量<script type="math/tex">y</script>和描述该变量的特征来定义的，此时数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,(x_{N},y_{N})\}</script>，其中<script type="math/tex">x_{i} = \left( x_{i}^{\left( 1 \right)},\ldots,x_{i}^{\left( n \right)} \right)^{T}</script>是描述<script type="math/tex">y_{i}</script>的特征向量、n 则是特征个数。我们可以先研究单一特征的情况（<script type="math/tex">n = 1</script>）：不妨设该特征叫<script type="math/tex">A</script>、数据集<script type="math/tex">D = \{\left( A_{1},y_{1} \right),\ldots,(A_{N},y_{N})\}</script>；此时所谓信息的增益，反映的就是特征<script type="math/tex">A</script>所能给我们带来的关于<script type="math/tex">y</script>的“信息量”的大小</p>
<p>可以引入条件熵<script type="math/tex">H(y|A)</script>的概念来定义信息的增益，它同样有着比较好的直观：</p>
<ul>
<li>所谓条件熵，就是根据特征<script type="math/tex">A</script>的不同取值<script type="math/tex">\{ a_{1},\ldots,a_{m}\}</script>对<script type="math/tex">y</script>进行限制后，先对这些被限制的<script type="math/tex">y</script>分别计算信息熵、再把这些信息熵（一共有 m 个）根据特征取值本身的概率加权求和、从而得到总的条件熵。换句话说，条件熵是由被<script type="math/tex">A</script>不同取值限制的各个部分的<script type="math/tex">y</script>的不确定性以取值本身的概率作为权重加总得到的</li>
</ul>
<p>所以，条件熵<script type="math/tex">H(y|A)</script>越小、意味着<script type="math/tex">y</script>被<script type="math/tex">A</script>限制后的总的不确定性越小、从而意味着<em>A</em>更能够帮助我们做出决策</p>
<p>接下来就是数学定义：</p>
<script type="math/tex; mode=display">
H\left( y \middle| A \right) = \sum_{j = 1}^{m}{p\left( A = a_{j} \right)H(y|A = a_{j})}</script><p>其中</p>
<script type="math/tex; mode=display">
H\left( y \middle| A = a_{j} \right) = - \sum_{k = 1}^{K}{p\left( y = c_{k} \middle| A = a_{j} \right)\log{p(y = c_{k}|A = a_{j})}}</script><p>同样可以用经验条件熵来估计真正的条件熵：</p>
<script type="math/tex; mode=display">
H\left( y \middle| A \right) = H\left( y \middle| D \right) = \sum_{j = 1}^{m}{\frac{\left| D_{j} \right|}{\left| D \right|}\sum_{k = 1}^{K}{\frac{|D_{\text{jk}}|}{|D_{j}|}\log\frac{|D_{\text{jk}}|}{|D_{j}|}}}</script><p>这里的<script type="math/tex">D_{j}</script>表示在<script type="math/tex">A = a_{j}</script>限制下的数据集。通常可以记<script type="math/tex">D_{j}</script>中的样本<script type="math/tex">y_{i}</script>满足<script type="math/tex">y_{i}^{A} = a_{j}</script>，亦即：</p>
<script type="math/tex; mode=display">
y_{i}^{A} = a_{j} \Leftrightarrow \left( A_{i},y_{i} \right) \in Y_{j} \Leftrightarrow A_{i} = a_{j}</script><p>而公式中的<script type="math/tex">|D_{\text{jk}}|</script>则代表着<script type="math/tex">D_{j}</script>中第 k 类样本的个数</p>
<p>从条件熵的直观含义，信息的增益就可以自然地定义为：</p>
<script type="math/tex; mode=display">
g\left( y,A \right) = H\left( y \right) - H(y|A)</script><p>这里的<script type="math/tex">g(y,A)</script>常被称为互信息（Mutual<br>Information），决策树中的 ID3 算法即是利用它来作为特征选取的标准的（相关定义会在后文给出）。但是，如果简单地以<script type="math/tex">g(y,A)</script>作为标准的话，会存在偏向于选择取值较多的特征、也就是 m 比较大的特征的问题。我们仍然可以从直观上去理解为什么会偏向于选取 m 较大的特征以及为什么这样做是不尽合理的：</p>
<ul>
<li>我们希望得到的决策树应该是比较深（又不会太深）的决策树，从而它可以基于多个方面而不是片面地根据某些特征来判断</li>
<li>如果单纯以<script type="math/tex">g(y,A)</script>作为标准，由于<script type="math/tex">g(y,A)</script>的直观意义是<script type="math/tex">y</script>被<script type="math/tex">A</script>划分后不确定性的减少量，可想而知，当<script type="math/tex">A</script>的取值很多时，<script type="math/tex">y</script>会被<script type="math/tex">A</script>划分成很多份、于是其不确定性自然会减少很多、从而 ID3 算法会倾向于选择<script type="math/tex">A</script>作为划分依据。但如果这样做的话，可以想象、我们最终得到的决策树将会是一颗很胖很矮的决策树，这并不是我们想要的</li>
</ul>
<p>为解决该问题、我们可以给 m 一个惩罚，由此我们可以得到信息增益比（Information<br>Gain Ratio）的概念，该概念对应着 C4.5 算法：</p>
<script type="math/tex; mode=display">
g_{R}(y,A) = \frac{g(y,A)}{H_{A}(y)}</script><p>其中<script type="math/tex">H_{A}(y)</script>是<script type="math/tex">y</script>关于<script type="math/tex">A</script>的熵，它的定义为：</p>
<script type="math/tex; mode=display">
H_{A}\left( y \right) = - \sum_{j = 1}^{m}{p\left( y^{A} = a_{j} \right)\log{p(y^{A} = a_{j})}}</script><p>同样可以用经验熵来进行估计：</p>
<script type="math/tex; mode=display">
H_{A}\left( y \right) = H_{A}\left( D \right) = - \sum_{j = 1}^{m}{\frac{\left| D_{j} \right|}{\left| D \right|}\log\frac{\left| D_{j} \right|}{\left| D \right|}}</script><p>该定义式和信息熵的定义式很像，它们的性质也有相通之处</p>
<p>需要指出的是，只需要类比上述的过程、我们同样可以使用基尼系数来定义信息的增益。具体而言，我们可以先定义条件基尼系数：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \middle| A \right) = \sum_{j = 1}^{m}{p\left( A = a_{j} \right)\text{Gini}(y|A = a_{j})}</script><p>其中</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \middle| A = a_{j} \right) = 1 - \sum_{k = 1}^{K}{p^{2}\left( y = c_{k} \middle| A = a_{j} \right)}</script><p>同样可以用经验条件基尼系数来进行估计：</p>
<script type="math/tex; mode=display">
{\text{Gini}\left( y \middle| A \right) = \text{Gini}(y|D) = \sum_{j = 1}^{m}{\frac{\left| D_{i} \right|}{\left| D \right|}\left\lbrack 1 - \sum_{k = 1}^{K}\left( \frac{\left| D_{\text{jk}} \right|}{\left| D_{j} \right|} \right)^{2} \right\rbrack}\backslash n}{= 1 - \sum_{j = 1}^{m}\frac{\left| D_{j} \right|}{\left| D \right|}\sum_{k = 1}^{K}\left( \frac{\left| D_{\text{Jk}} \right|}{\left| D_{j} \right|} \right)^{2}}</script><p>信息的增益则自然地定义为（不妨称之为“基尼增益”）：</p>
<script type="math/tex; mode=display">
g_{\text{Gini}}\left( y,A \right) = \text{Gini}\left( y \right) - \text{Gini}\left( y \middle| A \right)</script><p>决策树算法中的CART算法通常会应用这种定义</p>
]]></content>
      
        <categories>
            
            <category> 决策树 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[决策树综述]]></title>
      <url>/posts/dd35ec8b/</url>
      <content type="html"><![CDATA[<p>上个系列讲的朴素贝叶斯模型的理论基础大部分是数理统计和概率论相关的东西，可能从直观上不太好理解。这一章我们会讲解一种可以说是从直观上最好理解的模型——决策树。决策树是听上去比较厉害且又相对简单的模型，虽然它用到的数学知识确实不怎么多、但是在实现它的过程中可能可以获得对编程本身更深的理解，尤其是对递归的利用这一块可能会有更深的体会</p>
<p>以下是目录：</p>
<ul>
<li><a href="/posts/2ce87ace/" title="数据的信息">数据的信息</a></li>
<li><a href="/posts/c6faa205/" title="决策树的生成算法">决策树的生成算法</a></li>
<li><a href="/posts/e0705aab/" title="信息量计算的实现">信息量计算的实现</a></li>
<li><a href="/posts/41abb98b/" title="节点结构的实现">节点结构的实现</a></li>
<li><a href="/posts/b07c81ec/" title="树结构的实现">树结构的实现</a></li>
<li><a href="/posts/1a7aa546/" title="决策树的剪枝算法">决策树的剪枝算法</a></li>
<li><a href="/posts/602f7125/" title="剪枝算法的实现">剪枝算法的实现</a></li>
<li><a href="/posts/c12a819/" title="评估与可视化">评估与可视化</a></li>
<li><a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a></li>
<li><a href="/posts/88953f51/" title="“决策树”小结">“决策树”小结</a></li>
</ul>
]]></content>
      
        <categories>
            
            <category> 决策树 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
            <tag> 目录 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[“朴素贝叶斯”小结]]></title>
      <url>/posts/a75c0d1b/</url>
      <content type="html"><![CDATA[<ul>
<li>贝叶斯学派强调概率的“主观性”、而频率学派则强调“自然属性”</li>
<li>常见的参数估计有 ML 估计和 MAP 估计两种，其中 MAP 估计比 ML 估计多了对数先验概率这一项，体现了贝叶斯学派的思想</li>
<li>朴素贝叶斯算法下的模型一般分为三类：离散型、连续型和混合型。其中，离散型朴素贝叶斯不但能够进行对离散型数据进行分类、还能进行特征提取和可视化</li>
<li>朴素贝叶斯是简单而高效的算法，它是损失函数为 0-1 函数下的贝叶斯决策。朴素贝叶斯的基本假设是条件独立性假设，该假设一般来说太过苛刻，视情况可以通过另外两种贝叶斯分类器算法——半朴素贝叶斯和贝叶斯网来弱化</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 朴素贝叶斯 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 小结 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[推导与推广]]></title>
      <url>/posts/e312d61a/</url>
      <content type="html"><![CDATA[<p>本文旨在解决如下两个问题：</p>
<ul>
<li>为何后验概率最大化是贝叶斯决策？</li>
<li>如何导出离散型朴素贝叶斯的算法？</li>
</ul>
<p>以及旨在叙述一些朴素贝叶斯的推广。具体而言、我们会简要介绍：</p>
<ul>
<li>半朴素贝叶斯</li>
<li>贝叶斯网</li>
</ul>
<a id="more"></a>
<h1 id="朴素贝叶斯与贝叶斯决策"><a href="#朴素贝叶斯与贝叶斯决策" class="headerlink" title="朴素贝叶斯与贝叶斯决策"></a>朴素贝叶斯与贝叶斯决策</h1><p>可以证明，应用朴素贝叶斯算法得到的模型所做的决策就是 0-1 损失函数下的贝叶斯决策。这里先说一个直观：在损失函数为 0-1 损失函数的情况下，决策风险、亦即训练集的损失的期望就是示性函数某种线性组合的期望、从而就是相对应的概率；朴素贝叶斯本身就是运用相应的概率做决策、所以可以想象它们很有可能等价</p>
<p>下给出推导过程，首先我们要叙述一个定理：令<script type="math/tex">\rho(x_1,...,x_n)</script>满足：</p>
<script type="math/tex; mode=display">
\rho\left( x_{1},\ldots,x_{n} \right) = \inf_{a\in A}{\int_{\Theta}^{}{L\left( \theta,a \right)\xi\left( \theta \middle| x_{1},\ldots,x_{n} \right)\text{d}\theta}}</script><p>亦即<script type="math/tex">\rho(x_1,...,x_n)</script>是已知训练集<script type="math/tex">\tilde X=(x_1,...,x_n)</script>的最小后验期望损失。那么如果一个决策<script type="math/tex">\delta^*(x_1,...,x_n)</script>能能使任意一个含有 n 个样本的训练集的后验期望损失达到最小、亦即：</p>
<script type="math/tex; mode=display">
\int_{\Theta}^{}{L\left( \theta,\delta^{*}\left( x_{1},\ldots,x_{n} \right) \right)\xi\left( \theta \middle| x_{1},\ldots,x_{n} \right)d\theta = \rho\left( x_{1},\ldots,x_{n} \right)}\ (\forall x_{1},\ldots,x_{n})</script><p>的话，那么<script type="math/tex">\delta^*</script>就是贝叶斯决策。该定理的数学证明要用到比较深的数学知识、这里从略，但从直观上来说是可以理解的</p>
<p>是故如果我们想证明朴素贝叶斯算法能导出贝叶斯决策、我们只需证明它能使任一个训练集<script type="math/tex">\tilde X</script>上的后验期望损失<script type="math/tex">R\left( \theta,\delta(\tilde X)\right)</script>最小即可。为此，需要先计算<script type="math/tex">R\left( \theta,\delta(\tilde X)\right)</script>：</p>
<p><strong><em>注意：这里的期望是对联合分布取的，所以可以取成条件期望</em></strong></p>
<script type="math/tex; mode=display">
R\left( \theta,\delta(\tilde{X}) \right) = EL\left( \theta,\delta\left( \tilde{X} \right) \right) = E_{X}\sum_{k = 1}^{K}{\tilde{L}(c_{k},f\left( X \right))p(c_{k}|X)}</script><p>为了使上式达到最小，我们只需逐个对<script type="math/tex">X=x</script>最小化，从而有：</p>
<script type="math/tex; mode=display">
\begin{align}
  f\left( x \right) &= \arg{\min_{y\in S}{\sum_{k = 1}^{K}{\tilde{L}\left( c_{k},y \right)p\left( c_{k} \middle| X = x \right)}}} \\

  &= \arg{\min_{y\in S}{\sum_{k = 1}^{K}{p\left( y \neq c_{k} \middle| X = x \right)}}} \\

  &= \arg{\min_{c_k}\left\lbrack 1 - p\left( c_{k} \middle| X = x \right) \right\rbrack} \\

  &= \arg{\max_{c_k}{p\left( c_{k} \middle| X = x \right)}}
\end{align}</script><p>此即后验概率最大化准则、也就是朴素贝叶斯所采用的原理</p>
<h1 id="离散型朴素贝叶斯算法的推导"><a href="#离散型朴素贝叶斯算法的推导" class="headerlink" title="离散型朴素贝叶斯算法的推导"></a>离散型朴素贝叶斯算法的推导</h1><p>离散型朴素贝叶斯算法的推导相对简单但比较繁琐，核心的思想是利用示性函数将对数似然函数写成比较“整齐”的形式、再运用拉格朗日方法进行求解</p>
<p>在正式推导前，我们先说明一下符号约定：</p>
<ul>
<li>记已有的数据为<script type="math/tex">\tilde X=(x_1,x_2,...,x_N)</script>，其中：  <script type="math/tex; mode=display">
x_{i} = \left( x_{i}^{\left( 1 \right)},x_{i}^{\left( 2 \right)},\cdots,x_{i}^{\left( n \right)} \right)^{T}\ (i = 1,2,\cdots,N)</script></li>
<li><script type="math/tex">X^{\left( j \right)}</script>表示生成数据<script type="math/tex">x^{\left( j \right)}</script>的随机变量</li>
<li>随机变量<script type="math/tex">X^{\left( j \right)}</script>的取值限制在集合<script type="math/tex">K_{j} = \{ a_{j1},a_{j2},\ldots,a_{jS_j}\}\ (j = 1,2,\cdots,n)</script>中</li>
<li>类别<script type="math/tex">Y</script>的可能取值集合为<script type="math/tex">S = \{ c_{1},c_{2},\ldots,c_{K}\}</script></li>
<li>用<script type="math/tex">\theta^{c_{k}}(k = 1,2,\ldots,K)</script>表示先验概率<script type="math/tex">p(Y = c_{k})</script></li>
<li>用<script type="math/tex">\theta_{j,a_{jl}}^{c_{k}}</script>表示条件概率<script type="math/tex">p(X^{\left( j \right)} = a_{jl}|Y = c_{k})\ (j \in \left\{ 1,\ldots,n \right\},l \in \{ 1,\ldots,S_{j}\},k \in \{ 1,\ldots,K\}</script></li>
</ul>
<p>接下来就可以开始算法推导了：</p>
<h2 id="计算对数似然函数"><a href="#计算对数似然函数" class="headerlink" title="计算对数似然函数"></a>计算对数似然函数</h2><script type="math/tex; mode=display">
\begin{align}
  \ln L &= \ln{\prod_{i = 1}^{N}\left( \theta^{y_{i}} \cdot \prod_{j = 1}^{n}{\theta_{j,x_{i}^{\left( j \right)}}^{y_{i}}\ } \right)} \\

  &= \sum_{k = 1}^{K}{n_{k}\ln{\theta^{k} + \sum_{j = 1}^{n}{\sum_{k = 1}^{K}{\sum_{l = 1}^{S_{j}}{n_{j,l}^{k}\ln\theta_{j,a_{jl}}^{c_{k}}}\ }}}}
\end{align}</script><p>其中</p>
<script type="math/tex; mode=display">
\begin{align}
  n_{s} &= \sum_{i = 1}^{N}{I\left( y_{i} = c_{s} \right)} \\

  n_{j,l}^{k} &= \sum_{i = 1}^{N}{I\left( x_{i}^{\left( j \right)} = a_{jl}{,y}_{i} = c_{k} \right)}
\end{align}</script><h2 id="极大化似然函数"><a href="#极大化似然函数" class="headerlink" title="极大化似然函数"></a>极大化似然函数</h2><p>为此，只需分别最大化</p>
<script type="math/tex; mode=display">
f_{1} = \sum_{k = 1}^{K}{n_{k}\ln\theta^{k}}</script><p>和</p>
<script type="math/tex; mode=display">
f_{2} = \sum_{j = 1}^{n}{\sum_{k = 1}^{K}{\sum_{l = 1}^{S_{j}}{n_{j,l}^{k}\ln\theta_{j,a_{jl}}^{c_{k}}}\ }}</script><p>对于后者，由条件独立性假设可知、我们只需要对<script type="math/tex">j=1,2,...,n</script>分别最大化：</p>
<script type="math/tex; mode=display">f_{2}^{\left( j \right)} = \sum_{k = 1}^{K}{\sum_{l = 1}^{S_{j}}{n_{j,l}^{k}\ln\theta_{j,a_{jl}}^{c_{k}}}\ }</script><p>即可。我们利用拉格朗日方法来求解该问题，用到的约束条件是：</p>
<script type="math/tex; mode=display">
\begin{align}
  \sum_{k = 1}^{K}\theta^{k} &= \sum_{k = 1}^{K}{p\left( Y = c_{k} \right)} = 1 \\

  \sum_{l = 1}^{S_{j}}{\theta_{j,l}^{k}} &= \sum_{l = 1}^{S_{j}}{p\left( X^{\left( j \right)} = a_{jl} \middle| Y = c_{k} \right) = 1\ \left( \forall k \in \left\{ 1,\ldots,K \right\},j \in \left\{ 1,\ldots,n \right\} \right)}
\end{align}</script><p>从而可知</p>
<script type="math/tex; mode=display">
L_{1} = \sum_{k = 1}^{K}{n_{k}\ln{\theta^{k} + \alpha\left( \sum_{k = 1}^{K}{\theta^{k} - 1} \right)}}</script><p>由一阶条件</p>
<script type="math/tex; mode=display">
\frac{\partial L_{1}}{\partial\theta_{k}} = \frac{\partial L_{1}}{\partial\alpha} = 0</script><p>可以解得</p>
<script type="math/tex; mode=display">
p\left( Y = c_{k} \right) = \theta^{k} = \frac{n_{k}}{N} = \frac{\sum_{i = 1}^{N}{I(y_{i} = c_{k})}}{N}</script><p>同理，对<script type="math/tex">f_2^{(j)}</script>应用拉格朗日方法，可以得到</p>
<script type="math/tex; mode=display">
p\left( X^{\left( j \right)} = a_{jl} \middle| Y = c_{k} \right) = \theta_{j,l}^{k} = \frac{n_{j,l}^{k}}{\sum_{i = 1}^{N}{I(y_{i} = c_{k})}} = \frac{\sum_{i = 1}^{N}{I(x_{i}^{\left( j \right)} = a_{jl},y_{i} = c_{k})}}{\sum_{i = 1}^{N}{I(y_{i} = c_{k})}}</script><p>以上，我们完成了离散型朴素贝叶斯算法的推导</p>
<h1 id="半朴素贝叶斯"><a href="#半朴素贝叶斯" class="headerlink" title="半朴素贝叶斯"></a>半朴素贝叶斯</h1><p>由于提出条件独立性假设的原因正是联合概率难以求解，所以在弱化假设的时候同样应该避免引入过多的联合概率，这也正是半朴素贝叶斯的基本想法。比较常见的半朴素贝叶斯算法有如下三种：</p>
<h2 id="ODE-算法（One-Dependent-Estimator，可译为“独依赖估计”）"><a href="#ODE-算法（One-Dependent-Estimator，可译为“独依赖估计”）" class="headerlink" title="ODE 算法（One-Dependent Estimator，可译为“独依赖估计”）"></a>ODE 算法（One-Dependent Estimator，可译为“独依赖估计”）</h2><p><del>（常微分方程：？？？）</del><br>顾名思义，在该算法中、各个维度的特征至多依赖一个其它维度的特征。从公式上来说，它在描述条件概率时会多出一个条件：</p>
<script type="math/tex; mode=display">
p\left( c_{k} \middle| X = x \right) = p\left( y = c_{k} \right)\prod_{i = 1}^{n}{p\left( X^{\left( j \right)} = x^{\left( j \right)} \middle| Y = c_{k},X^{\left( pa_{j} \right)} = x^{\left( pa_{j} \right)} \right)}</script><p>这里的<script type="math/tex">\text{pa}_{j}</script>代表着维度 j 所“独依赖”的维度</p>
<h2 id="SPODE-算法（Super-Parent-ODE，可译为“超父独依赖估计”）"><a href="#SPODE-算法（Super-Parent-ODE，可译为“超父独依赖估计”）" class="headerlink" title="SPODE 算法（Super-Parent ODE，可译为“超父独依赖估计”）"></a>SPODE 算法（Super-Parent ODE，可译为“超父独依赖估计”）</h2><p>这是 ODE 算法的一个特例。在该算法中，所有维度的特征都独依赖于同一个维度的特征，这个被共同依赖的特征就叫“超父（Super-Parent）”。若它的维度是第 pa 维，知：</p>
<script type="math/tex; mode=display">
p\left( c_{k} \middle| X = x \right) = p\left( y = c_{k} \right)\prod_{i = 1}^{n}{p\left( X^{\left( j \right)} = x^{\left( j \right)} \middle| Y = c_{k},X^{\left( \text{pa} \right)} = x^{\left( \text{pa} \right)} \right)}</script><p>一般而言，会选择通过交叉验证来选择超父</p>
<h2 id="AODE-算法（Averaged-One-Dependent-Estimator，可译为“集成独依赖估计”）"><a href="#AODE-算法（Averaged-One-Dependent-Estimator，可译为“集成独依赖估计”）" class="headerlink" title="AODE 算法（Averaged One-Dependent Estimator，可译为“集成独依赖估计”）"></a>AODE 算法（Averaged One-Dependent Estimator，可译为“集成独依赖估计”）</h2><p>这种算法背后有提升方法的思想。AODE 算法会利用 SPODE 算法并尝试把许多个训练后的、有足够的训练数据量支撑的SPODE模型集成在一起来构建最终的模型。一般来说，AODE 会以所有维度的特征作为超父训练 n 个 SPODE 模型、然后线性组合出最终的模型</p>
<h1 id="贝叶斯网"><a href="#贝叶斯网" class="headerlink" title="贝叶斯网"></a>贝叶斯网</h1><p>贝叶斯网又称“信念网（Belief Network）”，比起朴素贝叶斯来说、它背后还蕴含了图论的思想。贝叶斯网有许多奇妙的性质，详细的讨论不可避免地要使用到图论的术语，这里仅拟对其做一个直观的介绍。 贝叶斯网既然带了“网”字，它的结构自然可以直观地想成是一张网络，其中：</p>
<ul>
<li>网络的节点就是单一样本的各个维度上的随机变量<script type="math/tex">X^{(1)},...,X^{(n)}</script> </li>
<li>连接节点的边就是节点之间的依赖关系</li>
</ul>
<p>需要注意的是，贝叶斯网一般要求这些边是“有方向的”、同时整张网络中不能出现“环”。无向的贝叶斯网通常是由有向贝叶斯网无向化得到的，此时它被称为 moral graph（除了把所有有向边改成无向边以外，moral graph 还需要将有向网络中不相互独立的随机变量之间连上一条无向边，细节不表），基于它能够非常直观、迅速地看出变量间的条件独立性</p>
<p>显然，有了代表各个维度随机变量的节点和代表这些节点之间的依赖关系的边之后，各个随机变量之间的条件依赖关系都可以通过这张网络表示出来。类似的东西在条件随机场中也有用到，可以说是一个适用范围非常宽泛的思想</p>
<p>贝叶斯网的学习在网络结构已经确定的情况下相对简单，其思想和朴素贝叶斯相去无多：只需要对训练集相应的条件进行“计数”即可，所以贝叶斯网的学习任务主要归结于如何找到最恰当的网络结构。常见的做法是定义一个用来打分的函数并基于该函数通过某种搜索手段来决定结构，但正如同很多最优化算法一样、在所有可能的结构空间中搜索最优结构是一个 NP 问题、无法在合理的时间内求解，所以一般会使用替代的方法求近似最优解。常见的方法有两种，一种是贪心法、比如：先定下一个初始的网络结构并从该结构出发，每次增添一条边、删去一条边或调整一条边的方向，期望通过这些手段能够使评分函数的值变大；另一种是直接限定假设空间、比如假设要求的贝叶斯网一定是一个树形的结构</p>
<p>相比起学习方法来说，贝叶斯网的决策方法相对来说显得比较不平凡。虽说最理想的情况是直接根据贝叶斯网的结构所决定的联合概率密度来计算后验概率，但是这样的计算被证明了是 NP 问题 [Cooper, 1990]。换句话说，只要贝叶斯网稍微复杂一点，这种精确的计算就无法在合理的时间内做完。所以我们同样要借助近似法求解，一种常见的做法是吉布斯采样（Gibbs Sampling），它的定义涉及到马尔科夫链相关的<del>（我还没有学的）</del>知识，这里就不详细展开了</p>
]]></content>
      
        <categories>
            
            <category> 朴素贝叶斯 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[MergedNB 的实现]]></title>
      <url>/posts/7c13f69c/</url>
      <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/b_NaiveBayes/Vectorized/MergedNB.py" target="_blank" rel="external">这里</a>）</p>
<p>本文主要介绍混合型朴素贝叶斯—— MergedNB 的实现。（我知道的）混合型朴素贝叶斯算法主要有两种提法：</p>
<ul>
<li>用某种分布的密度函数算出训练集中各个样本连续型特征相应维度的密度之后，根据这些密度的情况将该维度离散化、最后再训练离散型朴素贝叶斯模型</li>
<li>直接结合离散型朴素贝叶斯和连续型朴素贝叶斯：  <script type="math/tex; mode=display">
y = f(x^{*}) = \arg{\max_{c_k}{p\left( y = c_{k} \right)\prod_{j \in S_{1}}^{}{p(X^{\left( j \right)} = {x^{*}}^{\left( j \right)}|y = c_{k})}\prod_{j \in S_{2}}^{}{p(X^{j} = {x^{*}}^{\left( j \right)}|y = c_{k})}}}</script></li>
</ul>
<p>从直观可以看出、第二种提法可能会比第一种提法要“激进”一些，因为如果某个连续型维度采用的分布特别“大起大落”的话、该维度可能就会直接“主导”整个决策。但是考虑到实现的简洁和直观（……），我们还是演示第二种提法的实现。感兴趣的观众老爷可以尝试实现第一种提法，思路和过程都是没有太本质的区别的、只是会繁琐不少</p>
<a id="more"></a>
<p>我们可以对气球数据集 1.0 稍作变动、将“气球大小”这个特征改成“气球直径”，然后我们再手动做一次分类以加深对混合型朴素贝叶斯算法的理解。新数据集如下表所示（不妨称之为气球数据集 2.0）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>直径</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>10</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>15</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>9</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>9</td>
<td>小孩</td>
<td>用脚踩</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>19</td>
<td>成人</td>
<td>用手打</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>21</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>16</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>22</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>10</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>12</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>22</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>21</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>该数据集的电子版本可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/_Data/balloon2.0.txt" target="_blank" rel="external">这里</a>。我们想预测的是样本：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>大小</th>
<th>测试人员</th>
<th>测试动作</th>
</tr>
</thead>
<tbody>
<tr>
<td>  紫色</td>
<td>10</td>
<td>小孩</td>
<td>用脚踩</td>
</tr>
</tbody>
</table>
</div>
<p>除了“大小”变成了“直径”、其余特征都一点未变，所以我们只需再计算直径的条件概率（密度）即可。由 GaussianNB 的算法可知：</p>
<script type="math/tex; mode=display">{\hat{\mu}}_{不爆炸} = \frac{10 + 9 + 9 + 16 + 10 + 12}{6} = 11</script><script type="math/tex; mode=display">{\hat{\mu}}_{爆炸} = \frac{15 + 19 + 21 + 22 + 22 + 21}{6} = 20</script><script type="math/tex; mode=display">{\hat{\sigma}}_{不爆炸} = \frac{1}{6}\left\lbrack \left( 10 - {\hat{\mu}}_{不爆炸} \right)^{2} + \ldots + \left( 12 - {\hat{\mu}}_{不爆炸} \right)^{2} \right\rbrack = 6</script><script type="math/tex; mode=display">{\hat{\sigma}}_{爆炸} = \frac{1}{6}\left\lbrack \left( 15 - {\hat{\mu}}_{爆炸} \right)^{2} + \ldots + \left( 21 - {\hat{\mu}}_{爆炸} \right)^{2} \right\rbrack = 6</script><p>从而</p>
<script type="math/tex; mode=display">
\hat{p}\left( 不爆炸\right) = \frac{1}{\sqrt{2\pi}{\hat{\sigma}}_{不爆炸}}e^{- \frac{\left( 10 - {\hat{\mu}}_{不爆炸} \right)^{2}}{2{\hat{\sigma}}^{2}_{不爆炸}}} \times p\left( 小孩\middle| 不爆炸\right) \times p\left( 用脚踩\middle| 不爆炸\right) \approx 0.0073</script><script type="math/tex; mode=display">
\hat{p}\left( 爆炸\right) = \frac{1}{\sqrt{2\pi}{\hat{\sigma}}_{爆炸}}e^{- \frac{\left( 10 - {\hat{\mu}}_{爆炸} \right)^{2}}{2{\hat{\sigma}}^{2}_{爆炸}}} \times p\left( 小孩\middle| 爆炸\right) \times p\left( 用脚踩\middle| 爆炸\right) \approx 0.0046</script><p>因此我们应该认为给定样本所导致的结果是“不爆炸”，这和直观大体相符。接下来看看具体应该如何进行实现，首先是初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> b_NaiveBayes.Original.Basic <span class="keyword">import</span> *</div><div class="line"><span class="keyword">from</span> b_NaiveBayes.Original.MultinomialNB <span class="keyword">import</span> MultinomialNB</div><div class="line"><span class="keyword">from</span> b_NaiveBayes.Original.GaussianNB <span class="keyword">import</span> GaussianNB</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MergedNB</span><span class="params">(NaiveBayes)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._whether_discrete：记录各个维度的变量是否是离散型变量</div><div class="line">        self._whether_continuous：记录各个维度的变量是否是连续型变量</div><div class="line">        self._multinomial、self._gaussian：离散型、连续型朴素贝叶斯模型</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, whether_continuous)</span>:</span></div><div class="line">        self._multinomial, self._gaussian = (</div><div class="line">            MultinomialNB(), GaussianNB()</div><div class="line">        <span class="keyword">if</span> whether_continuous <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            self._whether_discrete = self._whether_continuous = <span class="keyword">None</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self._whether_continuous = np.array(whether_continuous)</div><div class="line">            self._whether_discrete = ~self._whether_continuous</div></pre></td></tr></table></figure>
<p>然后是和模型的训练相关的实现，这一块将会大量重用之前在 MultinomialNB 和 GaussianNB 里面写过的东西：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_data</span><span class="params">(self, x, y, sample_weight=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        sample_weight = np.array(sample_weight)</div><div class="line">    x, y, wc, features, feat_dics, label_dic = DataUtil.quantize_data(</div><div class="line">        x, y, wc=self._whether_continuous, separate=<span class="keyword">True</span>)</div><div class="line">    <span class="comment"># 若没有指定哪些维度连续，则用 quantize_data 中朴素的方法判定哪些维度连续</span></div><div class="line">    <span class="keyword">if</span> self._whether_continuous <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        self._whether_continuous = wc</div><div class="line">        <span class="comment"># 通过Numpy中对逻辑非的支持进行快速运算</span></div><div class="line">        self._whether_discrete = ~self._whether_continuous</div><div class="line">    <span class="comment"># 计算通用变量</span></div><div class="line">    self.label_dic = label_dic</div><div class="line">    discrete_x, continuous_x = x</div><div class="line">    cat_counter = np.bincount(y)</div><div class="line">    self._cat_counter = cat_counter</div><div class="line">    labels = [y == value <span class="keyword">for</span> value <span class="keyword">in</span> range(len(cat_counter))]</div><div class="line">    <span class="comment"># 训练离散型朴素贝叶斯</span></div><div class="line">    labelled_x = [discrete_x[ci].T <span class="keyword">for</span> ci <span class="keyword">in</span> labels]</div><div class="line">    self._multinomial._x, self._multinomial._y = x, y</div><div class="line">    self._multinomial._labelled_x, self._multinomial._label_zip = (</div><div class="line">        labelled_x, list(zip(labels, labelled_x)))</div><div class="line">    self._multinomial._cat_counter = cat_counter</div><div class="line">    self._multinomial._feat_dics = [_dic</div><div class="line">        <span class="keyword">for</span> i, _dic <span class="keyword">in</span> enumerate(feat_dics) <span class="keyword">if</span> self._whether_discrete[i]]</div><div class="line">    self._multinomial._n_possibilities = [len(feats)</div><div class="line">        <span class="keyword">for</span> i, feats <span class="keyword">in</span> enumerate(features) <span class="keyword">if</span> self._whether_discrete[i]]</div><div class="line">    self._multinomial.label_dic = label_dic</div><div class="line">    <span class="comment"># 训练连续型朴素贝叶斯</span></div><div class="line">    labelled_x = [continuous_x[label].T <span class="keyword">for</span> label <span class="keyword">in</span> labels]</div><div class="line">    self._gaussian._x, self._gaussian._y = continuous_x.T, y</div><div class="line">    self._gaussian._labelled_x, self._gaussian._label_zip = labelled_x, labels</div><div class="line">    self._gaussian._cat_counter, self._gaussian.label_dic = cat_counter, label_dic</div><div class="line">    <span class="comment"># 处理样本权重</span></div><div class="line">    self._feed_sample_weight(sample_weight)</div><div class="line"></div><div class="line"><span class="comment"># 分别利用 MultinomialNB 和 GaussianNB 处理样本权重的方法来处理样本权重</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_sample_weight</span><span class="params">(self, sample_weight=None)</span>:</span></div><div class="line">    self._multinomial.feed_sample_weight(sample_weight)</div><div class="line">    self._gaussian.feed_sample_weight(sample_weight)</div><div class="line"></div><div class="line"><span class="comment"># 分别利用 MultinomialNB 和 GaussianNB 的训练函数来进行训练</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_fit</span><span class="params">(self, lb)</span>:</span></div><div class="line">    self._multinomial.fit()</div><div class="line">    self._gaussian.fit()</div><div class="line">    p_category = self._multinomial.get_prior_probability(lb)</div><div class="line">    discrete_func, continuous_func = (</div><div class="line">        self._multinomial[<span class="string">"func"</span>], self._gaussian[<span class="string">"func"</span>])</div><div class="line">    <span class="comment"># 将 MultinomialNB 和 GaussianNB 的决策函数直接合成最终决策函数</span></div><div class="line">    <span class="comment"># 由于这两个决策函数都乘了先验概率、我们需要除掉一个先验概率</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(input_x, tar_category)</span>:</span></div><div class="line">        input_x = np.array(input_x)</div><div class="line">        <span class="keyword">return</span> discrete_func(</div><div class="line">            input_x[self._whether_discrete].astype(</div><div class="line">                np.int), tar_category) * continuous_func(</div><div class="line">            input_x[self._whether_continuous], tar_category) / p_category[tar_category]</div><div class="line">    <span class="keyword">return</span> func</div></pre></td></tr></table></figure>
<p><del>（又臭又长啊喂)</del></p>
<p>上述实现有一个显而易见的可以优化的地方：我们一共在代码中重复计算了三次先验概率、但其实只用计算一次就可以。考虑到这一点不是性能瓶颈，为了代码的连贯性和可读性、我们就没有进行这个优化<del>（？？？）</del></p>
<p>数据转换函数则相对而言要复杂一点，因为我们需要跳过连续维度、将离散维度挑出来进行数值化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 实现转换混合型数据的方法，要注意利用 MultinomialNB 的相应变量</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_transfer_x</span><span class="params">(self, x)</span>:</span></div><div class="line">    _feat_dics = self._multinomial[<span class="string">"feat_dics"</span>]</div><div class="line">    idx = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> d, discrete <span class="keyword">in</span> enumerate(self._whether_discrete):</div><div class="line">        <span class="comment"># 如果是连续维度，直接调用 float 方法将其转为浮点数</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> discrete:</div><div class="line">            x[d] = float(x[d])</div><div class="line">        <span class="comment"># 如果是离散维度，利用转换字典进行数值化</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            x[d] = _feat_dics[idx][x[d]]</div><div class="line">        <span class="keyword">if</span> discrete:</div><div class="line">            idx += <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure>
<p>至此，混合型朴素贝叶斯模型就搭建完毕了。为了比较合理地对它进行评估，我们不妨采用 UCI 上一个我认为有些病态的数据集进行测试。问题的描述大概可以概括如下：</p>
<p>“训练数据包含了某银行一项业务的目标客户的信息、电话销售记录以及后来他是否购买了这项业务的信息。我们希望做到：根据客户的基本信息和历史联系记录，预测他是否会购买这项业务”。UCI 上的原问题描述则如下图所示：</p>
<img src="/posts/7c13f69c/p1.png" alt="p1.png" title="">
<p>概括其主要内容、就是它是一个有 17 个属性的二类分类问题。之所以我认为它是病态的，是因为我发现即使是 17 个属性几乎完全一样的两个人，他们选择是否购买业务的结果也会截然相反。事实上从心理学的角度来说，想要很好地预测人的行为确实是一项非常困难的事情、尤其是当该行为直接牵扯到较大的利益时</p>
<p>完整的数据集可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/_Data/bank1.0.txt" target="_blank" rel="external">这里</a>（最后一列数据是类别）。按照数据的特性、我们可以通过和之前用来评估 MultinomialNB 的代码差不多的代码（注意额外定义一个记录离散型维度的数组即可）得出如下图所示的结果：</p>
<img src="/posts/7c13f69c/p2.png" alt="p2.png" title="">
<p>虽然准确率达到了 89%左右，但其实该问题不应该用准确率作为评判的标准。因为如果我们观察数据就会发现、数据存在着严重的非均衡现象。事实上，88%的客户最终都是没有购买这个业务的、但我们更关心的是那一小部分购买了业务的客户，这种情况我们通常会用 F1-score 来衡量模型的好坏。此外，该问题非常需要人为进行数据清洗、因为其原始数据非常杂乱。此外，我们可以对该问题中的各个离散维度进行可视化。该数据共 9 个离散维度，我们可以将它们合并在同一个图中以方便获得该数据离散部分的直观（如下图所示；由于各个特征的各个取值通常比较长（比如”manager”之类的），为整洁、我们直接将横坐标置为等差数列而没有进行转换）：</p>
<img src="/posts/7c13f69c/p3.png" alt="p3.png" title="">
<p>其中天蓝色代表类别 yes、亦即购买了业务；橙色则代表 no、亦即没有购买业务。可以看到、所有离散维度的特征都是前面所说的“无足轻重”的特征</p>
<p>连续维度的可视化是几乎同理的，唯一的差别在于它不是柱状图而是正态分布密度函数的函数曲线。具体的代码实现从略、感兴趣的观众老爷们可以尝试动手实现一下，这里仅放出程序运行的结果。该数据共 7 个连续维度，我们同样把它们放在同一个图中：</p>
<img src="/posts/7c13f69c/p4.png" alt="p4.png" title="">
<p>其中，天蓝色曲线代表类别 yes、橙色曲线代表类别 no。可以看到，两种类别的数据在各个维度上的正态分布的均值、方差都几乎一致</p>
<p>从以上的分析已经可以比较直观地感受到、该问题确实相当病态。特别地，考虑到朴素贝叶斯的算法、不难想象此时的混合型朴素贝叶斯模型基本就只是根据各类别的先验概率来进行分类决策</p>
<p>至此，朴素贝叶斯算法的理论、实现就差不多都说了一遍。作为收尾，下篇文章会补上之前没有展开叙述的一些细节、同时也会简要地介绍一下其余的贝叶斯分类器</p>
]]></content>
      
        <categories>
            
            <category> 朴素贝叶斯 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[GaussianNB 的实现]]></title>
      <url>/posts/c836ba35/</url>
      <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/b_NaiveBayes/Vectorized/GaussianNB.py" target="_blank" rel="external">这里</a>）</p>
<p>本文主要介绍连续型朴素贝叶斯——GaussianNB 的实现。在有了实现离散型朴素贝叶斯的经验后，实现连续型朴素贝叶斯模型其实只是个触类旁通的活了</p>
<a id="more"></a>
<p>不过在介绍实现之前，我们还是需要先要知道连续型朴素贝叶斯的算法是怎样的。处理连续型变量有一个最直观的方法：使用小区间切割、直接使其离散化。由于这种方法较难控制小区间的大小、而且对训练集质量的要求比较高，所以我们选用第二种方法：假设该变量服从正态分布（或称高斯分布，Gaussian Distribution）、再利用极大似然估计来计算该变量的“条件概率”。具体而言、GaussianNB 通过如下公式计算“条件概率”<script type="math/tex">p(X^{\left( j \right)} = a_{jl}|y = c_{k})</script>：</p>
<script type="math/tex; mode=display">
p\left( X^{\left( j \right)} = a_{jl} \middle| y = c_{k} \right) = \frac{1}{\sqrt{2\pi}\sigma_{jk}}e^{- \frac{\left( a_{jl} - \mu_{jk} \right)^{2}}{2\sigma_{jk}^{2}}}</script><p>这里有两个参数：<script type="math/tex">\mu_{jk}</script>、<script type="math/tex">\sigma_{jk}</script>，它们可以用极大似然估计法定出：</p>
<script type="math/tex; mode=display">
{\hat{\sigma}}_{jk}^{2} = \frac{1}{N_{k}}\sum_{i = 1}^{N}{\left( x_{i}^{\left( j \right)} - \mu_{jk} \right)^{2}I(y_{i} = c_{k})}</script><script type="math/tex; mode=display">
{\hat{\mu}}_{jk} = \frac{1}{N_{k}}\sum_{i = 1}^{N}{x_{i}^{\left( j \right)}I(y_{i} = c_{k})}</script><p>其中，<script type="math/tex">N_{k} = \sum_{i = 1}^{N}{I(y_{i} = c_{k})}</script>是类别<script type="math/tex">c_{k}</script>的样本数。需要注意的是，这里的“条件概率”其实是“条件概率密度”，真正的条件概率其实是 0（因为连续型变量单点概率为 0）。这样做的合理性涉及到了比较深的概率论知识，此处不表<del>（其实我想表也表不出来）</del></p>
<p>所以在实现 GaussianNB 之前、我们需要先实现一个能够计算正态分布密度和进行正态分布极大似然估计的类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> pi, exp</div><div class="line"></div><div class="line"><span class="comment"># 记录常量以避免重复运算</span></div><div class="line">sqrt_pi = (<span class="number">2</span> * pi) ** <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">NBFunctions</span>:</span></div><div class="line">    <span class="comment"># 定义正态分布的密度函数</span></div><div class="line"><span class="meta">    @staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gaussian</span><span class="params">(x, mu, sigma)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.exp(</div><div class="line">            -(x - mu) ** <span class="number">2</span> / (<span class="number">2</span> * sigma)) / (sqrt_pi * sigma ** <span class="number">0.5</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 定义进行极大似然估计的函数</span></div><div class="line">    <span class="comment"># 它能返回一个存储着计算条件概率密度的函数的列表</span></div><div class="line"><span class="meta">    @staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gaussian_maximum_likelihood</span><span class="params">(labelled_x, n_category, dim)</span>:</span></div><div class="line">        mu = [np.sum(</div><div class="line">            labelled_x[c][dim]) / </div><div class="line">            len(labelled_x[c][dim]) <span class="keyword">for</span> c <span class="keyword">in</span> range(n_category)]</div><div class="line">        sigma = [np.sum(</div><div class="line">            (labelled_x[c][dim]-mu[c])**<span class="number">2</span>) / </div><div class="line">            len(labelled_x[c][dim]) <span class="keyword">for</span> c <span class="keyword">in</span> range(n_category)]</div><div class="line">        <span class="comment"># 利用极大似然估计得到的和、定义生成计算条件概率密度的函数的函数 func</span></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(_c)</span>:</span></div><div class="line">            <span class="function"><span class="keyword">def</span> <span class="title">sub</span><span class="params">(xx)</span>:</span></div><div class="line">                <span class="keyword">return</span> NBFunctions.gaussian(xx, mu[_c], sigma[_c])</div><div class="line">            <span class="keyword">return</span> sub</div><div class="line">        <span class="comment"># 利用 func 返回目标列表</span></div><div class="line">        <span class="keyword">return</span> [func(_c=c) <span class="keyword">for</span> c <span class="keyword">in</span> range(n_category)]</div></pre></td></tr></table></figure>
<p>对于 GaussianNB 本身，由于算法中只有条件概率相关的定义变了、所以只需要将相关的函数重新定义即可。此外，由于输入数据肯定是数值数据、所以数据预处理会简单不少（至少不用因为要对输入进行特殊的数值化处理而记录其转换字典了）。考虑到上一章说明 MultinomialNB 的实现时已经基本把我们框架的思想都说明清楚了，在接下来的 GaussianNB 的代码实现中、我们会适当地减少注释以提高阅读流畅度<del>（其实主要还是为了偷懒）</del>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> b_NaiveBayes.Original.Basic <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GaussianNB</span><span class="params">(NaiveBayes)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feed_data</span><span class="params">(self, x, y, sample_weight=None)</span>:</span></div><div class="line">        <span class="comment"># 简单地调用 Python 自带的 float 方法将输入数据数值化</span></div><div class="line">        x = np.array([list(map(</div><div class="line">            <span class="keyword">lambda</span> c: float(c), sample)) <span class="keyword">for</span> sample <span class="keyword">in</span> x])</div><div class="line">        <span class="comment"># 数值化类别向量</span></div><div class="line">        labels = list(set(y))</div><div class="line">        label_dic = &#123;label: i <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels)&#125;</div><div class="line">        y = np.array([label_dic[yy] <span class="keyword">for</span> yy <span class="keyword">in</span> y])</div><div class="line">        cat_counter = np.bincount(y)</div><div class="line">        labels = [y == value <span class="keyword">for</span> value <span class="keyword">in</span> range(len(cat_counter))]</div><div class="line">        labelled_x = [x[label].T <span class="keyword">for</span> label <span class="keyword">in</span> labels]</div><div class="line">        <span class="comment"># 更新模型的各个属性</span></div><div class="line">        self._x, self._y = x.T, y</div><div class="line">        self._labelled_x, self._label_zip = labelled_x, labels</div><div class="line">        self._cat_counter, self.label_dic = (</div><div class="line">            cat_counter, &#123;i: _l <span class="keyword">for</span> _l, i <span class="keyword">in</span> label_dic.items()&#125;</div><div class="line">        self.feed_sample_weight(sample_weight)</div></pre></td></tr></table></figure>
<p>可以看到，数据预处理这一步确实要轻松很多。接下来只需要再定义训练用的代码就行，它们和 MultinomialNB 中的实现也大同小异： </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义处理样本权重的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_sample_weight</span><span class="params">(self, sample_weight=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        local_weights = sample_weight * len(sample_weight)</div><div class="line">        <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(self._label_zip):</div><div class="line">            self._labelled_x[i] *= local_weights[label]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_fit</span><span class="params">(self, lb)</span>:</span></div><div class="line">    n_category = len(self._cat_counter)</div><div class="line">    p_category = self.get_prior_probability(lb)</div><div class="line">    <span class="comment"># 利用极大似然估计获得计算条件概率的函数、使用数组变量 data 进行存储</span></div><div class="line">    data = [</div><div class="line">        NBFunctions.gaussian_maximum_likelihood(</div><div class="line">            self._labelled_x, n_category, dim)</div><div class="line">                <span class="keyword">for</span> dim <span class="keyword">in</span> range(len(self._x))]</div><div class="line">    self._data = data</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(input_x, tar_category)</span>:</span></div><div class="line">        <span class="comment"># 将输入转换成二维数组（矩阵）</span></div><div class="line">        input_x = np.atleast_2d(input_x).T</div><div class="line">        rs = np.ones(input_x.shape[<span class="number">1</span>])</div><div class="line">        <span class="keyword">for</span> d, xx <span class="keyword">in</span> enumerate(input_x):</div><div class="line">            rs *= data[d][tar_category](xx)</div><div class="line">        <span class="keyword">return</span> rs * p_category[tar_category]</div><div class="line"></div><div class="line"><span class="comment"># 由于数据本身就是数值的，所以数据转换函数只需直接返回输入值即可</span></div><div class="line"><span class="meta">@staticmethod</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_transfer_x</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure>
<p>至此，连续型朴素贝叶斯模型就搭建完毕了</p>
<p>连续型朴素贝叶斯同样能够进行和离散型朴素贝叶斯类似的可视化，不过由于我们接下来就要实现适用范围最广的朴素贝叶斯模型：混合型朴素贝叶斯了，所以我们这里不打算进行 GaussianNB 合理的评估、而打算把它归结到对混合型朴素贝叶斯的评估中</p>
]]></content>
      
        <categories>
            
            <category> 朴素贝叶斯 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[MultinomialNB 的实现]]></title>
      <url>/posts/74647589/</url>
      <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/b_NaiveBayes/Vectorized/MultinomialNB.py" target="_blank" rel="external">这里</a>）</p>
<p>本文主要介绍离散型朴素贝叶斯——MultinomialNB 的实现。对于离散型朴素贝叶斯模型的实现，由于核心算法都是在进行“计数”工作、所以问题的关键就转换为了如何进行计数。幸运的是、Numpy 中的一个方法：<code>bincount</code>就是专门用来计数的，它能够非常快速地数出一个数组中各个数字出现的频率；而且由于它是 Numpy 自带的方法，其速度比 Python 标准库<code>collections</code>中的计数器<code>Counter</code>还要快上非常多。不幸的是、该方法有如下两个缺点：</p>
<ul>
<li>只能处理非负整数型中数组</li>
<li>向量中的最大值即为返回的数组的长度，换句话说，如果用<code>bincount</code>方法对一个长度为 1、元素为 1000 的数组计数的话，返回的结果就是 999 个 0 加 1 个 1</li>
</ul>
<p>所以我们做数据预处理时就要充分考虑到这两点</p>
<a id="more"></a>
<p>在综述中我们曾经提到过在<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py#L189" target="_blank" rel="external">这里</a>可以找到将数据进行数值化的具体实现，该数据数值化的方法其实可以说是为<code>bincount</code>方法“量身定做”的。举个栗子，当原始数据形如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">x, s, n, t, p, f</div><div class="line">x, s, y, t, a, f</div><div class="line">b, s, w, t, l, f</div><div class="line">x, y, w, t, p, f</div><div class="line">x, s, g, f, n, f</div></pre></td></tr></table></figure>
<p>时，调用上述数值化数据的方法将会把数据数值化为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">0, 0, 0, 0, 0, 0</div><div class="line">0, 0, 1, 0, 1, 0</div><div class="line">1, 0, 2, 0, 2, 0</div><div class="line">0, 1, 2, 0, 0, 0</div><div class="line">0, 0, 3, 1, 3, 0</div></pre></td></tr></table></figure>
<p>单就实现这个功能而言、实现是平凡的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">quantize_data</span><span class="params">(x)</span>:</span></div><div class="line">    features = [set(feat) <span class="keyword">for</span> feat <span class="keyword">in</span> xt]</div><div class="line">    feat_dics = [&#123;</div><div class="line">        _l: i <span class="keyword">for</span> i, _l <span class="keyword">in</span> enumerate(feats)</div><div class="line">    &#125; <span class="keyword">if</span> <span class="keyword">not</span> wc[i] <span class="keyword">else</span> <span class="keyword">None</span> <span class="keyword">for</span> i, feats <span class="keyword">in</span> enumerate(features)]</div><div class="line">    x = np.array([[</div><div class="line">        feat_dics[i][_l] <span class="keyword">for</span> i, _l <span class="keyword">in</span> enumerate(sample)]</div><div class="line">            <span class="keyword">for</span> sample <span class="keyword">in</span> x])</div><div class="line">    <span class="keyword">return</span> x, feat_dics</div></pre></td></tr></table></figure>
<p>不过考虑到离散型朴素贝叶斯需要的东西比这要多很多，所以才有了<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py#L189" target="_blank" rel="external">这里</a>所实现的、相对而言繁复很多的版本。建议观众老爷们在看接下来的实现之前先把那个<code>quantize_data</code>函数的完整版看一遍、因为我接下来会直接用<del>（那你很棒棒哦）</del></p>
<p>当然，考虑到朴素贝叶斯的相关理论还是比较多的、我就不把实现一股脑扔出来了，那样估计大部分人<del>（包括我自己在内）</del>都看不懂……所以我决定把离散型朴素贝叶斯算法和对应的实现进行逐一讲解 ( σ’ω’)σ</p>
<h1 id="计算先验概率"><a href="#计算先验概率" class="headerlink" title="计算先验概率"></a>计算先验概率</h1><p>这倒是在将框架时就已经讲过了、但我还是打算重讲一遍以加深理解。首先把实现放出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_prior_probability</span><span class="params">(self, lb=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="keyword">return</span> [(_c_num + lb) / (len(self._y) + lb * len(self._cat_counter))</div><div class="line">        <span class="keyword">for</span> _c_num <span class="keyword">in</span> self._cat_counter]</div></pre></td></tr></table></figure>
<p>其中的<code>lb</code>为平滑系数（默认为 1、亦即拉普拉斯平滑），这对应的公式其实是带平滑项的、先验概率的极大似然估计：</p>
<script type="math/tex; mode=display">
p_\lambda(y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda},k=1,2,...,K</script><p>所以代码中的<code>self._cat_counter</code>的意义就很明确了——它存储着 K 个<script type="math/tex">\sum_{i=1}^NI(y_i=c_k)</script></p>
<p>（cat counter 是 category counter 的简称）<del>（我知道我命名很差所以不要打我）</del></p>
<h1 id="计算条件概率"><a href="#计算条件概率" class="headerlink" title="计算条件概率"></a>计算条件概率</h1><p>同样先看核心实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">data = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_dim)]</div><div class="line"><span class="keyword">for</span> dim, n_possibilities <span class="keyword">in</span> enumerate(self._n_possibilities):</div><div class="line">    data[dim] = [[</div><div class="line">        (self._con_counter[dim][c][p] + lb) / (</div><div class="line">            self._cat_counter[c] + lb * n_possibilities)</div><div class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> range(n_possibilities)] <span class="keyword">for</span> c <span class="keyword">in</span> range(n_category)]</div><div class="line">self._data = [np.array(dim_info) <span class="keyword">for</span> dim_info <span class="keyword">in</span> data]</div></pre></td></tr></table></figure>
<p>这对应的公式其实就是带平滑项（<code>lb</code>）的条件概率的极大似然估计：</p>
<script type="math/tex; mode=display">
p_{\lambda}\left( X^{\left( j \right)} = a_{jl} \middle| y = c_{k} \right) = \frac{\sum_{i = 1}^{N}{I\left( x_{i}^{\left( j \right)} = a_{jl},y_{i} = c_{k} \right) + \lambda}}{\sum_{i = 1}^{N}{I(y_{i} = c_{k})} + S_{j}\lambda}</script><p>其中</p>
<script type="math/tex; mode=display">
k=1,...,K;\ \ j=1,...,n;\ \ l=1,...,S_j</script><p>可以看到我们利用到了<code>self._cat_counter</code>属性来计算<script type="math/tex">\sum_{i=1}^NI(y_i=c_k)</script>。同时可以看出：</p>
<ul>
<li><code>n_category</code>即为 K </li>
<li><code>self._n_possibilities</code>储存着 n 个<script type="math/tex">S_j</script></li>
<li><code>self._con_counter</code>储存的即是各个<script type="math/tex">\sum_{i=1}^NI(x_i^{(j)}=a_{jl}, y_i=c_k)</script>的值。具体而言：  <script type="math/tex; mode=display">
\text{self._con_counter[d][c][p]}=p_\lambda(X^{(d)}=p|y=c)</script></li>
</ul>
<p>至于<code>self._data</code>、就只是为了向量化算法而存在的一个变量而已，它将<code>data</code>中的每一个列表都转成了 Numpy 数组、以便在计算后验概率时利用 Numpy 数组的 Fancy Indexing 来加速算法</p>
<p>聪明的观众老爷可能已经发现、其实<code>self._con_counter</code>才是计算条件概率的关键，事实上这里也正是<code>bincount</code>大放异彩的地方。以下为计算<code>self._con_counter</code>的函数的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_sample_weight</span><span class="params">(self, sample_weight=None)</span>:</span></div><div class="line">    self._con_counter = []</div><div class="line">    <span class="keyword">for</span> dim, _p <span class="keyword">in</span> enumerate(self._n_possibilities):</div><div class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            self._con_counter.append([</div><div class="line">                np.bincount(xx[dim], minlength=_p) <span class="keyword">for</span> xx <span class="keyword">in</span></div><div class="line">                    self._labelled_x])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self._con_counter.append([</div><div class="line">                np.bincount(xx[dim], weights=sample_weight[</div><div class="line">                    label] / sample_weight[label].mean(), minlength=_p)</div><div class="line">                <span class="keyword">for</span> label, xx <span class="keyword">in</span> self._label_zip])</div></pre></td></tr></table></figure>
<p>可以看到、<code>bincount</code>方法甚至能帮我们处理样本权重的问题</p>
<p>代码中有两个我们还没进行说明的属性：<code>self._labelled_x</code>和<code>self._label_zip</code>，不过从代码上下文不难推断出、它们储存的是应该是不同类别所对应的数据。具体而言：</p>
<ul>
<li><code>self._labelled_x</code>：记录按类别分开后的、输入数据的数组</li>
<li><code>self._label_zip</code>：比<code>self._labelled_x</code>多记录了个各个类别的数据所对应的下标</li>
</ul>
<p>这里就提前将它们的实现放出来以帮助理解吧：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 获得各类别数据的下标</span></div><div class="line">labels = [y == value <span class="keyword">for</span> value <span class="keyword">in</span> range(len(cat_counter))]</div><div class="line"><span class="comment"># 利用下标获取记录按类别分开后的输入数据的数组</span></div><div class="line">labelled_x = [x[ci].T <span class="keyword">for</span> ci <span class="keyword">in</span> labels]</div><div class="line">self._labelled_x, self._label_zip = labelled_x, list(zip(labels, labelled_x))</div></pre></td></tr></table></figure>
<h1 id="计算后验概率"><a href="#计算后验概率" class="headerlink" title="计算后验概率"></a>计算后验概率</h1><p>仍然先看核心实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(input_x, tar_category)</span>:</span></div><div class="line">    input_x = np.atleast_2d(input_x).T</div><div class="line">    rs = np.ones(input_x.shape[<span class="number">1</span>])</div><div class="line">    <span class="keyword">for</span> d, xx <span class="keyword">in</span> enumerate(input_x):</div><div class="line">        rs *= self._data[d][tar_category][xx]</div><div class="line">    <span class="keyword">return</span> rs * p_category[tar_category]</div></pre></td></tr></table></figure>
<p>这对应的公式其实就是决策公式：</p>
<script type="math/tex; mode=display">
y=f(x^*)=\arg\max_{c_k}\hat p(y=c_k)\prod_{i=1}^n\hat p(x^{(i)}=x^{*(i)}|y=c_k)</script><p>所以不难看出代码中的<code>p_category</code>存储着 K 个<script type="math/tex">\hat p(y=c_k)</script></p>
<h1 id="整合封装模型"><a href="#整合封装模型" class="headerlink" title="整合封装模型"></a>整合封装模型</h1><p>最后要做的、无非就是把上述三个步骤进行封装而已，首先是数据预处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_data</span><span class="params">(self, x, y, sample_weight=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        sample_weight = np.array(sample_weight)</div><div class="line">    <span class="comment"># 调用 quantize_data 函数获得诸多信息</span></div><div class="line">    x, y, _, features, feat_dics, label_dic = DataUtil.quantize_data(</div><div class="line">        x, y, wc=np.array([<span class="keyword">False</span>] * len(x[<span class="number">0</span>])))</div><div class="line">    <span class="comment"># 利用 bincount 函数直接获得 self._cat_counter</span></div><div class="line">    cat_counter = np.bincount(y)</div><div class="line">    <span class="comment"># 利用 features 变量获取各个维度的特征个数 Sj</span></div><div class="line">    n_possibilities = [len(feats) <span class="keyword">for</span> feats <span class="keyword">in</span> features]</div><div class="line">    <span class="comment"># 获得各类别数据的下标</span></div><div class="line">    labels = [y == value <span class="keyword">for</span> value <span class="keyword">in</span> range(len(cat_counter))]</div><div class="line">    <span class="comment"># 利用下标获取记录按类别分开后的输入数据的数组</span></div><div class="line">    labelled_x = [x[ci].T <span class="keyword">for</span> ci <span class="keyword">in</span> labels]</div><div class="line">    <span class="comment"># 更新模型的各个属性</span></div><div class="line">    self._x, self._y = x, y</div><div class="line">    self._labelled_x, self._label_zip = labelled_x, list(</div><div class="line">        zip(labels, labelled_x))</div><div class="line">    self._cat_counter, self._feat_dics, self._n_possibilities = cat_counter, feat_dics, n_possibilities</div><div class="line">    self.label_dic = label_dic</div><div class="line">    self.feed_sample_weight(sample_weight)</div></pre></td></tr></table></figure>
<p>然后利用上一章我们定义的框架的话、只需定义核心训练函数即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_fit</span><span class="params">(self, lb)</span>:</span></div><div class="line">    n_dim = len(self._n_possibilities)</div><div class="line">    n_category = len(self._cat_counter)</div><div class="line">    p_category = self.get_prior_probability(lb)</div><div class="line"></div><div class="line">    data = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_dim)]</div><div class="line">    <span class="keyword">for</span> dim, n_possibilities <span class="keyword">in</span> enumerate(self._n_possibilities):</div><div class="line">        data[dim] = [[</div><div class="line">            (self._con_counter[dim][c][p] + lb) / (</div><div class="line">                self._cat_counter[c] + lb * n_possibilities)</div><div class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> range(n_possibilities)] <span class="keyword">for</span> c <span class="keyword">in</span> range(n_category)]</div><div class="line">    self._data = [np.array(dim_info) <span class="keyword">for</span> dim_info <span class="keyword">in</span> data]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(input_x, tar_category)</span>:</span></div><div class="line">        input_x = np.atleast_2d(input_x).T</div><div class="line">        rs = np.ones(input_x.shape[<span class="number">1</span>])</div><div class="line">        <span class="keyword">for</span> d, xx <span class="keyword">in</span> enumerate(input_x):</div><div class="line">            rs *= self._data[d][tar_category][xx]</div><div class="line">        <span class="keyword">return</span> rs * p_category[tar_category]</div><div class="line">    <span class="keyword">return</span> func</div></pre></td></tr></table></figure>
<p>最后，我们需要定义一个将测试数据转化为模型所需的、数值化数据的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_transfer_x</span><span class="params">(self, x)</span>:</span></div><div class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(x):</div><div class="line">        <span class="keyword">for</span> j, char <span class="keyword">in</span> enumerate(sample):</div><div class="line">            x[i][j] = self._feat_dics[j][char]</div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure>
<p>至此，离散型朴素贝叶斯就全部实现完毕了<del>（鼓掌！）</del></p>
<h1 id="评估与可视化"><a href="#评估与可视化" class="headerlink" title="评估与可视化"></a>评估与可视化</h1><p>我们可以先拿之前的气球数据集 1.0、1.5 来简单地评估一下我们的模型。首先我们要定义一个能够将文件中的数据转化为 Python 数组的类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataUtil</span>:</span></div><div class="line">    <span class="comment"># 定义一个方法使其能从文件中读取数据</span></div><div class="line">    <span class="comment"># 该方法接受五个参数：</span></div><div class="line">        数据集的名字、数据集的路径、训练样本数、类别所在列、是否打乱数据</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span><span class="params">(name, path, train_num=None, tar_idx=None, shuffle=True)</span>:</span></div><div class="line">        x = []</div><div class="line">        <span class="comment"># 将编码设为utf8以便读入中文等特殊字符</span></div><div class="line">        <span class="keyword">with</span> open(path, <span class="string">"r"</span>, encoding=<span class="string">"utf8"</span>) <span class="keyword">as</span> file:</div><div class="line">            <span class="comment"># 如果是气球数据集的话、直接依逗号分割数据即可</span></div><div class="line">            <span class="keyword">if</span> <span class="string">"balloon"</span> <span class="keyword">in</span> name:</div><div class="line">                <span class="keyword">for</span> sample <span class="keyword">in</span> file:</div><div class="line">                    x.append(sample.strip().split(<span class="string">","</span>))</div><div class="line">        <span class="comment"># 默认打乱数据</span></div><div class="line">        <span class="keyword">if</span> shuffle:</div><div class="line">            np.random.shuffle(x)</div><div class="line">        <span class="comment"># 默认类别在最后一列</span></div><div class="line">        tar_idx = <span class="number">-1</span> <span class="keyword">if</span> tar_idx <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> tar_idx</div><div class="line">        y = np.array([xx.pop(tar_idx) <span class="keyword">for</span> xx <span class="keyword">in</span> x])</div><div class="line">        x = np.array(x)</div><div class="line">        <span class="comment"># 默认全都是训练样本</span></div><div class="line">        <span class="keyword">if</span> train_num <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">return</span> x, y</div><div class="line">        <span class="comment"># 若传入了训练样本数，则依之将数据集切分为训练集和测试集</span></div><div class="line">        <span class="keyword">return</span> (x[:train_num], y[:train_num]), (x[train_num:], y[train_num:])</div></pre></td></tr></table></figure>
<p>需要指出的是，今后获取各种数据的过程都会放在上述<code>DataUtil</code>中的这个<code>get_dataset</code>方法中，其完整版本可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py#L94" target="_blank" rel="external">这里</a>。下面就放出 MultinomialNB 的评估用代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="comment"># 导入标准库time以计时，导入DataUtil类以获取数据</span></div><div class="line">    <span class="keyword">import</span> time</div><div class="line">    <span class="keyword">from</span> Util <span class="keyword">import</span> DataUtil</div><div class="line">    <span class="comment"># 遍历1.0、1.5两个版本的气球数据集</span></div><div class="line">    <span class="keyword">for</span> dataset <span class="keyword">in</span> (<span class="string">"balloon1.0"</span>, <span class="string">"balloon1.5"</span>):</div><div class="line">        <span class="comment"># 读入数据</span></div><div class="line">        _x, _y = DataUtil.get_dataset(dataset, <span class="string">"../../_Data/&#123;&#125;.txt"</span>.format(dataset))</div><div class="line">        <span class="comment"># 实例化模型并进行训练、同时记录整个过程花费的时间</span></div><div class="line">        learning_time = time.time()</div><div class="line">        nb = MultinomialNB()</div><div class="line">        nb.fit(_x, _y)</div><div class="line">        learning_time = time.time() - learning_time</div><div class="line">        <span class="comment"># 评估模型的表现，同时记录评估过程花费的时间</span></div><div class="line">        estimation_time = time.time()</div><div class="line">        nb.evaluate(_x, _y)</div><div class="line">        estimation_time = time.time() - estimation_time</div><div class="line">        <span class="comment"># 将记录下来的耗时输出</span></div><div class="line">        print(</div><div class="line">            <span class="string">"Model building  : &#123;:12.6&#125; s\n"</span></div><div class="line">            <span class="string">"Estimation      : &#123;:12.6&#125; s\n"</span></div><div class="line">            <span class="string">"Total           : &#123;:12.6&#125; s"</span>.format(</div><div class="line">                learning_time, estimation_time,</div><div class="line">                learning_time + estimation_time</div><div class="line">            )</div><div class="line">        )</div></pre></td></tr></table></figure>
<p>上面这段代码的运行结果如下图所示：</p>
<img src="/posts/74647589/p4.png" alt="p4.png" title="">
<p>由于数据量太少、所以建模和评估的过程耗费的时间已是可以忽略不计的程度。同时正如前文所提及的，气球数据集1.5是“不太均衡”的数据集，所以朴素贝叶斯在其上的表现会比较差</p>
<p>仅仅在虚构的数据集上进行评估可能不太有说服力，我们可以拿 UCI 上一个比较出名<del>（简单）</del>的“蘑菇数据集（Mushroom Data Set）”来评估一下我们的模型。该数据集的大致描述如下：它有 8124 个样本、22 个属性，类别取值有两个：“能吃”或“有毒”；该数据每个单一样本都占一行、属性之间使用逗号隔开。选择该数据集的原因是它无需进行额外的数据预处理、样本量和属性量都相对合适、二类分类问题也相对来说具有代表性。更重要的是，它所有维度的特征取值都是离散的、从而非常适合用来测试我们的 MultinomialNB 模型</p>
<p>完整的数据集可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/_Data/mushroom.txt" target="_blank" rel="external">这里</a>（第一列数据是类别），我们的模型在其上的表现则如下图所示： </p>
<img src="/posts/74647589/p1.png" alt="p1.png" title="">
<p>其中第一、二行分别是训练集、测试集上的准确率，接下来三行则分别是建立模型、评估模型和总花费时间的记录</p>
<p>当然，仅仅看一个结果没有什么意思、也完全无法知道模型到底干了什么。为了获得更好的直观，我们可以进行一定的可视化，比如说将极大似然估计法得到的条件概率画出（如综述所示的那样）。可视化的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入 matplotlib 库以进行可视化</span></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="comment"># 进行一些设置使得 matplotlib 能够显示中文</span></div><div class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl</div><div class="line"><span class="comment"># 将字体设为“仿宋”</span></div><div class="line">mpl.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'FangSong'</span>]</div><div class="line">mpl.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="keyword">False</span></div><div class="line"><span class="comment"># 利用 MultinomialNB 搭建过程中记录的变量获取条件概率</span></div><div class="line">data = nb[<span class="string">"data"</span>]</div><div class="line"><span class="comment"># 定义颜色字典，将类别 e（能吃）设为天蓝色、类别 p（有毒）设为橙色</span></div><div class="line">colors = &#123;<span class="string">"e"</span>: <span class="string">"lightSkyBlue"</span>, <span class="string">"p"</span>: <span class="string">"orange"</span>&#125;</div><div class="line"><span class="comment"># 利用转换字典定义其“反字典”，后面可视化会用上</span></div><div class="line">_rev_feat_dics = [&#123;_val: _key <span class="keyword">for</span> _key, _val <span class="keyword">in</span> _feat_dic.items()&#125; </div><div class="line">    <span class="keyword">for</span> _feat_dic <span class="keyword">in</span> self._feat_dics]</div><div class="line"><span class="comment"># 遍历各维度进行可视化</span></div><div class="line"><span class="comment"># 利用 MultinomialNB 搭建过程中记录的变量，获取画图所需的信息</span></div><div class="line"><span class="keyword">for</span> _j <span class="keyword">in</span> range(nb[<span class="string">"x"</span>].shape[<span class="number">1</span>]):</div><div class="line">    sj = nb[<span class="string">"n_possibilities"</span>][_j]</div><div class="line">    tmp_x = np.arange(<span class="number">1</span>, sj+<span class="number">1</span>)</div><div class="line">    <span class="comment"># 利用 matplotlib 对 LaTeX 的支持来写标题，两个 $ 之间的即是 LaTeX 语句</span></div><div class="line">    title = <span class="string">"$j = &#123;&#125;; S_j = &#123;&#125;$"</span>.format(_j+<span class="number">1</span>, sj)</div><div class="line">    plt.figure()</div><div class="line">    plt.title(title)</div><div class="line">    <span class="comment"># 根据条件概率的大小画出柱状图</span></div><div class="line">    <span class="keyword">for</span> _c <span class="keyword">in</span> range(len(nb.label_dic)):</div><div class="line">        plt.bar(tmp_x<span class="number">-0.35</span>*_c, data[_j][_c, :], width=<span class="number">0.35</span>,</div><div class="line">                facecolor=colors[nb.label_dic[_c]], edgecolor=<span class="string">"white"</span>, </div><div class="line">                label=<span class="string">"class: &#123;&#125;"</span>.format(nb.label_dic[_c]))</div><div class="line">    <span class="comment"># 利用上文定义的“反字典”将横坐标转换成特征的各个取值</span></div><div class="line">    plt.xticks([i <span class="keyword">for</span> i <span class="keyword">in</span> range(sj + <span class="number">2</span>)], [<span class="string">""</span>] + [_rev_dic[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(sj)] + [<span class="string">""</span>])</div><div class="line">    plt.ylim(<span class="number">0</span>, <span class="number">1.0</span>)</div><div class="line">    plt.legend()</div><div class="line">    <span class="comment"># 保存画好的图像</span></div><div class="line">    plt.savefig(<span class="string">"d&#123;&#125;"</span>.format(j+<span class="number">1</span>))</div></pre></td></tr></table></figure>
<p>由于蘑菇数据一共有 22 维，所以上述代码会生成 22 张图，从这些图可以非常清晰地看出训练数据集各维度特征的分布。下选出几组有代表性的图片进行说明</p>
<p>一般来说，一组数据特征中会有相对“重要”的特征和相对“无足轻重”的特征，通过以上实现的可视化可以比较轻松地辨析出在离散型朴素贝叶斯中这两者的区别。比如说，在离散型朴素贝叶斯里、相对重要的特征的表现会如下图所示（左图对应第 5 维、右图对应第 19 维）：</p>
<img src="/posts/74647589/p2.png" alt="“优秀”的特征" title="“优秀”的特征">
<p>可以看出，蘑菇数据集在第 19 维上两个类别各自的“优势特征”都非常明显、第 5 维上两个类别各自特征的取值更是基本没有交集。可以想象，即使只根据第 5 维的取值来进行类别的判定、最后的准确率也一定会非常高</p>
<p>那么与之相反的、在 MultinomialNB 中相对没那么重要的特征的表现则会形如下图所示（左图对应第 3 维、右图对应第 16 维）：</p>
<img src="/posts/74647589/p3.png" alt="“无用”的特征" title="“无用”的特征">
<p>可以看出，蘑菇数据集在第 3 维上两个类的特征取值基本没有什么差异、第 16 维数据更是似乎完全没有存在的价值。像这样的数据就可以考虑直接剔除掉</p>
<p>看到这里的观众老爷如果再回过头去看上一篇文章所讲的框架、想必会有些新的体会吧 ( σ’ω’)σ</p>
]]></content>
      
        <categories>
            
            <category> 朴素贝叶斯 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[框架的实现]]></title>
      <url>/posts/fa51e28/</url>
      <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/b_NaiveBayes/Vectorized/Basic.py" target="_blank" rel="external">这里</a>）</p>
<p>对于我个人而言、光看这么一个框架是非常容易摸不着头脑的<br>毕竟之前花了许多时间在数学部分讲的那些算法完全没有体现在这个框架中、取而代之的是一些我抽象出来的和算法无关的结构性部分……<br>虽然从逻辑上来说应该先说明如何搭建这个框架，但从容易理解的角度来说、个人建议先不看这章的内容而是先看后续的实现具体算法的章节<br>然后如果那时有不懂的定义、再对照这一章的相关部分来看<br>不过如果是对朴素贝叶斯算法非常熟悉的观众老爷的话、直接看本章的抽象会引起一些共鸣也说不定 ( σ’ω’)σ</p>
<a id="more"></a>
<p>所谓的框架、自然是指三种朴素贝叶斯模型（离散、连续、混合）共性的抽象了。由于贝叶斯决策论就摆在那里、不难知道如下功能是通用的：</p>
<ul>
<li>计算类别的先验概率</li>
<li>训练出一个能输出后验概率的决策函数</li>
<li>利用该决策函数进行预测和评估</li>
</ul>
<p>虽说朴素贝叶斯大体上来说只是简单的计数、但是想以比较高的效率做好这件事却比想象中的要麻烦不少<del>（说实话麻烦到我有些不想讲的程度了）</del></p>
<p>总之先来看看这个框架的初始化步骤吧<del>（前方高能）</del></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">NaiveBayes</span><span class="params">(ClassifierBase)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._x, self._y：记录训练集的变量</div><div class="line">        self._data：核心数组，存储实际使用的条件概率的相关信息</div><div class="line">        self._func：模型核心——决策函数，能够根据输入的x、y输出对应的后验概率</div><div class="line">        self._n_possibilities：记录各个维度特征取值个数的数组</div><div class="line">        self._labelled_x：记录按类别分开后的输入数据的数组</div><div class="line">        self._label_zip：记录类别相关信息的数组，视具体算法、定义会有所不同</div><div class="line">        self._cat_counter：核心数组，记录第i类数据的个数（cat是category的缩写）</div><div class="line">        self._con_counter：核心数组，用于记录数据条件概率的原始极大似然估计</div><div class="line">        self.label_dic：核心字典，用于记录数值化类别时的转换关系</div><div class="line">        self._feat_dics：核心字典，用于记录数值化各维度特征（feat）时的转换关系</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self._x = self._y = <span class="keyword">None</span></div><div class="line">        self._data = self._func = <span class="keyword">None</span></div><div class="line">        self._n_possibilities = <span class="keyword">None</span></div><div class="line">        self._labelled_x = self._label_zip = <span class="keyword">None</span></div><div class="line">        self._cat_counter = self._con_counter = <span class="keyword">None</span></div><div class="line">        self.label_dic = self._feat_dics = <span class="keyword">None</span></div></pre></td></tr></table></figure>
<p>其中、<code>self._con_counter[d][c][p]</code><script type="math/tex">=\hat p(x^{(d)}=p|y=c)</script>（con 是 conditional 的缩写）</p>
<img src="/posts/fa51e28/p1.jpg" alt="注释比代码还多是想闹哪样？？？(╯‵□′)╯︵┻━┻" title="注释比代码还多是想闹哪样？？？(╯‵□′)╯︵┻━┻">
<p>总之和我一样陷入了茫然的观众老爷们可以先不太在意这一坨是什么玩意儿，毕竟这些东西是抽象程度比较高的属性……等结合具体算法时、这些属性的意义可能就会明确得多</p>
<p>不过需要注意的是，我们实现的<code>NaiveBayes</code>基类继承了一个叫<code>ClassifierBase</code>的基类、其实现是我们之前没有提及的。具体的代码可参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Bases.py" target="_blank" rel="external">GitHub</a>，这里仅大致说一下它所实现的、一些非常普适性的功能：</p>
<ul>
<li>可视化二维数据</li>
<li>重载 <strong>str</strong>、<strong>repr</strong> 和 <strong>getitem</strong> 方法</li>
<li>根据<code>predict</code>方法、输出某个数据集上的准确率</li>
</ul>
<p>这样做主要是为了合理地重用代码，从而能够使机器学习模型的开发更加便捷、高效。在今后的实现中，我们也会频繁地运用到<code>ClassifierBase</code>这个基类</p>
<p>下面进入正题……首先来看怎么计算先验概率（直接利用上面的<code>self._cat_counter</code>属性即可）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_prior_probability</span><span class="params">(self, lb=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="keyword">return</span> [(_c_num + lb) / (len(self._y) + lb * len(self._cat_counter))</div><div class="line">            <span class="keyword">for</span> _c_num <span class="keyword">in</span> self._cat_counter]</div></pre></td></tr></table></figure>
<p>其中参数<code>lb</code>即为平滑项，默认为 1 意味着默认使用拉普拉斯平滑 </p>
<p>然后看看训练步骤能如何进行抽象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x=None, y=None, sample_weight=None, lb=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="keyword">if</span> x <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        self.feed_data(x, y, sample_weight)</div><div class="line">    self._func = self._fit(lb)</div></pre></td></tr></table></figure>
<p><del>（岂可修不就只是调用了一下<code>feed_data</code>方法而已嘛还说成抽象什么的行不行啊）</del></p>
<p>其中用到的<code>feed_data</code>方法是留给各个子类定义的、进行数据预处理的方法；然后<code>self._fit</code>可说是核心训练函数、它会返回我们的决策函数<code>self._func</code></p>
<p>最后看看怎样利用<code>self._func</code>来预测未知数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x, get_raw_result=False)</span>:</span></div><div class="line">    <span class="comment"># 调用相应方法进行数据预处理（这在离散型朴素贝叶斯中尤为重要）</span></div><div class="line">    x = self._transfer_x(x)</div><div class="line">    <span class="comment"># 只有将算法进行向量化之后才能做以下的步骤</span></div><div class="line">    m_arg, m_probability = np.zeros(len(x), dtype=np.int8), np.zeros(len(x))</div><div class="line">    <span class="comment"># len(self._cat_counter) 其实就是类别个数</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self._cat_counter)):</div><div class="line">        <span class="comment"># 注意这里的 x 其实是矩阵、p 是对应的“后验概率矩阵”：p = p(y=i|x)</span></div><div class="line">        <span class="comment"># 这意味着决策函数 self._func 需要支持矩阵运算</span></div><div class="line">        p = self._func(x, i)</div><div class="line">        <span class="comment"># 利用 Numpy 进行向量化操作</span></div><div class="line">        _mask = p &gt; m_probability</div><div class="line">        m_arg[_mask], m_probability[_mask] = i, p[_mask]</div><div class="line">    <span class="comment"># 利用转换字典 self.label_dic 输出决策</span></div><div class="line">    <span class="comment"># 参数 get_raw_result 控制该函数是输出预测的类别还是输出相应的后验概率</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> get_raw_result:</div><div class="line">        <span class="keyword">return</span> np.array([self.label_dic[arg] <span class="keyword">for</span> arg <span class="keyword">in</span> m_arg])</div><div class="line">    <span class="keyword">return</span> m_probability</div></pre></td></tr></table></figure>
<p>其中<code>self.label_dic</code>大概是这个德性的：比如训练集的类别空间为 {red, green, blue} 然后第一个样本的类别是 red 且第二个样本的类别是 blue、那么就有</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.label_dic = np.array([<span class="string">"red"</span>, <span class="string">"blue"</span>, <span class="string">"green"</span>])</div></pre></td></tr></table></figure>
<p>以上就是朴素贝叶斯模型框架的搭建，下一篇文章则会在该框架的基础上实现离散型朴素贝叶斯模型</p>
]]></content>
      
        <categories>
            
            <category> 朴素贝叶斯 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[朴素贝叶斯算法]]></title>
      <url>/posts/ea9b7d09/</url>
      <content type="html"><![CDATA[<p>首先要叙述朴素贝叶斯算法的基本假设：</p>
<ul>
<li><strong>独立性假设</strong>：假设单一样本<script type="math/tex">X_i</script>的 n 个维度<script type="math/tex">X_i^{(1)},...,X_i^{(n)}</script>彼此之间在各种意义上相互独立</li>
</ul>
<p>这当然是很强的假设，在现实任务中也大多无法满足该假设。由此会衍生出所谓的半朴素贝叶斯和贝叶斯网，这里先按下不表</p>
<p>然后就是算法。我们打算先只叙述它的基本思想和各个公式，相关的定义和证明会放在后面的文章中。不过其实仅对着接下来的公式敲代码的话、就已经可以实现一个朴素贝叶斯模型了：</p>
<ul>
<li>基本思想：<strong>后验概率最大化</strong>、然后通过贝叶斯公式转换成先验概率乘条件概率最大化</li>
<li>各个公式（假设输入有 N 个、单个样本是 n 维的、一共有 K 类：<script type="math/tex">c_1,...,c_K</script>）<ul>
<li>计算先验概率的极大似然估计：  <script type="math/tex; mode=display">
\hat p(y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N},k=1,2,...,K</script></li>
<li>计算条件概率的极大似然估计：  <script type="math/tex; mode=display">
\hat p(x^{(j)}=a_{jl}|y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}</script>其中样本<script type="math/tex">x_i</script>第 j 维<script type="math/tex">x_i^{(j)}</script>的取值集合为<script type="math/tex">\{a_{j1},...,a_{jS_j}\}</script></li>
</ul>
</li>
<li>得到最终的分类器：  <script type="math/tex; mode=display">
y=f(x^*)=\arg\max_{c_k}\hat p(y=c_k)\prod_{i=1}^n\hat p(x^{(i)}=x^{*(i)}|y=c_k)</script></li>
</ul>
<p>在朴素贝叶斯算法思想下、一般来说会衍生出以下三种不同的模型：</p>
<ul>
<li>离散型朴素贝叶斯（MultinomialNB）：所有维度的特征都是离散型随机变量</li>
<li>连续型朴素贝叶斯（GaussianNB）：所有维度的特征都是连续型随机变量</li>
<li>混合型朴素贝叶斯（MergedNB）：各个维度的特征有离散型也有连续型</li>
</ul>
<p>接下来就简单<del>（并不简单啊喂）</del>讲讲朴素贝叶斯的数学背景。由浅入深，我们会用离散型朴素贝叶斯来说明一些普适性的概念，连续型和混合型的相关定义是类似的</p>
<a id="more"></a>
<h1 id="朴素贝叶斯与贝叶斯决策论的联系"><a href="#朴素贝叶斯与贝叶斯决策论的联系" class="headerlink" title="朴素贝叶斯与贝叶斯决策论的联系"></a>朴素贝叶斯与贝叶斯决策论的联系</h1><p>朴素贝叶斯的模型参数即是类别的选择空间：</p>
<script type="math/tex; mode=display">
\Theta = \left\{ y = c_{1},{y = c}_{2},\ldots,{y = c}_{K} \right\}</script><p>朴素贝叶斯总的参数空间<script type="math/tex">\tilde{\Theta}</script>本应包括模型参数的先验概率<script type="math/tex">p\left( \theta_{k} \right) = p(y = c_{k})</script>、样本空间在模型参数下的条件概率<script type="math/tex">p\left( X \middle| \theta_{k} \right) = p(X|y = c_{k})</script>和样本空间本身的概率<script type="math/tex">p(X)</script>；但由于我们采取样本空间的子集<script type="math/tex">\tilde{X}</script>作为训练集，所以在给定的<script type="math/tex">\tilde{X}</script>下、<script type="math/tex">p\left( X \right) = p(\tilde{X})</script>是常数，因此可以把它从参数空间中删去。换句话说，我们关心的东西只有模型参数的先验概率和样本空间在模型参数下的条件概率</p>
<script type="math/tex; mode=display">
\tilde{\Theta} = \left\{ p\left( \theta \right),p\left( X \middle| \theta \right):\theta \in \Theta \right\}</script><p>行动空间<script type="math/tex">A</script>就是朴素贝叶斯总的参数空间<script type="math/tex">\tilde{\Theta}</script></p>
<p>决策就是后验概率最大化（在<a href="/posts/e312d61a/" title="推导与推广">推导与推广</a>里，我们会证明该决策为贝叶斯决策）</p>
<script type="math/tex; mode=display">
\delta\left( \tilde{X} \right) = \hat{\theta} = \arg{\max_{\tilde\theta\in\tilde\Theta}{p\left( \tilde{\theta} \middle| \tilde{X} \right)}}</script><p>在<script type="math/tex">\hat{\theta}</script>确定后，模型的决策就可以具体写成（这一步用到了独立性假设）</p>
<script type="math/tex; mode=display">
\begin{align}
  f\left( x^{*} \right) &= \arg{\max_{c_k}{\hat{p}\left( c_{k} \middle| X = x^{*} \right)}} \\
  &= \arg{\max_{c_k}{\hat{p}\left( y = c_{k} \right)\prod_{j = 1}^{n}{\hat{p}\left( X^{\left( j \right)} = {x^{*}}^{\left( j \right)} \middle| y = c_{k} \right)}}}
\end{align}</script><p>损失函数会随模型的不同而不同。在离散型朴素贝叶斯中，损失函数就是比较简单的 0-1 损失函数</p>
<script type="math/tex; mode=display">
L\left( \theta,\delta\left( \tilde{X} \right) \right) = \sum_{i = 1}^{N}{\tilde{L}\left( y_{i},f\left( x_{i} \right) \right) =}\sum_{i = 1}^{N}{I(}y_{i} \neq f\left( x_{i} \right))</script><p>这里的<script type="math/tex">I</script>是示性函数，它满足：</p>
<script type="math/tex; mode=display">
I\left( y_{i} \neq f\left( x_{i} \right) \right) = \left\{ \begin{matrix}
1,\ if\ y_{i} \neq f\left( x_{i} \right) \\
0,if\ y_{i} \neq f\left( x_{i} \right) \\
\end{matrix} \right.\</script><p>从上述定义出发、可以利用<a href="/posts/d007d6bc/" title="上一篇文章">上一篇文章</a>中讲解的两种参数估计方法导出离散型朴素贝叶斯的算法（详见<a href="/posts/e312d61a/" title="推导与推广">推导与推广</a>）：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,(x_{N},y_{N})\}</script></li>
<li><strong>过程</strong>（利用 ML 估计导出模型的具体参数）：<ol>
<li>计算先验概率<script type="math/tex">p(y = c_{k})</script>的极大似然估计：  <script type="math/tex; mode=display">
\hat{p}\left( y = c_{k} \right) = \frac{\sum_{i = 1}^{N}{I(y_{i} = c_{k})}}{N},\ k = 1,2,\ldots,K</script></li>
<li>计算条件概率<script type="math/tex">p(X^{\left( j \right)} = a_{jl}|y = c_{k})</script>的极大似然估计（设每一个单独输入的 n 维向量<script type="math/tex">x_{i}</script>的第 j 维特征<script type="math/tex">x^{\left( j \right)}</script>可能的取值集合为<script type="math/tex">\{ a_{j1},\ldots,a_{jS_{j}}\}</script>）：  <script type="math/tex; mode=display">
\hat{p}\left( X^{\left( j \right)} = a_{jl} \middle| y = c_{k} \right) = \frac{\sum_{i = 1}^{N}{I(x_{i}^{\left( j \right)} = a_{jl},y_{i} = c_{k})}}{\sum_{i = 1}^{N}{I(y_{i} = c_{k})}}</script></li>
</ol>
</li>
<li><strong>输出</strong>（利用 MAP 估计进行决策）：朴素贝叶斯模型，能够估计数据<script type="math/tex">x^{*} = \left( {x^{*}}^{\left( 1 \right)},\ldots,{x^{*}}^{\left( n \right)} \right)^{T}</script>的类别：  <script type="math/tex; mode=display">
y = f(x^{*}) = \arg{\max_{c_k}{\hat{p}\left( y = c_{k} \right)\prod_{j = 1}^{n}{\hat{p}(X^{\left( j \right)} = {x^{*}}^{\left( j \right)}|y = c_{k})}}}</script></li>
</ol>
<p>由上述算法可以清晰地梳理出朴素贝叶斯算法背后的数学思想：</p>
<ul>
<li>使用极大似然估计导出模型的具体参数（先验概率、条件概率）</li>
<li>使用极大后验概率估计作为模型的决策（输出使得数据后验概率最大化的类别）</li>
</ul>
<h1 id="离散型朴素贝叶斯实例"><a href="#离散型朴素贝叶斯实例" class="headerlink" title="离散型朴素贝叶斯实例"></a>离散型朴素贝叶斯实例</h1><p>接下来我们在一个简单、虚拟的数据集上应用离散型朴素贝叶斯算法以加深对算法的理解，该数据集（不妨称之为气球数据集 1.0）如下表所示（参考了 UCI 上相应的数据集）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>大小</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>小</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>小孩</td>
<td>用脚踩</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>成人</td>
<td>用手打</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>大</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>大</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>该数据集的电子版本可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/_Data/balloon1.0.txt" target="_blank" rel="external">这里</a>。我们想预测的是样本：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>大小</th>
<th>测试人员</th>
<th>测试动作</th>
</tr>
</thead>
<tbody>
<tr>
<td>  紫色</td>
<td>小</td>
<td>小孩</td>
<td>用脚踩</td>
</tr>
</tbody>
</table>
</div>
<p>所导致的结果。容易观察到的是、气球的颜色对结果不起丝毫影响，所以在算法中该项特征可以直接去掉。因此从直观上来说，该样本所导致的结果应该是“不爆炸”，我们用离散型朴素贝叶斯算法来看看是否确实如此。首先我们需要计算类别的先验概率，易得：</p>
<script type="math/tex; mode=display">
p\left( 不爆炸\right) = p\left( 爆炸\right) = 0.5</script><p>亦即类别的先验概率也对决策不起作用。继而我们需要依次求出第2、3、4个特征（大小、测试人员、测试动作）的条件概率，它们才是决定新样本所属类别的关键。易得：</p>
<script type="math/tex; mode=display">p\left( 小气球\middle| 不爆炸\right) = \frac{5}{6},\ \ p\left( 大气球\middle| 不爆炸\right) = \frac{1}{6}</script><script type="math/tex; mode=display">p\left( 小气球\middle| 爆炸\right) = \frac{1}{6},\ \ p\left( 大气球\middle| 爆炸\right) = \frac{5}{6}</script><script type="math/tex; mode=display">p\left( 成人\middle| 不爆炸\right) = \frac{1}{3},\ \ p\left( 小孩\middle| 不爆炸\right) = \frac{2}{3}</script><script type="math/tex; mode=display">p\left( 成人\middle| 爆炸\right) = \frac{2}{3},\ \ p\left( 小孩\middle| 爆炸\right) = \frac{1}{3}</script><script type="math/tex; mode=display">p\left( 用手打\middle| 不爆炸\right) = \frac{5}{6},\ \ p\left( 用脚踩\middle| 不爆炸\right) = \frac{1}{6}</script><script type="math/tex; mode=display">p\left( 用手打\middle| 爆炸\right) = \frac{1}{6},\ \ p\left( 用脚踩\middle| 爆炸\right) = \frac{5}{6}</script><p>那么在条件“紫色小气球、小孩用脚踩”下，知（注意我们可以忽略颜色和先验概率）：</p>
<script type="math/tex; mode=display">\hat{p}\left( 不爆炸\right) = p\left( 小气球\middle| 不爆炸\right) \times p\left( 小孩\middle| 不爆炸\right) \times p\left( 用脚踩\middle| 不爆炸\right) = \frac{5}{54}</script><script type="math/tex; mode=display">\hat{p}\left( 爆炸\right) = p\left( 小气球\middle| 爆炸\right) \times p\left( 小孩\middle| 爆炸\right) \times p\left( 用脚踩\middle| 爆炸\right) = \frac{5}{108}</script><p>所以我们确实应该认为给定样本所导致的结果是“不爆炸”。</p>
<h1 id="不足与改进"><a href="#不足与改进" class="headerlink" title="不足与改进"></a>不足与改进</h1><p>需要指出的是，目前为止的算法存在一个问题：如果训练集中某个类别<script type="math/tex">c_{k}</script>的数据没有涵盖第 j 维特征的第 l 个取值的话、相应估计的条件概率<script type="math/tex">\hat{p}\left( X^{\left( j \right)} = a_{jl} \middle| y = c_{k} \right)</script>就是 0、从而导致模型可能会在测试集上的分类产生误差。解决这个问题的办法是在各个估计中加入平滑项（也有这种做法就叫贝叶斯估计的说法）：</p>
<ul>
<li>过程：<ul>
<li>计算先验概率<script type="math/tex">p_{\lambda}(y = c_{k})</script>：  <script type="math/tex; mode=display">
p_{\lambda}\left( y = c_{k} \right) = \frac{\sum_{i = 1}^{N}{I\left( y_{i} = c_{k} \right) + \lambda}}{N + K\lambda},\ k = 1,2,\ldots,K</script></li>
<li>计算条件概率<script type="math/tex">p_{\lambda}(X^{\left( j \right)} = a_{jl}|y = c_{k})</script>：  <script type="math/tex; mode=display">
p_{\lambda}\left( X^{\left( j \right)} = a_{jl} \middle| y = c_{k} \right) = \frac{\sum_{i = 1}^{N}{I\left( x_{i}^{\left( j \right)} = a_{jl},y_{i} = c_{k} \right) + \lambda}}{\sum_{i = 1}^{N}{I(y_{i} = c_{k})} + S_{j}\lambda}</script></li>
</ul>
</li>
</ul>
<p>可见当<script type="math/tex">\lambda = 0</script>时就是极大似然估计，而当<script type="math/tex">\lambda = 1</script>时、一般可以称之为拉普拉斯平滑（Laplace Smoothing）。拉普拉斯平滑是常见的做法、我们的实现中也会默认使用它。可以将气球数据集 1.0 稍作变动以彰显加入平滑项的重要性（新数据集如下表所示，不妨称之为气球数据集 1.5）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>大小</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>小</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>成人</td>
<td>用手打</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>大</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>该数据集的电子版本可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/_Data/balloon1.5.txt" target="_blank" rel="external">这里</a>。可以看到这个数据集是“不太均衡”的：它对样本“黄色小气球，小孩用脚踩”重复进行了三次实验、而对所有紫色气球样本实验的结果都是“不爆炸”。如果我们此时想预测“紫色小气球，小孩用脚踩”的结果，虽然从直观上来说应该是“爆炸”，但我们会发现、此时由于</p>
<script type="math/tex; mode=display">
p\left( 用脚踩| 不爆炸\right) = p\left( 紫色| 爆炸\right) = 0</script><p>所以会直接导致</p>
<script type="math/tex; mode=display">
\hat{p}\left( 不爆炸\right) = \hat{p}\left( 爆炸\right) = 0</script><p>从而我们只能随机进行决策，这不是一个令人满意的结果。此时加入平滑项就显得比较重要了，我们以拉普拉斯平滑为例、知（注意类别的先验概率仍然不造成影响）：</p>
<script type="math/tex; mode=display">p\left( 黄色\middle| 不爆炸\right) = \frac{3 + 1}{6 + 2},\ \ p\left( 紫色\middle| 不爆炸\right) = \frac{3 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 黄色\middle| 爆炸\right) = \frac{6 + 1}{6 + 2},\ \ p\left( 紫色\middle| 爆炸\right) = \frac{0 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 小气球\middle| 不爆炸\right) = \frac{4 + 1}{6 + 2},\ \ p\left( 大气球\middle| 不爆炸不爆炸\right) = \frac{2 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 小气球\middle| 爆炸\right) = \frac{4 + 1}{6 + 2},\ \ p\left( 大气球\middle| 爆炸\right) = \frac{2 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 成人\middle| 不爆炸\right) = \frac{2 + 1}{6 + 2},\ \ p\left( 小孩\middle| 不爆炸\right) = \frac{4 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 成人\middle| 爆炸\right) = \frac{3 + 1}{6 + 2},\ \ p\left( 小孩\middle| 爆炸\right) = \frac{3 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 用手打\middle| 不爆炸\right) = \frac{6 + 1}{6 + 2},\ \ p\left( 用脚踩\middle| 不爆炸\right) = \frac{0 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 用手打\middle| 爆炸\right) = \frac{1 + 1}{6 + 2},\ \ p\left( 用脚踩\middle| 爆炸\right) = \frac{5 + 1}{6 + 2}</script><p>从而可算得：</p>
<script type="math/tex; mode=display">\hat{p}\left( 不爆炸\right) = \frac{25}{1024},\ \ \hat{p}\left( 爆炸\right) = \frac{15}{512}</script><p>因此我们确实应该认为给定样本所导致的结果是“爆炸”</p>
]]></content>
      
        <categories>
            
            <category> 朴素贝叶斯 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 算法 </tag>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[参数估计]]></title>
      <url>/posts/d007d6bc/</url>
      <content type="html"><![CDATA[<p>无论是贝叶斯学派还是频率学派，一个无法避开的问题就是如何从已有的样本中获取信息并据此估计目标模型的参数。比较有名的“频率近似概率”其实就是（基于大数定律的）相当合理的估计之一，本章所叙述的两种参数估计方法在最后也通常会归结于它</p>
<a id="more"></a>
<h1 id="极大似然估计（ML-估计）"><a href="#极大似然估计（ML-估计）" class="headerlink" title="极大似然估计（ML 估计）"></a>极大似然估计（ML 估计）</h1><p>如果把模型描述成一个概率模型的话，一个自然的想法是希望得到的模型参数<script type="math/tex">\theta</script>能够使得在训练集<script type="math/tex">\tilde{X}</script>作为输入时、模型输出的概率达到极大。这里就有一个似然函数的概念，它能够输出<script type="math/tex">\tilde{X} = \left( x_{1},\ldots,x_{N} \right)^{T}</script>在模型参数为<script type="math/tex">\theta</script>下的概率：</p>
<script type="math/tex; mode=display">
p\left( \tilde{X} \middle| \theta \right) = \prod_{i = 1}^{N}{p(x_{i}|\theta)}</script><p>我们希望找到的<script type="math/tex">\hat{\theta}</script>就是使得似然函数在<script type="math/tex">\tilde{X}</script>作为输入时达到极大的参数：</p>
<script type="math/tex; mode=display">
\hat{\theta} = \arg{\max_\theta{p\left( \tilde{X} \middle| \theta \right) = \arg{\max_\theta{\prod_{i = 1}^{N}{p(x_{i}|\theta)}}}}}</script><p>举个栗子：假设一个暗箱中有白球、黑球共两个，虽然不知道具体的颜色分布情况、但是知道这两个球是完全一样的。现在有放回地从箱子里抽了 2 个球，发现两次抽出来的结果是 1 黑 1 白，那么该如何估计箱子里面球的颜色？从直观上来说似乎箱子中也是 1 黑 1 白会比较合理，下面我们就来说明“1 黑 1 白”这个估计就是极大似然估计。</p>
<p>在这个问题中，模型的参数<script type="math/tex">\theta</script>可以设为从暗箱中抽出黑球的概率，样本<script type="math/tex">x_{i}</script>可以描述为第i次取出的球是否是黑球；如果是就取 1、否则取 0。这样的话，似然函数就可以描述为：</p>
<script type="math/tex; mode=display">
p\left( \tilde{X} \middle| \theta \right) = \theta^{x_{1} + x_{2}}\left( 1 - \theta \right)^{2 - x_{1} - x_{2}}</script><p>直接对它求极大值（虽然可行但是）不太方便，通常的做法是将似然函数取对数之后再进行极大值的求解：</p>
<script type="math/tex; mode=display">
\ln{p\left( \tilde{X} \middle| \theta \right) = \left( x_{1} + x_{2} \right)\ln{\theta + \left( 2 - x_{1} - x_{2} \right)\ln{(1 - \theta)}}} \Rightarrow \frac{\partial\ln p}{\partial\theta} = \frac{x_{1} + x_{2}}{\theta} - \frac{2 - x_{1} - x_{2}}{1 - \theta}</script><p>从而可知：</p>
<script type="math/tex; mode=display">
\frac{\partial\ln p}{\partial\theta} = 0 \Rightarrow \theta = \frac{x_{1} + x_{2}}{2}</script><p>由于<script type="math/tex">x_{1} + x_{2} = 1</script>，所以得<script type="math/tex">\hat{\theta} = 0.5</script>、亦即应该估计从暗箱中抽出黑球的概率是 50%；进一步地、既然暗箱中的两个球完全一样，我们应该估计暗箱中的颜色分布为 1 黑 1 白。</p>
<p>从以上的讨论可以看出，极大似然估计视待估参数为一个未知但固定的量、不考虑“观察者”的影响（亦即不考虑先验知识的影响），是传统的频率学派的做法</p>
<h1 id="极大后验概率估计（MAP估计）"><a href="#极大后验概率估计（MAP估计）" class="headerlink" title="极大后验概率估计（MAP估计）"></a>极大后验概率估计（MAP估计）</h1><p>相比起极大似然估计，极大后验概率估计是更贴合贝叶斯学派思想的做法；事实上、甚至也有不少人直接称其为“贝叶斯估计”（注：贝叶斯估计的定义有许多，本人接触到的就有 3、4 种；囿于实力，本人无法辨析哪种才是真正的贝叶斯估计、所以我们不会进行相关的讨论）</p>
<p>在讨论 MAP 估计之前，我们有必要先知道何为后验概率<script type="math/tex">p(\theta|\tilde{X})</script>：它可以理解为参数<script type="math/tex">\theta</script>在训练集<script type="math/tex">\tilde{X}</script>下所谓的“真实的出现概率”，能够利用参数的先验概率<script type="math/tex">p\left( \theta \right)</script>、样本的先验概率<script type="math/tex">p(\tilde{X})</script>和条件概率<script type="math/tex">p\left( \tilde{X}|\theta \right) = \prod_{i = 1}^{N}{p\left( x_{i}|\theta \right)}</script>通过贝叶斯公式导出（详见<a href="/posts/e312d61a/" title="推导与推广">推导与推广</a>）</p>
<p>而 MAP 估计的核心思想、就是将待估参数<script type="math/tex">\theta</script>看成是一个随机变量、从而引入了极大似然估计里面没有引入的、参数<script type="math/tex">\theta</script>的先验分布。MAP 估计<script type="math/tex">{\hat{\theta}}_{\text{MAP}}</script>的定义为：</p>
<script type="math/tex; mode=display">
{\hat{\theta}}_{\text{MAP}} = \arg{\max_\theta{p(\theta|\tilde{X}) = \arg{\max_\theta{p(\theta)\prod_{i = 1}^{N}{p(x_{i}|\theta)}}}}}</script><p>同样的，为了计算简洁，我们通常对上式取对数：</p>
<script type="math/tex; mode=display">
{\hat{\theta}}_{\text{MAP}} = \arg{\max_\theta{\ln{p(\theta|\tilde{X})} = \arg{\max_\theta\left\lbrack \ln{p\left( \theta \right)} + \sum_{i = 1}^{N}{\ln{p\left( x_{i} \middle| \theta \right)}} \right\rbrack}}}</script><p>可以看到，从形式上、极大后验概率估计只比极大似然估计多了<script type="math/tex">\ln{p(\theta)}</script>这一项，不过它们背后的思想却相当不同。不过有意思的是，在之后具体讨论朴素贝叶斯算法时我们会看到、朴素贝叶斯在估计参数时选用了极大似然估计法、但是在做决策时则选用了 MAP 估计</p>
<p>和极大似然估计相比，MAP 估计的一个显著优势在于它可以引入所谓的“先验知识”，这正是贝叶斯学派的精髓。当然这个优势同时也伴随着劣势：它我们对模型参数有相对较好的认知、否则会相当大地影响到结果的合理性</p>
<p>既然先验分布如此重要，那么是否有比较合理的、先验分布的选取方法呢？事实上，如何确定先验分布这个问题，正是贝叶斯统计中最困难、最具有争议性却又必须解决的问题。虽然这个问题确实有许多现代的研究成果，但遗憾的是，尚未能有一个圆满的理论和普适的方法。这里拟介绍“协调性假说”这个相对而言拥有比较好的直观的理论：</p>
<ul>
<li>我们选择的参数<script type="math/tex">\theta</script>的先验分布、应该与由它和训练集确定的后验分布属同一类型</li>
</ul>
<p>此时先验分布又叫共轭先验分布。这里面所谓的“同一类型”其实又是难有恰当定义的概念，但是我们可以直观地理解为：概率性质相似的所有分布归为“同一类型”。比如，所有的正态分布都是“同一类型”的</p>
]]></content>
      
        <categories>
            
            <category> 朴素贝叶斯 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[贝叶斯决策论]]></title>
      <url>/posts/a0e8b2e/</url>
      <content type="html"><![CDATA[<p>贝叶斯决策论是在概率框架下进行决策的基本方法之一、更是统计模式识别的主要方法之一。从名字也许能看出来，贝叶斯决策论其实是贝叶斯统计学派进行决策的方法。为了更加深刻地理解贝叶斯分类器，我们需要先对贝叶斯学派和其决策理论有一个大致的认知</p>
<a id="more"></a>
<h1 id="贝叶斯学派与频率学派"><a href="#贝叶斯学派与频率学派" class="headerlink" title="贝叶斯学派与频率学派"></a>贝叶斯学派与频率学派</h1><p>贝叶斯学派强调概率的“主观性”，这一点和传统的、我们可能比较熟悉的频率学派不同。详细的论述牵扯到许多概率论和数理统计的知识，这里只说一个直观：</p>
<ul>
<li>频率学派强调频率的“自然属性”，认为应该使用事件在重复试验中发生的频率作为其发生的概率的估计</li>
<li>贝叶斯学派不强调事件的“客观随机性”，认为仅仅只是“观察者”不知道事件的结果。换句话说，贝叶斯学派认为：事件之所以具有随机性仅仅是因为“观察者”的知识不完备，对于“知情者”来说、该事件其实不具备随机性。随机性的根源不在于事件，而在于“观察者”对该事件的知识状态</li>
</ul>
<p>举个栗子：假设一个人抛了一枚均匀硬币到地上并迅速将其踩在脚底而在他面前从近到远坐了三个人。他本人看到了硬币是正面朝上的，而其他三个人也多多少少看到了一些信息，但显然坐得越远、看得就越模糊。频率学派会认为，该硬币是正是反、各自的概率都应该是 50%；但是贝叶斯学派会认为，对抛硬币的人来说、硬币是正面的概率就是 100%，然后可能对离他最近的人来说是 80%、对离他最远的人来说就可能还是 50%</p>
<p>所以相比起把模型参数固定、注重样本的随机性的频率学派而言，贝叶斯学派将样本视为是固定的、把模型的参数视为关键。在上面这个例子里面，样本就是抛出去的那枚硬币，模型的参数就是每个人从中获得的“信息”。对于频率学派而言，每个人获得的“信息”不应该有不同，所以自然会根据“均匀硬币抛出正面的概率是 50%”这个“样本的信息”来导出“硬币是正面的概率为 50%”这个结论。但是对贝叶斯学派而言，硬币抛出去就抛出去了，问题的关键在于模型的参数、亦即“观察者”从中获得的信息，所以会导出“对于抛硬币的人而言，硬币是正面的概率是 100%”这一类的结论</p>
<h1 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h1><p>大致知道贝叶斯学派的思想后，我们就可以介绍贝叶斯决策论了。这里不可避免地要牵扯到概率论和数理统计的相关定义和知识，但幸运的是它们都是比较基础且直观的部分、无需太多数学背景就可以知道它们的含义：</p>
<h2 id="行动空间"><a href="#行动空间" class="headerlink" title="行动空间"></a>行动空间</h2><p>行动空间（通常用<script type="math/tex">A</script>来表示）是某项实际工作中可能采取的各种“行动”所构成的集合。正如前文所提到的、贝叶斯学派注重的是模型参数，所以通常而言我们想要做出的“行动”是“决定模型的参数”。因此我们通常会将行动空间取为参数空间，亦即<script type="math/tex">A=\Theta</script></p>
<h2 id="决策"><a href="#决策" class="headerlink" title="决策"></a>决策</h2><p>决策（通常用<script type="math/tex">\delta(\tilde X)</script>来表示）是样本空间<script type="math/tex">X</script>到行动空间<script type="math/tex">A</script>的一个映射。换句话说，对于一个单一的样本<script type="math/tex">\tilde X</script>（<script type="math/tex">\tilde X\in X</script>），决策函数可以利用它得到<script type="math/tex">A</script>中的一个行动。需要注意的是，这里的样本<script type="math/tex">\tilde X</script>通常是高维的随机向量：<script type="math/tex">\tilde X=(x_1,...,x_N)^T</script>；尤其需要分清的是，这个（以及本节之后的所有）<script type="math/tex">\tilde X</script>其实是一般意义上的“训练集”、<script type="math/tex">x_i</script>才是一般意义上的“样本”。这是因为本节主要在叙述数理统计相关知识，所以在术语上和机器学习术语会有所冲突，需要分辨清它们的关系</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数（通常用<script type="math/tex">L(\theta,a)=L(\theta,\delta(\tilde X))</script>来表示）用于衡量当参数是<script type="math/tex">\theta</script>（<script type="math/tex">\theta\in\Theta</script>，<script type="math/tex">\Theta</script>是参数空间）时采取行动<script type="math/tex">a(a\in A)</script>所引起的损失</p>
<h2 id="决策风险"><a href="#决策风险" class="headerlink" title="决策风险"></a>决策风险</h2><p>决策风险（通常用<script type="math/tex">R(\theta,\delta)</script>来表示）是损失函数的期望：<script type="math/tex">R(\theta,\delta)=EL(\theta,\delta(\tilde X))</script></p>
<h2 id="先验分布"><a href="#先验分布" class="headerlink" title="先验分布"></a>先验分布</h2><p>先验分布描述了参数<script type="math/tex">\theta</script>在已知样本<script type="math/tex">\tilde X</script>中的分布</p>
<h2 id="平均风险"><a href="#平均风险" class="headerlink" title="平均风险"></a>平均风险</h2><p>平均风险（通常用<script type="math/tex">\rho(\delta)</script>来表示）定义为决策风险<script type="math/tex">R(\theta,\delta)</script>在先验分布下的期望：</p>
<script type="math/tex; mode=display">
\rho(\delta) = E_\xi R(\theta,\delta)</script><h2 id="贝叶斯决策"><a href="#贝叶斯决策" class="headerlink" title="贝叶斯决策"></a>贝叶斯决策</h2><p>贝叶斯决策（通常用<script type="math/tex">\delta^*</script>来表示）满足：</p>
<script type="math/tex; mode=display">
\rho(\delta^*)=\inf_\delta\rho(\delta)</script><p>换句话说，贝叶斯决策<script type="math/tex">\delta^*</script>是在某个先验分布下使得平均风险最小的决策</p>
<p>寻找一般意义下的贝叶斯决策是相当不平凡的数学问题，为简洁、我们需要结合具体的机器学习算法来推导相应的贝叶斯决策。相关的讨论会在<a href="/posts/ea9b7d09/" title="说明朴素贝叶斯算法的文章">说明朴素贝叶斯算法的文章</a>中进行，这里就暂时先按下不表</p>
]]></content>
      
        <categories>
            
            <category> 朴素贝叶斯 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[朴素贝叶斯综述]]></title>
      <url>/posts/1607712a/</url>
      <content type="html"><![CDATA[<p>朴素贝叶斯（Naive Bayes）是贝叶斯分类器的一种，而后者是一个相当宽泛的定义，它背后的数学理论根基是相当出名的贝叶斯决策论（Bayesian Decision Theory）。贝叶斯决策论和传统的统计学理论有着区别，其中最不可调和的就是它们各自关于概率的定义。因此，使用了贝叶斯决策论作为基石的贝叶斯分类器，在各个机器学习算法所导出的分类器中也算是比较标新立异的存在</p>
<p>由于朴素贝叶斯这一块能够扯到的理论还是相当多的，我们会把内容分成数学理论部分与程序实现部分，观众老爷们可以按需阅读 ( σ’ω’)σ</p>
<p>以下是目录：</p>
<ul>
<li><a href="/posts/a0e8b2e/" title="贝叶斯决策论">贝叶斯决策论</a></li>
<li><a href="/posts/d007d6bc/" title="参数估计">参数估计</a></li>
<li><a href="/posts/ea9b7d09/" title="朴素贝叶斯算法">朴素贝叶斯算法</a></li>
<li><a href="/posts/fa51e28/" title="框架的实现">框架的实现</a></li>
<li><a href="/posts/74647589/" title="MultinomialNB 的实现">MultinomialNB 的实现</a></li>
<li><a href="/posts/c836ba35/" title="GaussianNB 的实现">GaussianNB 的实现</a></li>
<li><a href="/posts/7c13f69c/" title="MergedNB 的实现">MergedNB 的实现</a></li>
<li><a href="/posts/e312d61a/" title="推导与推广">推导与推广</a></li>
<li><a href="/posts/a75c0d1b/" title="“朴素贝叶斯”小结">“朴素贝叶斯”小结</a>
</li>
</ul>
<a id="more"></a>
<h1 id="涉及到的理论"><a href="#涉及到的理论" class="headerlink" title="涉及到的理论"></a>涉及到的理论</h1><p>首先无法避开的自然就是贝叶斯决策论了，然而我本人对其可以说只有一个入门级别的理解、所以相应的说明更多只能起到一种抛砖引玉的作用</p>
<p>然后是参数估计，这一部分是比较基础的知识，但真要做起来还是相当繁琐的</p>
<p>至于其余的贝叶斯分类器，为了将重点放在朴素贝叶斯，我们仅会在最后进行一些简要的介绍，相关的实现和更深层次的推导等等则不会进行说明<del>（主要是我也不咋懂）</del></p>
<h1 id="程序实现的大体思路"><a href="#程序实现的大体思路" class="headerlink" title="程序实现的大体思路"></a>程序实现的大体思路</h1><ul>
<li>离散型朴素贝叶斯的实现围绕着 Numpy 中的<code>bincount</code>方法展开</li>
<li>连续型朴素贝叶斯的关键在于极大似然估计</li>
<li>混合型朴素贝叶斯则是以上两者的各种结合</li>
</ul>
<h1 id="程序运行的结果预览"><a href="#程序运行的结果预览" class="headerlink" title="程序运行的结果预览"></a>程序运行的结果预览</h1><p>本来要说的话、朴素贝叶斯是很难让人直观地看出来它干了什么的，不过<del>聪明而机智的</del>我想出了一个方法：把中间产生的那些条件概率可视化出来不就可以<del>蒙混过关</del>了！</p>
<p>于是我们的朴素贝叶斯模型可以生成类似于这样的图：</p>
<img src="/posts/1607712a/p1.png" alt="离散型朴素贝叶斯的可视化" title="离散型朴素贝叶斯的可视化">
<img src="/posts/1607712a/p2.png" alt="连续型朴素贝叶斯的可视化" title="连续型朴素贝叶斯的可视化">
<p><strong><em>注意：这些图是分开生成的，是我用画图软件<del>（很傻地）</del>很辛苦地把它们拼在一起的</em></strong></p>
<p>以上 16 张图（离散型特征 9 张、连续型特征 7 张）对应着一个有 16 个特征的、UCI 上的“银行业务数据集”，完整的原始数据集可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/_Data/bank1.0.txt" target="_blank" rel="external">这里</a></p>
<p>然后我们实现的朴素贝叶斯模型的最大特点就是：无需对数据进行太多预处理。还是以银行业务数据集为例，它每个样本大概长下面这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">58, management, married, tertiary, no, 2143, yes, no, unknown, 5, may, 261, 1, -1, 0, unknown</div></pre></td></tr></table></figure>
<p>我们可以直接把它输进模型而无需做其它多余的工作。当然、这是建立在我们自己写一套数值化数据的方法的基础上的，关于这个数值化数据的实现可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py#L189" target="_blank" rel="external">这里</a></p>
]]></content>
      
        <categories>
            
            <category> 朴素贝叶斯 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
            <tag> 目录 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[“绪论”小结]]></title>
      <url>/posts/c939f06b/</url>
      <content type="html"><![CDATA[<ul>
<li>与传统的计算机程序不同，机器学习是面向数据的算法、能够从数据中获得信息。它符合新时代脑力劳动代替体力劳动的趋势，是富有生命力的领域</li>
<li>Python 是一门优异的语言，代码清晰可读、功能广泛强大。其最大弱点——速度问题也可以通过很多不太困难的方法弥补</li>
<li>虽说机器学习算法很多，但通常而言、进行机器学习的过程会包含以下三步：<ul>
<li>获取与处理数据</li>
<li>选择与训练模型</li>
<li>评估与可视化结果</li>
</ul>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> 绪论 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 小结 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[第一个机器学习样例]]></title>
      <url>/posts/372587d5/</url>
      <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/a_FirstExample/Regression.py" target="_blank" rel="external">这里</a>）</p>
<p>作为“绪论”的总结，我们来运用 Python 解决一个实际问题以对机器学习有具体的感受吧。由于该样例只是为了提供直观，我们就拿比较有名的一个小问题来进行阐述。俗话云：“麻雀虽小，五脏俱全”，我们完全可以通过这个样例来对机器学习的一般性步骤进行一个大致的认知</p>
<p>该问题来自 Coursera 上斯坦福大学机器学习课程（which is 我的入坑课程），其叙述如下：现有包含 47 个房子的面积和价格，需要建立一个模型对新的房价进行预测。稍微翻译一下问题，可以得知：</p>
<ul>
<li>输入数据只有一维、亦即房子的面积</li>
<li>目标数据也只有一维、亦即房子的价格</li>
<li>我们需要做的、就是根据已知的房子的面积和价格的关系进行机器学习</li>
</ul>
<p>下面我们就来一步步地进行操作</p>
<a id="more"></a>
<h1 id="获取与处理数据"><a href="#获取与处理数据" class="headerlink" title="获取与处理数据"></a>获取与处理数据</h1><p>原始数据集的前 10 个样本如下表所示，这里房子面积和房子价格的单位可以随意定夺、因为它们不会对结果造成影响：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>房子面积</th>
<th>房子价格</th>
<th>房子面积</th>
<th>房子价格</th>
</tr>
</thead>
<tbody>
<tr>
<td>2104</td>
<td>399900</td>
<td>1600</td>
<td>329900</td>
</tr>
<tr>
<td>2400</td>
<td>369000</td>
<td>1416</td>
<td>232000</td>
</tr>
<tr>
<td>3000</td>
<td>539900</td>
<td>1985</td>
<td>299900</td>
</tr>
<tr>
<td>1534</td>
<td>314900</td>
<td>1427</td>
<td>198999</td>
</tr>
<tr>
<td>1380</td>
<td>212000</td>
<td>1494</td>
<td>242500</td>
</tr>
</tbody>
</table>
</div>
<p>完整的数据集可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/_Data/prices.txt" target="_blank" rel="external">这里</a>。虽然该数据集比较简单，但可以看到其中的数字都相当大。保留它原始形式确实有可能是有必要的，但一般而言、我们应该对它做简单的处理以期望能够降低问题的复杂度。在这个例子里，我们采取常用的、将输入数据标准化的做法，其数学公式为：</p>
<script type="math/tex; mode=display">
X = \frac{X - \bar X}{std(X)}</script><p>其中<script type="math/tex">\bar X</script>表示<script type="math/tex">X</script>（房子面积）的均值、<script type="math/tex">std(X)</script>表示<script type="math/tex">X</script>的标准差（Standard Deviation）。代码实现则如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入需要用到的库</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="comment"># 定义存储输入数据（x）和目标数据（y）的数组</span></div><div class="line">x, y = [], []</div><div class="line"><span class="comment"># 遍历数据集，变量 sample 对应的正是一个个样本</span></div><div class="line"><span class="keyword">for</span> sample <span class="keyword">in</span> open(<span class="string">"../_Data/prices.txt"</span>, <span class="string">"r"</span>):</div><div class="line">    <span class="comment"># 由于数据是用逗号隔开的，所以调用 Python 中的 split 方法并将逗号作为参数传入</span></div><div class="line">    _x, _y = sample.split(<span class="string">","</span>)</div><div class="line">    <span class="comment"># 将字符串数据转化为浮点数</span></div><div class="line">    x.append(float(_x))</div><div class="line">    y.append(float(_y))</div><div class="line"><span class="comment"># 读取完数据后，将它们转化为 Numpy 数组以方便进一步的处理</span></div><div class="line">x, y = np.array(x), np.array(y)</div><div class="line"><span class="comment"># 标准化</span></div><div class="line">x = (x - x.mean()) / x.std()</div><div class="line"><span class="comment"># 将原始数据以散点图的形式画出</span></div><div class="line">plt.figure()</div><div class="line">plt.scatter(x, y, c=<span class="string">"g"</span>, s=<span class="number">20</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>上面这段代码的运行结果如下图所示：</p>
<img src="/posts/372587d5/p1.png" alt="预处理后的数据散点图" title="预处理后的数据散点图">
<p>这里横轴是标准化后的房子面积，纵轴是房子价格。以上我们已经比较好地完成了机器学习任务的第一步：数据预处理</p>
<h1 id="选择与训练模型"><a href="#选择与训练模型" class="headerlink" title="选择与训练模型"></a>选择与训练模型</h1><p>在弄好数据之后、下一步就要开始选择相应的学习方法和模型了。幸运的是，通过可视化原始数据，我们可以非常直观地感受到：我们很有可能通过线性回归（Linear Regression）中的多项式拟合来得到一个不错的结果。其模型的数学表达式如下：</p>
<p><strong><em>注意：用多项式拟合散点只是线性回归的很小的一部分、但是它的直观意义比较明显。考虑到问题比较简单、我们才选用了多项式拟合。线性回归的详细讨论超出了本书的范围，这里不做赘述</em></strong></p>
<script type="math/tex; mode=display">
f(x|p;n)=p_0x^n+p_1x^{n-1}+...+p_{n-1}x+p_n

L(p;n)=\frac 12\sum_{i=1}^m[f(x|p;n)-y]^2</script><p>其中<script type="math/tex">f(x|p;n)</script>就是我们的模型，<code>p</code>、<code>n</code>都是模型的参数，其中<code>p</code>是多项式<code>f</code>的各个系数、<code>n</code>是多项式的次数。<script type="math/tex">L(p;n)</script>则是模型的损失函数，这里我们采用了常见的平方损失函数、也就是所谓的欧氏距离（或说向量的二范数）。<code>x</code>、<code>y</code>则分别是输入向量和目标向量；在我们这个样例中，<code>x</code>、<code>y</code>这两个向量都是 47 维的向量，分别由 47 个不同的房子面积、房子价格所构成</p>
<p>在确定好模型后，我们就可以开始编写代码来进行训练了。对于大多数机器学习算法，所谓的训练正是最小化某个损失函数的过程，我们这个多项式拟合的模型也不例外：我们的目的就是让上面定义的<script type="math/tex">L(p;n)</script>最小。在数理统计领域里面有专门的理论研究这种回归问题，其中比较有名的正规方程更是直接给出了一个简单的解的通式。不过由于有 Numpy 的存在，这个训练过程甚至变得还要更加简单一些：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 在(-2,4)这个区间上取 100 个点作为画图的基础</span></div><div class="line">x0 = np.linspace(<span class="number">-2</span>, <span class="number">4</span>, <span class="number">100</span>)</div><div class="line"></div><div class="line"><span class="comment"># 利用 Numpy 的函数定义训练并返回多项式回归模型的函数</span></div><div class="line"><span class="comment"># deg 参数代表着模型参数中的 n、亦即模型中多项式的次数</span></div><div class="line"><span class="comment"># 返回的模型能够根据输入的 x（默认是 x0）、返回相对应的预测的 y</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span><span class="params">(deg)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">lambda</span> input_x=x0: np.polyval(np.polyfit(x, y, deg), input_x)</div></pre></td></tr></table></figure>
<p>这里需要解释 Numpy 里面带的两个函数：<code>polyfit</code>和<code>polyval</code>的用法：</p>
<ul>
<li><code>polyfit(x, y, deg)</code>：该函数会返回使得上述（注：该公式中的<code>x</code>和<code>y</code>就是输入的<code>x</code>和<code>y</code>）<script type="math/tex">L(p;n)=\frac 12\sum_{i=1}^m[f(x|p;n)-y]^2</script>最小的参数<code>p</code>、亦即多项式的各项系数。换句话说，该函数就是模型的训练函数</li>
<li><code>polyval(p, x)</code>：根据多项式的各项系数<code>p</code>和多项式中<code>x</code>的值、返回多项式的值<code>y</code></li>
</ul>
<h1 id="评估与可视化结果"><a href="#评估与可视化结果" class="headerlink" title="评估与可视化结果"></a>评估与可视化结果</h1><p>模型做好后、我们就要尝试判断各种参数下模型的好坏了。为简洁，我们采用<script type="math/tex">n=1,4,10</script>这三组参数进行评估。由于我们训练的目的是最小化损失函数，所以用损失函数来衡量模型的好坏似乎是一个合理的做法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 根据参数 n、输入的 x、y 返回相对应的损失</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cost</span><span class="params">(deg, input_x, input_y)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * ((get_model(deg)(input_x) - input_y) ** <span class="number">2</span>).sum()</div><div class="line"></div><div class="line"><span class="comment"># 定义测试参数集并根据它进行各种实验</span></div><div class="line">test_set = (<span class="number">1</span>, <span class="number">4</span>, <span class="number">10</span>)</div><div class="line"><span class="keyword">for</span> d <span class="keyword">in</span> test_set:</div><div class="line">    <span class="comment"># 输出相应的损失</span></div><div class="line">    print(get_cost(d, x, y))</div></pre></td></tr></table></figure>
<p>所得的结果是：当<script type="math/tex">n=1,4,10</script>时，损失的头两个数字分别为 96、94 和 75。这么看来似乎是<script type="math/tex">n=10</script>优于<script type="math/tex">n=4</script>而<script type="math/tex">n=1</script>最差；但从上面那张图可以看出，似乎直接选择<script type="math/tex">n=1</script>作为模型的参数才是最好的选择。这里的矛盾的来源正是前文所提到过的过拟合情况</p>
<p>那么怎么最直观地了解是否出现过拟合了呢？当然还是画图了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 画出相应的图像</span></div><div class="line">plt.scatter(x, y, c=<span class="string">"g"</span>, s=<span class="number">20</span>)</div><div class="line"><span class="keyword">for</span> d <span class="keyword">in</span> test_set:</div><div class="line">    plt.plot(x0, get_model(d)(), label=<span class="string">"degree = &#123;&#125;"</span>.format(d))</div><div class="line">plt.xlim(<span class="number">-2</span>, <span class="number">4</span>)</div><div class="line"><span class="comment"># 将横轴、纵轴的范围分别限制在(-2,4)、(10^5,8 * 10^5)</span></div><div class="line">plt.ylim(<span class="number">1e5</span>, <span class="number">8e5</span>)</div><div class="line"><span class="comment"># 调用 legend 方法使曲线对应的 label 正确显示</span></div><div class="line">plt.legend()</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>上面这段代码的运行结果如下图所示：</p>
<img src="/posts/372587d5/p2.png" alt="线性回归的可视化" title="线性回归的可视化">
<p>其中，蓝线、绿线、红线分别代表<script type="math/tex">n=1</script>、<script type="math/tex">n=4</script>、<script type="math/tex">n=10</script>的情况（上图的右上角亦有说明）。可以看出，从<script type="math/tex">n=4</script>开始模型就已经开始出现过拟合现象了，到<script type="math/tex">n=10</script>时模型已经变得非常不合理</p>
<p>至此，可以说这个问题就已经基本解决了。在这个样例里面，除了交叉验证、我们涵盖了机器学习中的大部分主要步骤（之所以没有进行交叉验证是因为数据太少了……）。代码部分加起来总共 40~50 行，应该算是一个比较合适的长度。希望大家能够通过这个样例对机器学习有个大概的理解、也希望它能引起大家对机器学习的兴趣</p>
]]></content>
      
        <categories>
            
            <category> 绪论 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[人生苦短，我用 Python]]></title>
      <url>/posts/698a0893/</url>
      <content type="html"><![CDATA[<p>上一篇文章大概地介绍了一下机器学习的各种概念，这一篇文章我们则会主要讲讲脚本语言 Python 相关的一些东西。本文题目是在 Python 界流传甚广的“谚语”，它讲述了 Python 强大的功能与易于上手的特性</p>
<a id="more"></a>
<h1 id="为何选择-Python"><a href="#为何选择-Python" class="headerlink" title="为何选择 Python"></a>为何选择 Python</h1><p>援引开源运动的领袖人物 Eric Raymond 的说法：“Python 语言非常干净，设计优雅，具有出色的模块化特性。其最出色的地方在于，它鼓励清晰易读的代码，特别适合以渐进开发的方式构造项目”。Python 的可读性使得即使是刚学不久的人也看懂大部分的代码，Python 庞大的社区和大量的开发文档更是使得初学者能够快速地实现许许多多令人惊叹的功能。对于 Python 的程序，人们甚至有时会戏称其为“可执行的伪代码（executable pseudo-code）”以突显它的清晰性和可读性</p>
<p>Python 的强大是毋庸置疑的，上文提到的 Eric Raymond 甚至称其“过于强大了”。与之相对应的、就是 Python 的速度比较慢。然而比起 Python 开发环境提供的海量高级数据结构（如列表、元组、字典、集合等）和数之不尽的第三方库、再加上高速的 CPU 和近代发展起来的 GPU 编程，速度的问题就显得没那么尖锐。况且 Python 还能通过各种途径来使用 C / C++ 代码来编写核心代码，其强大的“胶水”功能使其速度（在程序员能力允许的情况下）和纯粹的 C / C++ 相比已经相去不远。一个典型的例子、也是我们会在本书常常运用到的、就是 Python 中 Numpy 这个第三方库。编写它的语言正是底层语言（C 和 Fortran），其支持向量、矩阵操作的特性和优异的速度使得 Python 在科学计算这一领域大放异彩</p>
<p><strong><em>注意：Python 及本博客会用到的两个非常优异的第三方库——Numpy 和 Tensorflow 的用法摘要我们会开单独的章节进行说明</em></strong></p>
<h1 id="Python-在机器学习领域的优势"><a href="#Python-在机器学习领域的优势" class="headerlink" title="Python 在机器学习领域的优势"></a>Python 在机器学习领域的优势</h1><p>虽然在上一节叙述了 Python 的种种好处，但不可否认的是，确实存在诸如 MATLAB 和 Mathematica 这样的高级程序语言、它们对机器学习的支持也不错，MATLAB 甚至还自带许多机器学习的应用。但是作为一个问心无愧的程序员，我们还是需要提倡支持正版、而 MATLAB 的正版软件需要数千美金。与之相对，由于 Python 是开源项目，几乎所有必要的组件都是完全免费的</p>
<p>之前也提到过 Python 的速度问题，但是更快更底层的语言、比如 C 和 C++，若使用它们来学习机器学习的话、会不可避免地引发这么一个问题：即使是实现一个非常简单的功能、也需要进行大量的编写和 debug 的过程；在这期间，程序员很有可能忘掉学习机器学习的初衷而迷失在代码的海洋中。笔者曾经尝试过将 Python 上的神经网络框架移植到 C++ 上，这之间的折腾至今难忘</p>
<p>此外，笔者认为、使用 Python 来学习机器学习是和“不要过早优化”这句编程界金句有着异曲同工之妙的。Python（几乎）唯一的缺陷——速度，在初期进行快速检验算法、思想正误及开发工作时，其实基本不是重要问题。这之中的道理是平凡的：如果解决问题的思想存在问题，那么即使拼命去提高程序的运行效率、也只能使问题越来越大而已。这种时候，先使用 Python 进行快速实现、有必要时再用底层代码重写核心代码，从各方面来说都是一个更好的选择</p>
<p><del>（有没有觉得这个人真能扯）</del></p>
]]></content>
      
        <categories>
            
            <category> 绪论 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[机器学习综述]]></title>
      <url>/posts/a0837b26/</url>
      <content type="html"><![CDATA[<p>“机器学习”在最近虽可能不至于到人尽皆知的程度、却也是非常火热的词汇。机器学习是英文单词“Machine Learning”（简称ML）的直译，从字面上便说明了这门技术是让机器进行“学习”的技术。然而我们知道机器终究是死的，所谓的“学习”归根结底亦只是人类“赋予”机器的一系列运算。这个“赋予”的过程可以有很多种实现，而 Python 正是其中相对容易上手、同时性能又相当不错的一门语言。作为综述，我们只打算谈谈机器学习相关的一些比较宽泛的知识，介绍与说明为何要使用 Python 来作为机器学习的工具的工作则交给下一篇文章来做。而在最后，我们会提供一个简短易懂的、具有实际意义的例子来给大家提供一个直观的感受</p>
<p>由于所涉及到的东西都比较基础，有相应知识背景的观众老爷大可不必看“绪论”这一分类下的文章 ( σ’ω’)σ</p>
<p>以下是目录：</p>
<ul>
<li><a href="/posts/698a0893/" title="人生苦短，我用 Python">人生苦短，我用 Python</a></li>
<li><a href="/posts/372587d5/" title="第一个机器学习样例">第一个机器学习样例</a></li>
<li><a href="/posts/c939f06b/" title="“绪论”小结">“绪论”小结</a>
</li>
</ul>
<a id="more"></a>
<h1 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a>什么是机器学习？</h1><p>正如前面所说，由于近期的各种最新成果、使得“机器学习”成为了非常热门的词汇。机器学习在各种邻域的优异表现（围棋界的Master是其中最具代表性的存在），使得各行各业的人们都或多或少对机器学习产生了兴趣与敬畏。然而与此同时，对机器学习有所误解的群体也日益壮大；他们或将机器学习想得过于神秘、或将它想得过于万能。然而事实上，清晨的一句“今天天气真好”、朋友之间的寒暄“你刚刚是去吃饭了吧”、考试过后的感叹“复习了那么久终有收获”……这些日常生活中随处可见的话语，其背后却已蕴含了“学习”的思想——它们都是利用以往的经验、对未知的新情况做出的有效的决策。而把这个决策的过程交给计算机来做、可以说就是“机器学习”的一个最浅白的定义</p>
<p>我们或许可以先说说机器学习与以往的计算机工作样式有什么不同。传统的计算机如果想要得到某个结果、需要人类赋予它一串实打实的指令，然后计算机就根据这串指令一步步地执行下去。这个过程中的因果关系非常明确，只要人类的理解不出偏差、运行结果是可以准确预测的。但是在机器学习中，这一传统样式被打破了：计算机确实仍然需要人类赋予它一串指令，但这串指令往往不能直接得到结果；相反，它是一串赋予了机器“学习能力”的指令。在此基础上，计算机需要进一步地接受“数据”并根据之前人类赋予它的“学习能力”从中“学习”出最终的结果，这个结果往往是无法仅仅通过直接编程得出的。是故这里就导出了稍微深一点的机器学习的定义：它是一种让计算机利用数据而非指令来进行各种工作的方法。在这背后，最关键的就是“统计”的思想，它所推崇的“相关而非因果”的概念是机器学习的理论根基。在此基础上，机器学习可以说是计算机使用输入给它的数据、利用人类赋予它的算法得到某种模型的过程，其最终的目的则是使用该模型、预测未来未知数据的信息</p>
<p>既然提到了统计，那么一定的数学理论就不可或缺。相关的、比较简短的定义会在第四章给出（PAC框架），这里我们就先只叙述一下机器学习在统计理论下的、比较深刻的本质：它追求的是合理的假设空间（Hypothesis Space）的选取和模型的泛化（Generalization）能力。该句中出现了一些专用术语，详细的定义会在介绍术语时提及，这里我们提供一个直观：</p>
<ul>
<li>所谓假设空间，就是我们的模型在数学上的“适用场合”</li>
<li>所谓的泛化能力，就是我们的模型在未知数据上的表现</li>
</ul>
<p><strong><em>注意：上述本质严格来说应该是 PAC Learning 的本质；在其余的理论框架下、机器学习是可以具有不同的内核的</em></strong></p>
<p>从上面的讨论可以看出，机器学习和人类思考的过程有或多或少的类似。事实上，我们在第六、第七章讲的神经网络（Neural Network，简称 NN）和卷积神经网络（Convolutional Neural Network，简称 CNN）背后确实有着相应的神经科学的理论背景。然而与此同时我们需要知道的是，机器学习并非是一个“会学习的机器人”和“具有学习的人造人”之类的，这一点从上面诸多讨论也可以明晰（惭愧的是，我在第一次听到“机器学习”四个字时，脑海中浮现的正是一个“聪明的机器人”的图像，甚至还幻想过它和人类一起生活的场景）。相反的，它是被人类利用的、用于发掘数据背后信息的工具</p>
<p>当然，现在也不乏“危险的人工智能”的说法，霍金大概是其中的“标杆”，这位伟大的英国理论物理学家甚至警告说“人工智能的发展可能意味着人类的灭亡”。孰好孰坏果然还是见仁见智，但可以肯定的是：本书所介绍的内容绝不至于导致世界的毁灭，大家大可轻松愉快地进行接下来的阅读 ( σ’ω’)σ</p>
<h1 id="机器学习常用术语"><a href="#机器学习常用术语" class="headerlink" title="机器学习常用术语"></a>机器学习常用术语</h1><p>机器学习领域有着许多非常基本的术语，这些术语在外人听来可能相当高深莫测、它们事实上也可能拥有非常复杂的数学背景，但我们需要知道：它们往往也拥有着相对浅显平凡的直观理解（上一节的假设空间和泛化能力就是两个例子）。本节会对这些常用的基本术语进行说明与解释，它们背后的数学理论会有所阐述、但不会涉及到过于本质的东西</p>
<p>正如前文反复强调的，数据在机器学习中发挥着不可或缺的作用；而用于描述数据的术语有好几个，它们是需要被牢牢记住的：</p>
<ul>
<li>“数据集”（Data Set）：就是数据的集合的意思。其中，每一条单独的数据被称为“样本”（Sample）。若没有进行特殊说明，本书都会假设数据集中样本之间在各种意义下相互独立。事实上，除了某些特殊的模型（如隐马尔可夫模型和条件随机场），该假设在大多数场景下都是相当合理的</li>
<li>对于每个样本，它通常具有一些“属性”（Attribute）或说“特征”（Feature），特征所具体取的值就被称为“特征值”（Feature Value）</li>
<li>特征和样本所张成的空间被称为“特征空间”（Feature Space）和“样本空间”（Sample Space），可以把它们简单地理解为特征和样本“可能存在的空间”。</li>
<li>相对应的，我们有“标签空间”（Label Space），它描述了模型的输出“可能存在的空间”；当模型是分类器时、我们通常会称之为“类别空间”</li>
</ul>
<p>其中、数据集又可以分为以下三类：</p>
<ul>
<li>训练集（Training Set）；顾名思义、它是总的数据集中用来训练我们模型的部分。虽说将所有数据集都拿来当做训练集也无不可，不过为了提高及合理评估模型的泛化能力、我们通常只会取数据集中的一部分来当训练集</li>
<li>测试集（Test Set）；顾名思义、它是用来测试、评估模型泛化能力的部分。测试集不会用在模型的训练部分；换句话说，测试集相对于模型而言是“未知”的、所以拿它来评估模型的泛化能力是相当合理的</li>
<li>交叉验证集（Cross-Validation Set，简称 CV Set）；这是比较特殊的一部分数据，它是用来调整模型具体参数的</li>
</ul>
<p><strong><em>注意：需要指出的是，获取数据集这个过程是不平凡的；尤其是当今“大数据”如日中天的情景下，诸如“得数据者得天下”的说法也不算诳语。在此我推荐一个非常著名的、含有大量真实数据集的网站——<a href="http://archive.ics.uci.edu/ml/datasets.html" target="_blank" rel="external">UCI</a>，接下来的篇章中也常常会用到其中一些合适的数据集来评估我们自己实现的模型</em></strong></p>
<p>我们可以通过具体的例子来理解上述概念。比如、我们假设小明是一个在北京读了一年书的学生，某天他想通过宿舍窗外的风景（能见度、温度、湿度、路人戴口罩的情况等）来判断当天的雾霾情况并据此决定是否戴口罩。此时，他过去一年的经验就是他拥有的数据集，过去一年中每一天的情况就是一个样本。“能见度”、“温度”、“湿度”、“路人戴口罩的情况”就是四个特征，而（能见度）“低”、（温度）“低”、（湿度）“高”、（路人戴口罩的）“多”就是相对应的特征值。现在小明想了想、决定在脑中建立一个模型来帮自己做决策，该模型将利用过去一年的数据集来对如今的情况作出“是否戴口罩”的决策。此时小明可以用过去一年中 8 个月的数据量来做训练集、2 个月的量来做测试集、2 个月的量来做交叉验证集，那么小明就需要不断地思考（训练模型）：</p>
<ul>
<li>用训练集训练出的模型是怎样的？</li>
<li>该模型在交叉验证集上的表现怎么样？<ul>
<li>如果足够好了，那么思考结束（得到最终模型）</li>
<li>如果不够好，那么根据模型在交叉验证集上的表现、重新思考（调整模型参数）</li>
</ul>
</li>
</ul>
<p>最后，小明可能会在测试集上评估一下自己刚刚思考后得到的模型的性能、然后根据这个性能和模型作出的“是否戴口罩”的决策来综合考虑自己到底戴不戴口罩<br>接下来说明一下上一节中提到过的重要概念：假设空间与泛化能力。泛化能力的含义在上文也有说明，为强调、这里再叙述一遍：</p>
<ul>
<li>泛化能力针对的其实是学习方法，它用于衡量该学习方法学习到的模型在整个样本空间上的表现</li>
</ul>
<p>这一点当然是十分重要的，因为我们拿来训练我们模型的数据终究只是样本空间的一个很小的采样，如果只是过分专注于它们的话、就会出现所谓的“过拟合”（Over Fitting）的情况。当然，如果过分罔顾训练数据，又会出现“欠拟合”（Under Fitting）。我们可以用一张图来直观感受过拟合和欠拟合（如下图所示，左为欠拟合、右为过拟合）：</p>
<img src="/posts/a0837b26/p1.png" alt="欠拟合与过拟合" title="欠拟合与过拟合">
<p>所以我们需要“张弛有度”、找到最好的那个平衡点。统计学习中的结构风险最小化（Structural Risk Minimization、简称 SRM）就是研究这个的，它和传统的经验风险最小化（Empirical Risk Minimization、简称 ERM）相比，注重于对风险上界的最小化、而不是单纯使经验风险最小化。它有一个原则：在使得风险上界最小的函数子集中、挑选出使得经验风险最小的函数。而这个函数子集，正是我们之前提到过的假设空间</p>
<p><strong><em>注意：所谓经验风险，可以理解为训练数据集上的风险。相对应的，ERM 则可以理解为只注重训练数据集的学习方法，它的理论基础是经验风险在某种足够合理的数学意义上一致收敛于期望风险、亦即所谓的“真正的”风险</em></strong></p>
<p>关于 SRM 和 ERM 的详细讨论会涉及到诸如 VC 维和正则化的概念，这里不进行详细展开、但我们需要有这么一个直观：为了使我们学习方法训练出的模型泛化能力足够好，我们需要对我们的模型做出一定的“限制”、而这个“限制”就表现在假设空间的选取上。一个非常普遍的做法是对模型的复杂度做出一定的惩罚、从而使模型趋于精简。这与所谓的“奥卡姆剃刀原理”<del>（奥卡姆：我的剃刀还能再战 500 年）</del>不谋而合：“如无必要，勿增实体”“切勿浪费较多的东西去做，用较少的东西、同样可以做好事情”</p>
<p>相比起通过选取合适的假设空间来规避过拟合，进行交叉验证（Cross Validation）则可以让我们知道过拟合的程度、从而帮助我们选择合适的模型。常见的交叉验证有三种：</p>
<ul>
<li>S-fold Cross Validation：中文可翻译成S折交叉验证，它是应用最多的一种方法。其方法大致如下：<ul>
<li>将数据分成 S 份：<script type="math/tex">D=\{ D_1,D_2,...,D_S\}</script>、一共作 S 次试验</li>
<li>在第 i 次试验中，使用作为<script type="math/tex">D-D_i</script>训练集、<script type="math/tex">D_i</script>作为测试集对模型进行训练、评测</li>
<li>最终选择平均测试误差最小的模型</li>
</ul>
</li>
<li>留一交叉验证（Leave-one-out Cross Validation）：这是S折交叉验证的特殊情况，此时 <script type="math/tex">S=N</script></li>
<li>简易交叉验证：这种实现起来最简单、也是本博客（在进行交叉验证时）所采用的方法。它简单地将数据进行随机分组、最后达到训练集约占原数据的 70% 的程度（这个比率可以视情况改变），选择模型时使用测试误差作为标准</li>
</ul>
<h1 id="机器学习的重要性"><a href="#机器学习的重要性" class="headerlink" title="机器学习的重要性"></a>机器学习的重要性</h1><p>道理说了不少，但到底为什么要学机器学习、机器学习的重要性又在哪里呢？事实上，回顾历史我们可以发现，人类的发展通常伴随着简单体力劳动向复杂脑力劳动的过渡。过去的工作基本都有着明确的定义，告诉你这一步怎么做、下一步再怎么做。而如今这一类的工作已是越来越少，取而代之的是更为宽泛模糊的、概念性的东西，比如说“将本季度的产品推向最合适的市场，在最大化期望利润的同时、尽量做到风险最小化”这种需求。想要做好这样的任务，我们需要获取相应的数据；虽说网络的存在让我们能够得到数之不尽的数据，然而从这些数据中获得信息与知识却不是一项平凡的工作。我们当然可以人工地、仔细地逐项甄选，但这样显然就又回到了最初的原点。机器学习这门技术，可以说正因此应运而生</p>
<p>单单抽象地说一大堆空话可能会让人头晕脑胀，我们就举一举机器学习具体的应用范围吧，从中大概能够比较直观地看出机器学习的强大与重要。<br>发展到如今，机器学习的“爪牙”可谓已经伸展到了各个角落、包括但不限于：</p>
<ul>
<li>机器视觉、也就是最近机器学习里很火热的深度学习的一种应用</li>
<li>语音识别、也就是微软 Cortana 背后的核心技术</li>
<li>数据挖掘、也就是耳熟能详的大数据相关的领域</li>
<li>统计学习、也就是本书讲解的主要范围之一，有许许多多著名的算法（比如支持向量机 SVM）都源于统计学习（但是统计学习还是和机器学习有着区别；简单地说，统计学习偏数学而机器学习偏实践）</li>
</ul>
<p>机器学习还能够进行模式识别、自然语言处理等等，之前提过的围棋界的 Master 和最新人工智能在德州扑克上的表现亦无不呈现着机器学习强大的潜力。一言以蔽之，机器学习是当今的热点，虽说不能保证它的热度能 100% 地一直延续下去，至少本人认为、它能在相当长的一段时间内保持强大的生命力</p>
]]></content>
      
        <categories>
            
            <category> 绪论 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 综述 </tag>
            
            <tag> 目录 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[PyML]]></title>
      <url>/posts/4f2fecf/</url>
      <content type="html"><![CDATA[<p>【 <a href="https://github.com/carefree0910/MachineLearning" target="_blank" rel="external">这里</a>是本博客的配套 GitHub，所有涉及到的代码都涵盖在了里面；如果觉得本博客有帮助的话，希望观众老爷们能给个 star ~ ( σ’ω’)σ 】</p>
<p>其实要说搭博客的想法的话，在相当久之前就已经有了。然而由于种种原因<del>（懒）</del>，导致一直搁置至今</p>
<p>不过搭一个相对好看的博客比我想象的要难不少，我在经历了打算从头搭建<script type="math/tex">\rightarrow</script>打算使用 WordPress<script type="math/tex">\rightarrow</script>打算使用 Jekyll 这三个阶段后，最终还是选择了 Hexo + Next 主题这一个组合</p>
<h1 id="主旨"><a href="#主旨" class="headerlink" title="主旨"></a>主旨</h1><p>虽然我大言不惭地将站点命名为了“Python 与机器学习”，但其实该博客更像是针对我的个人 repo——<a href="https://github.com/carefree0910/MachineLearning" target="_blank" rel="external">MachineLearning</a> 的一个文档（另一个文档为我的个人<a href="https://zhuanlan.zhihu.com/carefree0910-pyml" target="_blank" rel="external">知乎专栏</a>）。这里所说的文档不仅包括了代码的实现思路、调用方法，还包括了背后的理论基础等等。不过囿于本人学识尚浅，许多地方可能都会有错漏，届时希望大家能不吝指出 ( σ’ω’)σ</p>
<h1 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h1><p>由于我也在<del>苦逼</del>绝赞学习中，所以内容会不断更新；特别是在当前这种几乎一个星期就出一个新技术的时代，“啃老本”这种想法几乎是不可行的<del>（说得好像你有老本似的）</del></p>
<p>截止至 2017-4-19、本博客的内容包括：</p>
<ul>
<li><a href="/posts/a0837b26/" title="Python 与机器学习绪论（Introduction）">Python 与机器学习绪论（Introduction）</a></li>
<li><a href="/posts/1607712a/" title="朴素贝叶斯（Naive Bayes）">朴素贝叶斯（Naive Bayes）</a></li>
<li><a href="/posts/dd35ec8b/" title="决策树（Decision Tree）">决策树（Decision Tree）</a></li>
<li><a href="/posts/d8e97c87/" title="集成学习（Ensemble Learning）">集成学习（Ensemble Learning）</a></li>
<li><a href="/posts/487ba3a6/" title="支持向量机（Support Vector Machine）">支持向量机（Support Vector Machine）</a></li>
<li><a href="/posts/d94622d/" title="神经网络（Neural Network）">神经网络（Neural Network）</a></li>
<li><a href="/posts/78be0d7d/" title="卷积神经网络（Convolutional Neural Network）">卷积神经网络（Convolutional Neural Network）</a></li>
<li><a href="https://github.com/carefree0910/MachineLearning/tree/master/_Dist" target="_blank" rel="external">具体的应用实例（Applications）</a></li>
</ul>
<p>对于有相应博客的内容，链接会导向相应的综述，否则会导向相应的源代码</p>
]]></content>
      
        
        <tags>
            
            <tag> 综述 </tag>
            
            <tag> 目录 </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
