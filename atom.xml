<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Python 与机器学习</title>
  <subtitle>Python &amp; Machine Learning</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.carefree0910.com/"/>
  <updated>2017-04-28T11:26:50.000Z</updated>
  <id>http://www.carefree0910.com/</id>
  
  <author>
    <name>射命丸咲</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>“支持向量机”小结</title>
    <link href="http://www.carefree0910.com/posts/5b3e9c59/"/>
    <id>http://www.carefree0910.com/posts/5b3e9c59/</id>
    <published>2017-04-28T11:25:48.000Z</published>
    <updated>2017-04-28T11:26:50.000Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>感知机利用 SGD 能保证对线性可分数据集正确分类（无论学习速率为多少）、但它没怎么考虑泛化能力的问题</li>
<li>线性 SVM 通过引入间隔（硬、软）最大化的概念来增强模型的泛化能力</li>
<li>核技巧能够将线性算法“升级”为非线性算法，通过将原始问题转化为对偶问题能够非常自然地对核技巧进行应用</li>
<li>对于一个二分类模型，有许多方法能够直接将它拓展为多分类问题</li>
<li>SVM 的思想能用于做回归（SVR）；具体而言、SVR 容许模型输出和真值之间存在<script type="math/tex">\epsilon</script>的差距以期望提高泛化能力</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;感知机利用 SGD 能保证对线性可分数据集正确分类（无论学习速率为多少）、但它没怎么考虑泛化能力的问题&lt;/li&gt;
&lt;li&gt;线性 SVM 通过引入间隔（硬、软）最大化的概念来增强模型的泛化能力&lt;/li&gt;
&lt;li&gt;核技巧能够将线性算法“升级”为非线性算法，通过将原始
    
    </summary>
    
      <category term="支持向量机" scheme="http://www.carefree0910.com/categories/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
    
      <category term="小结" scheme="http://www.carefree0910.com/tags/%E5%B0%8F%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>相关数学理论</title>
    <link href="http://www.carefree0910.com/posts/613bbb2f/"/>
    <id>http://www.carefree0910.com/posts/613bbb2f/</id>
    <published>2017-04-28T11:01:43.000Z</published>
    <updated>2017-04-28T12:23:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章会叙述之前没有解决的纯数学问题，会涉及到相当庞杂的数学概念与思想，其中一些推导的难度相对而言可能会比较大</p>
<a id="more"></a>
<h1 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h1><p>前文已经相当充分地说明了梯度下降的直观，本节则打算用较严谨的数学语言来重新叙述一遍这个方法</p>
<p>首先说明其地位：梯度下降法（又称最速下降法）是求解无约束最优化问题的最常用的手段之一，同时由于现有的深度学习框架（比如 Tensorflow）基本都会含有自动求导并更新参数的功能、所以梯度下降法的实现往往会简单且高效</p>
<p>其次说明一下梯度下降法的大致步骤。正如前文所说、梯度下降法的核心在于在于函数的“求导”，而由于一般来说样本都是高维的样本（亦即<script type="math/tex">x \in \mathbb{R}^{n}</script>、<script type="math/tex">n \geq 2</script>）、所以此时我们要求的其实是函数的梯度。由于梯度是微积分里面的基础知识、这里就不“追本溯源”般地讲解梯度的定义之类的了，如果确实不甚了解且不满足于前文给出的直观解释的话、可以参见维基百科中的详细定义（<a href="https://zh.wikipedia.org/wiki/梯度" target="_blank" rel="external">中文版</a>和<a href="https://en.wikipedia.org/wiki/Gradient" target="_blank" rel="external">英文版</a>都有，个人建议尽量看英文版）</p>
<p>不管怎么说、函数梯度的这一点性质需要谨记：它是使函数值上升最快的方向，这就同时意味着负梯度是使函数值下降最快的“更新方向”。利用该性质，梯度下降法认为在每一步迭代中、都应该以梯度为更新方向“迈进”一步；在机器学习中、我们通常把这时迈进的“步长”称作“学习速率”：</p>
<ol>
<li><strong>输入</strong>：想要最小化的目标函数<script type="math/tex">f(x)</script>、迭代次数 M、学习速率<script type="math/tex">\eta</script>、计算精度<script type="math/tex">\epsilon</script>，其中<script type="math/tex">x \in \mathbb{R}^{n}</script></li>
<li><strong>过程</strong>：<ol>
<li>求出<script type="math/tex">f(x)</script>的梯度函数：  <script type="math/tex; mode=display">
g(x) \triangleq \nabla f(x)</script></li>
<li>取一个初始估计值<script type="math/tex">x^{\left( 0 \right)} \in \mathbb{R}^{n}</script></li>
<li>对<script type="math/tex">j = 1,\ldots,M</script>：<ol>
<li>计算负梯度——<script type="math/tex">g_{j} = - g(x^{\left( j \right)})</script>，若<script type="math/tex">\left\| g_{j} \right\| < \epsilon</script>则退出循环、令最终解<script type="math/tex">x^{*} = x^{\left( j \right)}</script></li>
<li>否则、向更新方向<script type="math/tex">g_{j}</script>迈进步长为<script type="math/tex">\eta</script>的一步：  <script type="math/tex; mode=display">
x^{\left( j + 1 \right)} = x^{\left( j \right)} + \eta g_{j}</script></li>
<li>若<script type="math/tex">\left\| f\left( x^{\left( j + 1 \right)} \right) - f(x^{\left( j \right)}) \right\| < \epsilon</script>或<script type="math/tex">\left\| x^{\left( j + 1 \right)} - x^{\left( j \right)} \right\| < \epsilon</script>则退出循环、令最终解<script type="math/tex">x^{*} = x^{\left( j + 1 \right)}</script></li>
</ol>
</li>
</ol>
</li>
<li><strong>输出</strong>：最终解<script type="math/tex">x^{*}</script></li>
</ol>
<p>上述算法是一个最为朴素的梯度下降法框架，通过在其基础上结合具体的模型进行改进、拓展能够衍生出一系列著名的算法。具体而言、这些拓展算法通常会针对如下两个部分进行改进：</p>
<ul>
<li>不是单纯地把梯度作为更新方向、而是利用更多的属性来定出更新方向</li>
<li>不把学习速率设成常量、而设法让其能够“适应算法”并根据具体情况进行调整</li>
</ul>
<p>有关梯度下降的拓展算法会在下一个系列的文章中进行比较详细的叙述，这里我们仅针对第二点来举一个非常直观的改进例子（仅写出与上述算法中不同的部分）：</p>
<ul>
<li><strong>算法 2.3.2 步</strong><br>对<script type="math/tex">j = 1,\ldots,M</script>：<ul>
<li>否则求出<script type="math/tex">\eta_{j}</script>、使得：  <script type="math/tex; mode=display">
f\left( x^{\left( j \right)} + {\eta_{j}g}_{j} \right) = \min_{\eta>0}{f(x^{\left( j \right)} + \eta g_{j})}</script>然后根据<script type="math/tex">\eta_{j}</script>来更新估计值：  <script type="math/tex; mode=display">
x^{\left( j + 1 \right)} = x^{\left( j \right)} + \eta_{j}g_{j}</script>这种算法又可以称作精确线性搜索准则。当优化问题为凸优化、亦即函数为凸函数时，可以证明若迭代次数 M 足够大、精确线性搜索必定能够收敛到全局最优解</li>
</ul>
</li>
</ul>
<p>考虑到对于具体的机器学习模型而言、其训练时一般会同时用到许多的样本，此时进行梯度下降法的话就不免会遇到一个问题：计算梯度时，是应该同时对多个样本进行求解然后将结果整合、还是对样本逐个进行求解？对该问题的不同解答对应着不同的算法、前文也已经有所提及。具体而言：</p>
<ul>
<li>对于随机梯度下降（SGD）、其求梯度的公式为：  <script type="math/tex; mode=display">
g_{j} = - \nabla f\left( x_{i} \right)</script>其中 i 是一个合适的下标。SGD 的优缺点都比较直观：虽然（在同样的迭代次数下）它的训练速度很快、但它搜索解空间的过程会显得比较盲目（就有种东走一下西走一下的感觉），这直接导致其收敛速度反而可能会更慢。同时如果考虑实际应用的话，由于 SGD 难以并行实现、所以其效率往往会比较低</li>
<li>对于小批量梯度下降（MBGD）、其求梯度的公式为：  <script type="math/tex; mode=display">
g_{j} = - \frac{1}{m}\left( \nabla f\left( x_{S_{1}} \right) + \ldots + \nabla f\left( x_{S_{m}} \right) \right)</script>其中 m 是一个合适的、小于总样本数 N 的数，<script type="math/tex">S_1,...,S_m</script>则是 m 个合适的下标；通常我们会称<script type="math/tex">\left\{ x_{S_1},...,x_{S_m}\right\}</script>为一个 batch。MBGD 可谓是应用得最广泛的梯度下降法，它在单步迭代中会比 SGD 慢、但它对解空间的搜索会显得“可控”很多、从而收敛速度一般反而会比 SGD 要快</li>
<li>对于批量梯度下降（BGD）、其求梯度的公式为：  <script type="math/tex; mode=display">
g_{j} = - \frac{1}{N}\sum_{i = 1}^{N}{\nabla f\left( x_{i} \right)}</script>BGD 会有一种“过犹不及”的感觉，由于它单步迭代中会用到所有样本，所以当训练集很大的时候、无论是时间开销还是空间开销都会变得难以忍受</li>
</ul>
<p>以上我们就大概综述了一遍梯度下降法的框架，更为细致的具体算法则会在下一个系列中介绍神经网络时进行部分说明</p>
<h1 id="拉格朗日对偶性"><a href="#拉格朗日对偶性" class="headerlink" title="拉格朗日对偶性"></a>拉格朗日对偶性</h1><p>如果按照最一般性的定义来讲的话，拉格朗日对偶性会显得太过“纯粹”、或说可以算是数学家的游戏。因此本小节拟打算通过推导如何将软间隔最大化 SVM 的原始最优化问题转化为对偶问题、来间接说明拉格朗日对偶性的一般性步骤</p>
<p>注意到原始问题为：</p>
<script type="math/tex; mode=display">
\min_{w,b}{L\left( w,b,x,y \right) =}\frac{1}{2}\left\| w \right\|^{2} + C\sum_{i = 1}^{N}\xi_{i}</script><p>使得：</p>
<script type="math/tex; mode=display">
y_{i}\left( w \cdot x_{i} + b \right) \geq 1 - \xi_{i}\ (i = 1,\ldots,N)</script><p>其中</p>
<script type="math/tex; mode=display">
\xi_{i} \geq 0\ (i = 1,\ldots,N)</script><p>那么原始问题的拉格朗日函数即为：</p>
<script type="math/tex; mode=display">
L\left( w,b,\xi,\alpha,\beta \right) = \frac{1}{2}\left\| w \right\|^{2} + C\sum_{i = 1}^{N}\xi_{i} - \sum_{i = 1}^{N}{\alpha_{i}\left\lbrack y_{i}\left( w \cdot x_{i} + b \right) - 1 + \xi_{i} \right\rbrack} - \sum_{i = 1}^{N}{\beta_{i}\xi_{i}}</script><p>为求解<script type="math/tex">L</script>的极小、我们需要对<script type="math/tex">w</script>、<script type="math/tex">b</script>和<script type="math/tex">\xi</script>求偏导并令偏导为 0。易知：</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla_{w}L &= w - \sum_{i = 1}^{N}{\alpha_{i}y_{i}x_{i}} = 0 \\
\nabla_{b}L &= \sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0 \\
\nabla_{\xi_{i}}L &= C - \alpha_{i} - \beta_{i} = 0
\end{align}</script><p>解得</p>
<script type="math/tex; mode=display">
w = \sum_{i = 1}^{N}{\alpha_{i}y_{i}x_{i}}</script><script type="math/tex; mode=display">
\sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0</script><p>以及对<script type="math/tex">i = 1,\ldots,N</script>、都有</p>
<script type="math/tex; mode=display">
\alpha_{i} + \beta_{i} = C</script><p>将它们带入<script type="math/tex">L\left( w,b,\xi,\alpha,\beta \right)</script>、得</p>
<script type="math/tex; mode=display">
L\left( w,b,\xi,\alpha,\beta \right) = - \frac{1}{2}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\alpha_{i}\alpha_{j}y_{i}y_{j}\left( x_{i} \cdot x_{j} \right)}} + \sum_{i = 1}^{N}\alpha_{i}</script><p>从而原始问题的对偶问题即为求上式的极大值、亦即</p>
<script type="math/tex; mode=display">
\max_{\alpha}{- \frac{1}{2}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\alpha_{i}\alpha_{j}y_{i}y_{j}\left( x_{i} \cdot x_{j} \right)}} + \sum_{i = 1}^{N}\alpha_{i}}</script><p>其中约束条件为：</p>
<script type="math/tex; mode=display">
\sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0</script><p>以及对<script type="math/tex">i = 1,\ldots,N</script>、都有</p>
<script type="math/tex; mode=display">
\alpha_{i} \geq 0,\ \ \beta_{i} \geq 0</script><script type="math/tex; mode=display">
\alpha_{i} + \beta_{i} = C\ (i = 1,\ldots,N)</script><p>易知上述约束可以简化为对<script type="math/tex">i = 1,\ldots,N</script>、都有</p>
<script type="math/tex; mode=display">
0 \leq \alpha_{i} \leq C</script><p>综上所述即得前文叙述过的软间隔最大化的对偶形式。注意到原始问题是凸二次规划、从而对偶形式的解<script type="math/tex">w^{*}</script>、<script type="math/tex">b^{*}</script>、<script type="math/tex">\xi^{*}</script>、<script type="math/tex">\alpha^{*}</script>和<script type="math/tex">\beta^{*}</script>满足 KKT 条件，亦即：</p>
<script type="math/tex; mode=display">
\nabla_{w}L(w^{*},b^{*},\xi^{*},\alpha^{*},\beta^{*}) = \nabla_{b}L(w^{*},b^{*},\xi^{*},\alpha^{*},\beta^{*}) = \nabla_{\xi}L(w^{*},b^{*},\xi^{*},\alpha^{*},\beta^{*}) = 0</script><p>以及对<script type="math/tex">i = 1,\ldots,N</script>、都有</p>
<script type="math/tex; mode=display">
\alpha_{i}^{*}\left\lbrack y_{i}\left( w^{*} \cdot x_{i} + b^{*} \right) - 1 + \xi^{*} \right\rbrack = 0</script><script type="math/tex; mode=display">
y_{i}\left( w^{*} \cdot x_{i} + b^{*} \right) - 1 + \xi^{*} \geq 0</script><script type="math/tex; mode=display">
\alpha^{*} \geq 0,\beta^{*} \geq 0,\xi^{*} \geq 0</script><script type="math/tex; mode=display">
\xi^{*}\beta^{*} = 0</script><p>由它们就可以推出前文说明过的、<script type="math/tex">w^{*}</script>和<script type="math/tex">b^{*}</script>关于<script type="math/tex">\alpha^{*}</script>的表达式了</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章会叙述之前没有解决的纯数学问题，会涉及到相当庞杂的数学概念与思想，其中一些推导的难度相对而言可能会比较大&lt;/p&gt;
    
    </summary>
    
      <category term="支持向量机" scheme="http://www.carefree0910.com/categories/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="综述" scheme="http://www.carefree0910.com/tags/%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>多分类与支持向量回归</title>
    <link href="http://www.carefree0910.com/posts/1dc4445a/"/>
    <id>http://www.carefree0910.com/posts/1dc4445a/</id>
    <published>2017-04-28T10:43:38.000Z</published>
    <updated>2017-04-28T12:19:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章将简要说明几种将二分类模型拓展为多分类模型的普适性方法，它们不仅能对前三篇文章叙述的感知机和 SVM 进行应用、同时还能应用于在上一个系列中进行了说明的 AdaBoost 二分类模型；在本篇的第四节（也是最后一节）、我们则会简要地说明一下如何将支持向量机的思想应用在回归问题上</p>
<a id="more"></a>
<h1 id="一对多方法（One-vs-Rest）"><a href="#一对多方法（One-vs-Rest）" class="headerlink" title="一对多方法（One-vs-Rest）"></a>一对多方法（One-vs-Rest）</h1><p>一对多方法常简称为 OvR、是一种比较比较“豪放”的方法：对于一个 K 类问题、OvR 将训练 K 个二分类模型<script type="math/tex">\{ G_{1},\ldots,G_{K}\}</script>，每个模型将训练集中的某一类的样本作为正样本、其余类的样本作为负样本。模型的输出空间为实数空间、它反映了模型对决策的“信心”</p>
<p>具体而言、模型<script type="math/tex">G_{i}</script>会把第<script type="math/tex">i</script>类看成一类、把其余类看成另一类并尝试通过训练来区分开第<script type="math/tex">i</script>类和剩余类别；若<script type="math/tex">G_{i}</script>有比较大的自信来判定输入样本<em>x</em>是（或不是）第<script type="math/tex">i</script>类、那么<script type="math/tex">G_{i}(x)</script>将会是一个比较大的正（负）数，否则、<script type="math/tex">G_{i}(x)</script>将会是一个比较小的正（负）数</p>
<p>训练好 K 个模型后、直接将输出最大的模型所对应的类别作为决策即可、亦即：</p>
<script type="math/tex; mode=display">
y_{\text{pred}} = \arg{\max_{i}{G_{i}(x)}}</script><p>之所以称这种方法比较“豪放”、主要是因为对每个模型的训练都存在比较严重的偏差：正样本集和负样本集的样本数之比在原始训练集均匀的情况下将会是<script type="math/tex">\frac{1}{K - 1}</script>。针对该缺陷、一种比较常见的做法是只抽取负样本集中的一部分来进行训练（比如抽取其中的三分之一）</p>
<h1 id="一对一方法（One-vs-One）"><a href="#一对一方法（One-vs-One）" class="headerlink" title="一对一方法（One-vs-One）"></a>一对一方法（One-vs-One）</h1><p>一对一方法常简称为 OvO、可谓是一种很直观的方法：对于一个 K 类问题、OvO 将直接训练出<script type="math/tex">\frac{K\left( K - 1 \right)}{2}</script>个二分类模型<script type="math/tex">\{ G_{12},\ldots,G_{1K},G_{23},\ldots,G_{2K},\ldots,G_{K - 1,K}\}</script>，每个模型都只从训练集中接受两个类的样本来进行训练。模型的输出空间为二值空间<script type="math/tex">\{ - 1, + 1\}</script>、亦即模型只需要具有投票的能力即可</p>
<p>具体而言、模型<script type="math/tex">G_{\text{ij}}(i < j)</script>将接受且仅接受所有第<script type="math/tex">i</script>类和第<script type="math/tex">j</script>类的样本并尝试通过训练来区分开第<script type="math/tex">i</script>类和第<script type="math/tex">j</script>类；同时，假设<script type="math/tex">c_{i}</script>代表第<script type="math/tex">i</script>类的样本空间、那么就有：</p>
<script type="math/tex; mode=display">
G_{ij}(x) = \left\{ \begin{matrix}
 - 1,\ \ & x \in c_{j} \\
 + 1,\ \ & x \in c_{i} \\
\end{matrix} \right.\</script><p>训练好<script type="math/tex">\frac{K\left( K - 1 \right)}{2}</script>个模型后，OvO 将通过投票表决来进行决策、在<script type="math/tex">\frac{K\left( K - 1 \right)}{2}</script>次投票中得票最多的类即为模型所预测的结果。具体而言，如果考察<script type="math/tex">G_{ij}</script>、那么若<script type="math/tex">G_{\text{ij}}</script>输出<script type="math/tex">- 1</script>则第<script type="math/tex">j</script>类得一票、若<script type="math/tex">G_{ij}</script>输出<script type="math/tex">+ 1</script>则第<script type="math/tex">i</script>类得一票。如果只有两个类别（比如第<script type="math/tex">i</script>类和第<script type="math/tex">j</script>类）得票一致、那么直接看针对这两个类别的模型（亦即<script type="math/tex">G_{ij}</script>）的结果即可；如果多于两个类别的得票一致、则需要具体情况具体分析</p>
<p>OvO 是一个相当不错的方法、没有类似于 OvR 中“有偏”的问题。然而它也是有一个显而易见的缺点的——由于模型的量级是<script type="math/tex">K^{2}</script>、所以它的时间开销会相当大</p>
<h1 id="有向无环图方法（Directed-Acyclic-Graph-Method）"><a href="#有向无环图方法（Directed-Acyclic-Graph-Method）" class="headerlink" title="有向无环图方法（Directed Acyclic Graph Method）"></a>有向无环图方法（Directed Acyclic Graph Method）</h1><p>有向无环图方法常简称为 DAG，它的训练过程和 OvO 的训练过程完全一致、区别只在于最后的决策过程。具体而言、DAG 会将<script type="math/tex">\frac{K\left( K - 1 \right)}{2}</script>个模型作为一个有向无环图中的<script type="math/tex">\frac{K\left( K - 1 \right)}{2}</script>节点并逐步进行决策。其工作原理可以用下图进行说明（假设<script type="math/tex">K = 4</script>）：</p>
<img src="/posts/1dc4445a/p1.png" alt="p1.png" title="">
<h1 id="支持向量回归（Support-Vector-Regression）"><a href="#支持向量回归（Support-Vector-Regression）" class="headerlink" title="支持向量回归（Support Vector Regression）"></a>支持向量回归（Support Vector Regression）</h1><p>支持向量回归常简称为 SVR，它的基本思想与“软”间隔的思想类似——传统的回归模型通常只有在模型预测值<script type="math/tex">f(x)</script>和真值<script type="math/tex">y</script>完全一致时损失函数的值才为 0（最经典的就是当损失函数为<script type="math/tex">\left\| f\left( x \right) - y \right\|^{2}</script>的情形），而 SVR 则允许<script type="math/tex">f(x)</script>和<script type="math/tex">y</script>之间有一个<script type="math/tex">\epsilon</script>的误差、亦即仅当：</p>
<script type="math/tex; mode=display">
\left| f\left( x \right) - y \right| > \epsilon</script><p>时、我们才认为模型在<script type="math/tex">(x,y)</script>点处有损失。这与支持向量机做分类时有种“恰好相反”的感觉：对于分类问题、只有当样本点离分界面足够远时才不计损失；对于回归问题、则只有当真值离预测值足够远时才计损失。但是仔细思考的话、就不难想通它们的思想和目的是完全一致的：都是为了提高模型的泛化能力</p>
<p>类比于之前讲过的 SVM 算法、可以很自然地写出 SVR 所对应的无约束优化问题：</p>
<script type="math/tex; mode=display">
\min_{w,b,x,y}{\frac{1}{2}\left\| w \right\|^{2} + C\sum_{i = 1}^{N}{l_{\epsilon}(w,b,x_{i},y_{i})}}</script><p>其中</p>
<script type="math/tex; mode=display">
l_{\epsilon}(w,b,x_{i},y_{i}) = \left\{ \begin{matrix}
0,\ \ &|f\left( x_{i} \right) - y_{i}| \leq \epsilon \\
\left| f\left( x_{i} \right) - y_{i} \right| - \epsilon,\ \ &|f\left( x_{i} \right) - y_{i}| > \epsilon \\
\end{matrix} \right.\</script><p>于是可以利用梯度下降法等进行求解。同样类比于 SVM 的对偶问题、我们可以提出 SVR 的对偶问题，细节就不展开叙述了</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章将简要说明几种将二分类模型拓展为多分类模型的普适性方法，它们不仅能对前三篇文章叙述的感知机和 SVM 进行应用、同时还能应用于在上一个系列中进行了说明的 AdaBoost 二分类模型；在本篇的第四节（也是最后一节）、我们则会简要地说明一下如何将支持向量机的思想应用在回归问题上&lt;/p&gt;
    
    </summary>
    
      <category term="支持向量机" scheme="http://www.carefree0910.com/categories/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="综述" scheme="http://www.carefree0910.com/tags/%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>核模型的实现与评估</title>
    <link href="http://www.carefree0910.com/posts/917ccef9/"/>
    <id>http://www.carefree0910.com/posts/917ccef9/</id>
    <published>2017-04-28T04:10:51.000Z</published>
    <updated>2017-04-28T10:48:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>有了上一篇文章的诸多准备、我们就能以之为基础实现核感知机和 SVM 了。不过需要指出的是，由于我们实现的 SVM 是一个朴素的版本、所以如果是要在实际任务中应用 SVM 的话，还是应该使用由前人开发、维护并经过长年考验的成熟的库（比如 LibSVM 等）；这些库能够处理更大的数据和更多的边值情况、运行的速度也会快上很多，这是因为它们通常都使用了底层语言来实现核心算法、且在算法上也做了许多数值稳定性和数值优化的处理</p>
<a id="more"></a>
<h1 id="核感知机的实现"><a href="#核感知机的实现" class="headerlink" title="核感知机的实现"></a>核感知机的实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> Util.Bases <span class="keyword">import</span> KernelBase</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KernelPerceptron</span><span class="params">(KernelBase)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        KernelBase.__init__(self)</div><div class="line">        <span class="comment"># 对于核感知机而言、循环体中所需的额外参数是学习速率（默认为1）</span></div><div class="line">        self._fit_args, self._fit_args_names = [<span class="number">1</span>], [<span class="string">"lr"</span>]</div><div class="line"></div><div class="line">    <span class="comment"># 更新dw</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_dw_cache</span><span class="params">(self, idx, lr, sample_weight)</span>:</span></div><div class="line">        self._dw_cache = lr * self._y[idx] * sample_weight[idx]</div><div class="line"></div><div class="line">    <span class="comment"># 更新db</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_db_cache</span><span class="params">(self, idx, lr, sample_weight)</span>:</span></div><div class="line">        self._db_cache = self._dw_cache</div><div class="line"></div><div class="line">    <span class="comment"># 利用和训练样本中的类别向量y来更新w和b</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_params</span><span class="params">(self)</span>:</span></div><div class="line">        self._w = self._alpha * self._y</div><div class="line">        self._b = np.sum(self._w)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_fit</span><span class="params">(self, sample_weight, lr)</span>:</span></div><div class="line">        <span class="comment"># 获取加权误差向量</span></div><div class="line">        _err = (np.sign(self._prediction_cache) != self._y) * sample_weight</div><div class="line">        <span class="comment"># 引入随机性以进行随机梯度下降</span></div><div class="line">        _indices = np.random.permutation(len(self._y))</div><div class="line">        <span class="comment"># 获取“错得最严重”的样本所对应的下标</span></div><div class="line">        _idx = _indices[np.argmax(_err[_indices])]</div><div class="line">        <span class="comment"># 若该样本被正确分类、则所有样本都已正确分类，此时返回真值、退出训练循环体</span></div><div class="line">        <span class="keyword">if</span> self._prediction_cache[_idx] == self._y[_idx]:</div><div class="line">            <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">        <span class="comment"># 否则、进行随机梯度下降</span></div><div class="line">        self._alpha[_idx] += lr</div><div class="line">        self._update_dw_cache(_idx, lr, sample_weight)</div><div class="line">        self._update_db_cache(_idx, lr, sample_weight)</div><div class="line">        self._update_pred_cache(_idx)</div></pre></td></tr></table></figure>
<p>可以看到代码清晰简洁，这主要得益于核感知机算法本身比较直白。我们可以先通过螺旋线数据集来大致看看它的分类能力、结果如下图所示：</p>
<img src="/posts/917ccef9/p1.png" alt="p1.png" title="">
<p>左图为 RBF 核感知机（<script type="math/tex">\gamma = 0.5</script>）、准确率为 90.0%；右图为多项式核感知机（<script type="math/tex">p = 12</script>）、准确率为 98.75%（迭代次数都是<script type="math/tex">10^{5}</script>）。虽说效果貌似还不错，但是由它们的训练曲线可以看出、训练过程其实是相当“不稳定”的：</p>
<img src="/posts/917ccef9/p2.png" alt="p2.png" title="">
<p>左、右图分别对应着 RBF 核感知机和多项式核感知机的训练曲线。之所以有这么大的波动、是因为我们采取的随机梯度下降每次只会进行非常局部的更新，而螺旋线数据集本身又具有比较特殊的结构，从而在直观上也能想象、模型的参数在训练的过程中很容易来回震荡。这一点在 SVM 上也会有体现、因为我们打算实现的 SMO 算法同样也是针对局部（两个变量）进行更新的</p>
<h1 id="核-SVM-的实现"><a href="#核-SVM-的实现" class="headerlink" title="核 SVM 的实现"></a>核 SVM 的实现</h1><p>接下来就看看核 SVM 的实现，虽说有些繁复、但其实只是一步一步地将之前说过的算法翻译出来而已，如果能理顺算法的逻辑的话、实现本身其实并不困难：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="keyword">from</span> Util.Bases <span class="keyword">import</span> KernelBase, KernelConfig</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span><span class="params">(KernelBase, metaclass=SubClassChangeNamesMeta)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        KernelBase.__init__(self)</div><div class="line">        <span class="comment"># 对于核SVM而言、循环体中所需的额外参数是容许误差（默认为）</span></div><div class="line">        self._fit_args, self._fit_args_names = [<span class="number">1e-3</span>], [<span class="string">"tol"</span>]</div><div class="line">        self._c = <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="comment"># 实现SMO算法中、挑选出第一个变量的方法</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_pick_first</span><span class="params">(self, tol)</span>:</span></div><div class="line">        con1 = self._alpha &gt; <span class="number">0</span></div><div class="line">        con2 = self._alpha &lt; self._c</div><div class="line">        <span class="comment"># 算出损失向量并拷贝成3份</span></div><div class="line">        err1 = self._y * self._prediction_cache - <span class="number">1</span></div><div class="line">        err2 = err1.copy()</div><div class="line">        err3 = err1.copy()</div><div class="line">        <span class="comment"># 将相应的数位置为0</span></div><div class="line">        err1[con1 | (err1 &gt;= <span class="number">0</span>)] = <span class="number">0</span></div><div class="line">        err2[(~con1 | ~con2) | (err2 == <span class="number">0</span>)] = <span class="number">0</span></div><div class="line">        err3[con2 | (err3 &lt;= <span class="number">0</span>)] = <span class="number">0</span></div><div class="line">        <span class="comment"># 算出总的损失向量并取出最大的一项</span></div><div class="line">        err = err1 ** <span class="number">2</span> + err2 ** <span class="number">2</span> + err3 ** <span class="number">2</span></div><div class="line">        idx = np.argmax(err)</div><div class="line">        <span class="comment"># 若该项的损失小于则返回返回空值</span></div><div class="line">        <span class="keyword">if</span> err[idx] &lt; tol:</div><div class="line">            <span class="keyword">return</span></div><div class="line">        <span class="comment"># 否则、返回对应的下标</span></div><div class="line">        <span class="keyword">return</span> idx</div><div class="line"></div><div class="line">    <span class="comment"># 实现SMO算法中、挑选出第二个变量的方法（事实上是随机挑选）</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_pick_second</span><span class="params">(self, idx1)</span>:</span></div><div class="line">        idx = np.random.randint(len(self._y))</div><div class="line">        <span class="keyword">while</span> idx == idx1:</div><div class="line">            idx = np.random.randint(len(self._y))</div><div class="line">        <span class="keyword">return</span> idx</div><div class="line"></div><div class="line">    <span class="comment"># 获取新的的下界</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_lower_bound</span><span class="params">(self, idx1, idx2)</span>:</span></div><div class="line">        <span class="keyword">if</span> self._y[idx1] != self._y[idx2]:</div><div class="line">            <span class="keyword">return</span> max(<span class="number">0.</span>, self._alpha[idx2] - self._alpha[idx1])</div><div class="line">        <span class="keyword">return</span> max(<span class="number">0.</span>, self._alpha[idx2] + self._alpha[idx1] - self._c)</div><div class="line"></div><div class="line">    <span class="comment"># 获取新的的上界</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_upper_bound</span><span class="params">(self, idx1, idx2)</span>:</span></div><div class="line">        <span class="keyword">if</span> self._y[idx1] != self._y[idx2]:</div><div class="line">            <span class="keyword">return</span> min(self._c, self._c + self._alpha[idx2] - self._alpha[idx1])</div><div class="line">        <span class="keyword">return</span> min(self._c, self._alpha[idx2] + self._alpha[idx1])</div><div class="line"></div><div class="line">    <span class="comment"># 更新dw</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_dw_cache</span><span class="params">(self, idx1, idx2, da1, da2, y1, y2)</span>:</span></div><div class="line">        self._dw_cache = np.array([da1 * y1, da2 * y2])</div><div class="line"></div><div class="line">    <span class="comment"># 更新db</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_db_cache</span><span class="params">(self, idx1, idx2, da1, da2, y1, y2, e1, e2)</span>:</span></div><div class="line">        gram_12 = self._gram[idx1][idx2]</div><div class="line">        b1 = -e1 - y1 * self._gram[idx1][idx1] * da1 - y2 * gram_12 * da2</div><div class="line">        b2 = -e2 - y1 * gram_12 * da1 - y2 * self._gram[idx2][idx2] * da2</div><div class="line">        self._db_cache = (b1 + b2) * <span class="number">0.5</span></div><div class="line"></div><div class="line">    <span class="comment"># 利用和训练样本中的类别向量y来更新w和b</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_update_params</span><span class="params">(self)</span>:</span></div><div class="line">        self._w = self._alpha * self._y</div><div class="line">        _idx = np.argmax((self._alpha != <span class="number">0</span>) &amp; (self._alpha != self._c))</div><div class="line">        self._b = self._y[_idx] – np.sum(self._alpha * self._y * self._gram[_idx])</div></pre></td></tr></table></figure>
<p>以上就是 SMO 算法中的核心步骤，接下来只需要将它们整合进一个大框架中即可（需要指出的是，随机选取第二个变量虽说效果也不错、但效率终究还是会差上一点；不过考虑到实现的复杂度、我们还是用随机选取的方法来进行实现）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义局部更新参数的方法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update_alpha</span><span class="params">(self, idx1, idx2)</span>:</span></div><div class="line">    l, h = self._get_lower_bound(idx1, idx2), self._get_upper_bound(idx1, idx2)</div><div class="line">    y1, y2 = self._y[idx1], self._y[idx2]</div><div class="line">    e1 = self._prediction_cache[idx1] - self._y[idx1]</div><div class="line">    e2 = self._prediction_cache[idx2] - self._y[idx2]</div><div class="line">    eta = self._gram[idx1][idx1] + self._gram[idx2][idx2] - <span class="number">2</span> * self._gram[idx1][idx2]</div><div class="line">    a2_new = self._alpha[idx2] + (y2 * (e1 - e2)) / eta</div><div class="line">    <span class="keyword">if</span> a2_new &gt; h:</div><div class="line">        a2_new = h</div><div class="line">    <span class="keyword">elif</span> a2_new &lt; l:</div><div class="line">        a2_new = l</div><div class="line">    a1_old, a2_old = self._alpha[idx1], self._alpha[idx2]</div><div class="line">    da2 = a2_new - a2_old</div><div class="line">    da1 = -y1 * y2 * da2</div><div class="line">    self._alpha[idx1] += da1</div><div class="line">    self._alpha[idx2] = a2_new</div><div class="line">    <span class="comment"># 根据、来更新dw和db并局部更新</span></div><div class="line">    self._update_dw_cache(idx1, idx2, da1, da2, y1, y2)</div><div class="line">    self._update_db_cache(idx1, idx2, da1, da2, y1, y2, e1, e2)</div><div class="line">    self._update_pred_cache(idx1, idx2)</div><div class="line"></div><div class="line"><span class="comment"># 初始化惩罚因子C</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_prepare</span><span class="params">(self, **kwargs)</span>:</span></div><div class="line">    self._c = kwargs.get(<span class="string">"c"</span>, KernelConfig.default_c)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_fit</span><span class="params">(self, sample_weight, tol)</span>:</span></div><div class="line">    idx1 = self._pick_first(tol)</div><div class="line">    <span class="comment"># 若没能选出第一个变量、则所有样本的误差都，此时返回真值、退出训练循环体</span></div><div class="line">    <span class="keyword">if</span> idx1 <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    idx2 = self._pick_second(idx1)</div><div class="line">    self._update_alpha(idx1, idx2)</div></pre></td></tr></table></figure>
<p>可以看到大部分代码确实只是算法的直译。同样可以先通过螺旋线数据集来大致看看核 SVM 的分类能力、结果如下图所示（图中用黑圈标注的样本点即是支持向量）：</p>
<img src="/posts/917ccef9/p3.png" alt="p3.png" title="">
<p>左图为 RBF 核 SVM（<script type="math/tex">\gamma = 0.5</script>）、迭代了 729 次即达到了停机条件（所有样本的误差都<script type="math/tex">< \epsilon</script>）、最终准确率为 51.25%；右图为多项式核 SVM（<script type="math/tex">p = 12</script>）、迭代了 6727 次即达到了停机条件、准确率为 97.5%。它们的训练曲线如下图所示：</p>
<img src="/posts/917ccef9/p4.png" alt="p4.png" title="">
<p>左、右图分别对应着 RBF 核 SVM 和多项式核 SVM 的训练曲线。虽说看上去似乎比核感知机的表现还要差、但这毕竟只是一个特殊的情形；事实上、即使是成熟的 SVM 库也并不是万能的。比如如果直接使用螺旋线数据集来训练 sklearn 中的、基于 LibSVM 进行实现的 SVM 模型的话、会得到如下图所示的结果：</p>
<img src="/posts/917ccef9/p5.png" alt="p5.png" title="">
<p>左图为 RBF 核 SVM（<script type="math/tex">\gamma = 0.5</script>）、最终准确率为 50.0%；右图为多项式核 SVM（<script type="math/tex">p = 12</script>）、准确率为 65.0%。造成这种差异的原因在于我们实现的多项式核函数和 sklearn 中的 SVM 所使用的多项式核函数不一样，如果将我们的核函数传进去、是可以得到相似结果的</p>
<p>作为本篇文章的收尾，我们可以通过画出两种核模型在蘑菇数据集上的训练曲线来简单地评估一下模型在真实数据下的表现。为了说明模型的泛化能力，我们只取 100 个样本作为训练样本、并用剩余 8000 多个样本作为测试样本来检验</p>
<p>首先来看一下核感知机的表现：</p>
<img src="/posts/917ccef9/p6.png" alt="p6.png" title="">
<p>左图为 RBF 核感知机（<script type="math/tex">\gamma \approx 0.04546</script>）的训练曲线、最终在测试集上的准确率为 92.53%；右图为多项式核感知机（<script type="math/tex">p = 3</script>）的训练曲线、最终在测试集上的准确率为 91.59%（迭代次数都是<script type="math/tex">10^{4}</script>）。由于只采用了 100 个样本训练、每次训练后的模型表现会波动得比较厉害；不过总体而言、RBF 核感知机会比多项式核感知机波动得更厉害一点</p>
<p>接下来看一下核 SVM 的表现：</p>
<img src="/posts/917ccef9/p7.png" alt="p7.png" title="">
<p>左图为 RBF 核 SVM（<script type="math/tex">\gamma \approx 0.04546</script>）、迭代了 462 次即达到了停机条件、最终在测试集上的准确率为 94.29%；右图为多项式核 SVM（<script type="math/tex">p = 3</script>）、迭代 1609 次即达到了停机条件、最终在测试集上的准确率为 92.96%</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;有了上一篇文章的诸多准备、我们就能以之为基础实现核感知机和 SVM 了。不过需要指出的是，由于我们实现的 SVM 是一个朴素的版本、所以如果是要在实际任务中应用 SVM 的话，还是应该使用由前人开发、维护并经过长年考验的成熟的库（比如 LibSVM 等）；这些库能够处理更大的数据和更多的边值情况、运行的速度也会快上很多，这是因为它们通常都使用了底层语言来实现核心算法、且在算法上也做了许多数值稳定性和数值优化的处理&lt;/p&gt;
    
    </summary>
    
      <category term="支持向量机" scheme="http://www.carefree0910.com/categories/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>从线性到非线性</title>
    <link href="http://www.carefree0910.com/posts/924abfe1/"/>
    <id>http://www.carefree0910.com/posts/924abfe1/</id>
    <published>2017-04-28T03:18:01.000Z</published>
    <updated>2017-04-28T12:18:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>前文已经提过，由于对偶形式中的样本点仅以内积的形式出现、所以利用核技巧能将线性算法“升级”为非线性算法。有一个与核技巧（Kernel Trick）类似的概念叫核方法（Kernel Method），这两者的区别可以简单地从字面意思去认知：当我们提及核方法（Method）时、我们比较注重它背后的原理；当我们提及核技巧（Trick）时、我们更注重它实际的应用。考虑到本书的主旨、我们还是选择了核技巧这一说法</p>
<p><strong><em>注意：以上关于核技巧和核方法这两个名词的区分不是一种共识、而是我个人为了简化问题而作的一种形象的说明，所以切忌将其作为严谨的叙述</em></strong></p>
<a id="more"></a>
<h1 id="核技巧简述"><a href="#核技巧简述" class="headerlink" title="核技巧简述"></a>核技巧简述</h1><p>虽说重视应用、但一些基本的概念还是需要稍微了解的。核方法本身要深究的话会牵扯到诸如正定核、内积空间、希尔伯特空间乃至于再生核希尔伯特空间（Reproducing Kernel Hilbert Space，常简称为 RKHS）、这些东西又会牵扯到泛函的相关理论，可谓是一个可以单独拿来出书的知识点。幸运的是，单就核技巧而言、我们仅需要知道其中的三个定理即可，这三个定理分别说明了核技巧的合理性、普适性和高效性。不过在叙述这三个定理之前，我们可以先来看看核技巧的直观解释</p>
<p>核技巧往简单地说，就是将一个低维的线性不可分的数据映射到一个高维的空间、并期望映射后的数据在高维空间里是线性可分的。我们以异或数据集为例：在二维空间中、异或数据集是线性不可分的；但是通过将其映射到三维空间、我们可以非常简单地让其在三维空间中变得线性可分。比如定义映射：</p>
<script type="math/tex; mode=display">
\phi\left( x,y \right) = \left\{ \begin{matrix}
\left( x,y,1 \right),\ \ & xy > 0 \\
\left( x,y,0 \right),\ \ & xy \leq 0 \\
\end{matrix} \right.\</script><p>该映射的效果如下图所示：</p>
<img src="/posts/924abfe1/p1.png" alt="p1.png" title="">
<p>可以看到，虽然左图的数据集线性不可分、但显然右图的数据集是线性可分的，这就是核技巧工作原理的一个不太严谨但仍然合理的解释</p>
<p><strong><em>注意：这里我们暂时采用了“从低维到高维的映射”这一说法、但该说法并不完全严谨，原因会在后文说明、这里只需留一个心眼即可</em></strong></p>
<p>从直观上来说，确实容易想象、同一份数据在越高维的空间中越有可能线性可分，但从理论上是否确实如此呢？1965 年提出的 Cover 定理解决了这个问题，它的具体叙述如下：若设 d 维空间中 N 个点线性可分的概率为<script type="math/tex">p(d,N)</script>，那么就有：</p>
<script type="math/tex; mode=display">
p\left( d,N \right) = \frac{2\sum_{i = 0}^{m}C_{N - 1}^{i}}{2^{N}} = \left\{ \begin{matrix}
\frac{\sum_{i = 1}^{d}C_{N - 1}^{i}}{2^{N - 1}},\ \ & N > d + 1 \\
1,\ \ & N \leq d + 1 \\
\end{matrix} \right.\</script><p>其中</p>
<script type="math/tex; mode=display">
m = \min{(d,\ N - 1)}</script><p>定理的证明细节从略，我们只需要知道它证明了当空间的维数 d 越大时、其中的 N 个点线性可分的概率就越大，这构成了核技巧的理论基础之一</p>
<p>至此，似乎问题就转化为了如何寻找合适的映射<script type="math/tex">\phi</script>、使得数据集在被它映射到高维空间后变得线性可分。不过可以想象的是，现实任务中的数据集要比上文我们拿来举例的异或数据集要复杂得多、直接构造一个恰当的<script type="math/tex">\phi</script>的难度甚至可能高于解决问题本身。而核技巧的巧妙之处就在于，它能将构造映射这个过程再次进行转化、从而使得问题变得简易：它通过核函数来避免显式定义映射<script type="math/tex">\phi</script>。往简单里说、核技巧会通过用核函数</p>
<script type="math/tex; mode=display">
K\left( x_{i},x_{j} \right) = \phi\left( x_{i} \right) \cdot \phi(x_{j})</script><p>替换各式算法中出现的内积</p>
<script type="math/tex; mode=display">
x_{i} \cdot x_{j}</script><p>来完成将数据从低维映射到高维的过程。换句话说、核技巧的思想如下：</p>
<ul>
<li>将算法表述成样本点内积的组合（这经常能通过算法的对偶形式实现）</li>
<li>设法找到核函数<script type="math/tex">K\left( x_i,x_j\right)</script>，它能返回样本点<script type="math/tex">x_i</script>、<script type="math/tex">x_j</script>被<script type="math/tex">\phi</script>作用后的内积</li>
<li>用<script type="math/tex">K\left( x_i,x_j\right)</script>替换<script type="math/tex">x_i\cdot x_j</script>、完成低维到高维的映射（同时也完成了从线性算法到非线性算法的转换）</li>
</ul>
<p>而核技巧事实上能够应用的场景更为宽泛——在 2002 年由 Sch<script type="math/tex">\ddot{o}</script>lkopf 和 Smola 证明的表示定理告诉我们：设<script type="math/tex">\mathcal{H}</script>为核函数<script type="math/tex">K</script>对应的映射后的空间（RKHS），<script type="math/tex">\left\| h \right\|_{\mathcal{H}}</script>表示<script type="math/tex">\mathcal{H}</script>中<script type="math/tex">h</script>的范数，那么对于任意单调递增的函数<script type="math/tex">C</script>和任意非负损失函数<script type="math/tex">L</script>、优化问题</p>
<script type="math/tex; mode=display">
\min_{h\in\mathcal{H}}{L\left( h\left( x_{1} \right),\ldots,h\left( x_{N} \right) \right) + C(\left\| h \right\|_{\mathcal{H}})}</script><p>的解总可以表述为核函数<script type="math/tex">K</script>的线性组合</p>
<script type="math/tex; mode=display">
h^{*}\left( x \right) = \sum_{i = 1}^{N}{\alpha_{i}K(x,x_{i})}</script><p>这意味着对于任意一个损失函数和一个单调递增的正则化项组成的优化问题、我们都能够对其应用核技巧。所以至此、大多数的问题就转化为如何找到能够表示成高维空间中内积的核函数了。幸运的是、1909 年提出的 Mercer 定理解决了这个问题，它的具体叙述如下：<script type="math/tex">K\left( x_{i},x_{j} \right)</script>若满足</p>
<script type="math/tex; mode=display">
K\left( x_{i},x_{j} \right) = K\left( x_{j},x_{i} \right)</script><p>亦即如果<script type="math/tex">K\left( x_{i},x_{j} \right)</script>是对称函数的话、那么它具有 Hilbert 空间中内积形式的充要条件有以下两个：</p>
<ul>
<li>对任何平方可积<script type="math/tex">g</script>、满足  <script type="math/tex; mode=display">
\int K\left( x_{i},x_{j} \right)g\left( x_{i} \right)g\left( x_{j} \right)dx_{i}dx_{j} \geq 0</script></li>
<li>对含任意 N 个样本的数据集<script type="math/tex">D=\left\{ x_1,...,x_N\right\}</script>、核矩阵：  <script type="math/tex; mode=display">
\mathbf{K} = \begin{bmatrix}
K\left( x_{1},x_{1} \right) & \ldots & K\left( x_{1},x_{N} \right) \\
\vdots & \ddots & \vdots \\
K\left( x_{N},x_{1} \right) & \ldots & K\left( x_{N},x_{N} \right) \\
\end{bmatrix}_{N \times N} = \left\lbrack K_{ij} \right\rbrack_{N \times N}</script>是半正定矩阵</li>
</ul>
<p><strong><em>注意：通常我们会称满足这两个充要条件之一的函数为 Mercer 核函数而把核函数定义得更宽泛。由于本书不打算在理论上深入太多、所以一律将 Mercer 核函数简称为核函数。此外，虽说 Mercer 核函数确实具有 Hilbert 空间中的内积形式、但此时的 Hilbert 空间并不一定具有“维度”这么好的概念（或说、可以认为此时 Hilbert 空间的维度为无穷大，比如下面马上就要讲到的 RBF 核、它映射后的空间就是无穷维的）。这也正是为何前文说“从低维到高维的映射”不完全严谨</em></strong></p>
<p>Mercer 定理为寻找核函数带来了极大的便利。可以证明如下两族函数都是核函数：</p>
<ul>
<li>多项式核  <script type="math/tex; mode=display">
K\left( x_{i},x_{j} \right) = \left( x_{i} \cdot x_{j} + 1 \right)^{p}</script></li>
<li>径向基（Radial Basis Function，常简称为RBF）核  <script type="math/tex; mode=display">
K\left( x_{i},x_{j} \right) = \exp\left( - \gamma\left\| x_{i} - x_{j} \right\|^{2} \right)</script></li>
</ul>
<h1 id="核技巧的应用"><a href="#核技巧的应用" class="headerlink" title="核技巧的应用"></a>核技巧的应用</h1><p>我们接下来会实现的也正是这两族核函数对应的、应用了核技巧的算法，具体而言、我们会利用核技巧来将感知机和支持向量机算法从原始的线性版本“升级”为非线性版本</p>
<p>由简入繁、先从核感知机讲起；由于感知机对偶算法十分简单、对其应用核技巧相应的也非常平凡——直接用核函数替换掉相应内积即可。不过需要注意的是，由于我们采用的是随机梯度下降、所以算法中也应尽量只更新局部参数以避免进行无用的计算：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,\left( x_{N},y_{N} \right)\}</script>、迭代次数 M、学习速率<script type="math/tex">\eta</script>，其中：  <script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script></li>
<li><p><strong>过程</strong>：</p>
<ol>
<li><p>初始化参数：  </p>
<script type="math/tex; mode=display">
\alpha = \left( \alpha_{1},\ldots,\alpha_{N} \right)^{T} = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{N}</script><script type="math/tex; mode=display">
\hat{y} = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{N}</script><p>同时计算核矩阵：  </p>
<script type="math/tex; mode=display">
\mathbf{K} = \left\lbrack K\left( x_{i},x_{j} \right) \right\rbrack_{N \times N}</script></li>
<li><p>对<script type="math/tex">j = 1,\ldots,M</script>：  </p>
<script type="math/tex; mode=display">
E = \left\{ \left( x_{i},y_{i} \right) \middle| {\hat{y}}_{i} \leq 0 \right\}</script><ol>
<li>若<script type="math/tex">E = \varnothing</script>（亦即没有误分类的样本点）则退出循环体</li>
<li>否则，任取<script type="math/tex">E</script>中的一个样本点<script type="math/tex">(x_{i},y_{i})</script>并利用其下标更新局部参数：  <script type="math/tex; mode=display">
\alpha_{i} \leftarrow \alpha_{i} + \eta</script><script type="math/tex; mode=display">
dw = db = \eta y_{i}</script></li>
<li>利用<script type="math/tex">dw</script>和<script type="math/tex">db</script>更新预测向量<script type="math/tex">\hat{y}</script>：  <script type="math/tex; mode=display">
\hat{y} \leftarrow \hat{y} + dw\mathbf{K}_{i\mathbf{\cdot}} + db\mathbf{1}</script>其中、<script type="math/tex">\mathbf{K}_{i\mathbf{\cdot}}</script>表示<script type="math/tex">\mathbf{K}</script>的第 i 行、<script type="math/tex">\mathbf{1}</script>表示全为 1 的向量</li>
</ol>
</li>
</ol>
</li>
<li><strong>输出</strong>：感知机模型<script type="math/tex">g\left( x \right) = \text{sign}\left( f\left( x \right) \right) = sign\left( \sum_{i = 1}^{N}{\alpha_{i}y_{i}\left( K\left( x_{i},x \right) + 1 \right)} \right)</script></li>
</ol>
<p>再来看如何对 SVM 应用核技巧。虽说在对偶算法上应用核技巧是非常自然、直观的，但是直接在原始算法上应用核技巧也无不可</p>
<p>注意原始问题可以表述为：</p>
<script type="math/tex; mode=display">
\min_{w,b}\hat{L}\left( w,b,x,y \right) = \min_{w,b}{\frac{1}{2}\left\| w \right\|^{2} + \sum_{i = 1}^{N}{\max(0,1 - y_{i}(w \cdot x_{i} + b)}}</script><p>若令<script type="math/tex">w = \sum_{i = 1}^{N}{u_{i}\phi(x_{i})} = u \cdot \phi(x)</script>、其中：</p>
<script type="math/tex; mode=display">
\begin{align}
u &= (u_{1},\ldots,u_{N}) \\
\phi\left( x \right) &= \left( \phi\left( x_{1} \right),\ldots,\phi\left( x_{N} \right) \right)^{T}
\end{align}</script><p>则可知上述问题能够通过<script type="math/tex">\phi</script>映射到高维空间上：</p>
<script type="math/tex; mode=display">
\min_{w,b}{\frac{1}{2}u^{T}\mathbf{K}u + \sum_{i = 1}^{N}{\max(0,1 - y_{i}\left( \sum_{j = 1}^{N}{u_{i}\phi\left( x_{j} \right) \cdot \phi\left( x_{i} \right)} \right))}}</script><p>亦即</p>
<script type="math/tex; mode=display">
\min_{w,b}{\frac{1}{2}u^{T}\mathbf{K}u + \sum_{i = 1}^{N}{max(0,1 - y_{i}u \cdot \mathbf{K}_{i\mathbf{\cdot}})}}</script><p>利用一定的技巧是可以直接利用梯度下降法直接对这个无约束最优化问题求解的，不过相关的数学理论基础都相当繁复、实现起来也有些麻烦；尽管如此、还是有许多优秀的算法是基于上述思想的</p>
<p>直观起见、我们还是将重点放在如何对 SMO 应用核技巧的讨论上。由于前文已经说明了 SMO 的大致步骤，所以我们先补充说明当时没有讲到的、选出两个变量后应该如何继续求解，然后再来看具体的算法应该如何叙述</p>
<script type="math/tex; mode=display">
\max_{\alpha}{- \frac{1}{2}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\alpha_{i}\alpha_{j}y_{i}y_{j}K_{ij}}} + \sum_{i = 1}^{N}\alpha_{i}}</script><p>使得对<script type="math/tex">i = 1,\ldots,N</script>、都有</p>
<script type="math/tex; mode=display">
\sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0</script><script type="math/tex; mode=display">
0 \leq \alpha_{i} \leq C</script><p>不妨设<script type="math/tex">i=1</script>、<script type="math/tex">j=2</script>，那么在针对<script type="math/tex">\alpha_1</script>、<script type="math/tex">\alpha_2</script>的情况下，<script type="math/tex">\alpha_3,...,\alpha_N</script>是固定的、且上述最优化问题可以转化为：</p>
<script type="math/tex; mode=display">\max_{\alpha_1,\alpha_2}{- \frac{1}{2}\left( K_{11}\alpha_{1}^{2} + y_{1}y_{2}K_{12}\alpha_{1}\alpha_{2} + K_{22}\alpha_{2}^{2} \right) - \left( y_{1}\alpha_{1}\sum_{i = 3}^{N}{y_{i}\alpha_{i}K_{i1}} + y_{2}\alpha_{2}\sum_{i = 3}^{N}K_{i2} \right) + {(\alpha}_{1} + \alpha_{2})}</script><p>使得对<script type="math/tex">i = 1</script>和<script type="math/tex">i = 2</script>、有</p>
<script type="math/tex; mode=display">
y_{1}\alpha_{1} + y_{2}\alpha_{2} = - \sum_{i = 3}^{N}{y_{i}\alpha_{i} = const}</script><script type="math/tex; mode=display">
0 \leq \alpha_{i} \leq C</script><p>其中<script type="math/tex">const</script>为常数。可以看出此时问题确实转化为了一个带约束的二次函数求极值问题、从而能够比较简单地求出其解析解。推导过程从略、以下就直接在算法中写出结果：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,\left( x_{N},y_{N} \right)\}</script>、迭代次数 M、容许误差<script type="math/tex">\epsilon</script>，其中：  <script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script></li>
<li><p><strong>过程</strong>：</p>
<ol>
<li>初始化参数：  <script type="math/tex; mode=display">
\alpha = \left( \alpha_{1},\ldots,\alpha_{N} \right)^{T} = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{N}</script><script type="math/tex; mode=display">
\hat{y} = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{N}</script>同时计算核矩阵：  <script type="math/tex; mode=display">
\mathbf{K} = \left\lbrack K\left( x_{i},x_{j} \right) \right\rbrack_{N \times N}</script></li>
<li><p>对<script type="math/tex">j = 1,\ldots,M</script>：</p>
<ol>
<li>选出违反 KKT 条件最严重的样本点<script type="math/tex">(x_{i},y_{i})</script>，若其违反程度小于<script type="math/tex">\epsilon</script>、则退出循环体</li>
<li><p>否则、选出异于i的任一个下标 j，针对<script type="math/tex">\alpha_{i}</script>和<script type="math/tex">\alpha_{j}</script>构造一个新的只有两个变量二次规划问题并求出解析解。具体而言，首先要更新的是<script type="math/tex">\alpha_{2}</script>、它由以下几个参数定出：  </p>
<script type="math/tex; mode=display">
\begin{align}
e_{i} &= {\hat{y}}_{i} - y_{i}\ (i = 1,2) \\
dK &= K_{11} + K_{22} - 2K_{12} \\
\alpha_{2}^{new,raw} &= \alpha_{2} + \frac{y_{2}\left( e_{1} - e_{2} \right)}{dK}
\end{align}</script><p>考虑到约束条件、我们需要定出新的<script type="math/tex">\alpha_{2}</script>下上界：  </p>
<script type="math/tex; mode=display">
\begin{align}
l &= \left\{ \begin{matrix}
max(0,\alpha_{2} - \alpha_{1}),\ \ & y_{1} \neq y_{2} \\
max(0,\alpha_{2} + \alpha_{1} - C),\ \ & y_{1} = y_{2} \\
\end{matrix} \right.\ \\

h &= \left\{ \begin{matrix}
min(C,C + \alpha_{2} - \alpha_{1}),\ \ & y_{1} \neq y_{2} \\
max(C,\alpha_{2} + \alpha_{1}),\ \ & y_{1} = y_{2} \\
\end{matrix} \right.\
\end{align}</script><p>继而根据<script type="math/tex">l</script>和<script type="math/tex">h</script>对<script type="math/tex">\alpha_{2}^{new,raw}</script>进行“裁剪”即可：  </p>
<script type="math/tex; mode=display">
\alpha_{2} \leftarrow \left\{ \begin{matrix}
l,\ \ &\alpha_{2}^{new,raw} < l \\
\alpha_{2}^{new,raw},\ \ &{l \leq \alpha}_{2}^{new,raw} \\
h,\ \ &\alpha_{2}^{new,raw} > h \\
\end{matrix} \right.\  \leq h</script><p>这里要注意记录<script type="math/tex">\alpha_{2}</script>的增量：  </p>
<script type="math/tex; mode=display">
\Delta\alpha_{2} = \alpha_{2}^{\text{new}} - \alpha_{2}^{\text{old}}</script></li>
<li>利用<script type="math/tex">\Delta\alpha_{2}</script>更新<script type="math/tex">\alpha_{1}</script>、同时注意记录<script type="math/tex">\alpha_{1}</script>的增量：  <script type="math/tex; mode=display">
\begin{align}
\alpha_{1} &\leftarrow \alpha_{1} - y_{1}y_{2}\Delta\alpha_{2} \\
\Delta\alpha_{1} &= \alpha_{1}^{\text{new}} - \alpha_{1}^{\text{old}}
\end{align}</script></li>
<li>利用<script type="math/tex">\Delta\alpha_{1}</script>、<script type="math/tex">\Delta\alpha_{2}</script>进行局部更新：  <script type="math/tex; mode=display">
\begin{align}
dw &= (y_{1}\Delta\alpha_{1},y_{2}\Delta\alpha_{2}) \\
db &= \frac{b_{1} + b_{2}}{2}
\end{align}</script>其中  <script type="math/tex; mode=display">
\begin{align}
b_{1} &= - e_{1} - y_{1}K_{11}\Delta\alpha_{1} - y_{2}K_{12}\Delta\alpha_{2} \\
b_{2} &= - e_{2} - y_{1}K_{12}\Delta\alpha_{1} - y_{2}K_{22}\Delta\alpha_{2}
\end{align}</script></li>
<li>利用<script type="math/tex">dw</script>和<script type="math/tex">db</script>更新预测向量<script type="math/tex">\hat{y}</script>：  <script type="math/tex; mode=display">
\hat{y} \leftarrow \hat{y} + dw_{1}\mathbf{K}_{1\mathbf{\cdot}} + dw_{2}\mathbf{K}_{2\mathbf{\cdot}} + db\mathbf{1}</script>其中、`$\mathbf{K}_{\mathbf{i}\mathbf{\cdot}}$表示$\mathbf{K}$的第i行、$\mathbf{1}$表示全为1的向量</li>
</ol>
</li>
</ol>
</li>
<li><strong>输出</strong>：感知机模型<script type="math/tex">g\left( x \right) = \text{sign}\left( f\left( x \right) \right) = sign\left( \sum_{i = 1}^{N}{\alpha_{i}y_{i}K\left( x_{i},x \right)} + b \right)</script>、其中：</li>
</ol>
<script type="math/tex; mode=display">
b = y_{k} - \sum_{i = 1}^{N}{y_{i}\alpha_{i}K_{ik}}</script><p>这里的下标 k 满足</p>
<script type="math/tex; mode=display">
0 < \alpha_{k} < C</script><p>可以用反证法证明这样的下标 k 必存在、具体步骤从略</p>
<p>从这两种算法应用核技巧的方式可以看出，虽然它们应用的训练算法完全不同（一个是随机梯度下降、一个是序列最小最优化）、但它们每一次迭代中做的事情却有相当多是一致的；为了合理重复利用代码、我们可以先把对应的实现都抽象出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KernelBase</span><span class="params">(ClassifierBase)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._fit_args, self._args_names：记录循环体中所需额外参数的信息的属性</div><div class="line">        self._x, self._y, self._gram：记录数据集和Gram矩阵的属性</div><div class="line">            self._w, self._b, self._alpha：记录各种参数的属性</div><div class="line">        self._kernel, self._kernel_name, self._kernel_param：记录核函数相关信息的属性</div><div class="line">            self._prediction_cache, self._dw_cache, self._db_cache：记录 、dw、db的属性</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(KernelBase, self).__init__()</div><div class="line">        self._fit_args, self._fit_args_names = <span class="keyword">None</span>, []</div><div class="line">        self._x = self._y = self._gram = <span class="keyword">None</span></div><div class="line">        self._w = self._b = self._alpha = <span class="keyword">None</span></div><div class="line">        self._kernel = self._kernel_name = self._kernel_param = <span class="keyword">None</span></div><div class="line">        self._prediction_cache = self._dw_cache = self._db_cache = <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="comment"># 定义计算多项式核矩阵的函数</span></div><div class="line"><span class="meta">    @staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_poly</span><span class="params">(x, y, p)</span>:</span></div><div class="line">        <span class="keyword">return</span> (x.dot(y.T) + <span class="number">1</span>) ** p</div><div class="line"></div><div class="line">    <span class="comment"># 定义计算RBF核矩阵的函数</span></div><div class="line"><span class="meta">    @staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_rbf</span><span class="params">(x, y, gamma)</span>:</span></div><div class="line">        <span class="keyword">return</span> np.exp(-gamma * np.sum((x[..., <span class="keyword">None</span>, :] - y) ** <span class="number">2</span>, axis=<span class="number">2</span>))</div></pre></td></tr></table></figure>
<p>其中定义 RBF 核函数时用到了升维的操作、这算是 Numpy 的高级使用技巧之一；具体的思想和机制会在后续的文章中进行简要说明、这里就暂时按下不表</p>
<p>以上我们就搭好了基本的框架、接下来要做的就是继续把具有普适性的训练过程进行抽象和实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 默认使用RBF核、默认迭代次数epoch为一万次</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, kernel=<span class="string">"rbf"</span>, epoch=<span class="number">10</span> ** <span class="number">4</span>, **kwargs)</span>:</span></div><div class="line">    self._x, self._y = np.atleast_2d(x), np.array(y)</div><div class="line">    <span class="keyword">if</span> kernel == <span class="string">"poly"</span>:</div><div class="line">        <span class="comment"># 对于多项式核、默认使用KernelConfig中的default_p作为p的取值</span></div><div class="line">        _p = kwargs.get(<span class="string">"p"</span>, KernelConfig.default_p)</div><div class="line">        self._kernel_name = <span class="string">"Polynomial"</span></div><div class="line">        self._kernel_param = <span class="string">"degree = &#123;&#125;"</span>.format(_p)</div><div class="line">        self._kernel = <span class="keyword">lambda</span> _x, _y: KernelBase._poly(_x, _y, _p)</div><div class="line">    <span class="keyword">elif</span> kernel == <span class="string">"rbf"</span>:</div><div class="line">         <span class="comment"># 对于RBF核、默认使用样本x的维数n的倒数1/n作为的取值</span></div><div class="line">        _gamma = kwargs.get(<span class="string">"gamma"</span>, <span class="number">1</span> / self._x.shape[<span class="number">1</span>])</div><div class="line">        self._kernel_name = <span class="string">"RBF"</span></div><div class="line">        self._kernel_param = <span class="string">"gamma = &#123;&#125;"</span>.format(_gamma)</div><div class="line">        self._kernel = <span class="keyword">lambda</span> _x, _y: KernelBase._rbf(_x, _y, _gamma)</div><div class="line">    <span class="comment"># 初始化参数</span></div><div class="line">    self._alpha, self._w, self._prediction_cache = (</div><div class="line">        np.zeros(len(x)), np.zeros(len(x)), np.zeros(len(x)))</div><div class="line">    self._gram = self._kernel(self._x, self._x)</div><div class="line">    self._b = <span class="number">0</span></div><div class="line">    <span class="comment"># 调用 _prepare方法进行特殊参数的初始化（比如SVM中的惩罚因子C）</span></div><div class="line">    self._prepare(**kwargs)</div><div class="line">    <span class="comment"># 获取在循环体中会用到的参数</span></div><div class="line">    _fit_args = []</div><div class="line">    <span class="keyword">for</span> _name, _arg <span class="keyword">in</span> zip(self._fit_args_names, self._fit_args):</div><div class="line">        <span class="keyword">if</span> _name <span class="keyword">in</span> kwargs:</div><div class="line">            _arg = kwargs[_name]</div><div class="line">        _fit_args.append(_arg)</div><div class="line">    <span class="comment"># 迭代、直至达到迭代次数epoch或 _fit核心方法返回真值</span></div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</div><div class="line">        <span class="keyword">if</span> self._fit(sample_weight, *_fit_args):</div><div class="line">            <span class="keyword">break</span></div><div class="line">    <span class="comment"># 利用和训练样本来更新w和b</span></div><div class="line">    self._update_params()</div></pre></td></tr></table></figure>
<p>注意到我们调用了一个叫<code>KernelConfig</code>的类、它的定义很简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">KernelConfig</span>:</span></div><div class="line">    default_c = <span class="number">1</span></div><div class="line">    default_p = <span class="number">3</span></div></pre></td></tr></table></figure>
<p>亦即默认惩罚因子<script type="math/tex">C</script>为 1、多项式核的次数<script type="math/tex">p</script>为 3。同时需要注意的是，我们在循环体里面调用了<code>_fit</code>核心方法、在最后调用了<code>_update_params</code>方法，这两个方法都是留给子类定义的；不过比较巧妙的是，无论是记录<script type="math/tex">\hat{y}</script>的<code>_prediction_cache</code>的更新还是预测函数<code>predict</code>的定义、都可以写成同一种形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义更新预测向量 _prediction_cache的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update_pred_cache</span><span class="params">(self, *args)</span>:</span></div><div class="line">    self._prediction_cache += self._db_cache</div><div class="line">    <span class="keyword">if</span> len(args) == <span class="number">1</span>:</div><div class="line">        self._prediction_cache += self._dw_cache * self._gram[args[<span class="number">0</span>]]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        self._prediction_cache += self._dw_cache.dot(self._gram[args, ...])</div><div class="line"></div><div class="line"><span class="comment"># 定义预测函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x, get_raw_results=False)</span>:</span></div><div class="line">    <span class="comment"># 计算测试集和训练集之间的核矩阵并利用它来做决策</span></div><div class="line">    x = self._kernel(np.atleast_2d(x), self._x)</div><div class="line">    y_pred = x.dot(self._w) + self._b</div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> get_raw_results:</div><div class="line">        <span class="keyword">return</span> np.sign(y_pred)</div><div class="line">    <span class="keyword">return</span> y_pred</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前文已经提过，由于对偶形式中的样本点仅以内积的形式出现、所以利用核技巧能将线性算法“升级”为非线性算法。有一个与核技巧（Kernel Trick）类似的概念叫核方法（Kernel Method），这两者的区别可以简单地从字面意思去认知：当我们提及核方法（Method）时、我们比较注重它背后的原理；当我们提及核技巧（Trick）时、我们更注重它实际的应用。考虑到本书的主旨、我们还是选择了核技巧这一说法&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;注意：以上关于核技巧和核方法这两个名词的区分不是一种共识、而是我个人为了简化问题而作的一种形象的说明，所以切忌将其作为严谨的叙述&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="支持向量机" scheme="http://www.carefree0910.com/categories/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>从感知机到支持向量机</title>
    <link href="http://www.carefree0910.com/posts/d455305a/"/>
    <id>http://www.carefree0910.com/posts/d455305a/</id>
    <published>2017-04-28T02:42:17.000Z</published>
    <updated>2017-04-28T12:11:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>感知机确实能够解决线性可分数据集的分类问题，但从它的解法容易看出、感知机的解是有无穷多个的。这主要是因为它对自己的要求太低：只需对训练集中所有样本点都能正确分类即可。换句话说、感知机基本没有考虑模型的泛化能力，这就导致感知机有时会训练出如下图所示的结果：</p>
<img src="/posts/d455305a/p1.png" alt="p1.png" title="">
<p>可以看出它们是不尽合理的。支持向量机（SVM）针对这一点提出了一种改进方法，本篇文章主要叙述的就是该改进的思想和具体内容</p>
<a id="more"></a>
<h1 id="间隔最大化与线性-SVM"><a href="#间隔最大化与线性-SVM" class="headerlink" title="间隔最大化与线性 SVM"></a>间隔最大化与线性 SVM</h1><p>上图的结果之所以显得不合理、主要是因为分离超平面离正负样本点集都显得“太近”了。因此一个自然的想法就是：在训练过程中考虑上超平面到点集的距离、并努力让这个距离最大化</p>
<p>然而直接从集合出发定义集合到平面的距离是相对困难的、所以通常会将它转化为点到平面的距离。前文已经说过，对于样本点<script type="math/tex">(x_{i},y_{i})</script>而言、它到超平面<script type="math/tex">\Pi:w \cdot x + b = 0</script>的相对距离即为：</p>
<script type="math/tex; mode=display">
d^{*}\left( x_{i},\Pi \right) = |w \cdot x_{i} + b|</script><p>这里的相对距离<script type="math/tex">d^{*}</script>有一个更学术一点称谓——函数间隔（Functional Margin）。函数间隔有一个比较明显的缺陷就是、当<script type="math/tex">w</script>和<script type="math/tex">b</script>等比例变大或变小时，虽然超平面不会改变、但是<script type="math/tex">d^{*}</script>却会随之等比例变大或变小。为解决这个问题、我们可以比较自然地定义出所谓的几何间隔（Geometric Distance）：</p>
<script type="math/tex; mode=display">
d\left( x_{i},\Pi \right) = \frac{1}{\left\| w \right\|} \cdot d^{*}\left( x_{i},\Pi \right) = \frac{1}{\left\| w \right\|} \cdot |w \cdot x_{i} + b|</script><p>这里的<script type="math/tex">\left\| w \right\|</script>是<script type="math/tex">w</script>的欧式范数。顾名思义、几何间隔描述的就是向量<script type="math/tex">x_{i}</script>到超平面<script type="math/tex">\Pi</script>的几何距离（欧氏距离），它不会随<script type="math/tex">w</script>和<script type="math/tex">b</script>的等比例变化而变化、是相对稳定且直观意义优良的距离的定义方法。SVM 在训练过程中所引入的也正是各个样本点到当前分离超平面的几何距离，结合前文所说的“努力让超平面到点集的距离最大化”、SVM 算法就可以比较自然地叙述为：最大化（几何间隔）<script type="math/tex">d</script>、使得：</p>
<script type="math/tex; mode=display">
\frac{1}{\left\| w \right\|} \cdot \left\lbrack y_{i}\left( w \cdot x_{i} + b \right) \right\rbrack \geq d\ (i = 1,\ldots,N)</script><p>考虑到几何间隔和函数间隔之间的转换关系、该问题可以等价为：最大化<script type="math/tex">\frac{d^{*}}{\left\| w \right\|}</script>、使得：</p>
<script type="math/tex; mode=display">
y_{i}\left( w \cdot x_{i} + b \right) \geq d^{*}\ (i = 1,\ldots,N)</script><p>可以发现函数间隔<script type="math/tex">d^{*}</script>的取值其实对该优化问题的解没有影响。这是因为当<script type="math/tex">d^{*}</script>变成<script type="math/tex">\lambda d^{*}</script>时、<script type="math/tex">w</script>和<script type="math/tex">b</script>也会相应地变成<script type="math/tex">\lambda w</script>和<script type="math/tex">\lambda b</script>（在超平面不变的情况下），此时<script type="math/tex">\frac{d^{*}}{\left\| w \right\|}</script>和不等式约束都没有变、所以对优化问题确实没有影响。这样的话我们就能不妨设<script type="math/tex">d^{*} = 1</script>、从而优化问题就可以转换为：最大化<script type="math/tex">\frac{1}{\left\| w \right\|}</script>、使得：</p>
<script type="math/tex; mode=display">
y_{i}\left( w \cdot x_{i} + b \right) \geq 1\ (i = 1,\ldots,N)</script><p>易知该优化问题又能转化为：最小化<script type="math/tex">\frac{1}{2}\left\| w \right\|^{2}</script>、使得：</p>
<script type="math/tex; mode=display">
y_{i}\left( w \cdot x_{i} + b \right) - 1 \geq 0\ (i = 1,\ldots,N)</script><p>这就是 SVM 算法的最原始的形式。可以证明，只要训练集<script type="math/tex">D</script>线性可分、那么 SVM 算法对应的这个优化问题的解就存在且唯一；其中存在性的证明相对直观、唯一性的证明需要用到反证法和一些数学上的技巧，细节从略</p>
<p>假设该优化问题的解为<script type="math/tex">w^{*}</script>和<script type="math/tex">b^{*}</script>，我们通常称超平面：</p>
<script type="math/tex; mode=display">
\Pi^{*}:w^{*} \cdot x + b^{*} = 0</script><p>为<script type="math/tex">D</script>的最大硬间隔分离超平面。之所以称它为“硬间隔”的理由会在后文叙述，这里暂时按下不表。需要指出的是，考虑到优化问题中的不等式约束、易知在超平面</p>
<script type="math/tex; mode=display">
\Pi_{1}^{*}:w^{*} \cdot x + b = - 1</script><p>和超平面</p>
<script type="math/tex; mode=display">
\Pi_{2}^{*}:w^{*} \cdot x + b = + 1</script><p>之间、是没有任何<script type="math/tex">D</script>中的样本点的。不过在<script type="math/tex">\Pi_{1}^{*}</script>和<script type="math/tex">\Pi_{2}^{*}</script>上、确实有可能有样本点。我们通常称<script type="math/tex">\Pi_{1}^{*}</script>和<script type="math/tex">\Pi_{2}^{*}</script>为间隔边界、称其上的某些点为支持向量</p>
<p><strong><em>注意：也有间隔边界上的样本点全是支持向量的说法，本书采用的支持向量的定义将更“苛刻”一些，具体细节会在 SVM 算法的对偶形式的叙述中讲到</em></strong></p>
<p>以上的叙述比较完整地说明了 SVM 如何应用于线性可分的数据集，接下来我们就看看如何将这种思想拓展到线性不可分数据集的分类之上。事实上，由于单用超平面的话、甚至连对线性不可分数据集正确分类都做不到，更不用提在此之上的将（硬）间隔最大化的问题了；但是考虑到间隔最大化的思想、我们可以做一定的“妥协”：将“硬”间隔转化为更加普适的“软”间隔。从数学的角度来说，这等价于将不等式约束放宽：</p>
<script type="math/tex; mode=display">
y_{i}\left( w \cdot x_{i} + b \right) \geq 1 \rightarrow y_{i}\left( w \cdot x_{i} + b \right) \geq 1 - \xi_{i}</script><p>其中的<script type="math/tex">\xi_{i}</script>通常被称为“松弛变量”，它需要满足<script type="math/tex">\xi_{i} \geq 0</script>。当然、这个约束的放宽并不是没有代价的，我们要在需要最小化的<script type="math/tex">{\frac{1}{2}\left\| w \right\|}^{2}</script>上加进一个“惩罚项”来“惩罚”<script type="math/tex">\xi_{i}</script>。换句话说，我们需要最小化的项将变为：</p>
<script type="math/tex; mode=display">
L\left( w,b,x,y \right) = \frac{1}{2}\left\| w \right\|^{2} + C\sum_{i = 1}^{N}\xi_{i}</script><p>式中<script type="math/tex">L(w,b,x,y)</script>即为损失函数、损失函数中的<script type="math/tex">C</script>（<script type="math/tex">> 0</script>）通常被称为“惩罚因子”，它描述了对松弛变量<script type="math/tex">\xi_{i}</script>的“惩罚力度”：<script type="math/tex">C</script>越大意味着最终的 SVM 模型越不能容忍误分类的点，越小则反之</p>
<p>综上所述、SVM 算法对应的优化问题可以拓展为：最小化<script type="math/tex">L(w,b,x,y)</script>、使得：</p>
<script type="math/tex; mode=display">
y_{i}\left( w \cdot x_{i} + b \right) \geq 1 - \xi_{i}\ (i = 1,\ldots,N)</script><p>其中</p>
<script type="math/tex; mode=display">
\xi_{i} \geq 0\ (i = 1,\ldots,N)</script><p>可以证明该优化问题的解存在、且<script type="math/tex">w</script>的解唯一但<script type="math/tex">b</script>的解不唯一，证明细节从略。同时参照感知机算法、自然希望能够写出使用随机梯度下降来训练软间隔最大化 SVM 的算法；但是注意到<script type="math/tex">L</script>表达式中的<script type="math/tex">\xi_{i}</script>是有约束的（需要不小于 0）、所以直接对其进行随机梯度下降存在一定的困难。为了将问题近似转化为无约束最优化问题、我们可以引入 Hinge 损失，其定义很简单：</p>
<script type="math/tex; mode=display">
l\left( w,b,x,y \right) = \max(0,1 - y\left( w \cdot x + b \right))</script><p>其中<script type="math/tex">y \in \left\{ - 1, + 1 \right\}</script>。换句话说，只有在模型作出足够肯定的正确的预测时、Hinge 损失才为 0；否则即使模型作出了正确的预测、Hinge 损失还是有可能给予模型一个惩罚</p>
<p>利用 Hinge 损失、我们可以把损失函数<script type="math/tex">L</script>写成：</p>
<script type="math/tex; mode=display">
\hat{L}\left( w,b,x,y \right) = \frac{1}{2}\left\| w \right\|^{2} + C\sum_{i = 1}^{N}{l(w,b,x_{i},y_{i})}</script><p>并通过最小化<script type="math/tex">\hat{L}</script>来求解上述 SVM 算法对应的最优化问题</p>
<p><strong><em>注意：最小化<script type="math/tex">\hat{L}</script>和上文最优化问题的等价性可能并不太显然，但是通过对比损失函数及逐条比对约束条件、完成等价性证明不算太困难（比如直接令<script type="math/tex">\xi_{i} = l(w,b,x_{i},y_{i})</script>）</em></strong></p>
<p>由于我们想要写出随机梯度下降的算法、所以求出<script type="math/tex">\hat{L}</script>在单一样本<script type="math/tex">(x_{i},y_{i})</script>上对<script type="math/tex">w</script>和<script type="math/tex">b</script>的偏导数是有必要的：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial\hat{L}(w,b,x_{i},y_{i})}{\partial w} &= w + \left\{ \begin{matrix}
0,\ \ & y_{i}\left( w \cdot x_{i} + b \right) \geq 1 \\
 - Cy_{i}x_{i},\ \ & y_{i}\left( w \cdot x_{i} + b \right) < 1 \\
\end{matrix} \right.\ \\

\frac{\partial\hat{L}(w,b,x_{i},y_{i})}{\partial b} &= \left\{ \begin{matrix}
0,\ \ & y_{i}\left( w \cdot x_{i} + b \right) \geq 1 \\
 - Cy_{i},\ \ & y_{i}\left( w \cdot x_{i} + b \right) < 1 \\
\end{matrix} \right.\
\end{align}</script><p>有了这两个偏导数之后，模仿感知机算法、我们就可以比较轻松地写出软间隔最大化 SVM 的随机梯度下降训练算法：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,\left( x_{N},y_{N} \right)\}</script>、迭代次数 M、惩罚因子<script type="math/tex">C</script>、学习速率<script type="math/tex">\eta</script>，其中：  <script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script></li>
<li><strong>过程</strong>：<ol>
<li>初始化参数：  <script type="math/tex; mode=display">
w = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{n},b = 0</script></li>
<li>对<script type="math/tex">j = 1,\ldots,M</script>：<ol>
<li>算出误差向量<script type="math/tex">e = \left( e_{1},\ldots,e_{N} \right)^{T}</script>、其中：  <script type="math/tex; mode=display">
e_{i} = 1 - y_{i}(w \cdot x_{i} + b)</script></li>
<li>取出误差最大的一项：  <script type="math/tex; mode=display">
i = \arg{\min_{i}e_{i}}</script></li>
<li>若<script type="math/tex">e_{i} \leq 0</script>则直接退出循环体、否则取对应的样本来进行随机梯度下降  <script type="math/tex; mode=display">
\begin{align}
w &\leftarrow (1 - \eta)w + \eta Cy_{i}x_{i} \\
b &\leftarrow b + \eta Cy_{i}
\end{align}</script></li>
</ol>
</li>
</ol>
</li>
<li><strong>输出</strong>：线性 SVM 模型<script type="math/tex">g\left( x \right) = \text{sign}\left( f\left( x \right) \right) = sign\left( w \cdot x + b \right)</script></li>
</ol>
<p>需要指出的是，虽然算法看上去差不多、内核也都是随机梯度下降，但其实在感知机模型对学习速率不敏感的同时、线性 SVM 对学习速率是相当敏感的。由前文提到过的 Novikoff 定理和凸优化相关理论可以从理论上解释这个现象，囿于篇幅、这里就不展开叙述了</p>
<p>由于上述线性 SVM 算法的实现和感知机算法的实现几乎一致、所以我们就略去对线性 SVM 实现的详细说明；观众老爷们可以参照感知机的代码来尝试进行实现、我个人实现的版本则可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/e_SVM/LinearSVM.py" target="_blank" rel="external">这里</a></p>
<p>可以通过二维线性可分数据集来简单直观地感受一下感知机和线性 SVM 的区别、结果如下图所示：</p>
<img src="/posts/d455305a/p2.png" alt="p2.png" title="">
<p>其中左、右图分别为感知机和线性 SVM 的表现，可以看出线性 SVM 要更合理</p>
<h1 id="SVM-算法的对偶形式"><a href="#SVM-算法的对偶形式" class="headerlink" title="SVM 算法的对偶形式"></a>SVM 算法的对偶形式</h1><p>与感知机类似、SVM 算法也是存在着对偶形式，不过这个转化的过程会比感知机那里的转化过程复杂不少。具体的推导步骤会放在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>中、这里我们就直接看结果：</p>
<ul>
<li>硬间隔最大化的对偶形式  <script type="math/tex; mode=display">
\max_{\alpha}{- \frac{1}{2}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\alpha_{i}\alpha_{j}y_{i}y_{j}\left( x_{i} \cdot x_{j} \right)}} + \sum_{i = 1}^{N}\alpha_{i}}</script>使得对<script type="math/tex">i = 1,\ldots,N</script>、都有：  <script type="math/tex; mode=display">
\sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0</script><script type="math/tex; mode=display">
\alpha_{i} \geq 0</script></li>
<li>软间隔最大化的对偶形式  <script type="math/tex; mode=display">
\max_{\alpha}{- \frac{1}{2}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\alpha_{i}\alpha_{j}y_{i}y_{j}\left( x_{i} \cdot x_{j} \right)}} + \sum_{i = 1}^{N}\alpha_{i}}</script>使得对<script type="math/tex">i = 1,\ldots,N</script>、都有：  <script type="math/tex; mode=display">
\sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0</script><script type="math/tex; mode=display">
0 \leq \alpha_{i} \leq C</script></li>
</ul>
<p>可以看到它们彼此之间相似度非常高、且转化的过程和感知机的转化过程也多少有些相似。同样的，由于对偶形式中样本点仅以内积的形式出现、我们通常会先把 Gram 矩阵算出来。现我们假设对偶形式的解为<script type="math/tex">\alpha^{*} = \left( \alpha_{1},\ldots,\alpha_{N} \right)^{T}</script>、那么就有：</p>
<script type="math/tex; mode=display">
\begin{align}
w^{*} &= \sum_{i = 1}^{N}{\alpha_{i}^{*}y_{i} \cdot x_{i}} \\
b^{*} &= y_{j} - \sum_{i = 1}^{N}{y_{i}\alpha_{i}^{*}\left( x_{i} \cdot x_{j} \right)}
\end{align}</script><p>其中<script type="math/tex">w^{*}</script>的表达式和感知机中<script type="math/tex">w^{*}</script>的表达式一致、<script type="math/tex">b^{*}</script>表达式中出现的 j 是满足<script type="math/tex">0 < \alpha_{j} < C</script>的下标（用反证法可以证明这种 j 必然存在，细节从略）</p>
<p>在有了对偶形式之后、我们就可以叙述支持向量的一个比较“苛刻”的定义了：假设支持向量的集合为<script type="math/tex">SV</script>、那么</p>
<ul>
<li>在硬间隔最大化 SVM 中：  <script type="math/tex; mode=display">
x_{i} \in SV \Leftrightarrow \alpha_{i}^{*} > 0</script></li>
<li>在软间隔最大化 SVM 中：  <script type="math/tex; mode=display">
x_{i} \in SV \Leftrightarrow 0 < \alpha_{i}^{*} \leq C</script></li>
</ul>
<p>其中在软间隔最大化 SVM 里，由于<script type="math/tex">\alpha_{i}^{*} \leq C</script>本身其实是由约束条件规定的、所以可以把上述两式统一写成：</p>
<script type="math/tex; mode=display">
x_{i} \in SV \Leftrightarrow \alpha_{i}^{*} > 0</script><p>我们可以通过下图来直观认知何谓支持向量：</p>
<img src="/posts/d455305a/p3.png" alt="p3.png" title="">
<p>图中的线段即为决策边界、被一个黑色圆圈给圈住的样本点即为支持向量，左图为线性可分数据集上的情况、右图为线性不可分数据集上的情况</p>
<p>此外，说明<script type="math/tex">\alpha_{i}^{*}</script>和<script type="math/tex">\xi_{i}^{*}</script>是如何定出各个样本点和间隔边界、分离超平面之间的位置关系是有必要的，它能加深我们对对偶形式求解过程中涉及到的 KKT 条件的理解与记忆（KKT 条件的相关定义会在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>中讲到）。具体而言：</p>
<ul>
<li>若<script type="math/tex">\alpha_{i}^{*} = 0</script>，那么<script type="math/tex">x_{i}</script>被正确分类且不在间隔边界上、又或被正确分类且在间隔边界上但不是支持向量</li>
<li>若<script type="math/tex">0 < \alpha_{i}^{*} < C</script>，那么就有<script type="math/tex">\xi_{i} = 0</script>、亦即<script type="math/tex">x_{i}</script>落在间隔边界上且为支持向量</li>
<li>若<script type="math/tex">\alpha_{i}^{*} = C</script>，那么：<ul>
<li>若<script type="math/tex">\xi_{i} = 0</script>、则<script type="math/tex">x_{i}</script>落在间隔边界上且为支持向量</li>
<li>若<script type="math/tex">0 < \xi_{i} < 1</script>、则<script type="math/tex">x_{i}</script>被正确分类且落在间隔边界和分离超平面之间</li>
<li>若<script type="math/tex">\xi_{i} = 1</script>、则<script type="math/tex">x_{i}</script>落在分离超平面上</li>
<li>若<script type="math/tex">\xi_{i} > 1</script>、则<script type="math/tex">x_{i}</script>被错误分类<br>由此可知、<script type="math/tex">\xi_{i}</script>其实刻画了<script type="math/tex">x_{i}</script>到相应间隔边界的函数间隔。换句话说、<script type="math/tex">\frac{\xi_{i}}{\left\| w \right\|}</script>即是<script type="math/tex">x_{i}</script>到间隔边间的距离（几何间隔）</li>
</ul>
</li>
</ul>
<h1 id="SVM-的训练"><a href="#SVM-的训练" class="headerlink" title="SVM 的训练"></a>SVM 的训练</h1><p>前文曾经提过、原始算法的对偶形式通常能将问题简化；虽然这点在感知机算法上没有太多体现，但是对于 SVM 来说，由于它的应用场景更为广泛、在许多问题的提法下转化成对偶形式的意义将非常重大。目前已经有许多针对 SVM 的成熟算法，本书拟介绍的是其中由 Platt 在 1998 年提出的、针对对偶问题求解的序列最小最优化算法（SMO）。本篇文章主要介绍 SMO 的思路和大概步骤，详细的叙述会在下一篇文章介绍完核技巧后进行</p>
<p>SMO 是一种启发式算法，其主要手段是在每次迭代中专注于只有两个变量的优问题以期望在可以接受的时间内得到一个较优解。具体而言、SMO 要解决的是软间隔最大化 SVM 的对偶问题：</p>
<script type="math/tex; mode=display">
\max_{\alpha}{- \frac{1}{2}\sum_{i = 1}^{N}{\sum_{j = 1}^{N}{\alpha_{i}\alpha_{j}y_{i}y_{j}\left( x_{i} \cdot x_{j} \right)}} + \sum_{i = 1}^{N}\alpha_{i}}</script><p>使得对<script type="math/tex">i = 1,\ldots,N</script>、都有：</p>
<script type="math/tex; mode=display">
\sum_{i = 1}^{N}{\alpha_{i}y_{i}} = 0</script><script type="math/tex; mode=display">
0 \leq \alpha_{i} \leq C</script><p>解决方案是在循环体中不断针对两个变量构造二次规划、并通过求出其解析解来优化原始的对偶问题。大致步骤如下：</p>
<ul>
<li>考察所有变量（<script type="math/tex">\alpha_{1},\ldots,\alpha_{N}</script>）及对应的样本点（<script type="math/tex">\left( x_{1},y_{1} \right),\ldots,(x_{N},y_{N})</script>）满足 KKT 条件的情况</li>
<li>若所有变量及对应样本在容许误差内都满足 KKT 条件，则退出循环体、完成训练</li>
<li>否则、通过如下步骤选出两个变量来构造新的规划问题：<ul>
<li>选出违反 KKT 条件最严重的样本点、以其对应的变量作为第一个变量</li>
<li>第二个变量的选取有一种比较繁复且高效的方法，但对于一个朴素的实现而言、第二个变量即使随机选取也无不可</li>
</ul>
</li>
<li>将上述步骤选出的变量以外的变量固定、仅针对这两个变量进行最优化。易知此时问题转化为了求二次函数的极大值、从而能简单地得到解析解</li>
</ul>
<p>这里仅简要说明一下 SVM 对偶算法中的 KKT 条件，详细的陈列则会放在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>中。具体而言、<script type="math/tex">\alpha_{i}</script>及其对应样本<script type="math/tex">(x_{i},y_{i})</script>的 KKT 条件为：</p>
<script type="math/tex; mode=display">
\begin{align}
\alpha_{i} = 0 &\Leftrightarrow y_{i}g\left( x_{i} \right) \geq 1 \\
0 < \alpha_{i} < C &\Leftrightarrow y_{i}g\left( x_{i} \right) = 1 \\
\alpha_{i} = C &\Leftrightarrow y_{i}g\left( x_{i} \right) \leq 1
\end{align}</script><p>所谓违反 KKT 条件最严重的样本点的定义也有许多种、其中一种简单有效的定义为：</p>
<script type="math/tex; mode=display">
c_{i} = \left\lbrack y_{i}g\left( x_{i} \right) - 1 \right\rbrack^{2}</script><p>所谓违反 KKT 条件最严重的样本点的定义也有许多种、其中一种简单有效的定义为：</p>
<ul>
<li>计算“损失向量”<script type="math/tex">c = \left( c_{1},\ldots,c_{N} \right)^{T}</script>、其中：  <script type="math/tex; mode=display">
c_{i} = \left\lbrack y_{i}g\left( x_{i} \right) - 1 \right\rbrack^{2}</script></li>
<li>将损失向量<script type="math/tex">c</script>复制三份（<script type="math/tex">c^{\left( 1 \right)}</script>、<script type="math/tex">c^{\left( 2 \right)}</script>、<script type="math/tex">c^{\left( 3 \right)}</script>）并分情况将相应位置的损失置为 0。具体而言：<ul>
<li>将<script type="math/tex">\alpha_{i} > 0</script>或<script type="math/tex">y_{i}g\left( x_{i} \right) \geq 1</script>对应的<script type="math/tex">c_{i}^{\left( 1 \right)}</script>置为 0</li>
<li>将<script type="math/tex">\alpha_{i} = 0</script>或<script type="math/tex">\alpha_{i} = C</script>或<script type="math/tex">y_{i}g\left( x_{i} \right) = 1</script>对应的<script type="math/tex">c_{i}^{\left( 2 \right)}</script>置为 0</li>
<li>将<script type="math/tex">\alpha_{i} < C</script>或<script type="math/tex">y_{i}g\left( x_{i} \right) \leq 1</script>对应的<script type="math/tex">c_{i}^{\left( 3 \right)}</script>置为 0</li>
</ul>
</li>
<li>将三份损失向量相加并取损失最大的样本对应<script type="math/tex">\alpha_i</script>的作为SMO的第一个变量、亦即：  <script type="math/tex; mode=display">
i = \arg{\max_{i}{\left\{ c_{i}^{\left( 1 \right)} + c_{i}^{\left( 2 \right)} + c_{i}^{\left( 3 \right)}|i = 1,\ldots,N\right\}}}</script></li>
</ul>
<p>在后面 SVM 的朴素实现中、我们打算采用的正是这种定义</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;感知机确实能够解决线性可分数据集的分类问题，但从它的解法容易看出、感知机的解是有无穷多个的。这主要是因为它对自己的要求太低：只需对训练集中所有样本点都能正确分类即可。换句话说、感知机基本没有考虑模型的泛化能力，这就导致感知机有时会训练出如下图所示的结果：&lt;/p&gt;
&lt;img src=&quot;/posts/d455305a/p1.png&quot; alt=&quot;p1.png&quot; title=&quot;&quot;&gt;
&lt;p&gt;可以看出它们是不尽合理的。支持向量机（SVM）针对这一点提出了一种改进方法，本篇文章主要叙述的就是该改进的思想和具体内容&lt;/p&gt;
    
    </summary>
    
      <category term="支持向量机" scheme="http://www.carefree0910.com/categories/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>感知机模型</title>
    <link href="http://www.carefree0910.com/posts/93db8ec2/"/>
    <id>http://www.carefree0910.com/posts/93db8ec2/</id>
    <published>2017-04-28T01:31:37.000Z</published>
    <updated>2017-04-28T12:04:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章所叙述的感知机模型是我们第一次应用到梯度下降法的模型，它的算法相当简单、但其框架却相当具有代表性。虽然说感知机模型只能处理非常特殊的问题（线性可分的数据集的分类问题）、但它的思想却是值得琢磨的</p>
<a id="more"></a>
<h1 id="线性可分性与感知机策略"><a href="#线性可分性与感知机策略" class="headerlink" title="线性可分性与感知机策略"></a>线性可分性与感知机策略</h1><p>在详细叙述感知机模型的原始算法之前，了解感知机适用范围和基本思想是必要的。正如前文所说，感知机只能用于给线性可分的数据集分类。其中，所谓的“线性可分性”的定义其实相当直观：对于一个数据集</p>
<script type="math/tex; mode=display">
D = \{\left( x_{1},y_{1} \right),\ldots,(x_{N},y_{N})\}</script><p>其中</p>
<script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script><p>如果存在一个超平面<script type="math/tex">\Pi</script>能够将<script type="math/tex">D</script>中的正负样本点精确地划分到<script type="math/tex">S</script>的两侧、亦即：</p>
<script type="math/tex; mode=display">
\exists\Pi:\ w \cdot x + b = 0</script><p>使得</p>
<script type="math/tex; mode=display">
\begin{align}
w \cdot x_{i} + b &< 0\ \left( \forall y_{i} = - 1 \right) \\
w \cdot x_{i} + b &> 0\ \left( \forall y_{i} = + 1 \right)
\end{align}</script><p>那么就称数据集<script type="math/tex">D</script>是线性可分的（Linearly Separable）；否则、就称<script type="math/tex">D</script>线性不可分</p>
<p>当维数<script type="math/tex">n = 2</script>时、数据集线性可分等价于正负样本点能在二维平面上被一条直线精确划分；当<script type="math/tex">n = 3</script>时则等价于能在三维空间中被一个平面精确划分。下给出一组线性可分和一组线性不可分的例子：</p>
<img src="/posts/93db8ec2/p1.png" alt="线性可分的数据集" title="线性可分的数据集">
<img src="/posts/93db8ec2/p2.png" alt="线性不可分的数据集" title="线性不可分的数据集">
<p>从数学的角度来说，线性可分性还有一个比较直观的等价定义：正负样本点集的凸包彼此不交。所谓凸包的定义如下：若集合<script type="math/tex">S \subset \mathbb{R}^{n}</script>由<script type="math/tex">N</script>个点组成：</p>
<script type="math/tex; mode=display">
S = \left\{ x_{1},\ldots,x_{N} \right\}\ (x_{i} \in \mathbb{R}^{n},\forall i = 1,\ldots,N)</script><p>那么<script type="math/tex">S</script>的凸包<script type="math/tex">\text{conv}(S)</script>即为：</p>
<script type="math/tex; mode=display">
\text{conv}\left( S \right) = \left\{ x = \sum_{i = 1}^{N}{\lambda_{i}x_{i}}|\sum_{i = 1}^{N}\lambda_{i} = 1,\lambda_{i} \geq 0(i = 1,\ldots,N)\right\}</script><p>比如，上述两个二维数据集的凸包将如下面两张图所示：</p>
<img src="/posts/93db8ec2/p3.png" alt="p3.png" title="">
<img src="/posts/93db8ec2/p4.png" alt="p4.png" title="">
<p>第一张图中，正负样本点集的凸包不交、所以数据集线性可分；第二张图中的橙色区域即为正负样本点集凸包的相交处、所以数据集线性不可分</p>
<p>该等价性的证明可以用反证法得出；由于过程不算困难且结论相当直观、所以具体的推导步骤从略</p>
<p>知道了线性可分性的定义之后、感知机模型的目的也就容易想到了——无非就是为了找到上文提到过的、能将线性可分数据集中的正负样本点精确划分到两侧的超平面<script type="math/tex">\Pi</script>。考虑到机器学习的共性，我们希望能将找超平面的过程转化为最小化一个损失函数的过程；感知机策略的具体表现、就在于如何定义这个损失函数上。考虑到<script type="math/tex">\Pi</script>的性质，损失函数的定义其实是很自然的：</p>
<script type="math/tex; mode=display">
L\left( w,b,x,y \right) = - \sum_{x_{i} \in E}^{}{y_{i}(w \cdot x_{i} + b)}</script><p>其中<script type="math/tex">E</script>是被当前感知机误分类的点集，亦即对<script type="math/tex">\forall x_{i} \in E</script>、有：</p>
<script type="math/tex; mode=display">
\begin{align}
w \cdot x_{i} + b &\geq 0\ \left( if\ y_{i} = - 1 \right) \\
w \cdot x_{i} + b &\leq 0\ \left( if\ y_{i} = + 1 \right)
\end{align}</script><p>换句话说、损失函数还可以表示为：</p>
<script type="math/tex; mode=display">
L\left( w,b,x,y \right) = \sum_{x_{i} \in E}^{}\left| w \cdot x_{i} + b \right|</script><p>注意到对于样本点<script type="math/tex">(x_{i},y_{i})</script>而言、<script type="math/tex">\left| w \cdot x_{i} + b \right|</script>能够相对地表示向量<script type="math/tex">x_{i}</script>到分离超平面<script type="math/tex">w \cdot x + b = 0</script>的距离，所以损失函数的几何解释即为：损失函数值<script type="math/tex">=</script>所有被误分类的样本点到当前分离超平面的相对距离的总和。如果感知机能将所有样本点正确分类的话、<script type="math/tex">E</script>就是空集、此时损失函数<script type="math/tex">L\left( w,b \right) = 0</script>；同时，若误分类的样本点越少或误分类的样本点离当前分离超平面越近、损失函数<script type="math/tex">L\left( w,b \right)</script>的值就越小。是故寻找最终正确的分离超平面<script type="math/tex">\Pi</script>的过程、确实可以转化为最小化损失函数<script type="math/tex">L\left( w,b \right)</script>的过程，而这也正是感知机所采用的训练策略</p>
<p><strong><em>注意：需要强调的是，<script type="math/tex">\left| w \cdot x_{i} + b \right|</script>所描述的“相对距离”和我们直观上的“欧氏距离”或说“几何距离”是不一样的（事实上它们之间相差了一个<script type="math/tex">\left\| w \right\|</script>）；相对严谨的叙述会放在<a href="/posts/d455305a/" title="从感知机到支持向量机">从感知机到支持向量机</a>中、这里就先按下不表</em></strong></p>
<h1 id="感知机算法"><a href="#感知机算法" class="headerlink" title="感知机算法"></a>感知机算法</h1><p>最小化损失函数这个过程在决策树的训练中也出现过，彼时我们采用的是一种启发式的算法——选取当前能使损失（信息的不确定性）减少最多的特征作为划分标准来划分数据。感知机算法采用的则是一种应用场景更广的方法——梯度下降法。梯度下降法的一般性定义和相关说明会放在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>中、这里我们只说一个直观：在许多情况下，损失函数是足够好的函数、从而它在每个点都能进行“求导”。求导之后我们就能得到一个损失函数增长最快的“方向”，此时我们沿反方向前进的话、就能期望损失函数以“最快的速度”减少（这也正是为何梯度下降法又叫最速下降法）</p>
<p>注意到在求得“方向”后、沿方向“走多远”也是需要考虑的参数。一般我们称该参数为“学习速率”或“步长”，通过调整和优化该参数的表现、常常能导出原理一致但表现迥异的算法。不过即使如此，梯度下降法的关键还是在于损失函数的“求导”。需要指出的是，梯度下降法本身还可以大致分为三种具体的算法——随机梯度下降法（Stochastic Gradient Descent，常简称为 SGD）、小批量梯度下降法（Mini-batch Gradient Descent，常简称为 MBGD）和批量梯度下降法（Batch Gradient Descent，常简称为 BGD）。这三种梯度下降法的说明、对比同样会放在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>中进行，这里只需要知道它们行为上的差别：随机梯度下降法在每个迭代中只使用一个样本来进行参数的更新、小批量梯度下降法则会同时选用多个样本来更新参数、批量梯度下降法则更是会同时选用所有样本来更新参数</p>
<p><strong><em>注意：也有认为 SGD 即为 MBGD 的说法、所以具体的含义需要结合具体情况分析</em></strong></p>
<p>简单起见、我们采用随机梯度下降来进行训练，此时我们需要求出损失函数在单一样本上对<script type="math/tex">w</script>和<script type="math/tex">b</script>的偏导数：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial L(w,b,x_{i},y_{i})}{\partial w} &= \left\{ \begin{matrix}
0,\ \ &(x_{i},y_{i}) \notin E \\
 - y_{i}x_{i},\ \ &{(x}_{i},y_{i}) \in E \\
\end{matrix} \right.\ \\

\frac{\partial L(w,b,x_{i},y_{i})}{\partial b} &= \left\{ \begin{matrix}
0,\ \ &(x_{i},y_{i}) \notin E \\
 - y_{i},\ \ &{(x}_{i},y_{i}) \in E \\
\end{matrix} \right.\
\end{align}</script><p>利用它们可以自然地写出感知机模型的随机梯度下降训练算法：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,\left( x_{N},y_{N} \right)\}</script>、迭代次数 M、学习速率<script type="math/tex">\eta</script>，其中：  <script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script></li>
<li><strong>过程</strong>：<ol>
<li>初始化参数：  <script type="math/tex; mode=display">
w = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{n},b = 0</script></li>
<li>对<script type="math/tex">j = 1,\ldots,M</script>：  <script type="math/tex; mode=display">
E = \left\{ \left( x_{i},y_{i} \right) \middle| y_{i}\left( w \cdot x_{i} + b \right) \leq 0 \right\}</script><ol>
<li>若<script type="math/tex">E = \varnothing</script>（亦即没有误分类的样本点）则退出循环体</li>
<li>否则，任取<script type="math/tex">E</script>中的一个样本点<script type="math/tex">(x_{i},y_{i})</script>并利用它更新参数：  <script type="math/tex; mode=display">
\begin{align}
w &\leftarrow w + \eta y_{i}x_{i} \\
b &\leftarrow b + \eta y_{i}
\end{align}</script></li>
</ol>
</li>
</ol>
</li>
<li><strong>输出</strong>：感知机模型<script type="math/tex">g\left( x \right) = \text{sign}\left( f\left( x \right) \right) = \text{sign}\left( w \cdot x + b \right)</script></li>
</ol>
<p>其中最后一步用到的 sign 是符号函数。由于感知机算法中更新一次参数的时间开销非常小、所以通常会把迭代次数 M 设置成一个比较大的数（比如<script type="math/tex">10^{4}</script>）</p>
<p>此外需要指出的是，虽说感知机只适用于线性可分数据集的分类、但它有个优点就是：无论学习速率<script type="math/tex">\eta</script>是多少，只要数据集线性可分，那么上述感知机算法在 M 足够大的情况下、必然能够训练出一个使得<script type="math/tex">E = \varnothing</script>的分离超平面（这其实就是著名的 Novikoff 定理，证明会用到比较纯粹的数学技巧、所以从略）</p>
<p>虽说所蕴含的梯度下降的思想并不平凡，但感知机算法单就复杂度而言、可以说是目前为止遇到过的最简单的算法了，其实现相对的也非常简单直观：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> Util.Bases <span class="keyword">import</span> ClassifierBase</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(ClassifierBase)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(Perceptron, self).__init__()</div><div class="line">        self._w = self._b = <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight=None, lr=<span class="number">0.01</span>, epoch=<span class="number">10</span> ** <span class="number">6</span>)</span>:</span></div><div class="line">        x, y = np.atleast_2d(x), np.array(y)</div><div class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            sample_weight = np.ones(len(y))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            sample_weight = np.array(sample_weight) * len(y)</div><div class="line">        <span class="comment"># 初始化参数</span></div><div class="line">        self._w = np.zeros(x.shape[<span class="number">1</span>])</div><div class="line">        self._b = <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</div><div class="line">            y_pred = self.predict(x)</div><div class="line">            <span class="comment"># 获取加权误差向量</span></div><div class="line">            _err = (y_pred != y) * sample_weight</div><div class="line">            <span class="comment"># 引入随机性以进行随机梯度下降</span></div><div class="line">            _indices = np.random.permutation(len(y))</div><div class="line">            _idx = _indices[np.argmax(_err[_indices])]</div><div class="line">            <span class="comment"># 若没有被误分类的样本点则完成了训练</span></div><div class="line">            <span class="keyword">if</span> y_pred[_idx] == y[_idx]:</div><div class="line">                <span class="keyword">return</span></div><div class="line">            <span class="comment"># 否则，根据选出的样本点更新参数</span></div><div class="line">            _delta = lr * y[_idx] * sample_weight[_idx]</div><div class="line">            self._w += _delta * x[_idx]</div><div class="line">            self._b += _delta</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x, get_raw_results=False)</span>:</span></div><div class="line">        rs = np.sum(self._w * x, axis=<span class="number">1</span>) + self._b</div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> get_raw_results:</div><div class="line">            <span class="keyword">return</span> np.sign(rs)</div><div class="line">        <span class="keyword">return</span> rs</div></pre></td></tr></table></figure>
<p>可以用之前提到过的两个二维数据集来简单评估一下我们实现的感知机模型，结果如下图所示：</p>
<img src="/posts/93db8ec2/p5.png" alt="p5.png" title="">
<p>其中在线性可分的数据集上、感知机只用了一次迭代便得到了上面左图中的效果；在线性不可分的数据集上、感知机在迭代 1000 次后可以得到右图中的效果，此时正确率为 97.5%</p>
<h1 id="感知机算法的对偶形式"><a href="#感知机算法的对偶形式" class="headerlink" title="感知机算法的对偶形式"></a>感知机算法的对偶形式</h1><p>本节将会简要介绍一个非常重要的概念：拉格朗日对偶性（Lagrange Duality）；在有约束的最优化问题中，为了便于求解、我们常常会利用它来将比较原始问题转化为更好解决的对偶问题。对于特定的问题，原始算法的对偶形式也常常会有一些共性存在。比如对于感知机和后文会介绍的支持向量机来说，它们的对偶算法都会将模型的参数表示为样本点的某种线性组合、并把问题转化为求解线性组合中的各个系数</p>
<p>虽说感知机算法的原始形式已经非常简单，但是通过将它转化为对偶形式、我们可以比较清晰地感受到转化的过程，这有助于理解和记忆后文介绍的、较为复杂的支持向量机的对偶形式</p>
<p>考虑到原始算法的核心步骤为：</p>
<script type="math/tex; mode=display">
\begin{align}
w &\leftarrow w + \eta y_{i}x_{i} \\
b &\leftarrow b + \eta y_{i}
\end{align}</script><p>其中<script type="math/tex">\left( x_{i},y_{i} \right) \in E</script>、<script type="math/tex">E</script>是当前被误分类的样本点的集合；可以看见、参数的更新是完全基于样本点的。考虑到我们要将参数<script type="math/tex">w</script>和<script type="math/tex">b</script>表示为样本点的线性组合，一个自然的想法就是记录下在核心步骤中、各个样本点分别被利用了多少次、然后利用这个次数来将<script type="math/tex">w</script>和<script type="math/tex">b</script>表示出来。比如说，若设样本点<script type="math/tex">\left( x_{i},y_{i} \right)</script>一共在上述核心步骤中被利用了<script type="math/tex">n_{i}</script>次、那么就有（假设初始化参数时<script type="math/tex">w = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{n},b = 0</script>）：</p>
<script type="math/tex; mode=display">
\begin{align}
w &= \eta\sum_{i = 1}^{N}{n_{i}y_{i}x_{i}} \\
b &= \eta\sum_{i = 1}^{N}{n_{i}y_{i}}
\end{align}</script><p>如果进一步设<script type="math/tex">\alpha_{i} = \eta n_{i}</script>、则有：</p>
<script type="math/tex; mode=display">
\begin{align}
w &= \sum_{i = 1}^{N}{\alpha_{i}y_{i}x_{i}} \\
b &= \sum_{i = 1}^{N}{\alpha_{i}y_{i}}
\end{align}</script><p>在此基础上，感知机算法的对偶形式就能很自然地写出来了：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,\left( x_{N},y_{N} \right)\}</script>、迭代次数 M、学习速率<script type="math/tex">\eta</script>，其中：  <script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script></li>
<li><strong>过程</strong>：<ol>
<li>初始化参数：  <script type="math/tex; mode=display">
\alpha = \left( \alpha_{1},\ldots,\alpha_{N} \right)^{T} = \left( 0,\ldots,0 \right)^{T} \in \mathbb{R}^{N}</script></li>
<li>对<script type="math/tex">j = 1,\ldots,M</script>：  <script type="math/tex; mode=display">
E = \left\{ \left( x_{i},y_{i} \right) \middle| y_{i}\left( \sum_{k = 1}^{N}{\alpha_{k}y_{k}\left( x_{k} \cdot x_{i} + 1 \right)} \right) \leq 0 \right\}</script><ol>
<li>若<script type="math/tex">E = \varnothing</script>（亦即没有误分类的样本点）则退出循环体</li>
<li>否则，任取<script type="math/tex">E</script>中的一个样本点<script type="math/tex">(x_{i},y_{i})</script>并利用其下标更新参数：  <script type="math/tex; mode=display">
\alpha_{i} \leftarrow \alpha_{i} + \eta</script></li>
</ol>
</li>
</ol>
</li>
<li><strong>输出</strong>：感知机模型<script type="math/tex">g\left( x \right) = \text{sign}\left( f\left( x \right) \right) = sign\left( \sum_{k = 1}^{N}{\alpha_{k}y_{k}\left( x_{k} \cdot x_{i} + 1 \right)} \right)</script></li>
</ol>
<p>需要指出的是，在对偶形式中、样本点里面的<script type="math/tex">x</script>仅以内积的形式（<script type="math/tex">x_{k} \cdot x_{i}</script>）出现；这是一个非常重要且深刻的性质，利用它和后文将进行介绍核技巧、能够将许多算法从线性算法“升级”成为非线性算法</p>
<p>注意到对偶形式的训练过程常常会重复用到大量的、样本点之间的内积，我们通常会提前将样本点两两之间的内积计算出来并存储在一个矩阵中；这个矩阵就是著名的 Gram 矩阵、其数学定义即为：</p>
<script type="math/tex; mode=display">
G = \left( x_{i} \cdot x_{j} \right)_{N \times N}</script><p>这样的话，在训练过程中如果要用到相应的内积、只需从 Gram 矩阵中提取即可，这样在大多数情况下都能大大提高效率</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章所叙述的感知机模型是我们第一次应用到梯度下降法的模型，它的算法相当简单、但其框架却相当具有代表性。虽然说感知机模型只能处理非常特殊的问题（线性可分的数据集的分类问题）、但它的思想却是值得琢磨的&lt;/p&gt;
    
    </summary>
    
      <category term="支持向量机" scheme="http://www.carefree0910.com/categories/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机综述</title>
    <link href="http://www.carefree0910.com/posts/487ba3a6/"/>
    <id>http://www.carefree0910.com/posts/487ba3a6/</id>
    <published>2017-04-28T01:21:08.000Z</published>
    <updated>2017-04-28T12:24:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>目前为止讲过的模型中，朴素贝叶斯模型属于生成模型：它的训练过程其实很难称之为“训练”——毕竟它只是对输入的训练数据集进行了若干“计数”的操作；决策树模型的训练过程虽然确实有一些训练的意思在里面，但其本质——各种信息不确定性的度量仍然脱不出“计数”的范畴。换句话说，朴素贝叶斯和决策树的核心步骤似乎都只是“计数”而已。随机森林和 AdaBoost 自不用提，它们都只是将已有的模型进行集成、其本身的训练过程可谓不是主体</p>
<p>然而我们都知道、机器学习当然不只是“计数”那么简单的一回事。因此我们拟在本系列及以后的系列中介绍另一大类训练方法——梯度下降法（Gradient Decent；有时我们也称之为最速下降法（Steepest Descent））。就本系列而言，我们会先介绍一个比较简易的、应用到了梯度下降法的模型——感知机（Perceptron），然后我们会介绍一个思想和感知机类似、但是应用场景更加广泛的模型——支持向量机（Support Vector Machine，常简称为SVM）</p>
<p>以下是目录：</p>
<ul>
<li><a href="/posts/93db8ec2/" title="感知机模型">感知机模型</a></li>
<li><a href="/posts/d455305a/" title="从感知机到支持向量机">从感知机到支持向量机</a></li>
<li><a href="/posts/924abfe1/" title="从线性到非线性">从线性到非线性</a></li>
<li><a href="/posts/917ccef9/" title="核模型的实现与评估">核模型的实现与评估</a></li>
<li><a href="/posts/1dc4445a/" title="多分类与支持向量回归">多分类与支持向量回归</a></li>
<li><a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a></li>
<li><a href="/posts/5b3e9c59/" title="“支持向量机”小结">“支持向量机”小结</a>
</li>
</ul>
<p>需要指出的是、本系列的前三篇文章讨论的都是二类分类问题，回归问题和把二类算法拓展成多类算法的手段会放在<a href="/posts/1dc4445a/" title="多分类与支持向量回归">多分类与支持向量回归</a>中进行简要介绍（注意虽然我们上一个系列叙述 AdaBoost 算法时同样也只针对二类分类问题进行了说明、但是应用<a href="/posts/1dc4445a/" title="多分类与支持向量回归">多分类与支持向量回归</a>中的内容是可以将上一个系列涉及到的的诸多 AdaBoost 二分类模型推广成多分类模型的）</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前为止讲过的模型中，朴素贝叶斯模型属于生成模型：它的训练过程其实很难称之为“训练”——毕竟它只是对输入的训练数据集进行了若干“计数”的操作；决策树模型的训练过程虽然确实有一些训练的意思在里面，但其本质——各种信息不确定性的度量仍然脱不出“计数”的范畴。换句话说，朴素贝叶斯
    
    </summary>
    
      <category term="支持向量机" scheme="http://www.carefree0910.com/categories/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
    
      <category term="综述" scheme="http://www.carefree0910.com/tags/%E7%BB%BC%E8%BF%B0/"/>
    
      <category term="目录" scheme="http://www.carefree0910.com/tags/%E7%9B%AE%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>“集成学习”小结</title>
    <link href="http://www.carefree0910.com/posts/48a0211a/"/>
    <id>http://www.carefree0910.com/posts/48a0211a/</id>
    <published>2017-04-25T16:41:26.000Z</published>
    <updated>2017-04-25T16:42:32.000Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>集成学习是将个体模型进行集成的方法，大致可分为 Bagging 和 Boosting 两类</li>
<li>随机森林是 Bagging 算法的一种常见拓展、性能优异；它不仅对样本的选取引入随机性、还对个体模型（决策树）的特征选取步骤引入随机性</li>
<li>AdaBoost 是 Boosting 族算法的代表，通过以下三步进行提升：<ul>
<li>根据样本权重训练弱分类器</li>
<li>根据该弱分类器的加权错误率为其分配“话语权”</li>
<li>根据该弱分类器的表现更新样本权重</li>
</ul>
</li>
<li>集成模型具有相当不错的正则化能力、但该正则化能力并不是必然存在的</li>
<li>AdaBoost 可以用前向分步算法和加法模型来解释</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;集成学习是将个体模型进行集成的方法，大致可分为 Bagging 和 Boosting 两类&lt;/li&gt;
&lt;li&gt;随机森林是 Bagging 算法的一种常见拓展、性能优异；它不仅对样本的选取引入随机性、还对个体模型（决策树）的特征选取步骤引入随机性&lt;/li&gt;
&lt;li
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="小结" scheme="http://www.carefree0910.com/tags/%E5%B0%8F%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>相关数学理论</title>
    <link href="http://www.carefree0910.com/posts/613bbb2f/"/>
    <id>http://www.carefree0910.com/posts/613bbb2f/</id>
    <published>2017-04-25T16:19:04.000Z</published>
    <updated>2017-04-28T10:51:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章会叙述之前没有解决的纯数学问题，同样会涉及到概率论的一些基础概念和思想，可能会有一定的难度</p>
<a id="more"></a>
<h1 id="经验分布函数"><a href="#经验分布函数" class="headerlink" title="经验分布函数"></a>经验分布函数</h1><p>正如前文所说，经验分布函数的数学表达式为：</p>
<script type="math/tex; mode=display">
F_{N}\left( x \right) = \frac{1}{N}\sum_{i = 1}^{N}{I_{\left( - \infty,x \right\rbrack}(x_{i})}</script><p>如果将<script type="math/tex">x_{1},\ldots,x_{N}</script>按从小到大的顺序排成<script type="math/tex">x_{(1)},\ldots,x_{\left( N \right)}</script>，我们通常称其中的<script type="math/tex">x_{\left( i \right)}</script>为第 i 个次序统计量。易知可以利用次序统计量将<script type="math/tex">F_{N}(x)</script>表示成更直观的形式：</p>
<script type="math/tex; mode=display">
F_{N}\left( x \right) = \left\{ \begin{matrix}
0,\ \ &x < x_{\left( 1 \right)} \\
\frac{i}{N},\ \ &x \in \left\lbrack x_{\left( i \right)},x_{\left( i + 1 \right)} \right)\ (i = 1,\ldots,N - 1) \\
1,\ \ &x \geq x_{\left( N \right)} \\
\end{matrix} \right.\</script><p>关于其优良性，前文所说的“频率估计概率”的严谨叙述其实就是强大数律：</p>
<script type="math/tex; mode=display">
p\left( \lim_{N}{F_{N}\left( x \right) - F\left( x \right) = 0} \right) = 1\ (\forall x)</script><p>亦即</p>
<script type="math/tex; mode=display">
F_{N}(x) \xrightarrow{a.s.} F(x)</script><p>同时还有一个更强的结论（Glivenko-Cantelli 定理）：</p>
<script type="math/tex; mode=display">
p\left( \lim_{N}{\sup_{x}\left| F_{N}\left( x \right) - F\left( x \right) \right| = 0} \right) = 1</script><p>亦即</p>
<script type="math/tex; mode=display">
\left\| F_{N}\left( x \right) - F\left( x \right) \right\|_{\infty} \equiv \sup_{x}\left| F_{N}\left( x \right) - F\left( x \right) \right|\xrightarrow{a.s.}0</script><p>其中，<script type="math/tex">\sup_{x}\left| F_{N}\left( x \right) - F\left( x \right) \right|</script>就是著名的柯尔莫诺夫-斯米尔诺夫检验（Kolmogorov-Smirnov Statistic）。值得一提的是，用其它范数来代替这里的无穷范数有时也是合理的。比如说用二范数来代替时、对应的就是 Cramér-von Mises Criterion</p>
<p>此外，我们还可以利用中心极限定理等来研究经验分布函数（比如与正态分布扯上关系等等），这里就不详细展开了。总之，经验分布函数的优良性是相当有保证的，与其本质类似的 Bootstrap 的优良性也因而有了保证。当然、Bootstrap 自己是有一套成熟理论的，不过如果就这点展开来叙述的话、多多少少会偏离了本系列文章的主旨，所以这里就仅通过讨论经验分布函数来间接地感受 Bootstrap 的优良性</p>
<h1 id="AdaBoost-与前向分步加法模型"><a href="#AdaBoost-与前向分步加法模型" class="headerlink" title="AdaBoost 与前向分步加法模型"></a>AdaBoost 与前向分步加法模型</h1><p>本节主要用于推导如下定理：AdaBoost 分类模型可以等价为损失函数为指数函数的前向分步加法模型</p>
<p>假设经过<script type="math/tex">k</script>轮迭代后、前项分布算法已经得到了加法模型<script type="math/tex">f_{k}(x)</script>，亦即：</p>
<script type="math/tex; mode=display">
\begin{align}
f_{k}\left( x \right) &= f_{k - 1}\left( x \right) + \alpha_{k}g_{k}\left( x \right) = f_{k - 2}\left( x \right) + \alpha_{k - 1}g_{k - 1}\left( x \right) + \alpha_{k}g_{k}(x) \\

&= \ldots = \sum_{i = 1}^{k}{\alpha_{i}g_{i}(x)}
\end{align}</script><p>可知、第<script type="math/tex">k + 1</script>轮的模型<script type="math/tex">f_{k + 1}</script>能表示为：</p>
<script type="math/tex; mode=display">
f_{k + 1}\left( x \right) = f_{k}\left( x \right) + \alpha_{k + 1}g_{k + 1}(x)</script><p>我们关心的问题就是，如何在<script type="math/tex">f_{k}(x)</script>确定下来的情况下、训练出第<script type="math/tex">k + 1</script>轮的个体分类器<script type="math/tex">g_{k + 1}(x)</script>及其权重<script type="math/tex">\alpha_{k + 1}</script>。注意到我们的损失函数是指数函数，亦即：</p>
<script type="math/tex; mode=display">
\begin{align}
L &= \sum_{i = 1}^{N}{\exp\left\lbrack - y_{i}f_{k + 1}\left( x_{i} \right) \right\rbrack} \\

&= \sum_{i = 1}^{N}w_{ki}\exp\lbrack - y_{i}\alpha_{k + 1}g_{k + 1}(x_{i})\rbrack
\end{align}</script><p>其中</p>
<script type="math/tex; mode=display">
w_{ki} = \exp\lbrack - y_{i}f_{k}(x_{i})\rbrack</script><p>在<script type="math/tex">f_{k}(x)</script>确定下来的情况下是常数。由于我们的最终目的是最小化损失函数、所以<script type="math/tex">\alpha_{k + 1}</script>和<script type="math/tex">g_{k + 1}(x)</script>就可以表示为：</p>
<script type="math/tex; mode=display">
\begin{align}
\left( \alpha_{k + 1},g_{k + 1}\left( x \right) \right) &= \arg{\min_{\alpha,g}{\sum_{i = 1}^{N}{w_{ki}\exp\left\lbrack - y_{i}\alpha g\left( x_{i} \right) \right\rbrack}}} \\

&= \arg{\min_{\alpha,g}{\sum_{y_{i} = g\left( x_{i} \right)}^{}{w_{ki}e^{- \alpha}} + \sum_{y_{i} \neq g\left( x_{i} \right)}^{}{w_{ki}e^{\alpha}}}} \\

&= \arg{\min_{\alpha,g}{\left( e^{\alpha} - e^{- \alpha} \right)\sum_{i = 1}^{N}{w_{ki}I\left( y_{i} \neq g\left( x_{i} \right) \right)} + e^{- \alpha}\sum_{i = 1}^{N}w_{ki}}} \\

&= \arg{\min_{\alpha,g}{\left( e^{\alpha} - e^{- \alpha} \right)\sum_{i = 1}^{N}{w_{ki}I\left( y_{i} \neq g\left( x_{i} \right) \right)} + e^{- \alpha}}}
\end{align}</script><p>上式可以分两步求解。先看当<script type="math/tex">\alpha</script>确定下来后应该如何定出<script type="math/tex">g_{k + 1}(x)</script>，易知：</p>
<script type="math/tex; mode=display">
g_{k + 1}\left( x \right) = \arg{\min_{g}{\sum_{i = 1}^{N}{w_{ki}I\left( y_{i} \neq g\left( x_{i} \right) \right)}}}</script><p>亦即第<script type="math/tex">k + 1</script>步的个体分类器应该使训练集上的加权错误率最小。不妨设解出的<script type="math/tex">g_{k + 1}(x)</script>在训练集上的加权错误率为<script type="math/tex">e_{k + 1}</script>、亦即：</p>
<script type="math/tex; mode=display">
\sum_{i = 1}^{N}{w_{ki}I\left( y_{i} \neq g_{k + 1}\left( x_{i} \right) \right)} \triangleq e_{k + 1}</script><p>我们需要利用它来定出<script type="math/tex">\alpha_{k + 1}</script>。注意到对目标函数求偏导后易知：</p>
<script type="math/tex; mode=display">
\begin{align}
\alpha_{k + 1} &= \arg{\min_{\alpha}{\left( e^{\alpha} - e^{- \alpha} \right)e_{k + 1} + e^{- \alpha}}} \\

&\Leftrightarrow \left( e^{\alpha_{k + 1}} + e^{- \alpha_{k + 1}} \right)e_{k + 1} - e^{- \alpha_{k + 1}} = 0 \\

&\Leftrightarrow \alpha_{k + 1} = \frac{1}{2}\ln\frac{1 - e_{k + 1}}{e_{k + 1}}
\end{align}</script><p>这和 AdaBoost 中确定个体分类器权值的式子一模一样。接下来只需要证明样本权重更新的式子也彼此一致即可得证定理，而事实上、由于：</p>
<script type="math/tex; mode=display">
f_{k + 1}\left( x \right) = f_{k}\left( x \right) + \alpha_{k + 1}g_{k + 1}\left( x \right)</script><p>从而</p>
<script type="math/tex; mode=display">
\begin{align}
w_{k + 1,i} &= \exp\left\lbrack - y_{i}f_{k + 1}\left( x_{i} \right) \right\rbrack \\

&= \exp\left\lbrack - y_{i}f_{k}\left( x_{i} \right) \right\rbrack \cdot \exp\left\lbrack - y_{i}\alpha_{k + 1}g_{k + 1}\left( x_{i} \right) \right\rbrack \\

&= w_{ki} \cdot \exp\left\lbrack - y_{i}\alpha_{k + 1}g_{k + 1}\left( x_{i} \right) \right\rbrack
\end{align}</script><p>注意到我们要将样本权重归一化，所以须有：</p>
<script type="math/tex; mode=display">
w_{k + 1,i} \leftarrow \frac{w_{k + 1,i}}{Z_{k}}</script><p>其中</p>
<script type="math/tex; mode=display">
Z_{k} = \sum_{i = 1}^{N}w_{k + 1,i} = \sum_{i = 1}^{N}{w_{ki} \cdot \exp\left\lbrack - \alpha_{k + 1}y_{i}g_{k + 1}\left( x_{i} \right) \right\rbrack}</script><p>是故</p>
<script type="math/tex; mode=display">
w_{k + 1,i} = \frac{w_{ki}}{Z_{k}} \cdot \exp\left\lbrack - y_{i}\alpha_{k + 1}g_{k + 1}\left( x_{i} \right) \right\rbrack</script><p>这和 AdaBoost 中更新样本权重的式子也一模一样。综上所述、定理得证</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章会叙述之前没有解决的纯数学问题，同样会涉及到概率论的一些基础概念和思想，可能会有一定的难度&lt;/p&gt;
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>AdaBoost 算法的解释</title>
    <link href="http://www.carefree0910.com/posts/707464b/"/>
    <id>http://www.carefree0910.com/posts/707464b/</id>
    <published>2017-04-25T16:06:50.000Z</published>
    <updated>2017-04-28T10:51:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>我们前面提到过 Bagging 的数学基础是 Bootstrap 理论、但还没有讲 Boosting 的数学基础。本篇文章拟打算直观地阐述 Boosting 族的代表算法——AdaBoost 算法的解释，由于具体的推导相当繁琐，相关的细节我们会放在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>里面说明</p>
<a id="more"></a>
<p>首先将结论给出：AdaBoost 算法是前向分步算法的特例，AdaBoost 模型等价于损失函数为指数函数的加法模型</p>
<p>其中，加法模型的定义是直观且熟悉的：</p>
<script type="math/tex; mode=display">
f\left( x \right) = \sum_{k = 1}^{M}{\alpha_{k}g(x;\Theta_{k})}</script><p>这里的<script type="math/tex">g(x;\Theta_{k})</script>为基函数，<script type="math/tex">\alpha_{k}</script>是基函数的权重，<script type="math/tex">\Theta_{k}</script>是基函数的参数。显然的是，我们的 AdaBoost 算法的最后一步生成的模型正是这么一个加法模型</p>
<p>而所谓的前向分步算法，就是从前向后、一步一步地学习加法模型中的每一个基函数及其权重而非将<script type="math/tex">f(x)</script>作为一个整体来训练，这也正是 AdaBoost 的思想</p>
<p>如果此时需要最小化的损失函数是指数损失函数<script type="math/tex">L\left( y,f\left( x \right) \right) = \exp\left\lbrack - yf\left( x \right) \right\rbrack</script>的话，通过一系列的数学推导后可以证明、此时的加法模型确实等价于 AdaBoost 模型</p>
<p>可能大家会觉得这里面有一些别扭：为什么一个实现起来非常简便的模型，它背后的数学原理却如此复杂？事实上有趣的是，AdaBoost 是为数不多的、先有算法后有解释的模型。也就是说，是先有了 AdaBoost 这个东西，然后数学家们看到它的表现非常好之后、才开始绞尽脑汁并想出了一套适用于 AdaBoost 的数学理论。更有意思的是，该数学理论并非毫无意义：在 AdaBoost 的回归问题中，就可以用前向分步算法的理论、将每一步的训练转化为了拟合当前模型的残差、从而简化了训练步骤。我们可以简单地叙述一下其原理：</p>
<p>加法模型的等价叙述为</p>
<script type="math/tex; mode=display">
f_{k + 1}\left( x \right) = f_{k}\left( x \right) + g_{k + 1}(x;\Theta_{k + 1})</script><p>其中<script type="math/tex">g_{k + 1}</script>为第<script type="math/tex">k + 1</script>步的基函数（亦即 AdaBoost 中的弱分类器），<script type="math/tex">\Theta_{k + 1}</script>为其参数。当采用平方误差损失函数<script type="math/tex">L\left( y,f\left( x \right) \right) = {\lbrack y - f\left( x \right)\rbrack}^{2}</script>时，可知第<script type="math/tex">k + 1</script>步的损失变为：</p>
<script type="math/tex; mode=display">
L = {\left\lbrack y - f_{k + 1}\left( x \right) \right\rbrack^{2} = \left\lbrack y - f_{k}\left( x \right) - g_{k + 1} \right\rbrack}^{2} = \left\lbrack r_{k}(x) - g_{k + 1}(x) \right\rbrack^{2}</script><p>其中<script type="math/tex">r_{k}(x) = y - f_{k}(x)</script>是第<script type="math/tex">k</script>步模型的残差。</p>
<p>从上式可以看出在第<script type="math/tex">k + 1</script>步时，为了最小化损失<script type="math/tex">L</script>，只需让当前的基函数<script type="math/tex">g_{k + 1}</script>拟合当前模型的残差<script type="math/tex">r_{k}</script>即可，这就完成了 AdaBoost 回归问题的转化。比较具有代表性的是回归问题的提升树算法，它正是利用了以上叙述的转化技巧来进行模型训练的</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们前面提到过 Bagging 的数学基础是 Bootstrap 理论、但还没有讲 Boosting 的数学基础。本篇文章拟打算直观地阐述 Boosting 族的代表算法——AdaBoost 算法的解释，由于具体的推导相当繁琐，相关的细节我们会放在&lt;a href=&quot;/posts/613bbb2f/&quot; title=&quot;相关数学理论&quot;&gt;相关数学理论&lt;/a&gt;里面说明&lt;/p&gt;
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>集成模型的性能分析</title>
    <link href="http://www.carefree0910.com/posts/fb0d2f02/"/>
    <id>http://www.carefree0910.com/posts/fb0d2f02/</id>
    <published>2017-04-25T15:51:10.000Z</published>
    <updated>2017-04-28T10:52:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>正如前文所说，在实现完 AdaBoost 框架后，我们需要先用 sklearn 中的分类器进行检验、然后再用我们前两章实现的模型进行对比实验。检验的步骤就不在这里详述（毕竟只是一些调试的活），我们在此仅展示在随机森林模型和经过检验的 AdaBoost 模型上进行的一系列的分析</p>
<p>直观起见，我们先采用二维的数据进行实验、并通过可视化来加深对随机森林和 AdaBoost 的理解，然后再用蘑菇数据集做比较贴近现实的实验。为讨论方便，我们一律采用决策树作为 AdaBoost 的弱分类器（亦即采用提升树模型进行讨论）、其强度可以通过调整其最深层数来控制。我们可以利用<code>DataUtil</code>类来生成或获取原始数据集，其完整代码可参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py" target="_blank" rel="external">这里</a>、生成数据集的代码则会在前三节分别放出</p>
<p>对于二维数据，我们拟打算使用三种数据集来进行评估：</p>
<ul>
<li>随机数据集。该数据集主要用于直观地感受模型的分类能力</li>
<li>异或数据集。该数据集主要用于直观地理解：<ul>
<li>集成模型正则化的能力</li>
<li>为何说 AdaBoost 不要选用分类能力太强的弱分类器</li>
</ul>
</li>
<li>螺旋线数据集，主要用于直观认知随机森林和提升树的不足</li>
</ul>
<a id="more"></a>
<h1 id="随机数据集上的表现"><a href="#随机数据集上的表现" class="headerlink" title="随机数据集上的表现"></a>随机数据集上的表现</h1><p>生成随机数据集的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_random</span><span class="params">(size=<span class="number">100</span>)</span>:</span></div><div class="line">    xy = np.random.rand(size, <span class="number">2</span>)</div><div class="line">    z = np.random.randint(<span class="number">2</span>, size=size)</div><div class="line">    <span class="comment"># 注意：我们的AdaBoost框架要求类别空间为(-1,+1)</span></div><div class="line">    z[z == <span class="number">0</span>] = <span class="number">-1</span></div><div class="line">    <span class="keyword">return</span> xy, z</div></pre></td></tr></table></figure>
<p>随机森林在随机数据集上的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p1.png" alt="p1.png" title="">
<p>左图为包含 1 棵 CART 树的随机森林，准确率为 78.0%；右图则为包含 10 棵 CART 树的随机森林，准确率为 93.0%。如果将树的数量继续往上抬、达到 100%准确率并非难事。比如，包含 5 0棵 CART 树的随机森林的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p2.png" alt="随机数据集上准确率为 100%的随机森林" title="随机数据集上准确率为 100%的随机森林">
<p>提升树（弱模型为决策树的 AdaBoost）在随机数据集上的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p3.png" alt="p3.png" title="">
<p>左图为包含 1 棵 CART 树的 AdaBoost，准确率为 93.0%；右图则为包含 10 棵 CART 树的 AdaBoost，准确率为 99.0%</p>
<h1 id="异或数据集上的表现"><a href="#异或数据集上的表现" class="headerlink" title="异或数据集上的表现"></a>异或数据集上的表现</h1><p>这里主要是想说明随机森林和提升树正则化的效果。从直观上来说，由于随机森林的理论基础是 Bootstrap、所以自然是包含越多树越好；至于 AdaBoost，可以想象它会对难以分类的数据特别在意、从而导致如下两种可能的结果：</p>
<ul>
<li>太过注重噪声，导致过拟合</li>
<li>专注于类似于下一个系列要讲的 SVM 中的“支持向量”，从而达到正则化</li>
</ul>
<p>事实上正如之前提到过的，即使 AdaBoost 在某一步迭代时、所得的模型在训练集上的加权错误率已经达到了 0，继续进行训练仍然可以使模型进一步提升（因为单个模型的正确率没有那么高、从而能使模型继续专注于“支持向量”。所谓支持向量、可以暂时直观地理解为“非常重要的”样本）。为说明这一点，我们可以比较同一数据集上、同样使用最深层数为 3 层的决策树作为弱分类器时、两种不同训练策略在异或数据集上的表现。为了比较准确地衡量正则化能力，我们需要进行交叉验证。、</p>
<p>生成异或数据集的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_xor</span><span class="params">(size=<span class="number">100</span>)</span>:</span></div><div class="line">    x = np.random.randn(size)</div><div class="line">    y = np.random.randn(size)</div><div class="line">    z = np.ones(size)</div><div class="line">    z[x * y &lt; <span class="number">0</span>] = <span class="number">-1</span></div><div class="line">    <span class="keyword">return</span> np.c_[x, y].astype(np.float32), z</div></pre></td></tr></table></figure>
<p>随机森林在异或数据集上的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p4.png" alt="p4.png" title="">
<p><strong><em>注意：该异或数据集和上一章用到的异或数据集是同一个数据集，感兴趣的读者可以进行一些对比</em></strong></p>
<p>左图为包含 1 棵 CART 树的随机森林，准确率为 93.0%；右图则为包含 10 棵 CART 树的随机森林，准确率为 98.0%。虽说右图中随机森林的表现已经足够好，由前文讨论可知、我们应该尝试训练一个更复杂的随机森林来看看其正则化能力。比如，包含 1000 棵 CART 树的随机森林的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p5.png" alt="p5.png" title="">
<p>仔细观察决策边界，可以发现它会倾向于画在使得样本和边界“间隔较大”的地方。关于“间隔”的详细讨论会放在下一个系列，这里只需直观地感受一下即可</p>
<p>对于提升树，首先看一下不提前停止训练时的表现。为更好地说明问题，这里我们换了一个异或数据集来进行分析：</p>
<img src="/posts/fb0d2f02/p6.png" alt="p6.png" title="">
<p>此时在测试数据集上的正确率为 97.0%。然后看当模型在训练集上错误率足够小就马上停止训练时的表现：</p>
<img src="/posts/fb0d2f02/p7.png" alt="p7.png" title="">
<p>此时在测试数据集上的正确率为 94.0%</p>
<p>当然，正如前面所说，事实上确实有论文（G. Ratsch et al. ML, 2001）给出了 AdaBoost 会很快就过拟合的例子。但总体而言，笔者认为 AdaBoost 在正则化这一方面的表现还是相当优异的</p>
<p>由前面的诸多讨论可以得知，AdaBoost 的正则化能力是来源于各个弱分类器的“分而治之”，那么如果使用分类能力强的弱分类器会有什么结果呢？下面就放出当选用不限制层数的决策树作为弱模型的、异或数据集上的表现，相信会带来很好的直观：</p>
<img src="/posts/fb0d2f02/p8.png" alt="p8.png" title="">
<p>此时在测试数据集上的正确率为 90.0%。值得一提的是，用单独的决策树做出来的效果和上图的效果几乎完全一致。换句话说、此时使用 AdaBoost 没有太大的意义</p>
<h1 id="螺旋数据集上的表现"><a href="#螺旋数据集上的表现" class="headerlink" title="螺旋数据集上的表现"></a>螺旋数据集上的表现</h1><p>随机森林和提升树虽然确实都相当强大、但它同样具有其基本组成单元——决策树所具有的某些缺点。比如说，它们在处理连续性比较强的数据时可能会有些吃力、因为它们的决策边界一般而言都是“不太光滑”的。下面我们就统一使用个体决策树不做层数限制的随机森林和提升树以及螺旋线数据集作为样例来进行说明</p>
<p>生成螺旋数据集的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_spin</span><span class="params">(size=<span class="number">30</span>)</span>:</span></div><div class="line">    xs = np.zeros((size * <span class="number">4</span>, <span class="number">2</span>), dtype=np.float32)</div><div class="line">    ys = np.zeros(size * <span class="number">4</span>, dtype=np.int8)</div><div class="line">    <span class="comment"># 根据螺旋线在极坐标中的公式、生成四条螺旋线</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</div><div class="line">        ix = range(size * i, size * (i + <span class="number">1</span>))</div><div class="line">        <span class="comment"># 去掉原点以避免出现原点同时从属于两类的不合理情况</span></div><div class="line">        r = np.linspace(<span class="number">0.0</span>, <span class="number">1</span>, size + <span class="number">1</span>)[<span class="number">1</span>:]</div><div class="line">        t = np.linspace(<span class="number">2</span> * i * pi / <span class="number">4</span>, <span class="number">2</span> * (i + <span class="number">4</span>) * pi / <span class="number">4</span>, size) + np.random.random(</div><div class="line">            size=size) * <span class="number">0.1</span></div><div class="line">        xs[ix] = np.c_[r * np.sin(t), r * np.cos(t)]</div><div class="line">        ys[ix] = <span class="number">2</span> * (i % <span class="number">2</span>) - <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> xs, ys</div></pre></td></tr></table></figure>
<p>随机森林和提升树在其上的表现分别如下两张图所示：</p>
<img src="/posts/fb0d2f02/p9.png" alt="p9.png" title="">
<img src="/posts/fb0d2f02/p10.png" alt="p10.png" title="">
<p>上面两组图的左边都是包含 10 棵 CART 树的模型、右边都是包含 1000 棵 CART 树的模型，准确率则都是为 100.0%。可以看到，虽然它们都确实能够将大致的趋势给描述出来、但是决策边界相对而言都是“直来直去”的，这一点要比支持向量机、神经网络等模型的训练出来的结果要差不少。总之，决策树那使用二类问题的解决方案来处理连续型特征的做法、导致了随机森林和提升树在处理连续特征上的一些不足</p>
<h1 id="蘑菇数据集上的表现"><a href="#蘑菇数据集上的表现" class="headerlink" title="蘑菇数据集上的表现"></a>蘑菇数据集上的表现</h1><p>目前为止我们对二维数据上的测试做了比较详尽的说明，接下来我们不妨拿蘑菇数据集来测试一下我们的模型在真实数据下的表现；鉴于该数据集比较简单、我们只使用 100 个样本进行训练并用剩余的 8000 多个样本进行测试。为了直观感受模型的分类能力，我们可以画出当个体模型为 CART 决策树桩时、两种集成模型在测试集上的准确率随训练迭代次数变化而变化的曲线：</p>
<img src="/posts/fb0d2f02/p11.png" alt="p11.png" title="">
<p>其中蓝线是随机森林的训练曲线、绿线是提升树的训练曲线。这个结果是符合直观的，毕竟从个体模型来讲，引入了随机性的、随机森林中的决策树桩要比提升树中正常的决策树桩要弱，所以提升树的收敛速度理应比随机森林的要快；此外，由于随机森林和提升树相比、受个体模型分类能力的影响更大、我们采用的又是 CART 决策树桩这种相当弱的个体模型，所以随机森林收敛后的表现也要比提升树收敛后的表现要差</p>
<p>不过需要指出的是，当我们取消个体 CART 决策树的层数限制时，虽然随机森林的收敛速度仍会比提升树的收敛速度慢、但是收敛后的表现却很有可能比提升树收敛后的表现要好。这是因为取消了层数限制的决策树是相当强力的模型，而且：</p>
<ul>
<li>一方面正如刚刚所说的，随机森林受个体模型的分类能力影响较大、所以取消个体树的层数限制后、随机森林的分类能力自然大大增强</li>
<li>另一方面则如之前所讨论的，具有较强分类能力的个体模型与 AdaBoost 的原理可能不太兼容，这就使得 AdaBoost 本身的优势被抑制了</li>
</ul>
<p>取消层数限制后重复上述实验，此时两种集成模型的训练曲线如下图所示：</p>
<img src="/posts/fb0d2f02/p12.png" alt="p12.png" title="">
<p>可能观众老爷们已经发现，取消层数限制后的提升树似乎还没有取消限制之前的提升树的表现好；事实上，由于我们只用了 100 个样本来进行训练，所以容易想象、取消限制后的提升树将会产生比较严重的过拟合。可以把取消层数限制前后的训练曲线放在一起来进行直观对比，结果如下图所示：</p>
<img src="/posts/fb0d2f02/p13.png" alt="p13.png" title="">
<p>其中蓝线是个体模型为 CART 决策树桩时的训练曲线、绿线是个体模型为正常 CART 决策树时的训练曲线</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;正如前文所说，在实现完 AdaBoost 框架后，我们需要先用 sklearn 中的分类器进行检验、然后再用我们前两章实现的模型进行对比实验。检验的步骤就不在这里详述（毕竟只是一些调试的活），我们在此仅展示在随机森林模型和经过检验的 AdaBoost 模型上进行的一系列的分析&lt;/p&gt;
&lt;p&gt;直观起见，我们先采用二维的数据进行实验、并通过可视化来加深对随机森林和 AdaBoost 的理解，然后再用蘑菇数据集做比较贴近现实的实验。为讨论方便，我们一律采用决策树作为 AdaBoost 的弱分类器（亦即采用提升树模型进行讨论）、其强度可以通过调整其最深层数来控制。我们可以利用&lt;code&gt;DataUtil&lt;/code&gt;类来生成或获取原始数据集，其完整代码可参见&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py&quot;&gt;这里&lt;/a&gt;、生成数据集的代码则会在前三节分别放出&lt;/p&gt;
&lt;p&gt;对于二维数据，我们拟打算使用三种数据集来进行评估：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;随机数据集。该数据集主要用于直观地感受模型的分类能力&lt;/li&gt;
&lt;li&gt;异或数据集。该数据集主要用于直观地理解：&lt;ul&gt;
&lt;li&gt;集成模型正则化的能力&lt;/li&gt;
&lt;li&gt;为何说 AdaBoost 不要选用分类能力太强的弱分类器&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;螺旋线数据集，主要用于直观认知随机森林和提升树的不足&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>AdaBoost 算法</title>
    <link href="http://www.carefree0910.com/posts/f5f50863/"/>
    <id>http://www.carefree0910.com/posts/f5f50863/</id>
    <published>2017-04-25T15:31:03.000Z</published>
    <updated>2017-04-28T10:51:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>由前文的讨论可知，问题的关键主要在如下两点：</p>
<ul>
<li>如何根据弱模型的表现更新训练集的权重</li>
<li>如何根据弱模型的表现决定弱模型的话语权</li>
</ul>
<p>我们接下来就看看 AdaBoost 算法是怎样解决上述两个问题的。事实上，能够将这两个问题的解决方案有机地糅合在一起、正是 AdaBoost 的巧妙之处之一</p>
<a id="more"></a>
<h1 id="AdaBoost-算法陈述"><a href="#AdaBoost-算法陈述" class="headerlink" title="AdaBoost 算法陈述"></a>AdaBoost 算法陈述</h1><p>不失一般性、我们以二类分类问题来进行讨论，易知此时我们的弱模型、强模型和最终模型为弱分类器、强分类器和最终分类器。再不妨假设我们现在有的是一个二类分类的训练数据集：</p>
<script type="math/tex; mode=display">
D = \{\left( x_{1},y_{1} \right),\left( x_{2},y_{2} \right),\ldots,(x_{n},\ y_{n})\}</script><p>其中，每个样本点都是由实例<script type="math/tex">x_{i}</script>和类别<script type="math/tex">y_{i}</script>组成、且：</p>
<script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script><p>这里的<script type="math/tex">X</script>是样本空间、<script type="math/tex">Y</script>是类别空间。AdaBoost 会利用如下的步骤、从训练数据中训练出一系列的弱分类器、然后把这些弱分类器集成为一个强分类器：</p>
<ol>
<li><strong>输入</strong>：训练数据集（包含 N 个数据）、弱学习算法及对应的弱分类器、迭代次数 M</li>
<li><strong>过程</strong>：<ol>
<li>初始化训练数据的权值分布  <script type="math/tex; mode=display">
W_{0} = (w_{01},\ldots,w_{0N})</script></li>
<li>对<script type="math/tex">k = 0,1,\ldots,\ M - 1</script>：<ol>
<li>使用权值分布为<script type="math/tex">W_{k}</script>的训练数据集训练弱分类器  <script type="math/tex; mode=display">g_{k + 1}(x)$$：$$X \rightarrow \{ - 1,\  + 1\}</script></li>
<li>计算<script type="math/tex">g_{k + 1}(x)</script>在训练数据集上的加权错误率  <script type="math/tex; mode=display">
e_{k + 1} = \sum_{i = 1}^{N}{w_{\text{ki}}I(g_{k + 1}\left( x_{i} \right) \neq y_{i})}</script></li>
<li>根据加权错误率计算<script type="math/tex">g_{k + 1}(x)</script>的“话语权”  <script type="math/tex; mode=display">
\alpha_{k + 1} = \frac{1}{2}\ln\frac{1 - e_{k + 1}}{e_{k + 1}}</script></li>
<li>根据<script type="math/tex">g_{k + 1}(x)</script>的表现更新训练数据集的权值分布：被<script type="math/tex">g_{k + 1}\left( x \right)</script>误分的样本（<script type="math/tex">y_{i}g_{k + 1}\left( x_{i} \right) < 0</script>的样本）要相对地（以<script type="math/tex">e^{\alpha_{k + 1}}</script>为比例地）增大其权重，反之则要（以<script type="math/tex">e^{- \alpha_{k + 1}}</script>为比例地）减少其权重  <script type="math/tex; mode=display">
w_{k + 1,i} = \frac{w_{\text{ki}}}{Z_{k}} \cdot exp( - \alpha_{k + 1}y_{i}g_{k + 1}(x_{i}))</script><script type="math/tex; mode=display">
W_{k + 1} = (w_{k + 1,1},\ldots,w_{k + 1,N})</script>这里的<script type="math/tex">Z_{k}</script>是规范化因子  <script type="math/tex; mode=display">
Z_{k} = \sum_{i = 1}^{N}{w_{\text{ki}} \cdot exp( - \alpha_{k + 1}y_{i}g_{k + 1}(x_{i}))}</script>它的作用是将<script type="math/tex">W_{k + 1}</script>归一化成为一个概率分布</li>
</ol>
</li>
<li>加权集成弱分类器  <script type="math/tex; mode=display">
f\left( x \right) = \sum_{k = 1}^{M}{\alpha_{k}g_{k}(x)}</script></li>
</ol>
</li>
<li><strong>输出</strong>：最终分类器<script type="math/tex">g(x)</script>  <script type="math/tex; mode=display">
g\left( x \right) = sign\left( f\left( x \right) \right) = \text{sign}\left( \sum_{k = 1}^{M}{\alpha_{k}g_{k}\left( x \right)} \right)</script></li>
</ol>
<p><strong><em>注意：2.2.2 步骤得到的加权错误率如果足够小的话，可以考虑提前停止训练，但这样做往往不是最合理的选择（这点会在后文进行模型性能分析时进行较详细的说明）</em></strong></p>
<p>我们在分配弱分类器的话语权时用到了一个公式：<script type="math/tex">\alpha_{k + 1} = \frac{1}{2}\ln\frac{1 - e_{k + 1}}{e_{k + 1}}</script>。在该公式中，话语权<script type="math/tex">\alpha_{k + 1}</script>会随着加权错误率<script type="math/tex">e_{k + 1} \in \lbrack 0,\ 1\rbrack</script>的增大而减小。它们之间的函数关系如下图所示：</p>
<img src="/posts/f5f50863/p1.png" alt="p1.png" title="">
<p>大多数情况我们训练出来的弱分类器的<script type="math/tex">e_{k} < 0.5</script>、对应着的是上图左半边的部分；不过即使我们的弱分类器非常差、以至于<script type="math/tex">e_{k} > 0.5</script>，由于此时<script type="math/tex">\alpha_{k} < 0</script>、亦即我们知道该分类器的表决应该反着来看、所以也不会出问题（有一种做法是如果训练到<script type="math/tex">e_{k} > 0.5</script>的话就停止训练，个人感觉也有道理）</p>
<h1 id="弱模型的选择"><a href="#弱模型的选择" class="headerlink" title="弱模型的选择"></a>弱模型的选择</h1><p>看到这里，观众老爷们可能会产生这么一个疑问：如果我们不拘泥于对弱模型进行提升、转而对强模型或比较强的弱模型进行提升的话，会不会提升出更好的模型呢？从 Boosting 的思想来看、需要指出的是：用 Boosting 进行提升的弱模型的学习能力不宜太强，否则使用 Boosting 就没有太大的意义、甚至从原理上不太兼容。直观地说，Boosting 是为了让各个弱模型专注于“某一方面”、最后加权表决，如果使用了较强的弱模型，可能一个弱模型就包揽了好几方面，最后可能反而会模棱两可、起不到“提升”的效果。而且从迭代的角度来说，可以想象：如果使用较强弱模型的话，可能第一次产生的模型就已经达到“最优”、从而使得模型没有“提升空间”</p>
<p><strong><em>注意：虽然笔者认为在 Boosting 中的弱模型就应该选择足够弱的模型，但确实亦有对强模型（如核 SVM）应用 Boosting 也很好的说法。详细而严谨的讨论会牵扯大量的数学理论、这里就不详细展开了</em></strong></p>
<p>可能观众老爷们此时又会产生一个新的疑问：如果说 Boosting 中的弱模型不宜太强的话，是不是说 Bagging 中的个体模型也不宜太强呢？需要指出的是，虽然从理论上来说使用弱模型进行集成就已足以获得一个相当不错的最终模型，但使用较强的模型来进行集成从原理上是不太矛盾的。考虑到不同的场合，有时确实可以选用较强的模型来作为个体模型</p>
<p>那么所谓的不太强的弱模型大概是个什么东西呢？一个比较直观的例子就是限制层数的决策树。极端的情况就是限定它只能有一层、亦即上一章我们提到过的“决策树桩”，对应的进行了提升后的模型就是相当有名的提升树（Boosting Tree），它被认为是统计学习中性能最好的方法之一、既可以用来做分类也可以拿来做回归，是个相当强力的模型</p>
<h1 id="AdaBoost-的实现"><a href="#AdaBoost-的实现" class="headerlink" title="AdaBoost 的实现"></a>AdaBoost 的实现</h1><p>从之前的算法讲解其实可以看出，虽然 AdaBoost 算法本身很不平凡，但它给出的步骤都是相当便于实现的，基本上一个步骤就对应着 Python 里面的一行代码。在实现的过程中，困难之处可能主要在于如何让实现出来的 AdaBoost 框架易于扩展并具有方便调用的接口，而不在于实现算法本身。同时，为了能够更好地理解 AdaBoost 算法，我们需要对其性能作一系列的分析</p>
<p>由于 AdaBoost 是一个用于提升弱模型的算法，所以我们整体的实现思路大致是（不失一般性、我们先讨论二类分类问题）：</p>
<ul>
<li>搭建 AdaBoost 框架</li>
<li>使用 sklearn 中的分类器对框架的正确性进行检验</li>
<li>使用前两章实现的分类器进行对比实验</li>
</ul>
<p>所以我们要先把 AdaBoost 框架实现出来。为此，先来看 AdaBoost 框架的初始化步骤（其中<a href="https://github.com/carefree0910/MachineLearning/tree/master/_SKlearn" target="_blank" rel="external">_SKlearn</a>是我对 sklearn 中的模型做了一定程度的拓展后的模型包）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</div><div class="line"><span class="comment"># 导入我们之前实现的朴素贝叶斯模型和决策树模型</span></div><div class="line"><span class="keyword">from</span> b_NaiveBayes.Vectorized.MultinomialNB <span class="keyword">import</span> MultinomialNB</div><div class="line"><span class="keyword">from</span> b_NaiveBayes.Vectorized.GaussianNB <span class="keyword">import</span> GaussianNB</div><div class="line"><span class="keyword">from</span> c_CvDTree.Tree <span class="keyword">import</span> *</div><div class="line"><span class="keyword">from</span> _SKlearn.NaiveBayes <span class="keyword">import</span> *</div><div class="line"><span class="keyword">from</span> _SKlearn.Tree <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdaBoost</span>:</span></div><div class="line">    <span class="comment"># 弱分类器字典，如果想要测试新的弱分类器的话、只需将其加入该字典即可</span></div><div class="line">    _weak_clf = &#123;</div><div class="line">        <span class="string">"SKMNB"</span>: SKMultinomialNB,</div><div class="line">        <span class="string">"SKGNB"</span>: SKGaussianNB,</div><div class="line">        <span class="string">"SKTree"</span>: SKTree,</div><div class="line"></div><div class="line">        <span class="string">"MNB"</span>: MultinomialNB,</div><div class="line">        <span class="string">"GNB"</span>: GaussianNB,</div><div class="line">        <span class="string">"ID3"</span>: ID3Tree,</div><div class="line">        <span class="string">"C45"</span>: C45Tree,</div><div class="line">        <span class="string">"Cart"</span>: CartTree</div><div class="line">    &#125;</div><div class="line">    <span class="string">"""</span></div><div class="line">        AdaBoost框架的朴素实现</div><div class="line">        使用的弱分类器需要有如下两个方法：</div><div class="line">            1) 'fit'      方法，它需要支持输入样本权重</div><div class="line">            2) 'predict'  方法, 它用于返回预测的类别向量</div><div class="line">        初始化结构</div><div class="line">        self._clf：记录弱分类器名称的变量</div><div class="line">        self._clfs：记录弱分类器的列表</div><div class="line">        self._clfs_weights：记录弱分类器“话语权”的列表</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self._clf, self._clfs, self._clfs_weights = <span class="string">""</span>, [], []</div></pre></td></tr></table></figure>
<p>接下来就是训练和预测部分的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight=None, clf=None, epoch=<span class="number">10</span>, eps=<span class="number">1e-12</span>, **kwargs)</span>:</span></div><div class="line">    <span class="comment"># 默认使用10个CART决策树桩作为弱分类器</span></div><div class="line">    <span class="keyword">if</span> clf <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> AdaBoost._weak_clf[clf] <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        clf = <span class="string">"Cart"</span></div><div class="line">        kwargs = &#123;<span class="string">"max_depth"</span>: <span class="number">1</span>&#125;</div><div class="line">    self._clf = clf</div><div class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        sample_weight = np.ones(len(y)) / len(y)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        sample_weight = np.array(sample_weight)</div><div class="line">    <span class="comment"># AdaBoost算法的主循环，epoch为迭代次数</span></div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</div><div class="line">        <span class="comment"># 根据样本权重训练弱分类器</span></div><div class="line">        tmp_clf = AdaBoost._weak_clf[clf](**kwargs)</div><div class="line">        tmp_clf.fit(x, y, sample_weight)</div><div class="line">        <span class="comment"># 调用弱分类器的predict方法进行预测</span></div><div class="line">        y_pred = tmp_clf.predict(x)</div><div class="line">        <span class="comment"># 计算加权错误率；考虑到数值稳定性，在边值情况加了一个小的常数</span></div><div class="line">        em = min(max((y_pred != y).dot(self._sample_weight[:, <span class="keyword">None</span>])[<span class="number">0</span>], eps), <span class="number">1</span> - eps)</div><div class="line">        <span class="comment"># 计算该弱分类器的“话语权”</span></div><div class="line">        am = <span class="number">0.5</span> * log(<span class="number">1</span> / em - <span class="number">1</span>)</div><div class="line">        <span class="comment"># 更新样本权重并利用deepcopy将该弱分类器记录在列表中</span></div><div class="line">        sample_weight *= np.exp(-am * y * y_pred)</div><div class="line">        sample_weight /= np.sum(sample_weight)</div><div class="line">        self._clfs.append(deepcopy(tmp_clf))</div><div class="line">        self._clfs_weights.append(am)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></div><div class="line">    x = np.atleast_2d(x)</div><div class="line">    rs = np.zeros(len(x))</div><div class="line">    <span class="comment"># 根据各个弱分类器的“话语权”进行决策</span></div><div class="line">    <span class="keyword">for</span> clf, am <span class="keyword">in</span> zip(self._clfs, self._clfs_weights):</div><div class="line">        rs += am * clf.predict(x)</div><div class="line">    <span class="comment"># 将预测值大于0的判为类别1，小于0的判为类别-1</span></div><div class="line">    <span class="keyword">return</span> np.sign(rs)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由前文的讨论可知，问题的关键主要在如下两点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何根据弱模型的表现更新训练集的权重&lt;/li&gt;
&lt;li&gt;如何根据弱模型的表现决定弱模型的话语权&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们接下来就看看 AdaBoost 算法是怎样解决上述两个问题的。事实上，能够将这两个问题的解决方案有机地糅合在一起、正是 AdaBoost 的巧妙之处之一&lt;/p&gt;
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>随机森林算法</title>
    <link href="http://www.carefree0910.com/posts/c0a9c025/"/>
    <id>http://www.carefree0910.com/posts/c0a9c025/</id>
    <published>2017-04-25T12:42:41.000Z</published>
    <updated>2017-04-25T16:45:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>由前文讨论可知，我们在实现 RF 算法之前，需要先在决策树模型的生成过程中加一个参数、使得我们能够对特征选取加入随机性。这个过程相当平凡，下给出代码片段以进行粗略的说明。首先在<code>CvDBase</code>的<code>fit</code>方法中加入一个参数<code>feature_bound</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight=None, alpha=None, eps=<span class="number">1e-8</span>,</span></span></div><div class="line">    cv_rate=<span class="number">0.2</span>, train_only=False, feature_bound=None):</div></pre></td></tr></table></figure>
<p>然后在同一个方法里面、把这个参数传给<code>CvDNode</code>的<code>fit</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.root.fit(x_train, y_train, _train_weights, feature_bound, eps)</div></pre></td></tr></table></figure>
<p>在<code>CvDNode</code>的<code>fit</code>方法中，原始代码中有一个对可选特征空间的遍历：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> self.feats:</div></pre></td></tr></table></figure>
<p>根据参数<code>feature_bound</code>对它加入随机性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">feat_len = len(self.feats)</div><div class="line"><span class="comment"># 默认没有随机性</span></div><div class="line"><span class="keyword">if</span> feature_bound <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">    indices = range(<span class="number">0</span>, feat_len)</div><div class="line"><span class="keyword">elif</span> feature_bound == <span class="string">"log"</span>:</div><div class="line">    <span class="comment"># np.random.permutation(n)：将数组打乱后返回</span></div><div class="line">    indices = np.random.permutation(feat_len)[:max(<span class="number">1</span>, int(log2(feat_len)))]</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    indices = np.random.permutation(feat_len)[:feature_bound]</div><div class="line">tmp_feats = [self.feats[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices]</div><div class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> tmp_feats:</div></pre></td></tr></table></figure>
<p>然后要在同一个方法里面、把<code>feature_bound</code>传给<code>_gen_children</code>方法，而在<code>_gen_children</code>中、再把<code>feature_bound</code>传给子节点的<code>fit</code>方法即可</p>
<p>以上所有实现细节可参见<a href="https://github.com/carefree0910/MachineLearning/tree/master/c_CvDTree" target="_blank" rel="external">这里</a>中的 Tree.py 和 Node.py</p>
<p>有了这些准备，我们就可以来看看 RF 的算法陈述了（以分类问题为例）：</p>
<a id="more"></a>
<ol>
<li><strong>输入</strong>：训练数据集（包含 N 个数据）、决策树模型、迭代次数 M</li>
<li><strong>过程</strong>：<ol>
<li>对<script type="math/tex">j=1,2,...,M</script>：<ol>
<li>通过 Bootstrap 生成包含 N 个数据的数据集<script type="math/tex">D_k</script></li>
<li>利用<script type="math/tex">D_j</script>和输入的决策树模型进行训练，注意不用对训练好的决策树模型<script type="math/tex">g_j</script>进行剪枝。同时需要注意的是，在训练决策树的过程中、每一步的生成都要对特征的选取加入随机性</li>
</ol>
</li>
<li>对个体决策树进行简单组合。不妨用符号<script type="math/tex">\text{freq}(c_{k})</script>表示类别<script type="math/tex">c_{k}</script>在 M 个决策树模型的决策中出现的频率，那么：  <script type="math/tex; mode=display">
g\left( x \right) = \arg{\max_{c_k}{\text{freq}(c_{k})}}</script></li>
</ol>
</li>
<li><strong>输出</strong>：最终分类器<script type="math/tex">g(x)</script></li>
</ol>
<p>从算法即可看出随机森林算法的实现（在实现好决策树模型后）是相当平凡的，需要额外做的工作只有定义一个能够计算上述算法第 2.2 步中<script type="math/tex">\arg{\max_{c_k}{freq(c_{k})}}</script>的函数而已：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入我们自己实现的决策树模型</span></div><div class="line"><span class="keyword">from</span> c_CvDTree.Tree <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomForest</span><span class="params">(ClassifierBase)</span>:</span></div><div class="line">    <span class="comment"># 建立一个决策树字典，以便调用</span></div><div class="line">    _cvd_trees = &#123;</div><div class="line">        <span class="string">"id3"</span>: ID3Tree,</div><div class="line">        <span class="string">"c45"</span>: C45Tree,</div><div class="line">        <span class="string">"cart"</span>: CartTree</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(RandomForest, self).__init__()</div><div class="line">        self._trees = []</div><div class="line"></div><div class="line">    <span class="comment"># 实现计算的函数</span></div><div class="line"><span class="meta">    @staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">most_appearance</span><span class="params">(arr)</span>:</span></div><div class="line">        u, c = np.unique(arr, return_counts=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">return</span> u[np.argmax(c)]</div><div class="line"></div><div class="line">    <span class="comment"># 默认使用 10 棵 CART 树、默认 k = log(d)</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight=None, tree=<span class="string">"cart"</span>, epoch=<span class="number">10</span>, feature_bound=<span class="string">"log"</span>,</span></span></div><div class="line">            *args, **kwargs):</div><div class="line">        x, y = np.atleast_2d(x), np.array(y)</div><div class="line">        n_sample = len(y)</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</div><div class="line">            tmp_tree = RandomForest._cvd_trees[tree](*args, **kwargs)</div><div class="line">            _indices = np.random.randint(n_sample, size=n_sample)</div><div class="line">            <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">                _local_weight = <span class="keyword">None</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                _local_weight = sample_weight[_indices]</div><div class="line">                _local_weight /= _local_weight.sum()</div><div class="line">            tmp_tree.fit(x[_indices], y[_indices],</div><div class="line">                sample_weight=_local_weight, feature_bound=feature_bound)</div><div class="line">            self._trees.append(deepcopy(tmp_tree))</div><div class="line"></div><div class="line">    <span class="comment"># 对个体决策树进行简单组合</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></div><div class="line">        _matrix = np.array([_tree.predict(x) <span class="keyword">for</span> _tree <span class="keyword">in</span> self._trees]).T</div><div class="line">        <span class="keyword">return</span> np.array([RandomForest.most_appearance(rs) <span class="keyword">for</span> rs <span class="keyword">in</span> _matrix])</div></pre></td></tr></table></figure>
<p>需要指出的是，<code>most_appearance</code>函数用到了 Numpy 中的<code>unique</code>方法、它和标准库<code>collections</code>中的<code>Counter</code>具有差不多的用法。举个小栗子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = np.array([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">"dcbabcd"</span>])</div><div class="line">np.unique(x, return_counts=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p>这两行代码会返回：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">(</div><div class="line">    array([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>], dtype=<span class="string">'&lt;U1'</span>),</div><div class="line">    array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], dtype=int64)</div><div class="line">)</div></pre></td></tr></table></figure>
<p>换句话说，<code>unique</code>方法能够提取出一个 Numpy 数组中出现过的元素并对它们计数、同时输出的 Numpy 数组是经过排序的</p>
<p>以上就完成了一个简易可行的随机森林模型的实现，我们可以把对随机森林模型的评估与对 AdaBoost 的评估放在一起进行以便于对比、这里就先按下不表</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由前文讨论可知，我们在实现 RF 算法之前，需要先在决策树模型的生成过程中加一个参数、使得我们能够对特征选取加入随机性。这个过程相当平凡，下给出代码片段以进行粗略的说明。首先在&lt;code&gt;CvDBase&lt;/code&gt;的&lt;code&gt;fit&lt;/code&gt;方法中加入一个参数&lt;code&gt;feature_bound&lt;/code&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, x, y, sample_weight=None, alpha=None, eps=&lt;span class=&quot;number&quot;&gt;1e-8&lt;/span&gt;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    cv_rate=&lt;span class=&quot;number&quot;&gt;0.2&lt;/span&gt;, train_only=False, feature_bound=None)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;然后在同一个方法里面、把这个参数传给&lt;code&gt;CvDNode&lt;/code&gt;的&lt;code&gt;fit&lt;/code&gt;方法：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;self.root.fit(x_train, y_train, _train_weights, feature_bound, eps)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;在&lt;code&gt;CvDNode&lt;/code&gt;的&lt;code&gt;fit&lt;/code&gt;方法中，原始代码中有一个对可选特征空间的遍历：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; feat &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; self.feats:&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;根据参数&lt;code&gt;feature_bound&lt;/code&gt;对它加入随机性：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;feat_len = len(self.feats)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 默认没有随机性&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; feature_bound &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    indices = range(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, feat_len)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;elif&lt;/span&gt; feature_bound == &lt;span class=&quot;string&quot;&gt;&quot;log&quot;&lt;/span&gt;:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# np.random.permutation(n)：将数组打乱后返回&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    indices = np.random.permutation(feat_len)[:max(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, int(log2(feat_len)))]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    indices = np.random.permutation(feat_len)[:feature_bound]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;tmp_feats = [self.feats[i] &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; indices]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; feat &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; tmp_feats:&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;然后要在同一个方法里面、把&lt;code&gt;feature_bound&lt;/code&gt;传给&lt;code&gt;_gen_children&lt;/code&gt;方法，而在&lt;code&gt;_gen_children&lt;/code&gt;中、再把&lt;code&gt;feature_bound&lt;/code&gt;传给子节点的&lt;code&gt;fit&lt;/code&gt;方法即可&lt;/p&gt;
&lt;p&gt;以上所有实现细节可参见&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/tree/master/c_CvDTree&quot;&gt;这里&lt;/a&gt;中的 Tree.py 和 Node.py&lt;/p&gt;
&lt;p&gt;有了这些准备，我们就可以来看看 RF 的算法陈述了（以分类问题为例）：&lt;/p&gt;
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>“集成”的思想</title>
    <link href="http://www.carefree0910.com/posts/7081b0ee/"/>
    <id>http://www.carefree0910.com/posts/7081b0ee/</id>
    <published>2017-04-25T11:51:31.000Z</published>
    <updated>2017-04-28T10:51:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文首先会介绍何谓“集成”、然后会介绍两种常见的集成学习方法：Bagging、AdaBoost 的基本定义。这些概念的背后有着深刻的数学理论，但是它们同时也拥有着很好的直观。获得对它们的直观有助于加深对各种模型的分类性能的理解、同时也有助于根据具体的数据集来挑选相应的、合适的模型来进行学习</p>
<a id="more"></a>
<h1 id="众擎易举"><a href="#众擎易举" class="headerlink" title="众擎易举"></a>众擎易举</h1><p>集成学习基于这样的思想：对于比较复杂的任务，综合许多人的意见来进行决策会比“一家独大”要更好。换句话说、就是通过适当的方式集成许多“个体模型”所得到的最终模型要比单独的“个体模型”的性能更优。我们可以通过下图来直观感知这个过程：</p>
<img src="/posts/7081b0ee/p1.png" alt="p1.png" title="">
<p>所以问题的关键转化为了两点：如何选择、生成弱分类器和如何对它们进行提升（集成）。在此基础上，通常有三种不同的思路：</p>
<ul>
<li>将不同类型的弱分类器进行提升</li>
<li>将相同类型但参数不同的弱分类器进行提升</li>
<li>将相同类型但训练集不同的弱分类器进行提升</li>
</ul>
<p>其中第一种思路的应用相对来说可能不太广泛，而第二、第三种思路则指导着两种常见的做法，这两种做法的区别主要体现在基本组成单元——弱分类器的生成方式：</p>
<p>第一种做法期望各个弱分类器之间依赖性不强、可以同时进行生成。这种做法又称并行方法，其代表为 Bagging，而 Bagging 一个著名的拓展应用便是本系列的主题之一——随机森林（Random Forest，常简称为 RF）。</p>
<p>第二种做法中弱分类器之间具有强依赖性、只能序列生成。这种做法又称串行方法，其代表为 Boosting，而 Boosting 族算法中的代表即是本系列的另一主题——AdaBoost</p>
<h1 id="Bagging-与随机森林"><a href="#Bagging-与随机森林" class="headerlink" title="Bagging 与随机森林"></a>Bagging 与随机森林</h1><p>Bagging 是 1996 年由 Breiman 提出的，它的思想根源是数理统计中非常重要的 Bootstrap 理论。Bootstrap 可以翻译成“自举”，它通过模拟的方法来逼近样本的概率分布函数。可以想象这样一个场景：现在有一个包含 N 个样本的数据集<script type="math/tex">X = \{ x_{1},\ldots,x_{N}\}</script>，这 N 个样本是由随机变量<script type="math/tex">x</script>独立生成的。我们想要研究<script type="math/tex">x</script>的均值估计<script type="math/tex">\bar{x} = \frac{1}{N}\sum_{i = 1}^{N}x_{i}</script>的统计特性（误差、方差等等），但由于研究统计特性是需要大量样本的、而数据集<script type="math/tex">X</script>只能给我们提供一个<script type="math/tex">\bar{x}</script>的样本，从而导致无法进行研究</p>
<p>在这种场景下，容易想到的一种解决方案是：通过<script type="math/tex">x</script>的分布生成出更多的数据集<script type="math/tex">X_{1},\ldots X_{M}</script>、每个数据集都包含 N 个样本。这 M 个数据集都能产生一个均值估计、从而就有了 M 个均值估计的样本。那么只要 M 足够大、我们就能研究<script type="math/tex">\bar{x}</script>的统计特性了</p>
<p>当然这种解决方案的一个最大的困难就是：我们并不知道<script type="math/tex">x</script>的真实分布。Bootstrap 就是针对这个困难提出了一个解决办法：通过不断地“自采样”来模拟随机变量真实分布生成的数据集。具体而言，Bootstrap 的做法是：</p>
<ul>
<li>从<script type="math/tex">X</script>中随机抽出一个样本（亦即抽出<script type="math/tex">x_1,...,x_N</script>的概率相同）</li>
<li>将该样本的拷贝放入数据集<script type="math/tex">X_j</script></li>
<li>将该样本放回<script type="math/tex">X</script>中</li>
</ul>
<p>以上三个步骤将重复 N 次、从而使得<script type="math/tex">X_{j}</script>中有 N 个样本。这个过程将对<script type="math/tex">j = 1,\ldots,M</script>都进行一遍、从而我们最终能得到 M 个含有 N 个样本的数据集<script type="math/tex">X_{1},\ldots X_{M}</script></p>
<p>简单来说的话、Bootstrap 其实就是一个有放回的随机抽样过程，所以原始数据<script type="math/tex">\{ x_{1},\ldots,x_{N}\}</script>中可能会在<script type="math/tex">X_{1},\ldots X_{M}</script>中重复出现、也有可能不出现在<script type="math/tex">X_{1},\ldots X_{M}</script>中。事实上，由于<script type="math/tex">X</script>中一个样本在 N 次采样中始终不被采到的概率为<script type="math/tex">\left( 1 - \frac{1}{N} \right)^{N}</script>、且：</p>
<script type="math/tex; mode=display">
\lim_{N\rightarrow\infty}\left( 1 - \frac{1}{N} \right)^{N} \rightarrow \frac{1}{e} \approx 0.368</script><p>所以在统计意义上可以认为、<script type="math/tex">X_{j}</script>中含有<script type="math/tex">X</script>中 63.2%的样本<script type="math/tex">\left( \forall j = 1,\ldots,M \right)</script></p>
<p>这种模拟的方法在理论上是具有最优性的。事实上、这种模拟的本质和经验分布函数对真实分布函数的模拟几乎一致：</p>
<ul>
<li>Bootstrap 以<script type="math/tex">\frac 1N</script>的概率、有放回地从<script type="math/tex">X</script>中抽取 N 个样本作为数据集、并以之估计真实分布生成的具有 N 个样本的数据集</li>
<li>经验分布函数则是在 N 个样本点上以每点的概率为<script type="math/tex">\frac 1N</script>作为概率密度函数、然后进行积分的函数</li>
</ul>
<p>经验分布函数的数学表达式为：</p>
<script type="math/tex; mode=display">
F_{N}\left( x \right) = \frac{1}{N}\sum_{i = 1}^{N}{I_{\left( - \infty,x \right\rbrack}(x_{i})}</script><p>可以看出、经验分布函数用到了频率估计概率的思想。用它来模拟真实分布函数是具有很好的优良性的，详细的讨论可参见<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a></p>
<p>知道 Bootstrap 是什么之后、我们就可以来看 Bagging 的具体定义了。Bagging 的全称是 Bootstrap Aggregating，其思想非常简单：</p>
<ul>
<li>用 Bootstrap 生成出 M 个数据集</li>
<li>用这 M 个数据集训练出 M 个弱分类器</li>
<li>最终模型即为这M个弱分类器的简单组合</li>
</ul>
<p>所谓简单组合就是：</p>
<ul>
<li>对于分类问题使用简单的投票表决</li>
<li>对于回归问题则进行简单的取平均</li>
</ul>
<p>简单组合虽说简单、其背后仍然是有数学理论支撑的。考虑二分类问题：</p>
<script type="math/tex; mode=display">
y \in \{ - 1, + 1\}</script><p>假设样本空间到类别空间的真实映射为<script type="math/tex">f</script>、我们得到的 M 个弱分类器模型<script type="math/tex">G_{1},\ldots,G_{M}</script>所对应的映射为<script type="math/tex">g_{1},\ldots,g_{M}</script>，那么简单组合下的最终模型对应的映射即为：</p>
<script type="math/tex; mode=display">
g\left( x \right) = \text{sign}\left( \sum_{j = 1}^{M}{g_{j}\left( x \right)} \right)</script><p>这里的 sign 是符号函数、满足：</p>
<script type="math/tex; mode=display">
\text{sign}\left( x \right) = \left\{ \begin{matrix}
 - 1,\ \ if\ x < 0 \\
 + 1,\ \ if\ x > 0 \\
\end{matrix} \right.\</script><p>其中，<script type="math/tex">\text{sign}(0)</script>则可为<script type="math/tex">-1</script>也可为<script type="math/tex">+1</script>，令其有 50%的概率输出<script type="math/tex">-</script>、<script type="math/tex">+1</script>也是可行的</p>
<p>如果我们此时假设每个弱分类器的错误率为<script type="math/tex">\epsilon</script>：</p>
<script type="math/tex; mode=display">
p\left( g_{i}\left( x \right) \neq f\left( x \right) \right) = \epsilon</script><p>如果我们假设弱分类器的错误率相互独立，那么由霍夫丁不等式（Hoeffding’s Inequality）可以得知：</p>
<script type="math/tex; mode=display">
p\left( G\left( x \right) \neq f\left( x \right) \right) = \sum_{j = 0}^{\left\lfloor \frac{M}{2} \right\rfloor}\left( \frac{M}{j} \right)\left( 1 - \epsilon \right)^{j}\epsilon^{M - j} \leq \exp\left( - \frac{1}{2}M\left( 1 - 2\epsilon \right)^{2} \right)</script><p>亦即最终模型的错误率随弱分类器的个数 M 的增加、将会以指数级下降并最终趋于 0</p>
<p>虽说这个结果看上去很振奋人心，但需要注意的是、我们做了一个非常强的关键假设：假设弱分类器的错误率相互独立。这可以说是不可能做到的，因为这些弱分类器想要解决的都是同一个问题、且使用的训练集也都源自于同一份数据集</p>
<p>但不管怎么说，以上的分析给了我们这样一个重要信息：弱分类器之间的“差异”似乎应该尽可能的大。基于此，结合 Bagging 的特点、我们可以得出这样一个结论：对于“不稳定”（或说对训练集敏感：若训练样本稍有改变，得到的从样本空间到类别空间的映射 g 就会产生较大的变化）的分类器，Bagging 能够显著地对其进行提升。这也是被大量实验结果所证实了的</p>
<p>正如前文提过的，Bagging 有一个著名的拓展应用叫“随机森林”，从名字就容易想到、它是当个体模型为决策树时的 Bagging 算法。不过需要指出的是，随机森林算法不仅对样本进行 Bootstrap 采样，对每个 Node 调用生成算法时都会随机挑选出一个可选特征空间的子空间作为该决策树的可选特征空间；同时，生成好个体决策树后不进行剪枝、而是保持原始的形式。换句话说、随机森林算法流程大致如下：</p>
<ul>
<li>用 Bootstrap 生成出 M 个数据集</li>
<li>用这 M 个数据集训练出 M 颗不进行后剪枝决策树，且在每颗决策树的生成过程中，每次对 Node 进行划分时、都从可选特征（比如说有 d 个）中随机挑选出 k 个（<script type="math/tex">k\le d</script>）特征，然后依信息增益的定义从这 k 个特征中选出信息增益最大的特征作为划分标准</li>
<li>最终模型即为这 M 个弱分类器的简单组合</li>
</ul>
<p><strong><em>注意：有一种说法是随机森林中的个体决策树模型只能使用 CART 树</em></strong></p>
<p>也就是说，除了和一般 Bagging 算法那样对样本进行随机采样以外、随机森林还对特征进行了某种意义上的随机采样。这样做的意义是直观的：通过对特征引入随机扰动，可以使个体模型之间的差异进一步增加、从而提升最终模型的泛化能力。而这个特征选取的随机性，恰恰被上述算法第二步中的参数k所控制：</p>
<ul>
<li>若<script type="math/tex">k=d</script>，那么训练出来的决策树和一般意义下的决策树别无二致、亦即特征选取这一部分不具有随机性</li>
<li>若<script type="math/tex">k=1</script>，那么生成决策树的每一步都是在随机选择属性、亦即特征选取的随机性达到最大</li>
</ul>
<p>Breiman 在提出随机森林算法的同时指出，一般情况下、推荐取<script type="math/tex">k=\log_2{d}</script></p>
<h1 id="PAC-框架与-Boosting"><a href="#PAC-框架与-Boosting" class="headerlink" title="PAC 框架与 Boosting"></a>PAC 框架与 Boosting</h1><p>虽然同属集成学习方法，但 Boosting 和 Bagging 的数学理论根基不尽相同：Boosting 产生于计算学习理论（Computational Learning Theory）[Valiant, 1984]。一般而言，如果只是应用机器学习的话、我们无需对它进行太多的了解（甚至可以说对它一知半解反而有害），所以本节只打算对其最基本的概率近似正确（PAC）学习理论中的“可学习性（PAC Learnability）”进行简要的介绍</p>
<p>PAC 学习整体来说是一个比较纯粹的数学理论。有一种说法是、PAC 学习是统计学家研究机器学习的方式，它关心模型的可解释性、然而机器学习专家通常更关心模型的预测能力。这也正是为何说无需太过了解它，因为我们的目的终究不是成为统计的专家、而是更希望成为一个能够应用机器学习的人。不过幸运的是，虽然为了叙述 Boosting、PAC 学习中“可学习性”的概念难以避开，但其本身却是具有很直观的解释的。下面我们就来看看这个直观解释：</p>
<p>PAC 提出的一个主要的假设、就是它要求数据是从某个稳定的概率分布中产生。直观地说，就是样本在样本空间中的分布状况不能随时间的变化而变化、否则就失去了学习的意义（因为学习到的永远只是“某个时间”的分布，如果未知数据所处时间的分布状况和该时间数据的分布状况不同的话、模型就直接失效了）。然后所谓的PAC可学习性，就是看学习的算法是否能够在合理的时间（多项式时间）内、以足够高的概率输出一个错误率足够低的模型。由此，所谓的“强可学习”和“弱可学习”的概念就很直观了：</p>
<ul>
<li>若存在一个多项式算法可以学习出准确率很高的模型，则称为强可学习</li>
<li>若在在一个多项式算法可以学习但准确率仅仅略高于随机猜测，则称为弱可学习</li>
</ul>
<p><strong><em>注意：由于进行机器学习时、我们只能针对训练数据集进行学习、所以和真实情况相比肯定是有偏差的。这正是需要提出 PAC 可学习这个概念的原因之一</em></strong></p>
<p>虽然我们区分定义了这两个概念，不过神奇之处在于，这两个概念在PAC学习框架下是完全等价的[Schapire, 1990]。这意味着对于一个学习问题，只要我们找到了一个“弱学习算法”，就可以把它变成一个“强学习算法”。这当然是意义深刻的，因为往往比较粗糙的“弱学习算法”比较好找、而相对精确的“强学习算法”却难得一求</p>
<p>那么具体而言应该怎么做呢？这里就需要用到所谓的 Boosting（提升方法）了。提升方法可以定义为用于将由“弱学习算法”生成的“弱模型”、提升成和“强学习算法”所生成的“强模型”性能差不多的模型的方法，它的基本组成单元是许许多多的“弱模型”、然后通过某种手段把它们集成为最终模型。虽然该过程听上去和之前介绍的 Bagging 差不多、但它们的思想和背后的数学理论却有较大区别，加以辨析是有必要的</p>
<p>需要指出的是，Boosting 事实上是一族算法、该族算法有一个类似的框架：</p>
<ul>
<li>根据当前的数据训练出一个弱模型</li>
<li>根据该弱模型的表现调整数据的样本权重。具体而言：<ul>
<li>让该弱模型做错的样本在后续训练中获得更多的关注</li>
<li>让该弱模型做对的样本在后续训练中获得较少的关注</li>
</ul>
</li>
<li>最后再根据该弱模型的表现决定该弱模型的“话语权”、亦即投票表决时的“可信度”。自然、表现越好就越有话语权</li>
</ul>
<p>可以证明，当训练样本有无穷多时、Boosting 能让弱模型集成出一个对训练样本集的准确率任意高的模型。然而实际任务中训练样本当然不可能有无穷多，所以问题就转为了如何在固定的训练集上应用 Boosting 方法。而在 1996 年，Freund 和 Schapire 所提出的 AdaBoost（Adaptive Boosting）正是一个相当不错的解决方案，在理论和实验上均有优异的表现。虽然 AdaBoost 背后的理论深究起来可能会有些繁复、但它的思想并没有脱离 Boosting 族算法的那一套框架。值得一提的是、Boosting 还有一套比较有意思的解释方法；我们会在后文详细讨论其中的代表性算法——AdaBoost 的解释、这里就先按下不表</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文首先会介绍何谓“集成”、然后会介绍两种常见的集成学习方法：Bagging、AdaBoost 的基本定义。这些概念的背后有着深刻的数学理论，但是它们同时也拥有着很好的直观。获得对它们的直观有助于加深对各种模型的分类性能的理解、同时也有助于根据具体的数据集来挑选相应的、合适的模型来进行学习&lt;/p&gt;
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="综述" scheme="http://www.carefree0910.com/tags/%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>集成学习综述</title>
    <link href="http://www.carefree0910.com/posts/d8e97c87/"/>
    <id>http://www.carefree0910.com/posts/d8e97c87/</id>
    <published>2017-04-25T09:27:57.000Z</published>
    <updated>2017-04-25T11:50:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>目前为止我们已经讲过了若干的分类器了。从它们的复杂程度可以感受到，它们有些是比较“强”的、有些是比较“弱”的。这一章我们将会阐述所谓的“强”与“弱”的定义、它们之间的联系以及阐述如何将一个“弱分类器”通过集成学习来集成出一个“强分类器”。而由于集成学习有许多种具体的方法，我们会挑选出其中的随机森林和 AdaBoost 来作比较详细的说明</p>
<p>以下是目录：</p>
<ul>
<li><a href="/posts/7081b0ee/" title="“集成”的思想">“集成”的思想</a></li>
<li><a href="/posts/c0a9c025/" title="随机森林算法">随机森林算法</a></li>
<li><a href="/posts/f5f50863/" title="AdaBoost 算法">AdaBoost 算法</a></li>
<li><a href="/posts/fb0d2f02/" title="集成模型的性能分析">集成模型的性能分析</a></li>
<li><a href="/posts/707464b/" title="AdaBoost 算法的解释">AdaBoost 算法的解释</a></li>
<li><a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a></li>
<li><a href="/posts/48a0211a/" title="“集成学习”小结">“集成学习”小结</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前为止我们已经讲过了若干的分类器了。从它们的复杂程度可以感受到，它们有些是比较“强”的、有些是比较“弱”的。这一章我们将会阐述所谓的“强”与“弱”的定义、它们之间的联系以及阐述如何将一个“弱分类器”通过集成学习来集成出一个“强分类器”。而由于集成学习有许多种具体的方法，我
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="综述" scheme="http://www.carefree0910.com/tags/%E7%BB%BC%E8%BF%B0/"/>
    
      <category term="目录" scheme="http://www.carefree0910.com/tags/%E7%9B%AE%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>“决策树”小结</title>
    <link href="http://www.carefree0910.com/posts/88953f51/"/>
    <id>http://www.carefree0910.com/posts/88953f51/</id>
    <published>2017-04-23T02:43:07.000Z</published>
    <updated>2017-04-23T02:44:13.000Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>决策树是从直观上很好理解的模型，可以把它理解为一个划分规则的序列</li>
<li>决策树常用的生成算法包括：<ul>
<li>ID3 算法，它使用互信息作为信息增益的度量</li>
<li>C4.5 算法，它使用信息增益比作为信息增益的度量</li>
<li>CART 算法，它规定生成出来的决策树为二叉树、且一般使用基尼增益作为信息增益的度量</li>
</ul>
</li>
<li>决策树常用的剪枝算法有两种，它们都是为了适当地降低模型复杂度、从而期望模型在未知数据上的表现更好</li>
<li>决策树的代码实现从始到终都贯彻着递归的思想，可以说是递归的一个经典应用</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;决策树是从直观上很好理解的模型，可以把它理解为一个划分规则的序列&lt;/li&gt;
&lt;li&gt;决策树常用的生成算法包括：&lt;ul&gt;
&lt;li&gt;ID3 算法，它使用互信息作为信息增益的度量&lt;/li&gt;
&lt;li&gt;C4.5 算法，它使用信息增益比作为信息增益的度量&lt;/li&gt;
&lt;li&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="小结" scheme="http://www.carefree0910.com/tags/%E5%B0%8F%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>相关数学理论</title>
    <link href="http://www.carefree0910.com/posts/613bbb2f/"/>
    <id>http://www.carefree0910.com/posts/613bbb2f/</id>
    <published>2017-04-23T02:33:22.000Z</published>
    <updated>2017-04-28T10:49:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章会叙述琴生不等式（Jensen’s Inequality）及其一个简单的应用，为此我们需要知道凸函数的概念：</p>
<ul>
<li>若函数<script type="math/tex">f(x)</script>对<script type="math/tex">\forall p\in [0,1]</script>、都满足：  <script type="math/tex; mode=display">
pf\left( x_{1} \right) + \left( 1 - p \right)f\left( x_{2} \right) \leq f\left( px_{1} + \left( 1 - p \right)x_{2} \right)</script>则称<script type="math/tex">f(x)</script>为凸函数（有时又叫上凸函数）</li>
</ul>
<p>琴生不等式是针对凸函数提出的，下面就具体说一下其内容与证明</p>
<a id="more"></a>
<h1 id="定理内容"><a href="#定理内容" class="headerlink" title="定理内容"></a>定理内容</h1><p>对于<script type="math/tex">\lbrack a,b\rbrack</script>上的凸函数，若</p>
<script type="math/tex; mode=display">
p_{1},...,p_{K} \in \left\lbrack 0,1 \right\rbrack,\ \ p_{1} + p_{2} + \ldots + p_{K} = 1</script><p>则有</p>
<script type="math/tex; mode=display">
\sum_{k = 1}^{K}{p_{i}f(x_{i})} \leq f(\sum_{k = 1}^{K}{p_{i}x_{i}})</script><p>接下来我们会利用它来证明等概率分布具有最大熵。注意到可以证明函数</p>
<script type="math/tex; mode=display">
\hat{H}\left( p \right) = - p\log p</script><p>是一个凸函数，于是熵的定义式可以写成</p>
<script type="math/tex; mode=display">
H\left( y \right) = - \sum_{k = 1}^{K}{p_{k}\log p_{k}} = \sum_{k = 1}^{K}{\hat{H}\left( p_{k} \right)}</script><p>从而</p>
<script type="math/tex; mode=display">
\frac{1}{K}H\left( y \right) = \frac{1}{K}\sum_{k = 1}^{K}{\hat{H}(p_{k})} \leq \hat{H}\left( \sum_{k = 1}^{K}{\frac{1}{K}p_{k}} \right) = \hat{H}\left( \frac{1}{K} \right) = - \frac{1}{K}\log\frac{1}{K} = \frac{1}{K}\log K</script><p>亦即</p>
<script type="math/tex; mode=display">
H\left( y \right) \leq \log K</script><p>等式当且仅当</p>
<script type="math/tex; mode=display">
p_{1} = p_{2} = \ldots = p_{K} = \frac{1}{K}</script><p>时取得</p>
<h1 id="定理证明"><a href="#定理证明" class="headerlink" title="定理证明"></a>定理证明</h1><p>应用数学归纳法可以比较简单地完成证明：</p>
<ul>
<li>当时<script type="math/tex">K=2</script>、由凸函数定义直接证毕，此为奠基</li>
<li>假设<script type="math/tex">K = n</script>时成立、考虑<script type="math/tex">K = n + 1</script>的情况，令  <script type="math/tex; mode=display">
s_{n} = \sum_{k = 1}^{n}p_{k}</script>则  <script type="math/tex; mode=display">
\sum_{k = 1}^{n + 1}{p_{k}f\left( x_{k} \right)} = s_{n}\sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} + p_{n + 1}f\left( x_{n + 1} \right)</script>注意到  <script type="math/tex; mode=display">
\sum_{k = 1}^{n}\frac{p_{k}}{s_{n}} = 1</script>从而由<script type="math/tex">K = n</script>时的琴生不等式可知  <script type="math/tex; mode=display">
\sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} \leq f\left( \sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} \right)</script>注意到  <script type="math/tex; mode=display">
s_{n} + p_{n + 1} = 1</script>从而由凸函数定义知  <script type="math/tex; mode=display">
s_{n}f\left( \sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} \right) + p_{n + 1}f\left( x_{n + 1} \right) \leq f\left( s_{n}\sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} + p_{n + 1}x_{n + 1} \right)</script>综上所述、即得  <script type="math/tex; mode=display">
\sum_{k = 1}^{n + 1}{p_{k}f(x_{k})} \leq f\left( \sum_{k = 1}^{n + 1}{p_{k}f\left( x_{k} \right)} \right)</script></li>
</ul>
<p>琴生不等式的应用非常广泛，证明等概率分布具有最大熵只是其中一个小应用。在许许多多涉及到凸问题的算法中、琴生不等式都显示出了强大的威力</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇文章会叙述琴生不等式（Jensen’s Inequality）及其一个简单的应用，为此我们需要知道凸函数的概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若函数&lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;对&lt;script type=&quot;math/tex&quot;&gt;\forall p\in [0,1]&lt;/script&gt;、都满足：  &lt;script type=&quot;math/tex; mode=display&quot;&gt;
pf\left( x_{1} \right) + \left( 1 - p \right)f\left( x_{2} \right) \leq f\left( px_{1} + \left( 1 - p \right)x_{2} \right)&lt;/script&gt;则称&lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;为凸函数（有时又叫上凸函数）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;琴生不等式是针对凸函数提出的，下面就具体说一下其内容与证明&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>评估与可视化</title>
    <link href="http://www.carefree0910.com/posts/c12a819/"/>
    <id>http://www.carefree0910.com/posts/c12a819/</id>
    <published>2017-04-23T01:45:37.000Z</published>
    <updated>2017-04-23T03:08:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前我们实现了一个 Node 基类<code>CvDNode</code>和一个 Tree 基类<code>CvDBase</code>；为了评估决策树模型的表现、我们需要先在这两个基类的基础上根据不同的算法实现出各种具体的决策树。由于我们在基类里面已经完成了绝大部分工作、所以在其上进行扩展是平凡的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ID3Node</span><span class="params">(CvDNode)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></div><div class="line">        CvDNode.__init__(self, *args, **kwargs)</div><div class="line">        self.criterion = <span class="string">"ent"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">C45Node</span><span class="params">(CvDNode)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></div><div class="line">        CvDNode.__init__(self, *args, **kwargs)</div><div class="line">        self.criterion = <span class="string">"ratio"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CartNode</span><span class="params">(CvDNode)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></div><div class="line">        CvDNode.__init__(self, *args, **kwargs)</div><div class="line">        self.criterion = <span class="string">"gini"</span></div><div class="line">        self.is_cart = <span class="keyword">True</span></div></pre></td></tr></table></figure>
<p>在<code>CvDBase</code>的基础上定义三种算法对应的 Tree 结构的方法是类似的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ID3Tree</span><span class="params">(CvDBase, ID3Node, metaclass=CvDMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">C45Tree</span><span class="params">(CvDBase, C45Node, metaclass=CvDMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CartTree</span><span class="params">(CvDBase, CartNode, metaclass=CvDMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>其中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CvDMeta</span><span class="params">(type)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(mcs, *args, **kwargs)</span>:</span></div><div class="line">        name, bases, attr = args[:<span class="number">3</span>]</div><div class="line">        _, _node = bases</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, whether_continuous=None, max_depth=None, node=None, **_kwargs)</span>:</span></div><div class="line">            tmp_node = node <span class="keyword">if</span> isinstance(node, CvDNode) <span class="keyword">else</span> _node</div><div class="line">            CvDBase.__init__(self, whether_continuous, max_depth, tmp_node(**_kwargs))</div><div class="line">            self._name = name</div><div class="line"></div><div class="line">        attr[<span class="string">"__init__"</span>] = __init__</div><div class="line">        <span class="keyword">return</span> type(name, bases, attr)</div></pre></td></tr></table></figure>
<p>接下来就是具体的评估与相应的可视化</p>
<a id="more"></a>
<p>我们同样可以使用蘑菇数据集来评估决策树模型的表现，结果如下所示：</p>
<img src="/posts/c12a819/p1.png" alt="蘑菇数据集上 ID3 算法的表现" title="蘑菇数据集上 ID3 算法的表现">
<img src="/posts/c12a819/p2.png" alt="蘑菇数据集上 C4.5 算法的表现" title="蘑菇数据集上 C4.5 算法的表现">
<img src="/posts/c12a819/p3.png" alt="蘑菇数据集上 CART 算法的表现" title="蘑菇数据集上 CART 算法的表现">
<p>可以看到 CART 算法的表现相对来说要差不少，可能的原因有如下三条：</p>
<ul>
<li>CART 算法在选择划分标准时是从所有二分标准里面进行选择的，这里就会比 ID3 和 C4.5 算法多出不少倍的运算量</li>
<li>由于我们在实现 CART 剪枝算法时为了追求简洁、直接调用了标准库 copy 中的 deepcopy 方法对整颗决策树进行了深拷贝。这一步可能会连不必要的东西也进行了拷贝、从而导致了一些不必要的开销</li>
<li>CART 算法生成的是二叉决策树，所以可能生成出来的树会更深、各叶节点中的样本数可能也会分布得比较均匀、从而无论是建模过程还是预测过程都会要慢一些</li>
</ul>
<p>当然，如果结合蘑菇数据集来说的话、笔者认为最大的问题在于：CART 算法不适合应用于蘑菇数据集。一方面是因为蘑菇数据集全是离散型特征且各特征取值都挺多，另一方面是因为蘑菇数据集相对简单、有一些特征非常具有代表性（我们在说明朴素贝叶斯时也有所提及），仅仅用二分标准划分数据的话、会显得比较没有效率</p>
<p>为了更客观地评估我们模型的表现，我们可以对成熟第三方库 sklearn 中的决策树模型进行恰当的封装并看看它在蘑菇数据集上的表现：</p>
<img src="/posts/c12a819/p4.png" alt="蘑菇数据集上 sklearn 决策树的表现（criterion=entropy）" title="蘑菇数据集上 sklearn 决策树的表现（criterion=entropy）">
<img src="/posts/c12a819/p5.png" alt="蘑菇数据集上 sklearn 决策树的表现（criterion=gini）" title="蘑菇数据集上 sklearn 决策树的表现（criterion=gini）">
<p>不得不承认、成熟第三方库的效率确实要高很多（比我们的要快 5 倍左右）；这是因为虽然算法思想可能大致相同，但 sklearn 的核心实现都经过了高度优化、且（如不出意料的话）应该都是用 C 或者其它底层语言直接写的。不过正如第一章说过的，要想应用 sklearn 中的决策树、就必须先将数据数值化（即使是离散型数据）；而我们实现的决策树在处理离散型数据时却无需这一步数据预处理、可以直接应用在原始数据上（但处理混合型数据时还是要先进行数值化处理、而且将离散型数据数值化也能显著提升模型的运行速度）</p>
<p>我们在本系列的综述里面曾说过、决策树可能是从直观上最好理解的模型；事实上，我们之前画过的一些决策树示意图也确实非常直观易懂、于是我们可能自然就会希望程序能将生成类似的东西。虽然不能做到那么漂亮、不过我们确实是能在之前实现的决策树模型的基础上做出类似效果的：</p>
<img src="/posts/c12a819/p6.png" alt="蘑菇数据集上 ID3 决策树的可视化" title="蘑菇数据集上 ID3 决策树的可视化">
<img src="/posts/c12a819/p7.png" alt="蘑菇数据集上 C4.5 决策树的可视化" title="蘑菇数据集上 C4.5 决策树的可视化">
<img src="/posts/c12a819/p8.png" alt="蘑菇数据集上 CART 决策树的可视化" title="蘑菇数据集上 CART 决策树的可视化">
<p>其中，红色数字代表该 Node 作为划分标准的特征所属的维度，位于各条连线中央的字母代表着该维度特征的各个取值、加号“+”代表着“其它”，绿色字母代表类别标记。以上三张图在一定程度上验证了我们之前的很多说法，比如说 ID3 会倾向选择取值比较多的特征、C4.5 可能会倾向选择取值比较少的特征且倾向于在每个二叉分枝处留下一个小 Node 作为叶节点、CART 各个叶节点上的样本分布较均匀且生成出的决策树会比较深……等等</p>
<p>我们在说明朴素贝叶斯时曾经提过，即使只根据第 5 维的取值来进行类别的判定、最后的准确率也一定会非常高。验证这一命题的方法很简单——只需将决策树的最大深度设为 1 即可，结果如下图所示：</p>
<img src="/posts/c12a819/p9.png" alt="p9.png" title="">
<p>此时模型的表现如下图所示：</p>
<img src="/posts/c12a819/p10.png" alt="p10.png" title="">
<p>可以看到其表现确实不错。值得一提的是，单层决策树又可称为“决策树桩（Decision Stump）”、它是有特殊应用场景的（比如我们在下个系列中讲 AdaBoost 时就会用到它）</p>
<p>至今为止我们用到的数据集都是离散型数据集，为了更全面地进行评估、使用连续型混合型数据集进行评估是有必要的；同时为了增强直观、我们可以用异或数据集来进行评估。原始数据集如下图所示：</p>
<img src="/posts/c12a819/p11.png" alt="p11.png" title="">
<p>生成异或数据集（及其它二维数据集）的代码定义在之前提过 DataUtil 类中（可参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py" target="_blank" rel="external">这里</a>），读者也可以在下一章中找到相应的讲解。为使评估更具有直观性、我们可以把四种决策树（ID3、C4.5、CART 决策树和 sklearn 的决策树）在异或数据集上的表现直接画出来：</p>
<img src="/posts/c12a819/p12.png" alt="异或数据集上 ID3、CART 和 sklearn 决策树的表现" title="异或数据集上 ID3、CART 和 sklearn 决策树的表现">
<img src="/posts/c12a819/p13.png" alt="异或数据集上 C4.5 决策树的表现" title="异或数据集上 C4.5 决策树的表现">
<p>可以看到 C4.5 决策树的过拟合现象比较严重。正如我们之前所分析的一般、这很有可能是因为 C4.5 在二叉分枝时会倾向于进行“不均匀的二分”（从上图也可以大概看出）</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前我们实现了一个 Node 基类&lt;code&gt;CvDNode&lt;/code&gt;和一个 Tree 基类&lt;code&gt;CvDBase&lt;/code&gt;；为了评估决策树模型的表现、我们需要先在这两个基类的基础上根据不同的算法实现出各种具体的决策树。由于我们在基类里面已经完成了绝大部分工作、所以在其上进行扩展是平凡的：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ID3Node&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDNode)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, *args, **kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        CvDNode.__init__(self, *args, **kwargs)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.criterion = &lt;span class=&quot;string&quot;&gt;&quot;ent&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;C45Node&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDNode)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, *args, **kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        CvDNode.__init__(self, *args, **kwargs)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.criterion = &lt;span class=&quot;string&quot;&gt;&quot;ratio&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;CartNode&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDNode)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, *args, **kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        CvDNode.__init__(self, *args, **kwargs)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.criterion = &lt;span class=&quot;string&quot;&gt;&quot;gini&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.is_cart = &lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;在&lt;code&gt;CvDBase&lt;/code&gt;的基础上定义三种算法对应的 Tree 结构的方法是类似的：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ID3Tree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDBase, ID3Node, metaclass=CvDMeta)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;C45Tree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDBase, C45Node, metaclass=CvDMeta)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;CartTree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDBase, CartNode, metaclass=CvDMeta)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;其中&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;CvDMeta&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(type)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__new__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(mcs, *args, **kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        name, bases, attr = args[:&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        _, _node = bases&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, whether_continuous=None, max_depth=None, node=None, **_kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            tmp_node = node &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; isinstance(node, CvDNode) &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; _node&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            CvDBase.__init__(self, whether_continuous, max_depth, tmp_node(**_kwargs))&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            self._name = name&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        attr[&lt;span class=&quot;string&quot;&gt;&quot;__init__&quot;&lt;/span&gt;] = __init__&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; type(name, bases, attr)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;接下来就是具体的评估与相应的可视化&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>剪枝算法的实现</title>
    <link href="http://www.carefree0910.com/posts/602f7125/"/>
    <id>http://www.carefree0910.com/posts/602f7125/</id>
    <published>2017-04-23T01:25:20.000Z</published>
    <updated>2017-04-23T03:00:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py" target="_blank" rel="external">这里</a>和<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Tree.py" target="_blank" rel="external">这里</a>）</p>
<p>决策树剪枝算法的实现相对而言比较平凡，只需要把算法依次翻译成程序语言即可</p>
<a id="more"></a>
<h1 id="ID3、C4-5-剪枝算法的实现"><a href="#ID3、C4-5-剪枝算法的实现" class="headerlink" title="ID3、C4.5 剪枝算法的实现"></a>ID3、C4.5 剪枝算法的实现</h1><p>回忆算法本身，可以知道我们需要获取“从下往上”这个顺序，为此我们需要先在<code>CvDNode</code>中利用递归定义一个函数来更新 Tree 的<code>self.layers</code>属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_layers</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 根据该Node的深度、在self.layers对应位置的列表中记录自己</span></div><div class="line">    self.tree.layers[self._depth].append(self)</div><div class="line">    <span class="comment"># 遍历所有子节点、完成递归</span></div><div class="line">    <span class="keyword">for</span> _node <span class="keyword">in</span> sorted(self.children):</div><div class="line">        _node = self.children[_node]</div><div class="line">        <span class="keyword">if</span> _node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            _node.update_layers()</div></pre></td></tr></table></figure>
<p>然后、在<code>CvDBase</code>中定义一个对应的函数进行封装：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update_layers</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 根据整颗决策树的高度、在self.layers里面放相应数量的列表</span></div><div class="line">    self.layers = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.root.height)]</div><div class="line">    self.root.update_layers()</div></pre></td></tr></table></figure>
<p>同时，为了做到合理的代码重用、我们可以先在<code>CvDNode</code>中定义一个计算损失的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(self, pruned=False)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pruned:</div><div class="line">        <span class="keyword">return</span> sum([leaf[<span class="string">"chaos"</span>] * len(leaf[<span class="string">"y"</span>]) <span class="keyword">for</span> leaf <span class="keyword">in</span> self.leafs.values()])</div><div class="line">    <span class="keyword">return</span> self.chaos * len(self._y)</div></pre></td></tr></table></figure>
<p>有了以上两个函数，算法本身的实现就很直观了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_prune</span><span class="params">(self)</span>:</span></div><div class="line">    self._update_layers()</div><div class="line">    _tmp_nodes = []</div><div class="line">    <span class="comment"># 更新完决策树每一“层”的Node之后，从后往前地向 _tmp_nodes中加Node</span></div><div class="line">    <span class="keyword">for</span> _node_lst <span class="keyword">in</span> self.layers[::<span class="number">-1</span>]:</div><div class="line">        <span class="keyword">for</span> _node <span class="keyword">in</span> _node_lst[::<span class="number">-1</span>]:</div><div class="line">            <span class="keyword">if</span> _node.category <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">                _tmp_nodes.append(_node)</div><div class="line">    _old = np.array([node.cost() + self.prune_alpha * len(node.leafs)</div><div class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> _tmp_nodes])</div><div class="line">    _new = np.array([node.cost(pruned=<span class="keyword">True</span>) + self.prune_alpha</div><div class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> _tmp_nodes])</div><div class="line">    <span class="comment"># 使用 _mask变量存储 _old和 _new对应位置的大小关系</span></div><div class="line">    _mask = _old &gt;= _new</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># 若只剩根节点就退出循环体</span></div><div class="line">        <span class="keyword">if</span> self.root.height == <span class="number">1</span>:</div><div class="line">            <span class="keyword">return</span></div><div class="line">        p = np.argmax(_mask)</div><div class="line">        <span class="comment"># 如果 _new中有比 _old中对应损失小的损失、则进行局部剪枝</span></div><div class="line">        <span class="keyword">if</span> _mask[p]:</div><div class="line">            _tmp_nodes[p].prune()</div><div class="line">            <span class="comment"># 根据被影响了的Node、更新 _old、_mask对应位置的值</span></div><div class="line">            <span class="keyword">for</span> i, node <span class="keyword">in</span> enumerate(_tmp_nodes):</div><div class="line">                <span class="keyword">if</span> node.affected:</div><div class="line">                    _old[i] = node.cost() + self.prune_alpha * len(node.leafs)</div><div class="line">                    _mask[i] = _old[i] &gt;= _new[i]</div><div class="line">                    node.affected = <span class="keyword">False</span></div><div class="line">            <span class="comment"># 根据被剪掉的Node、将各个变量对应的位置除去（注意从后往前遍历）</span></div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(_tmp_nodes) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</div><div class="line">                <span class="keyword">if</span> _tmp_nodes[i].pruned:</div><div class="line">                    _tmp_nodes.pop(i)</div><div class="line">                    _old = np.delete(_old, i)</div><div class="line">                    _new = np.delete(_new, i)</div><div class="line">                    _mask = np.delete(_mask, i)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">    self.reduce_nodes()</div></pre></td></tr></table></figure>
<p>上述代码的第 25 行和第 28 行出现了 Node 的<code>affected</code>属性，这是我们之前没有进行定义的（因为若在彼时定义会显得很突兀）；不过由剪枝算法可知，这个属性的用处与其名字一致——标记一个 Node 是否是“被影响到的”Node。事实上，在一个 Node 进行了局部剪枝后，会有两类 Node “被影响到”：</p>
<ul>
<li>该 Node 的子节点、子节点的子节点……等等，它们属于被剪掉的 Node、应该要将它们在<code>_old</code>、<code>_tmp_nodes</code>中对应的位置从这些列表中除去</li>
<li>该 Node 的父节点、父节点的父节点……等等，它们存储叶节点的列表会因局部剪枝而发生改变、所以要更新<code>_old</code>和<code>_mask</code>列表中对应位置的值</li>
</ul>
<p>其中，我们之前定义的 Node 中是用<code>pruned</code>属性来标记该 Node 是否已被剪掉、且介绍了如何通过递归来更新<code>pruned</code>属性；<code>affected</code>属性和<code>pruned</code>属性的本质几乎没什么区别，所以我们同样可以通过递归来更新<code>affected</code>属性。具体而言，我们只需：</p>
<ul>
<li>在初始化时令<code>self.affected = False</code></li>
<li>在局部剪枝函数内部插入<code>_parent.affected = True</code></li>
</ul>
<p>即可，其余部分可以保持不变。</p>
<h1 id="CART-剪枝算法的实现"><a href="#CART-剪枝算法的实现" class="headerlink" title="CART 剪枝算法的实现"></a>CART 剪枝算法的实现</h1><p>同样的，为了做到合理的代码重用、我们先利用之前实现的<code>cost</code>函数、在<code>CvDNode</code>里面定义一个获取 Node 阈值的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_threshold</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> (self.cost(pruned=<span class="keyword">True</span>) - self.cost()) / (len(self.leafs) - <span class="number">1</span>)</div></pre></td></tr></table></figure>
<p>由于算法本身的实现的思想以及用到的工具都和第一种剪枝算法大同小异、所以代码写起来也差不多：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cart_prune</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 暂时将所有节点记录所属Tree的属性置为None</span></div><div class="line">    <span class="comment"># 这样做的必要性会在后文进行说明</span></div><div class="line">    self.root.cut_tree()</div><div class="line">    _tmp_nodes = [node <span class="keyword">for</span> node <span class="keyword">in</span> self.nodes <span class="keyword">if</span> node.category <span class="keyword">is</span> <span class="keyword">None</span>]</div><div class="line">    _thresholds = np.array([node.get_threshold() <span class="keyword">for</span> node <span class="keyword">in</span> _tmp_nodes])</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># 利用deepcopy对当前根节点进行深拷贝、存入self.roots列表</span></div><div class="line">        <span class="comment"># 如果前面没有把记录Tree的属性置为None，那么这里就也会对整个Tree做</span></div><div class="line">        <span class="comment"># 深拷贝。可以想象、这样会引发严重的内存问题，速度也会被拖慢非常多</span></div><div class="line">        root_copy = deepcopy(self.root)</div><div class="line">        self.roots.append(root_copy)</div><div class="line">        <span class="keyword">if</span> self.root.height == <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        p = np.argmin(_thresholds)</div><div class="line">        _tmp_nodes[p].prune()</div><div class="line">        <span class="keyword">for</span> i, node <span class="keyword">in</span> enumerate(_tmp_nodes):</div><div class="line">            <span class="comment"># 更新被影响到的Node的阈值</span></div><div class="line">            <span class="keyword">if</span> node.affected:</div><div class="line">                _thresholds[i] = node.get_threshold()</div><div class="line">                node.affected = <span class="keyword">False</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(_tmp_nodes) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</div><div class="line">            <span class="comment"># 去除掉各列表相应位置的元素</span></div><div class="line">            <span class="keyword">if</span> _tmp_nodes[i].pruned:</div><div class="line">                _tmp_nodes.pop(i)</div><div class="line">                _thresholds = np.delete(_thresholds, i)</div><div class="line">    self.reduce_nodes()</div></pre></td></tr></table></figure>
<p>代码第 4 行对根节点调用的<code>cut_tree</code>方法同样是利用递归实现的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_tree</span><span class="params">(self)</span>:</span></div><div class="line">    self.tree = <span class="keyword">None</span></div><div class="line">    <span class="keyword">for</span> child <span class="keyword">in</span> self.children.values():</div><div class="line">        <span class="keyword">if</span> child <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            child.cut_tree()</div></pre></td></tr></table></figure>
<p>然后就是最后一步、通过交叉验证选出最优树了。注意到之前我们封装生成算法时、最后一行调用了剪枝算法的封装——<code>self.prune</code>方法。由于该方法是第一个接收了交叉验证集<code>x_cv</code>和<code>y_cv</code>的方法、所以我们应该让该方法来做交叉验证。简洁起见，我们直接选用加权正确率作为交叉验证的标准：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算加权正确率的函数</span></div><div class="line"><span class="meta">@staticmethod</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">acc</span><span class="params">(y, y_pred, weights)</span>:</span></div><div class="line">    <span class="keyword">if</span> weights <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> np.sum((np.array(y) == np.array(y_pred)) * weights) / len(y)</div><div class="line">    <span class="keyword">return</span> np.sum(np.array(y) == np.array(y_pred)) / len(y)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prune</span><span class="params">(self, x_cv, y_cv, weights)</span>:</span></div><div class="line">    <span class="keyword">if</span> self.root.is_cart:</div><div class="line">        <span class="comment"># 如果该Node使用CART剪枝，那么只有在确实传入了交叉验证集的情况下</span></div><div class="line">        <span class="comment"># 才能调用相关函数、否则没有意义</span></div><div class="line">        <span class="keyword">if</span> x_cv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> y_cv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            self._cart_prune()</div><div class="line">            _arg = np.argmax([CvDBase.acc(</div><div class="line">                y_cv, tree.predict(x_cv), weights) <span class="keyword">for</span> tree <span class="keyword">in</span> self.roots])</div><div class="line">            _tar_root = self.roots[_arg]</div><div class="line">            <span class="comment"># 由于Node的feed_tree方法会递归地更新nodes属性、所以要先重置</span></div><div class="line">            self.nodes = []</div><div class="line">            _tar_root.feed_tree(self)</div><div class="line">            self.root = _tar_root</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        self._prune()</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;（本文会用到的所有代码都在&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py&quot;&gt;这里&lt;/a&gt;和&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Tree.py&quot;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;决策树剪枝算法的实现相对而言比较平凡，只需要把算法依次翻译成程序语言即可&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
</feed>
