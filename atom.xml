<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Python 与机器学习</title>
  <subtitle>Python &amp; Machine Learning</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.carefree0910.com/"/>
  <updated>2017-04-25T16:42:32.000Z</updated>
  <id>http://www.carefree0910.com/</id>
  
  <author>
    <name>射命丸咲</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>“集成学习”小结</title>
    <link href="http://www.carefree0910.com/posts/48a0211a/"/>
    <id>http://www.carefree0910.com/posts/48a0211a/</id>
    <published>2017-04-25T16:41:26.000Z</published>
    <updated>2017-04-25T16:42:32.000Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>集成学习是将个体模型进行集成的方法，大致可分为 Bagging 和 Boosting 两类</li>
<li>随机森林是 Bagging 算法的一种常见拓展、性能优异；它不仅对样本的选取引入随机性、还对个体模型（决策树）的特征选取步骤引入随机性</li>
<li>AdaBoost 是 Boosting 族算法的代表，通过以下三步进行提升：<ul>
<li>根据样本权重训练弱分类器</li>
<li>根据该弱分类器的加权错误率为其分配“话语权”</li>
<li>根据该弱分类器的表现更新样本权重</li>
</ul>
</li>
<li>集成模型具有相当不错的正则化能力、但该正则化能力并不是必然存在的</li>
<li>AdaBoost 可以用前向分步算法和加法模型来解释</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;集成学习是将个体模型进行集成的方法，大致可分为 Bagging 和 Boosting 两类&lt;/li&gt;
&lt;li&gt;随机森林是 Bagging 算法的一种常见拓展、性能优异；它不仅对样本的选取引入随机性、还对个体模型（决策树）的特征选取步骤引入随机性&lt;/li&gt;
&lt;li
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="小结" scheme="http://www.carefree0910.com/tags/%E5%B0%8F%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>相关数学理论</title>
    <link href="http://www.carefree0910.com/posts/613bbb2f/"/>
    <id>http://www.carefree0910.com/posts/613bbb2f/</id>
    <published>2017-04-25T16:19:04.000Z</published>
    <updated>2017-04-25T16:49:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>这一节会叙述之前没有解决的纯数学问题，同样会涉及到概率论的一些基础概念和思想，可能会有一定的难度</p>
<a id="more"></a>
<h1 id="经验分布函数"><a href="#经验分布函数" class="headerlink" title="经验分布函数"></a>经验分布函数</h1><p>正如前文所说，经验分布函数的数学表达式为：</p>
<script type="math/tex; mode=display">
F_{N}\left( x \right) = \frac{1}{N}\sum_{i = 1}^{N}{I_{\left( - \infty,x \right\rbrack}(x_{i})}</script><p>如果将<script type="math/tex">x_{1},\ldots,x_{N}</script>按从小到大的顺序排成<script type="math/tex">x_{(1)},\ldots,x_{\left( N \right)}</script>，我们通常称其中的<script type="math/tex">x_{\left( i \right)}</script>为第 i 个次序统计量。易知可以利用次序统计量将<script type="math/tex">F_{N}(x)</script>表示成更直观的形式：</p>
<script type="math/tex; mode=display">
F_{N}\left( x \right) = \left\{ \begin{matrix}
0,\ \ &x < x_{\left( 1 \right)} \\
\frac{i}{N},\ \ &x \in \left\lbrack x_{\left( i \right)},x_{\left( i + 1 \right)} \right)\ (i = 1,\ldots,N - 1) \\
1,\ \ &x \geq x_{\left( N \right)} \\
\end{matrix} \right.\</script><p>关于其优良性，前文所说的“频率估计概率”的严谨叙述其实就是强大数律：</p>
<script type="math/tex; mode=display">
p\left( \lim_{N}{F_{N}\left( x \right) - F\left( x \right) = 0} \right) = 1\ (\forall x)</script><p>亦即</p>
<script type="math/tex; mode=display">
F_{N}(x) \xrightarrow{a.s.} F(x)</script><p>同时还有一个更强的结论（Glivenko-Cantelli 定理）：</p>
<script type="math/tex; mode=display">
p\left( \lim_{N}{\sup_{x}\left| F_{N}\left( x \right) - F\left( x \right) \right| = 0} \right) = 1</script><p>亦即</p>
<script type="math/tex; mode=display">
\left\| F_{N}\left( x \right) - F\left( x \right) \right\|_{\infty} \equiv \sup_{x}\left| F_{N}\left( x \right) - F\left( x \right) \right|\xrightarrow{a.s.}0</script><p>其中，<script type="math/tex">\sup_{x}\left| F_{N}\left( x \right) - F\left( x \right) \right|</script>就是著名的柯尔莫诺夫-斯米尔诺夫检验（Kolmogorov-Smirnov Statistic）。值得一提的是，用其它范数来代替这里的无穷范数有时也是合理的。比如说用二范数来代替时、对应的就是 Cramér-von Mises Criterion</p>
<p>此外，我们还可以利用中心极限定理等来研究经验分布函数（比如与正态分布扯上关系等等），这里就不详细展开了。总之，经验分布函数的优良性是相当有保证的，与其本质类似的 Bootstrap 的优良性也因而有了保证。当然、Bootstrap 自己是有一套成熟理论的，不过如果就这点展开来叙述的话、多多少少会偏离了本系列文章的主旨，所以这里就仅通过讨论经验分布函数来间接地感受 Bootstrap 的优良性</p>
<h1 id="AdaBoost-与前向分步加法模型"><a href="#AdaBoost-与前向分步加法模型" class="headerlink" title="AdaBoost 与前向分步加法模型"></a>AdaBoost 与前向分步加法模型</h1><p>本节主要用于推导如下定理：AdaBoost 分类模型可以等价为损失函数为指数函数的前向分步加法模型</p>
<p>假设经过<script type="math/tex">k</script>轮迭代后、前项分布算法已经得到了加法模型<script type="math/tex">f_{k}(x)</script>，亦即：</p>
<script type="math/tex; mode=display">
\begin{align}
f_{k}\left( x \right) &= f_{k - 1}\left( x \right) + \alpha_{k}g_{k}\left( x \right) = f_{k - 2}\left( x \right) + \alpha_{k - 1}g_{k - 1}\left( x \right) + \alpha_{k}g_{k}(x) \\

&= \ldots = \sum_{i = 1}^{k}{\alpha_{i}g_{i}(x)}
\end{align}</script><p>可知、第<script type="math/tex">k + 1</script>轮的模型<script type="math/tex">f_{k + 1}</script>能表示为：</p>
<script type="math/tex; mode=display">
f_{k + 1}\left( x \right) = f_{k}\left( x \right) + \alpha_{k + 1}g_{k + 1}(x)</script><p>我们关心的问题就是，如何在<script type="math/tex">f_{k}(x)</script>确定下来的情况下、训练出第<script type="math/tex">k + 1</script>轮的个体分类器<script type="math/tex">g_{k + 1}(x)</script>及其权重<script type="math/tex">\alpha_{k + 1}</script>。注意到我们的损失函数是指数函数，亦即：</p>
<script type="math/tex; mode=display">
\begin{align}
L &= \sum_{i = 1}^{N}{\exp\left\lbrack - y_{i}f_{k + 1}\left( x_{i} \right) \right\rbrack} \\

&= \sum_{i = 1}^{N}w_{ki}\exp\lbrack - y_{i}\alpha_{k + 1}g_{k + 1}(x_{i})\rbrack
\end{align}</script><p>其中</p>
<script type="math/tex; mode=display">
w_{ki} = \exp\lbrack - y_{i}f_{k}(x_{i})\rbrack</script><p>在<script type="math/tex">f_{k}(x)</script>确定下来的情况下是常数。由于我们的最终目的是最小化损失函数、所以<script type="math/tex">\alpha_{k + 1}</script>和<script type="math/tex">g_{k + 1}(x)</script>就可以表示为：</p>
<script type="math/tex; mode=display">
\begin{align}
\left( \alpha_{k + 1},g_{k + 1}\left( x \right) \right) &= \arg{\min_{\alpha,g}{\sum_{i = 1}^{N}{w_{ki}\exp\left\lbrack - y_{i}\alpha g\left( x_{i} \right) \right\rbrack}}} \\

&= \arg{\min_{\alpha,g}{\sum_{y_{i} = g\left( x_{i} \right)}^{}{w_{ki}e^{- \alpha}} + \sum_{y_{i} \neq g\left( x_{i} \right)}^{}{w_{ki}e^{\alpha}}}} \\

&= \arg{\min_{\alpha,g}{\left( e^{\alpha} - e^{- \alpha} \right)\sum_{i = 1}^{N}{w_{ki}I\left( y_{i} \neq g\left( x_{i} \right) \right)} + e^{- \alpha}\sum_{i = 1}^{N}w_{ki}}} \\

&= \arg{\min_{\alpha,g}{\left( e^{\alpha} - e^{- \alpha} \right)\sum_{i = 1}^{N}{w_{ki}I\left( y_{i} \neq g\left( x_{i} \right) \right)} + e^{- \alpha}}}
\end{align}</script><p>上式可以分两步求解。先看当<script type="math/tex">\alpha</script>确定下来后应该如何定出<script type="math/tex">g_{k + 1}(x)</script>，易知：</p>
<script type="math/tex; mode=display">
g_{k + 1}\left( x \right) = \arg{\min_{g}{\sum_{i = 1}^{N}{w_{ki}I\left( y_{i} \neq g\left( x_{i} \right) \right)}}}</script><p>亦即第<script type="math/tex">k + 1</script>步的个体分类器应该使训练集上的加权错误率最小。不妨设解出的<script type="math/tex">g_{k + 1}(x)</script>在训练集上的加权错误率为<script type="math/tex">e_{k + 1}</script>、亦即：</p>
<script type="math/tex; mode=display">
\sum_{i = 1}^{N}{w_{ki}I\left( y_{i} \neq g_{k + 1}\left( x_{i} \right) \right)} \triangleq e_{k + 1}</script><p>我们需要利用它来定出<script type="math/tex">\alpha_{k + 1}</script>。注意到对目标函数求偏导后易知：</p>
<script type="math/tex; mode=display">
\begin{align}
\alpha_{k + 1} &= \arg{\min_{\alpha}{\left( e^{\alpha} - e^{- \alpha} \right)e_{k + 1} + e^{- \alpha}}} \\

&\Leftrightarrow \left( e^{\alpha_{k + 1}} + e^{- \alpha_{k + 1}} \right)e_{k + 1} - e^{- \alpha_{k + 1}} = 0 \\

&\Leftrightarrow \alpha_{k + 1} = \frac{1}{2}\ln\frac{1 - e_{k + 1}}{e_{k + 1}}
\end{align}</script><p>这和 AdaBoost 中确定个体分类器权值的式子一模一样。接下来只需要证明样本权重更新的式子也彼此一致即可得证定理，而事实上、由于：</p>
<script type="math/tex; mode=display">
f_{k + 1}\left( x \right) = f_{k}\left( x \right) + \alpha_{k + 1}g_{k + 1}\left( x \right)</script><p>从而</p>
<script type="math/tex; mode=display">
\begin{align}
w_{k + 1,i} &= \exp\left\lbrack - y_{i}f_{k + 1}\left( x_{i} \right) \right\rbrack \\

&= \exp\left\lbrack - y_{i}f_{k}\left( x_{i} \right) \right\rbrack \cdot \exp\left\lbrack - y_{i}\alpha_{k + 1}g_{k + 1}\left( x_{i} \right) \right\rbrack \\

&= w_{ki} \cdot \exp\left\lbrack - y_{i}\alpha_{k + 1}g_{k + 1}\left( x_{i} \right) \right\rbrack
\end{align}</script><p>注意到我们要将样本权重归一化，所以须有：</p>
<script type="math/tex; mode=display">
w_{k + 1,i} \leftarrow \frac{w_{k + 1,i}}{Z_{k}}</script><p>其中</p>
<script type="math/tex; mode=display">
Z_{k} = \sum_{i = 1}^{N}w_{k + 1,i} = \sum_{i = 1}^{N}{w_{ki} \cdot \exp\left\lbrack - \alpha_{k + 1}y_{i}g_{k + 1}\left( x_{i} \right) \right\rbrack}</script><p>是故</p>
<script type="math/tex; mode=display">
w_{k + 1,i} = \frac{w_{ki}}{Z_{k}} \cdot \exp\left\lbrack - y_{i}\alpha_{k + 1}g_{k + 1}\left( x_{i} \right) \right\rbrack</script><p>这和 AdaBoost 中更新样本权重的式子也一模一样。综上所述、定理得证</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一节会叙述之前没有解决的纯数学问题，同样会涉及到概率论的一些基础概念和思想，可能会有一定的难度&lt;/p&gt;
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>AdaBoost 算法的解释</title>
    <link href="http://www.carefree0910.com/posts/707464b/"/>
    <id>http://www.carefree0910.com/posts/707464b/</id>
    <published>2017-04-25T16:06:50.000Z</published>
    <updated>2017-04-25T16:18:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>我们前面提到过 Bagging 的数学基础是 Bootstrap 理论、但还没有讲 Boosting 的数学基础。本节拟打算直观地阐述 Boosting 族的代表算法——AdaBoost 算法的解释，由于具体的推导相当繁琐，相关的细节我们会放在<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>里面说明</p>
<a id="more"></a>
<p>首先将结论给出：AdaBoost 算法是前向分步算法的特例，AdaBoost 模型等价于损失函数为指数函数的加法模型</p>
<p>其中，加法模型的定义是直观且熟悉的：</p>
<script type="math/tex; mode=display">
f\left( x \right) = \sum_{k = 1}^{M}{\alpha_{k}g(x;\Theta_{k})}</script><p>这里的<script type="math/tex">g(x;\Theta_{k})</script>为基函数，<script type="math/tex">\alpha_{k}</script>是基函数的权重，<script type="math/tex">\Theta_{k}</script>是基函数的参数。显然的是，我们的 AdaBoost 算法的最后一步生成的模型正是这么一个加法模型</p>
<p>而所谓的前向分步算法，就是从前向后、一步一步地学习加法模型中的每一个基函数及其权重而非将<script type="math/tex">f(x)</script>作为一个整体来训练，这也正是 AdaBoost 的思想</p>
<p>如果此时需要最小化的损失函数是指数损失函数<script type="math/tex">L\left( y,f\left( x \right) \right) = \exp\left\lbrack - yf\left( x \right) \right\rbrack</script>的话，通过一系列的数学推导后可以证明、此时的加法模型确实等价于 AdaBoost 模型</p>
<p>可能大家会觉得这里面有一些别扭：为什么一个实现起来非常简便的模型，它背后的数学原理却如此复杂？事实上有趣的是，AdaBoost 是为数不多的、先有算法后有解释的模型。也就是说，是先有了 AdaBoost 这个东西，然后数学家们看到它的表现非常好之后、才开始绞尽脑汁并想出了一套适用于 AdaBoost 的数学理论。更有意思的是，该数学理论并非毫无意义：在 AdaBoost 的回归问题中，就可以用前向分步算法的理论、将每一步的训练转化为了拟合当前模型的残差、从而简化了训练步骤。我们可以简单地叙述一下其原理：</p>
<p>加法模型的等价叙述为</p>
<script type="math/tex; mode=display">
f_{k + 1}\left( x \right) = f_{k}\left( x \right) + g_{k + 1}(x;\Theta_{k + 1})</script><p>其中<script type="math/tex">g_{k + 1}</script>为第<script type="math/tex">k + 1</script>步的基函数（亦即 AdaBoost 中的弱分类器），<script type="math/tex">\Theta_{k + 1}</script>为其参数。当采用平方误差损失函数<script type="math/tex">L\left( y,f\left( x \right) \right) = {\lbrack y - f\left( x \right)\rbrack}^{2}</script>时，可知第<script type="math/tex">k + 1</script>步的损失变为：</p>
<script type="math/tex; mode=display">
L = {\left\lbrack y - f_{k + 1}\left( x \right) \right\rbrack^{2} = \left\lbrack y - f_{k}\left( x \right) - g_{k + 1} \right\rbrack}^{2} = \left\lbrack r_{k}(x) - g_{k + 1}(x) \right\rbrack^{2}</script><p>其中<script type="math/tex">r_{k}(x) = y - f_{k}(x)</script>是第<script type="math/tex">k</script>步模型的残差。</p>
<p>从上式可以看出在第<script type="math/tex">k + 1</script>步时，为了最小化损失<script type="math/tex">L</script>，只需让当前的基函数<script type="math/tex">g_{k + 1}</script>拟合当前模型的残差<script type="math/tex">r_{k}</script>即可，这就完成了 AdaBoost 回归问题的转化。比较具有代表性的是回归问题的提升树算法，它正是利用了以上叙述的转化技巧来进行模型训练的</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们前面提到过 Bagging 的数学基础是 Bootstrap 理论、但还没有讲 Boosting 的数学基础。本节拟打算直观地阐述 Boosting 族的代表算法——AdaBoost 算法的解释，由于具体的推导相当繁琐，相关的细节我们会放在&lt;a href=&quot;/posts/613bbb2f/&quot; title=&quot;相关数学理论&quot;&gt;相关数学理论&lt;/a&gt;里面说明&lt;/p&gt;
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>集成模型的性能分析</title>
    <link href="http://www.carefree0910.com/posts/fb0d2f02/"/>
    <id>http://www.carefree0910.com/posts/fb0d2f02/</id>
    <published>2017-04-25T15:51:10.000Z</published>
    <updated>2017-04-25T16:11:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>正如前文所说，在实现完 AdaBoost 框架后，我们需要先用 sklearn 中的分类器进行检验、然后再用我们前两章实现的模型进行对比实验。检验的步骤就不在这里详述（毕竟只是一些调试的活），我们在此仅展示在随机森林模型和经过检验的 AdaBoost 模型上进行的一系列的分析</p>
<p>直观起见，我们先采用二维的数据进行实验、并通过可视化来加深对随机森林和 AdaBoost 的理解，然后再用蘑菇数据集做比较贴近现实的实验。为讨论方便，我们一律采用决策树作为 AdaBoost 的弱分类器（亦即采用提升树模型进行讨论）、其强度可以通过调整其最深层数来控制。我们可以利用<code>DataUtil</code>类来生成或获取原始数据集，其完整代码可参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py" target="_blank" rel="external">这里</a>、生成数据集的代码则会在前三小节分别放出</p>
<p>对于二维数据，我们拟打算使用三种数据集来进行评估：</p>
<ul>
<li>随机数据集。该数据集主要用于直观地感受模型的分类能力</li>
<li>异或数据集。该数据集主要用于直观地理解：<ul>
<li>集成模型正则化的能力</li>
<li>为何说 AdaBoost 不要选用分类能力太强的弱分类器</li>
</ul>
</li>
<li>螺旋线数据集，主要用于直观认知随机森林和提升树的不足</li>
</ul>
<a id="more"></a>
<h1 id="随机数据集上的表现"><a href="#随机数据集上的表现" class="headerlink" title="随机数据集上的表现"></a>随机数据集上的表现</h1><p>生成随机数据集的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_random</span><span class="params">(size=<span class="number">100</span>)</span>:</span></div><div class="line">    xy = np.random.rand(size, <span class="number">2</span>)</div><div class="line">    z = np.random.randint(<span class="number">2</span>, size=size)</div><div class="line">    <span class="comment"># 注意：我们的AdaBoost框架要求类别空间为(-1,+1)</span></div><div class="line">    z[z == <span class="number">0</span>] = <span class="number">-1</span></div><div class="line">    <span class="keyword">return</span> xy, z</div></pre></td></tr></table></figure>
<p>随机森林在随机数据集上的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p1.png" alt="p1.png" title="">
<p>左图为包含 1 棵 CART 树的随机森林，准确率为 78.0%；右图则为包含 10 棵 CART 树的随机森林，准确率为 93.0%。如果将树的数量继续往上抬、达到 100%准确率并非难事。比如，包含 5 0棵 CART 树的随机森林的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p2.png" alt="随机数据集上准确率为 100%的随机森林" title="随机数据集上准确率为 100%的随机森林">
<p>提升树（弱模型为决策树的 AdaBoost）在随机数据集上的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p3.png" alt="p3.png" title="">
<p>左图为包含 1 棵 CART 树的 AdaBoost，准确率为 93.0%；右图则为包含 10 棵 CART 树的 AdaBoost，准确率为 99.0%</p>
<h1 id="异或数据集上的表现"><a href="#异或数据集上的表现" class="headerlink" title="异或数据集上的表现"></a>异或数据集上的表现</h1><p>这里主要是想说明随机森林和提升树正则化的效果。从直观上来说，由于随机森林的理论基础是 Bootstrap、所以自然是包含越多树越好；至于 AdaBoost，可以想象它会对难以分类的数据特别在意、从而导致如下两种可能的结果：</p>
<ul>
<li>太过注重噪声，导致过拟合</li>
<li>专注于类似于下一个系列要讲的 SVM 中的“支持向量”，从而达到正则化</li>
</ul>
<p>事实上正如之前提到过的，即使 AdaBoost 在某一步迭代时、所得的模型在训练集上的加权错误率已经达到了 0，继续进行训练仍然可以使模型进一步提升（因为单个模型的正确率没有那么高、从而能使模型继续专注于“支持向量”。所谓支持向量、可以暂时直观地理解为“非常重要的”样本）。为说明这一点，我们可以比较同一数据集上、同样使用最深层数为 3 层的决策树作为弱分类器时、两种不同训练策略在异或数据集上的表现。为了比较准确地衡量正则化能力，我们需要进行交叉验证。、</p>
<p>生成异或数据集的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_xor</span><span class="params">(size=<span class="number">100</span>)</span>:</span></div><div class="line">    x = np.random.randn(size)</div><div class="line">    y = np.random.randn(size)</div><div class="line">    z = np.ones(size)</div><div class="line">    z[x * y &lt; <span class="number">0</span>] = <span class="number">-1</span></div><div class="line">    <span class="keyword">return</span> np.c_[x, y].astype(np.float32), z</div></pre></td></tr></table></figure>
<p>随机森林在异或数据集上的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p4.png" alt="p4.png" title="">
<p><strong><em>注意：该异或数据集和上一章用到的异或数据集是同一个数据集，感兴趣的读者可以进行一些对比</em></strong></p>
<p>左图为包含 1 棵 CART 树的随机森林，准确率为 93.0%；右图则为包含 10 棵 CART 树的随机森林，准确率为 98.0%。虽说右图中随机森林的表现已经足够好，由前文讨论可知、我们应该尝试训练一个更复杂的随机森林来看看其正则化能力。比如，包含 1000 棵 CART 树的随机森林的表现如下图所示：</p>
<img src="/posts/fb0d2f02/p5.png" alt="p5.png" title="">
<p>仔细观察决策边界，可以发现它会倾向于画在使得样本和边界“间隔较大”的地方。关于“间隔”的详细讨论会放在下一个系列，这里只需直观地感受一下即可</p>
<p>对于提升树，首先看一下不提前停止训练时的表现。为更好地说明问题，这里我们换了一个异或数据集来进行分析：</p>
<img src="/posts/fb0d2f02/p6.png" alt="p6.png" title="">
<p>此时在测试数据集上的正确率为 97.0%。然后看当模型在训练集上错误率足够小就马上停止训练时的表现：</p>
<img src="/posts/fb0d2f02/p7.png" alt="p7.png" title="">
<p>此时在测试数据集上的正确率为 94.0%</p>
<p>当然，正如前面所说，事实上确实有论文（G. Ratsch et al. ML, 2001）给出了 AdaBoost 会很快就过拟合的例子。但总体而言，笔者认为 AdaBoost 在正则化这一方面的表现还是相当优异的</p>
<p>由前面的诸多讨论可以得知，AdaBoost 的正则化能力是来源于各个弱分类器的“分而治之”，那么如果使用分类能力强的弱分类器会有什么结果呢？下面就放出当选用不限制层数的决策树作为弱模型的、异或数据集上的表现，相信会带来很好的直观：</p>
<img src="/posts/fb0d2f02/p8.png" alt="p8.png" title="">
<p>此时在测试数据集上的正确率为 90.0%。值得一提的是，用单独的决策树做出来的效果和上图的效果几乎完全一致。换句话说、此时使用 AdaBoost 没有太大的意义</p>
<h1 id="螺旋数据集上的表现"><a href="#螺旋数据集上的表现" class="headerlink" title="螺旋数据集上的表现"></a>螺旋数据集上的表现</h1><p>随机森林和提升树虽然确实都相当强大、但它同样具有其基本组成单元——决策树所具有的某些缺点。比如说，它们在处理连续性比较强的数据时可能会有些吃力、因为它们的决策边界一般而言都是“不太光滑”的。下面我们就统一使用个体决策树不做层数限制的随机森林和提升树以及螺旋线数据集作为样例来进行说明</p>
<p>生成螺旋数据集的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_spin</span><span class="params">(size=<span class="number">30</span>)</span>:</span></div><div class="line">    xs = np.zeros((size * <span class="number">4</span>, <span class="number">2</span>), dtype=np.float32)</div><div class="line">    ys = np.zeros(size * <span class="number">4</span>, dtype=np.int8)</div><div class="line">    <span class="comment"># 根据螺旋线在极坐标中的公式、生成四条螺旋线</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</div><div class="line">        ix = range(size * i, size * (i + <span class="number">1</span>))</div><div class="line">        <span class="comment"># 去掉原点以避免出现原点同时从属于两类的不合理情况</span></div><div class="line">        r = np.linspace(<span class="number">0.0</span>, <span class="number">1</span>, size + <span class="number">1</span>)[<span class="number">1</span>:]</div><div class="line">        t = np.linspace(<span class="number">2</span> * i * pi / <span class="number">4</span>, <span class="number">2</span> * (i + <span class="number">4</span>) * pi / <span class="number">4</span>, size) + np.random.random(</div><div class="line">            size=size) * <span class="number">0.1</span></div><div class="line">        xs[ix] = np.c_[r * np.sin(t), r * np.cos(t)]</div><div class="line">        ys[ix] = <span class="number">2</span> * (i % <span class="number">2</span>) - <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> xs, ys</div></pre></td></tr></table></figure>
<p>随机森林和提升树在其上的表现分别如下两张图所示：</p>
<img src="/posts/fb0d2f02/p9.png" alt="p9.png" title="">
<img src="/posts/fb0d2f02/p10.png" alt="p10.png" title="">
<p>上面两组图的左边都是包含 10 棵 CART 树的模型、右边都是包含 1000 棵 CART 树的模型，准确率则都是为 100.0%。可以看到，虽然它们都确实能够将大致的趋势给描述出来、但是决策边界相对而言都是“直来直去”的，这一点要比支持向量机、神经网络等模型的训练出来的结果要差不少。总之，决策树那使用二类问题的解决方案来处理连续型特征的做法、导致了随机森林和提升树在处理连续特征上的一些不足</p>
<h1 id="蘑菇数据集上的表现"><a href="#蘑菇数据集上的表现" class="headerlink" title="蘑菇数据集上的表现"></a>蘑菇数据集上的表现</h1><p>目前为止我们对二维数据上的测试做了比较详尽的说明，接下来我们不妨拿蘑菇数据集来测试一下我们的模型在真实数据下的表现；鉴于该数据集比较简单、我们只使用 100 个样本进行训练并用剩余的 8000 多个样本进行测试。为了直观感受模型的分类能力，我们可以画出当个体模型为 CART 决策树桩时、两种集成模型在测试集上的准确率随训练迭代次数变化而变化的曲线：</p>
<img src="/posts/fb0d2f02/p11.png" alt="p11.png" title="">
<p>其中蓝线是随机森林的训练曲线、绿线是提升树的训练曲线。这个结果是符合直观的，毕竟从个体模型来讲，引入了随机性的、随机森林中的决策树桩要比提升树中正常的决策树桩要弱，所以提升树的收敛速度理应比随机森林的要快；此外，由于随机森林和提升树相比、受个体模型分类能力的影响更大、我们采用的又是 CART 决策树桩这种相当弱的个体模型，所以随机森林收敛后的表现也要比提升树收敛后的表现要差</p>
<p>不过需要指出的是，当我们取消个体 CART 决策树的层数限制时，虽然随机森林的收敛速度仍会比提升树的收敛速度慢、但是收敛后的表现却很有可能比提升树收敛后的表现要好。这是因为取消了层数限制的决策树是相当强力的模型，而且：</p>
<ul>
<li>一方面正如刚刚所说的，随机森林受个体模型的分类能力影响较大、所以取消个体树的层数限制后、随机森林的分类能力自然大大增强</li>
<li>另一方面则如之前所讨论的，具有较强分类能力的个体模型与 AdaBoost 的原理可能不太兼容，这就使得 AdaBoost 本身的优势被抑制了</li>
</ul>
<p>取消层数限制后重复上述实验，此时两种集成模型的训练曲线如下图所示：</p>
<img src="/posts/fb0d2f02/p12.png" alt="p12.png" title="">
<p>可能观众老爷们已经发现，取消层数限制后的提升树似乎还没有取消限制之前的提升树的表现好；事实上，由于我们只用了 100 个样本来进行训练，所以容易想象、取消限制后的提升树将会产生比较严重的过拟合。可以把取消层数限制前后的训练曲线放在一起来进行直观对比，结果如下图所示：</p>
<img src="/posts/fb0d2f02/p13.png" alt="p13.png" title="">
<p>其中蓝线是个体模型为 CART 决策树桩时的训练曲线、绿线是个体模型为正常 CART 决策树时的训练曲线</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;正如前文所说，在实现完 AdaBoost 框架后，我们需要先用 sklearn 中的分类器进行检验、然后再用我们前两章实现的模型进行对比实验。检验的步骤就不在这里详述（毕竟只是一些调试的活），我们在此仅展示在随机森林模型和经过检验的 AdaBoost 模型上进行的一系列的分析&lt;/p&gt;
&lt;p&gt;直观起见，我们先采用二维的数据进行实验、并通过可视化来加深对随机森林和 AdaBoost 的理解，然后再用蘑菇数据集做比较贴近现实的实验。为讨论方便，我们一律采用决策树作为 AdaBoost 的弱分类器（亦即采用提升树模型进行讨论）、其强度可以通过调整其最深层数来控制。我们可以利用&lt;code&gt;DataUtil&lt;/code&gt;类来生成或获取原始数据集，其完整代码可参见&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py&quot;&gt;这里&lt;/a&gt;、生成数据集的代码则会在前三小节分别放出&lt;/p&gt;
&lt;p&gt;对于二维数据，我们拟打算使用三种数据集来进行评估：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;随机数据集。该数据集主要用于直观地感受模型的分类能力&lt;/li&gt;
&lt;li&gt;异或数据集。该数据集主要用于直观地理解：&lt;ul&gt;
&lt;li&gt;集成模型正则化的能力&lt;/li&gt;
&lt;li&gt;为何说 AdaBoost 不要选用分类能力太强的弱分类器&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;螺旋线数据集，主要用于直观认知随机森林和提升树的不足&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>AdaBoost 算法</title>
    <link href="http://www.carefree0910.com/posts/f5f50863/"/>
    <id>http://www.carefree0910.com/posts/f5f50863/</id>
    <published>2017-04-25T15:31:03.000Z</published>
    <updated>2017-04-25T15:50:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>由前文的讨论可知，问题的关键主要在如下两点：</p>
<ul>
<li>如何根据弱模型的表现更新训练集的权重</li>
<li>如何根据弱模型的表现决定弱模型的话语权</li>
</ul>
<p>我们接下来就看看 AdaBoost 算法是怎样解决上述两个问题的。事实上，能够将这两个问题的解决方案有机地糅合在一起、正是 AdaBoost 的巧妙之处之一</p>
<a id="more"></a>
<h1 id="AdaBoost-算法陈述"><a href="#AdaBoost-算法陈述" class="headerlink" title="AdaBoost 算法陈述"></a>AdaBoost 算法陈述</h1><p>不失一般性、我们以二类分类问题来进行讨论，易知此时我们的弱模型、强模型和最终模型为弱分类器、强分类器和最终分类器。再不妨假设我们现在有的是一个二类分类的训练数据集：</p>
<script type="math/tex; mode=display">
D = \{\left( x_{1},y_{1} \right),\left( x_{2},y_{2} \right),\ldots,(x_{n},\ y_{n})\}</script><p>其中，每个样本点都是由实例<script type="math/tex">x_{i}</script>和类别<script type="math/tex">y_{i}</script>组成、且：</p>
<script type="math/tex; mode=display">
x_{i} \in X \subseteq \mathbb{R}^{n}\ ;y_{i} \in Y = \{ - 1,\  + 1\}</script><p>这里的<script type="math/tex">X</script>是样本空间、<script type="math/tex">Y</script>是类别空间。AdaBoost 会利用如下的步骤、从训练数据中训练出一系列的弱分类器、然后把这些弱分类器集成为一个强分类器：</p>
<ol>
<li><strong>输入</strong>：训练数据集（包含 N 个数据）、弱学习算法及对应的弱分类器、迭代次数 M</li>
<li><strong>过程</strong>：<ol>
<li>初始化训练数据的权值分布  <script type="math/tex; mode=display">
W_{0} = (w_{01},\ldots,w_{0N})</script></li>
<li>对<script type="math/tex">k = 0,1,\ldots,\ M - 1</script>：<ol>
<li>使用权值分布为<script type="math/tex">W_{k}</script>的训练数据集训练弱分类器  <script type="math/tex; mode=display">g_{k + 1}(x)$$：$$X \rightarrow \{ - 1,\  + 1\}</script></li>
<li>计算<script type="math/tex">g_{k + 1}(x)</script>在训练数据集上的加权错误率  <script type="math/tex; mode=display">
e_{k + 1} = \sum_{i = 1}^{N}{w_{\text{ki}}I(g_{k + 1}\left( x_{i} \right) \neq y_{i})}</script></li>
<li>根据加权错误率计算<script type="math/tex">g_{k + 1}(x)</script>的“话语权”  <script type="math/tex; mode=display">
\alpha_{k + 1} = \frac{1}{2}\ln\frac{1 - e_{k + 1}}{e_{k + 1}}</script></li>
<li>根据<script type="math/tex">g_{k + 1}(x)</script>的表现更新训练数据集的权值分布：被<script type="math/tex">g_{k + 1}\left( x \right)</script>误分的样本（<script type="math/tex">y_{i}g_{k + 1}\left( x_{i} \right) < 0</script>的样本）要相对地（以<script type="math/tex">e^{\alpha_{k + 1}}</script>为比例地）增大其权重，反之则要（以<script type="math/tex">e^{- \alpha_{k + 1}}</script>为比例地）减少其权重  <script type="math/tex; mode=display">
w_{k + 1,i} = \frac{w_{\text{ki}}}{Z_{k}} \cdot exp( - \alpha_{k + 1}y_{i}g_{k + 1}(x_{i}))</script><script type="math/tex; mode=display">
W_{k + 1} = (w_{k + 1,1},\ldots,w_{k + 1,N})</script>这里的<script type="math/tex">Z_{k}</script>是规范化因子  <script type="math/tex; mode=display">
Z_{k} = \sum_{i = 1}^{N}{w_{\text{ki}} \cdot exp( - \alpha_{k + 1}y_{i}g_{k + 1}(x_{i}))}</script>它的作用是将<script type="math/tex">W_{k + 1}</script>归一化成为一个概率分布</li>
</ol>
</li>
<li>加权集成弱分类器  <script type="math/tex; mode=display">
f\left( x \right) = \sum_{k = 1}^{M}{\alpha_{k}g_{k}(x)}</script></li>
</ol>
</li>
<li><strong>输出</strong>：最终分类器<script type="math/tex">g(x)</script>  <script type="math/tex; mode=display">
g\left( x \right) = sign\left( f\left( x \right) \right) = \text{sign}\left( \sum_{k = 1}^{M}{\alpha_{k}g_{k}\left( x \right)} \right)</script></li>
</ol>
<p><strong><em>注意：2.2.2 步骤得到的加权错误率如果足够小的话，可以考虑提前停止训练，但这样做往往不是最合理的选择（这点会在后文进行模型性能分析时进行较详细的说明）</em></strong></p>
<p>我们在分配弱分类器的话语权时用到了一个公式：<script type="math/tex">\alpha_{k + 1} = \frac{1}{2}\ln\frac{1 - e_{k + 1}}{e_{k + 1}}</script>。在该公式中，话语权<script type="math/tex">\alpha_{k + 1}</script>会随着加权错误率<script type="math/tex">e_{k + 1} \in \lbrack 0,\ 1\rbrack</script>的增大而减小。它们之间的函数关系如下图所示：</p>
<img src="/posts/f5f50863/p1.png" alt="p1.png" title="">
<p>大多数情况我们训练出来的弱分类器的<script type="math/tex">e_{k} < 0.5</script>、对应着的是上图左半边的部分；不过即使我们的弱分类器非常差、以至于<script type="math/tex">e_{k} > 0.5</script>，由于此时<script type="math/tex">\alpha_{k} < 0</script>、亦即我们知道该分类器的表决应该反着来看、所以也不会出问题（有一种做法是如果训练到<script type="math/tex">e_{k} > 0.5</script>的话就停止训练，个人感觉也有道理）</p>
<h1 id="弱模型的选择"><a href="#弱模型的选择" class="headerlink" title="弱模型的选择"></a>弱模型的选择</h1><p>看到这里，观众老爷们可能会产生这么一个疑问：如果我们不拘泥于对弱模型进行提升、转而对强模型或比较强的弱模型进行提升的话，会不会提升出更好的模型呢？从 Boosting 的思想来看、需要指出的是：用 Boosting 进行提升的弱模型的学习能力不宜太强，否则使用 Boosting 就没有太大的意义、甚至从原理上不太兼容。直观地说，Boosting 是为了让各个弱模型专注于“某一方面”、最后加权表决，如果使用了较强的弱模型，可能一个弱模型就包揽了好几方面，最后可能反而会模棱两可、起不到“提升”的效果。而且从迭代的角度来说，可以想象：如果使用较强弱模型的话，可能第一次产生的模型就已经达到“最优”、从而使得模型没有“提升空间”</p>
<p><strong><em>注意：虽然笔者认为在 Boosting 中的弱模型就应该选择足够弱的模型，但确实亦有对强模型（如核 SVM）应用 Boosting 也很好的说法。详细而严谨的讨论会牵扯大量的数学理论、这里就不详细展开了</em></strong></p>
<p>可能观众老爷们此时又会产生一个新的疑问：如果说 Boosting 中的弱模型不宜太强的话，是不是说 Bagging 中的个体模型也不宜太强呢？需要指出的是，虽然从理论上来说使用弱模型进行集成就已足以获得一个相当不错的最终模型，但使用较强的模型来进行集成从原理上是不太矛盾的。考虑到不同的场合，有时确实可以选用较强的模型来作为个体模型</p>
<p>那么所谓的不太强的弱模型大概是个什么东西呢？一个比较直观的例子就是限制层数的决策树。极端的情况就是限定它只能有一层、亦即上一章我们提到过的“决策树桩”，对应的进行了提升后的模型就是相当有名的提升树（Boosting Tree），它被认为是统计学习中性能最好的方法之一、既可以用来做分类也可以拿来做回归，是个相当强力的模型</p>
<h1 id="AdaBoost-的实现"><a href="#AdaBoost-的实现" class="headerlink" title="AdaBoost 的实现"></a>AdaBoost 的实现</h1><p>从第一节的算法讲解其实可以看出，虽然 AdaBoost 算法本身很不平凡，但它给出的步骤都是相当便于实现的，基本上一个步骤就对应着 Python 里面的一行代码。在实现的过程中，困难之处可能主要在于如何让实现出来的 AdaBoost 框架易于扩展并具有方便调用的接口，而不在于实现算法本身。同时，为了能够更好地理解 AdaBoost 算法，我们需要对其性能作一系列的分析</p>
<p>由于 AdaBoost 是一个用于提升弱模型的算法，所以我们整体的实现思路大致是（不失一般性、我们先讨论二类分类问题）：</p>
<ul>
<li>搭建 AdaBoost 框架</li>
<li>使用 sklearn 中的分类器对框架的正确性进行检验</li>
<li>使用前两章实现的分类器进行对比实验</li>
</ul>
<p>所以我们要先把 AdaBoost 框架实现出来。为此，先来看 AdaBoost 框架的初始化步骤（其中<a href="https://github.com/carefree0910/MachineLearning/tree/master/_SKlearn" target="_blank" rel="external">_SKlearn</a>是我对 sklearn 中的模型做了一定程度的拓展后的模型包）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</div><div class="line"><span class="comment"># 导入我们之前实现的朴素贝叶斯模型和决策树模型</span></div><div class="line"><span class="keyword">from</span> b_NaiveBayes.Vectorized.MultinomialNB <span class="keyword">import</span> MultinomialNB</div><div class="line"><span class="keyword">from</span> b_NaiveBayes.Vectorized.GaussianNB <span class="keyword">import</span> GaussianNB</div><div class="line"><span class="keyword">from</span> c_CvDTree.Tree <span class="keyword">import</span> *</div><div class="line"><span class="keyword">from</span> _SKlearn.NaiveBayes <span class="keyword">import</span> *</div><div class="line"><span class="keyword">from</span> _SKlearn.Tree <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdaBoost</span>:</span></div><div class="line">    <span class="comment"># 弱分类器字典，如果想要测试新的弱分类器的话、只需将其加入该字典即可</span></div><div class="line">    _weak_clf = &#123;</div><div class="line">        <span class="string">"SKMNB"</span>: SKMultinomialNB,</div><div class="line">        <span class="string">"SKGNB"</span>: SKGaussianNB,</div><div class="line">        <span class="string">"SKTree"</span>: SKTree,</div><div class="line"></div><div class="line">        <span class="string">"MNB"</span>: MultinomialNB,</div><div class="line">        <span class="string">"GNB"</span>: GaussianNB,</div><div class="line">        <span class="string">"ID3"</span>: ID3Tree,</div><div class="line">        <span class="string">"C45"</span>: C45Tree,</div><div class="line">        <span class="string">"Cart"</span>: CartTree</div><div class="line">    &#125;</div><div class="line">    <span class="string">"""</span></div><div class="line">        AdaBoost框架的朴素实现</div><div class="line">        使用的弱分类器需要有如下两个方法：</div><div class="line">            1) 'fit'      方法，它需要支持输入样本权重</div><div class="line">            2) 'predict'  方法, 它用于返回预测的类别向量</div><div class="line">        初始化结构</div><div class="line">        self._clf：记录弱分类器名称的变量</div><div class="line">        self._clfs：记录弱分类器的列表</div><div class="line">        self._clfs_weights：记录弱分类器“话语权”的列表</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self._clf, self._clfs, self._clfs_weights = <span class="string">""</span>, [], []</div></pre></td></tr></table></figure>
<p>接下来就是训练和预测部分的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight=None, clf=None, epoch=<span class="number">10</span>, eps=<span class="number">1e-12</span>, **kwargs)</span>:</span></div><div class="line">    <span class="comment"># 默认使用10个CART决策树桩作为弱分类器</span></div><div class="line">    <span class="keyword">if</span> clf <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> AdaBoost._weak_clf[clf] <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        clf = <span class="string">"Cart"</span></div><div class="line">        kwargs = &#123;<span class="string">"max_depth"</span>: <span class="number">1</span>&#125;</div><div class="line">    self._clf = clf</div><div class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        sample_weight = np.ones(len(y)) / len(y)</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        sample_weight = np.array(sample_weight)</div><div class="line">    <span class="comment"># AdaBoost算法的主循环，epoch为迭代次数</span></div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</div><div class="line">        <span class="comment"># 根据样本权重训练弱分类器</span></div><div class="line">        tmp_clf = AdaBoost._weak_clf[clf](**kwargs)</div><div class="line">        tmp_clf.fit(x, y, sample_weight)</div><div class="line">        <span class="comment"># 调用弱分类器的predict方法进行预测</span></div><div class="line">        y_pred = tmp_clf.predict(x)</div><div class="line">        <span class="comment"># 计算加权错误率；考虑到数值稳定性，在边值情况加了一个小的常数</span></div><div class="line">        em = min(max((y_pred != y).dot(self._sample_weight[:, <span class="keyword">None</span>])[<span class="number">0</span>], eps), <span class="number">1</span> - eps)</div><div class="line">        <span class="comment"># 计算该弱分类器的“话语权”</span></div><div class="line">        am = <span class="number">0.5</span> * log(<span class="number">1</span> / em - <span class="number">1</span>)</div><div class="line">        <span class="comment"># 更新样本权重并利用deepcopy将该弱分类器记录在列表中</span></div><div class="line">        sample_weight *= np.exp(-am * y * y_pred)</div><div class="line">        sample_weight /= np.sum(sample_weight)</div><div class="line">        self._clfs.append(deepcopy(tmp_clf))</div><div class="line">        self._clfs_weights.append(am)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></div><div class="line">    x = np.atleast_2d(x)</div><div class="line">    rs = np.zeros(len(x))</div><div class="line">    <span class="comment"># 根据各个弱分类器的“话语权”进行决策</span></div><div class="line">    <span class="keyword">for</span> clf, am <span class="keyword">in</span> zip(self._clfs, self._clfs_weights):</div><div class="line">        rs += am * clf.predict(x)</div><div class="line">    <span class="comment"># 将预测值大于0的判为类别1，小于0的判为类别-1</span></div><div class="line">    <span class="keyword">return</span> np.sign(rs)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由前文的讨论可知，问题的关键主要在如下两点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何根据弱模型的表现更新训练集的权重&lt;/li&gt;
&lt;li&gt;如何根据弱模型的表现决定弱模型的话语权&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们接下来就看看 AdaBoost 算法是怎样解决上述两个问题的。事实上，能够将这两个问题的解决方案有机地糅合在一起、正是 AdaBoost 的巧妙之处之一&lt;/p&gt;
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>随机森林算法</title>
    <link href="http://www.carefree0910.com/posts/c0a9c025/"/>
    <id>http://www.carefree0910.com/posts/c0a9c025/</id>
    <published>2017-04-25T12:42:41.000Z</published>
    <updated>2017-04-25T16:45:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>由前文讨论可知，我们在实现 RF 算法之前，需要先在决策树模型的生成过程中加一个参数、使得我们能够对特征选取加入随机性。这个过程相当平凡，下给出代码片段以进行粗略的说明。首先在<code>CvDBase</code>的<code>fit</code>方法中加入一个参数<code>feature_bound</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight=None, alpha=None, eps=<span class="number">1e-8</span>,</span></span></div><div class="line">    cv_rate=<span class="number">0.2</span>, train_only=False, feature_bound=None):</div></pre></td></tr></table></figure>
<p>然后在同一个方法里面、把这个参数传给<code>CvDNode</code>的<code>fit</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.root.fit(x_train, y_train, _train_weights, feature_bound, eps)</div></pre></td></tr></table></figure>
<p>在<code>CvDNode</code>的<code>fit</code>方法中，原始代码中有一个对可选特征空间的遍历：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> self.feats:</div></pre></td></tr></table></figure>
<p>根据参数<code>feature_bound</code>对它加入随机性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">feat_len = len(self.feats)</div><div class="line"><span class="comment"># 默认没有随机性</span></div><div class="line"><span class="keyword">if</span> feature_bound <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">    indices = range(<span class="number">0</span>, feat_len)</div><div class="line"><span class="keyword">elif</span> feature_bound == <span class="string">"log"</span>:</div><div class="line">    <span class="comment"># np.random.permutation(n)：将数组打乱后返回</span></div><div class="line">    indices = np.random.permutation(feat_len)[:max(<span class="number">1</span>, int(log2(feat_len)))]</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    indices = np.random.permutation(feat_len)[:feature_bound]</div><div class="line">tmp_feats = [self.feats[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices]</div><div class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> tmp_feats:</div></pre></td></tr></table></figure>
<p>然后要在同一个方法里面、把<code>feature_bound</code>传给<code>_gen_children</code>方法，而在<code>_gen_children</code>中、再把<code>feature_bound</code>传给子节点的<code>fit</code>方法即可</p>
<p>以上所有实现细节可参见<a href="https://github.com/carefree0910/MachineLearning/tree/master/c_CvDTree" target="_blank" rel="external">这里</a>中的 Tree.py 和 Node.py</p>
<p>有了这些准备，我们就可以来看看 RF 的算法陈述了（以分类问题为例）：</p>
<a id="more"></a>
<ol>
<li><strong>输入</strong>：训练数据集（包含 N 个数据）、决策树模型、迭代次数 M</li>
<li><strong>过程</strong>：<ol>
<li>对<script type="math/tex">j=1,2,...,M</script>：<ol>
<li>通过 Bootstrap 生成包含 N 个数据的数据集<script type="math/tex">D_k</script></li>
<li>利用<script type="math/tex">D_j</script>和输入的决策树模型进行训练，注意不用对训练好的决策树模型<script type="math/tex">g_j</script>进行剪枝。同时需要注意的是，在训练决策树的过程中、每一步的生成都要对特征的选取加入随机性</li>
</ol>
</li>
<li>对个体决策树进行简单组合。不妨用符号<script type="math/tex">\text{freq}(c_{k})</script>表示类别<script type="math/tex">c_{k}</script>在 M 个决策树模型的决策中出现的频率，那么：  <script type="math/tex; mode=display">
g\left( x \right) = \arg{\max_{c_k}{\text{freq}(c_{k})}}</script></li>
</ol>
</li>
<li><strong>输出</strong>：最终分类器<script type="math/tex">g(x)</script></li>
</ol>
<p>从算法即可看出随机森林算法的实现（在实现好决策树模型后）是相当平凡的，需要额外做的工作只有定义一个能够计算上述算法第 2.2 步中<script type="math/tex">\arg{\max_{c_k}{freq(c_{k})}}</script>的函数而已：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入我们自己实现的决策树模型</span></div><div class="line"><span class="keyword">from</span> c_CvDTree.Tree <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomForest</span><span class="params">(ClassifierBase)</span>:</span></div><div class="line">    <span class="comment"># 建立一个决策树字典，以便调用</span></div><div class="line">    _cvd_trees = &#123;</div><div class="line">        <span class="string">"id3"</span>: ID3Tree,</div><div class="line">        <span class="string">"c45"</span>: C45Tree,</div><div class="line">        <span class="string">"cart"</span>: CartTree</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        super(RandomForest, self).__init__()</div><div class="line">        self._trees = []</div><div class="line"></div><div class="line">    <span class="comment"># 实现计算的函数</span></div><div class="line"><span class="meta">    @staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">most_appearance</span><span class="params">(arr)</span>:</span></div><div class="line">        u, c = np.unique(arr, return_counts=<span class="keyword">True</span>)</div><div class="line">        <span class="keyword">return</span> u[np.argmax(c)]</div><div class="line"></div><div class="line">    <span class="comment"># 默认使用 10 棵 CART 树、默认 k = log(d)</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight=None, tree=<span class="string">"cart"</span>, epoch=<span class="number">10</span>, feature_bound=<span class="string">"log"</span>,</span></span></div><div class="line">            *args, **kwargs):</div><div class="line">        x, y = np.atleast_2d(x), np.array(y)</div><div class="line">        n_sample = len(y)</div><div class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(epoch):</div><div class="line">            tmp_tree = RandomForest._cvd_trees[tree](*args, **kwargs)</div><div class="line">            _indices = np.random.randint(n_sample, size=n_sample)</div><div class="line">            <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">                _local_weight = <span class="keyword">None</span></div><div class="line">            <span class="keyword">else</span>:</div><div class="line">                _local_weight = sample_weight[_indices]</div><div class="line">                _local_weight /= _local_weight.sum()</div><div class="line">            tmp_tree.fit(x[_indices], y[_indices],</div><div class="line">                sample_weight=_local_weight, feature_bound=feature_bound)</div><div class="line">            self._trees.append(deepcopy(tmp_tree))</div><div class="line"></div><div class="line">    <span class="comment"># 对个体决策树进行简单组合</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></div><div class="line">        _matrix = np.array([_tree.predict(x) <span class="keyword">for</span> _tree <span class="keyword">in</span> self._trees]).T</div><div class="line">        <span class="keyword">return</span> np.array([RandomForest.most_appearance(rs) <span class="keyword">for</span> rs <span class="keyword">in</span> _matrix])</div></pre></td></tr></table></figure>
<p>需要指出的是，<code>most_appearance</code>函数用到了 Numpy 中的<code>unique</code>方法、它和标准库<code>collections</code>中的<code>Counter</code>具有差不多的用法。举个小栗子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = np.array([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">"dcbabcd"</span>])</div><div class="line">np.unique(x, return_counts=<span class="keyword">True</span>)</div></pre></td></tr></table></figure>
<p>这两行代码会返回：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">(</div><div class="line">    array([<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>], dtype=<span class="string">'&lt;U1'</span>),</div><div class="line">    array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], dtype=int64)</div><div class="line">)</div></pre></td></tr></table></figure>
<p>换句话说，<code>unique</code>方法能够提取出一个 Numpy 数组中出现过的元素并对它们计数、同时输出的 Numpy 数组是经过排序的</p>
<p>以上就完成了一个简易可行的随机森林模型的实现，我们可以把对随机森林模型的评估与对 AdaBoost 的评估放在一起进行以便于对比、这里就先按下不表</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由前文讨论可知，我们在实现 RF 算法之前，需要先在决策树模型的生成过程中加一个参数、使得我们能够对特征选取加入随机性。这个过程相当平凡，下给出代码片段以进行粗略的说明。首先在&lt;code&gt;CvDBase&lt;/code&gt;的&lt;code&gt;fit&lt;/code&gt;方法中加入一个参数&lt;code&gt;feature_bound&lt;/code&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, x, y, sample_weight=None, alpha=None, eps=&lt;span class=&quot;number&quot;&gt;1e-8&lt;/span&gt;,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    cv_rate=&lt;span class=&quot;number&quot;&gt;0.2&lt;/span&gt;, train_only=False, feature_bound=None)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;然后在同一个方法里面、把这个参数传给&lt;code&gt;CvDNode&lt;/code&gt;的&lt;code&gt;fit&lt;/code&gt;方法：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;self.root.fit(x_train, y_train, _train_weights, feature_bound, eps)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;在&lt;code&gt;CvDNode&lt;/code&gt;的&lt;code&gt;fit&lt;/code&gt;方法中，原始代码中有一个对可选特征空间的遍历：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; feat &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; self.feats:&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;根据参数&lt;code&gt;feature_bound&lt;/code&gt;对它加入随机性：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;feat_len = len(self.feats)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 默认没有随机性&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; feature_bound &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    indices = range(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, feat_len)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;elif&lt;/span&gt; feature_bound == &lt;span class=&quot;string&quot;&gt;&quot;log&quot;&lt;/span&gt;:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# np.random.permutation(n)：将数组打乱后返回&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    indices = np.random.permutation(feat_len)[:max(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, int(log2(feat_len)))]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    indices = np.random.permutation(feat_len)[:feature_bound]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;tmp_feats = [self.feats[i] &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; indices]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; feat &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; tmp_feats:&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;然后要在同一个方法里面、把&lt;code&gt;feature_bound&lt;/code&gt;传给&lt;code&gt;_gen_children&lt;/code&gt;方法，而在&lt;code&gt;_gen_children&lt;/code&gt;中、再把&lt;code&gt;feature_bound&lt;/code&gt;传给子节点的&lt;code&gt;fit&lt;/code&gt;方法即可&lt;/p&gt;
&lt;p&gt;以上所有实现细节可参见&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/tree/master/c_CvDTree&quot;&gt;这里&lt;/a&gt;中的 Tree.py 和 Node.py&lt;/p&gt;
&lt;p&gt;有了这些准备，我们就可以来看看 RF 的算法陈述了（以分类问题为例）：&lt;/p&gt;
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>“集成”的思想</title>
    <link href="http://www.carefree0910.com/posts/7081b0ee/"/>
    <id>http://www.carefree0910.com/posts/7081b0ee/</id>
    <published>2017-04-25T11:51:31.000Z</published>
    <updated>2017-04-25T14:29:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文首先会介绍何谓“集成”、然后会介绍两种常见的集成学习方法：Bagging、AdaBoost 的基本定义。这些概念的背后有着深刻的数学理论，但是它们同时也拥有着很好的直观。获得对它们的直观有助于加深对各种模型的分类性能的理解、同时也有助于根据具体的数据集来挑选相应的、合适的模型来进行学习</p>
<a id="more"></a>
<h1 id="众擎易举"><a href="#众擎易举" class="headerlink" title="众擎易举"></a>众擎易举</h1><p>集成学习基于这样的思想：对于比较复杂的任务，综合许多人的意见来进行决策会比“一家独大”要更好。换句话说、就是通过适当的方式集成许多“个体模型”所得到的最终模型要比单独的“个体模型”的性能更优。我们可以通过下图来直观感知这个过程：</p>
<img src="/posts/7081b0ee/p1.png" alt="p1.png" title="">
<p>所以问题的关键转化为了两点：如何选择、生成弱分类器和如何对它们进行提升（集成）。在此基础上，通常有三种不同的思路：</p>
<ul>
<li>将不同类型的弱分类器进行提升</li>
<li>将相同类型但参数不同的弱分类器进行提升</li>
<li>将相同类型但训练集不同的弱分类器进行提升</li>
</ul>
<p>其中第一种思路的应用相对来说可能不太广泛，而第二、第三种思路则指导着两种常见的做法，这两种做法的区别主要体现在基本组成单元——弱分类器的生成方式：</p>
<p>第一种做法期望各个弱分类器之间依赖性不强、可以同时进行生成。这种做法又称并行方法，其代表为 Bagging，而 Bagging 一个著名的拓展应用便是本系列的主题之一——随机森林（Random Forest，常简称为 RF）。</p>
<p>第二种做法中弱分类器之间具有强依赖性、只能序列生成。这种做法又称串行方法，其代表为 Boosting，而 Boosting 族算法中的代表即是本系列的另一主题——AdaBoost</p>
<h1 id="Bagging-与随机森林"><a href="#Bagging-与随机森林" class="headerlink" title="Bagging 与随机森林"></a>Bagging 与随机森林</h1><p>Bagging 是 1996 年由 Breiman 提出的，它的思想根源是数理统计中非常重要的 Bootstrap 理论。Bootstrap 可以翻译成“自举”，它通过模拟的方法来逼近样本的概率分布函数。可以想象这样一个场景：现在有一个包含 N 个样本的数据集<script type="math/tex">X = \{ x_{1},\ldots,x_{N}\}</script>，这 N 个样本是由随机变量<script type="math/tex">x</script>独立生成的。我们想要研究<script type="math/tex">x</script>的均值估计<script type="math/tex">\bar{x} = \frac{1}{N}\sum_{i = 1}^{N}x_{i}</script>的统计特性（误差、方差等等），但由于研究统计特性是需要大量样本的、而数据集<script type="math/tex">X</script>只能给我们提供一个<script type="math/tex">\bar{x}</script>的样本，从而导致无法进行研究</p>
<p>在这种场景下，容易想到的一种解决方案是：通过<script type="math/tex">x</script>的分布生成出更多的数据集<script type="math/tex">X_{1},\ldots X_{M}</script>、每个数据集都包含 N 个样本。这 M 个数据集都能产生一个均值估计、从而就有了 M 个均值估计的样本。那么只要 M 足够大、我们就能研究<script type="math/tex">\bar{x}</script>的统计特性了</p>
<p>当然这种解决方案的一个最大的困难就是：我们并不知道<script type="math/tex">x</script>的真实分布。Bootstrap 就是针对这个困难提出了一个解决办法：通过不断地“自采样”来模拟随机变量真实分布生成的数据集。具体而言，Bootstrap 的做法是：</p>
<ul>
<li>从<script type="math/tex">X</script>中随机抽出一个样本（亦即抽出<script type="math/tex">x_1,...,x_N</script>的概率相同）</li>
<li>将该样本的拷贝放入数据集<script type="math/tex">X_j</script></li>
<li>将该样本放回<script type="math/tex">X</script>中</li>
</ul>
<p>以上三个步骤将重复 N 次、从而使得<script type="math/tex">X_{j}</script>中有 N 个样本。这个过程将对<script type="math/tex">j = 1,\ldots,M</script>都进行一遍、从而我们最终能得到 M 个含有 N 个样本的数据集<script type="math/tex">X_{1},\ldots X_{M}</script></p>
<p>简单来说的话、Bootstrap 其实就是一个有放回的随机抽样过程，所以原始数据<script type="math/tex">\{ x_{1},\ldots,x_{N}\}</script>中可能会在<script type="math/tex">X_{1},\ldots X_{M}</script>中重复出现、也有可能不出现在<script type="math/tex">X_{1},\ldots X_{M}</script>中。事实上，由于<script type="math/tex">X</script>中一个样本在 N 次采样中始终不被采到的概率为<script type="math/tex">\left( 1 - \frac{1}{N} \right)^{N}</script>、且：</p>
<script type="math/tex; mode=display">
\lim_{N\rightarrow\infty}\left( 1 - \frac{1}{N} \right)^{N} \rightarrow \frac{1}{e} \approx 0.368</script><p>所以在统计意义上可以认为、<script type="math/tex">X_{j}</script>中含有<script type="math/tex">X</script>中 63.2%的样本<script type="math/tex">\left( \forall j = 1,\ldots,M \right)</script></p>
<p>这种模拟的方法在理论上是具有最优性的。事实上、这种模拟的本质和经验分布函数对真实分布函数的模拟几乎一致：</p>
<ul>
<li>Bootstrap 以<script type="math/tex">\frac 1N</script>的概率、有放回地从<script type="math/tex">X</script>中抽取 N 个样本作为数据集、并以之估计真实分布生成的具有 N 个样本的数据集</li>
<li>经验分布函数则是在 N 个样本点上以每点的概率为<script type="math/tex">\frac 1N</script>作为概率密度函数、然后进行积分的函数</li>
</ul>
<p>经验分布函数的数学表达式为：</p>
<script type="math/tex; mode=display">
F_{N}\left( x \right) = \frac{1}{N}\sum_{i = 1}^{N}{I_{\left( - \infty,x \right\rbrack}(x_{i})}</script><p>可以看出、经验分布函数用到了频率估计概率的思想。用它来模拟真实分布函数是具有很好的优良性的，详细的讨论可参见<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a></p>
<p>知道 Bootstrap 是什么之后、我们就可以来看 Bagging 的具体定义了。Bagging 的全称是 Bootstrap Aggregating，其思想非常简单：</p>
<ul>
<li>用 Bootstrap 生成出 M 个数据集</li>
<li>用这 M 个数据集训练出 M 个弱分类器</li>
<li>最终模型即为这M个弱分类器的简单组合</li>
</ul>
<p>所谓简单组合就是：</p>
<ul>
<li>对于分类问题使用简单的投票表决</li>
<li>对于回归问题则进行简单的取平均</li>
</ul>
<p>简单组合虽说简单、其背后仍然是有数学理论支撑的。考虑二分类问题：</p>
<script type="math/tex; mode=display">
y \in \{ - 1, + 1\}</script><p>假设样本空间到类别空间的真实映射为<script type="math/tex">f</script>、我们得到的 M 个弱分类器模型<script type="math/tex">G_{1},\ldots,G_{M}</script>所对应的映射为<script type="math/tex">g_{1},\ldots,g_{M}</script>，那么简单组合下的最终模型对应的映射即为：</p>
<script type="math/tex; mode=display">
g\left( x \right) = \text{sign}\left( \sum_{j = 1}^{M}{g_{j}\left( x \right)} \right)</script><p>这里的 sign 是符号函数、满足：</p>
<script type="math/tex; mode=display">
\text{sign}\left( x \right) = \left\{ \begin{matrix}
 - 1,\ \ if\ x < 0 \\
 + 1,\ \ if\ x > 0 \\
\end{matrix} \right.\</script><p>其中，<script type="math/tex">\text{sign}(0)</script>则可为<script type="math/tex">-1</script>也可为<script type="math/tex">+1</script>，令其有 50%的概率输出<script type="math/tex">-</script>、<script type="math/tex">+1</script>也是可行的</p>
<p>如果我们此时假设每个弱分类器的错误率为<script type="math/tex">\epsilon</script>：</p>
<script type="math/tex; mode=display">
p\left( g_{i}\left( x \right) \neq f\left( x \right) \right) = \epsilon</script><p>如果我们假设弱分类器的错误率相互独立，那么由霍夫丁不等式（Hoeffding’s Inequality）可以得知：</p>
<script type="math/tex; mode=display">
p\left( G\left( x \right) \neq f\left( x \right) \right) = \sum_{j = 0}^{\left\lfloor \frac{M}{2} \right\rfloor}\left( \frac{M}{j} \right)\left( 1 - \epsilon \right)^{j}\epsilon^{M - j} \leq \exp\left( - \frac{1}{2}M\left( 1 - 2\epsilon \right)^{2} \right)</script><p>亦即最终模型的错误率随弱分类器的个数 M 的增加、将会以指数级下降并最终趋于 0</p>
<p>虽说这个结果看上去很振奋人心，但需要注意的是、我们做了一个非常强的关键假设：假设弱分类器的错误率相互独立。这可以说是不可能做到的，因为这些弱分类器想要解决的都是同一个问题、且使用的训练集也都源自于同一份数据集</p>
<p>但不管怎么说，以上的分析给了我们这样一个重要信息：弱分类器之间的“差异”似乎应该尽可能的大。基于此，结合 Bagging 的特点、我们可以得出这样一个结论：对于“不稳定”（或说对训练集敏感：若训练样本稍有改变，得到的从样本空间到类别空间的映射 g 就会产生较大的变化）的分类器，Bagging 能够显著地对其进行提升。这也是被大量实验结果所证实了的</p>
<p>正如前文提过的，Bagging 有一个著名的拓展应用叫“随机森林”，从名字就容易想到、它是当个体模型为决策树时的 Bagging 算法。不过需要指出的是，随机森林算法不仅对样本进行 Bootstrap 采样，对每个 Node 调用生成算法时都会随机挑选出一个可选特征空间的子空间作为该决策树的可选特征空间；同时，生成好个体决策树后不进行剪枝、而是保持原始的形式。换句话说、随机森林算法流程大致如下：</p>
<ul>
<li>用 Bootstrap 生成出 M 个数据集</li>
<li>用这 M 个数据集训练出 M 颗不进行后剪枝决策树，且在每颗决策树的生成过程中，每次对 Node 进行划分时、都从可选特征（比如说有 d 个）中随机挑选出 k 个（<script type="math/tex">k\le d</script>）特征，然后依信息增益的定义从这 k 个特征中选出信息增益最大的特征作为划分标准</li>
<li>最终模型即为这 M 个弱分类器的简单组合</li>
</ul>
<p><strong><em>注意：有一种说法是随机森林中的个体决策树模型只能使用 CART 树</em></strong></p>
<p>也就是说，除了和一般 Bagging 算法那样对样本进行随机采样以外、随机森林还对特征进行了某种意义上的随机采样。这样做的意义是直观的：通过对特征引入随机扰动，可以使个体模型之间的差异进一步增加、从而提升最终模型的泛化能力。而这个特征选取的随机性，恰恰被上述算法第二步中的参数k所控制：</p>
<ul>
<li>若<script type="math/tex">k=d</script>，那么训练出来的决策树和一般意义下的决策树别无二致、亦即特征选取这一部分不具有随机性</li>
<li>若<script type="math/tex">k=1</script>，那么生成决策树的每一步都是在随机选择属性、亦即特征选取的随机性达到最大</li>
</ul>
<p>Breiman 在提出随机森林算法的同时指出，一般情况下、推荐取<script type="math/tex">k=\log_2{d}</script></p>
<h1 id="PAC-框架与-Boosting"><a href="#PAC-框架与-Boosting" class="headerlink" title="PAC 框架与 Boosting"></a>PAC 框架与 Boosting</h1><p>虽然同属集成学习方法，但 Boosting 和 Bagging 的数学理论根基不尽相同：Boosting 产生于计算学习理论（Computational Learning Theory）[Valiant, 1984]。一般而言，如果只是应用机器学习的话、我们无需对它进行太多的了解（甚至可以说对它一知半解反而有害），所以本小节只打算对其最基本的概率近似正确（PAC）学习理论中的“可学习性（PAC Learnability）”进行简要的介绍</p>
<p>PAC 学习整体来说是一个比较纯粹的数学理论。有一种说法是、PAC 学习是统计学家研究机器学习的方式，它关心模型的可解释性、然而机器学习专家通常更关心模型的预测能力。这也正是为何说无需太过了解它，因为我们的目的终究不是成为统计的专家、而是更希望成为一个能够应用机器学习的人。不过幸运的是，虽然为了叙述 Boosting、PAC 学习中“可学习性”的概念难以避开，但其本身却是具有很直观的解释的。下面我们就来看看这个直观解释：</p>
<p>PAC 提出的一个主要的假设、就是它要求数据是从某个稳定的概率分布中产生。直观地说，就是样本在样本空间中的分布状况不能随时间的变化而变化、否则就失去了学习的意义（因为学习到的永远只是“某个时间”的分布，如果未知数据所处时间的分布状况和该时间数据的分布状况不同的话、模型就直接失效了）。然后所谓的PAC可学习性，就是看学习的算法是否能够在合理的时间（多项式时间）内、以足够高的概率输出一个错误率足够低的模型。由此，所谓的“强可学习”和“弱可学习”的概念就很直观了：</p>
<ul>
<li>若存在一个多项式算法可以学习出准确率很高的模型，则称为强可学习</li>
<li>若在在一个多项式算法可以学习但准确率仅仅略高于随机猜测，则称为弱可学习</li>
</ul>
<p><strong><em>注意：由于进行机器学习时、我们只能针对训练数据集进行学习、所以和真实情况相比肯定是有偏差的。这正是需要提出 PAC 可学习这个概念的原因之一</em></strong></p>
<p>虽然我们区分定义了这两个概念，不过神奇之处在于，这两个概念在PAC学习框架下是完全等价的[Schapire, 1990]。这意味着对于一个学习问题，只要我们找到了一个“弱学习算法”，就可以把它变成一个“强学习算法”。这当然是意义深刻的，因为往往比较粗糙的“弱学习算法”比较好找、而相对精确的“强学习算法”却难得一求</p>
<p>那么具体而言应该怎么做呢？这里就需要用到所谓的 Boosting（提升方法）了。提升方法可以定义为用于将由“弱学习算法”生成的“弱模型”、提升成和“强学习算法”所生成的“强模型”性能差不多的模型的方法，它的基本组成单元是许许多多的“弱模型”、然后通过某种手段把它们集成为最终模型。虽然该过程听上去和上一小节介绍的 Bagging 差不多、但它们的思想和背后的数学理论却有较大区别，加以辨析是有必要的</p>
<p>需要指出的是，Boosting 事实上是一族算法、该族算法有一个类似的框架：</p>
<ul>
<li>根据当前的数据训练出一个弱模型</li>
<li>根据该弱模型的表现调整数据的样本权重。具体而言：<ul>
<li>让该弱模型做错的样本在后续训练中获得更多的关注</li>
<li>让该弱模型做对的样本在后续训练中获得较少的关注</li>
</ul>
</li>
<li>最后再根据该弱模型的表现决定该弱模型的“话语权”、亦即投票表决时的“可信度”。自然、表现越好就越有话语权</li>
</ul>
<p>可以证明，当训练样本有无穷多时、Boosting 能让弱模型集成出一个对训练样本集的准确率任意高的模型。然而实际任务中训练样本当然不可能有无穷多，所以问题就转为了如何在固定的训练集上应用 Boosting 方法。而在 1996 年，Freund 和 Schapire 所提出的 AdaBoost（Adaptive Boosting）正是一个相当不错的解决方案，在理论和实验上均有优异的表现。虽然 AdaBoost 背后的理论深究起来可能会有些繁复、但它的思想并没有脱离 Boosting 族算法的那一套框架。值得一提的是、Boosting 还有一套比较有意思的解释方法；我们会在后文详细讨论其中的代表性算法——AdaBoost 的解释、这里就先按下不表</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文首先会介绍何谓“集成”、然后会介绍两种常见的集成学习方法：Bagging、AdaBoost 的基本定义。这些概念的背后有着深刻的数学理论，但是它们同时也拥有着很好的直观。获得对它们的直观有助于加深对各种模型的分类性能的理解、同时也有助于根据具体的数据集来挑选相应的、合适的模型来进行学习&lt;/p&gt;
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="综述" scheme="http://www.carefree0910.com/tags/%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>集成学习综述</title>
    <link href="http://www.carefree0910.com/posts/d8e97c87/"/>
    <id>http://www.carefree0910.com/posts/d8e97c87/</id>
    <published>2017-04-25T09:27:57.000Z</published>
    <updated>2017-04-25T11:50:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>目前为止我们已经讲过了若干的分类器了。从它们的复杂程度可以感受到，它们有些是比较“强”的、有些是比较“弱”的。这一章我们将会阐述所谓的“强”与“弱”的定义、它们之间的联系以及阐述如何将一个“弱分类器”通过集成学习来集成出一个“强分类器”。而由于集成学习有许多种具体的方法，我们会挑选出其中的随机森林和 AdaBoost 来作比较详细的说明</p>
<p>以下是目录：</p>
<ul>
<li><a href="/posts/7081b0ee/" title="“集成”的思想">“集成”的思想</a></li>
<li><a href="/posts/c0a9c025/" title="随机森林算法">随机森林算法</a></li>
<li><a href="/posts/f5f50863/" title="AdaBoost 算法">AdaBoost 算法</a></li>
<li><a href="/posts/fb0d2f02/" title="集成模型的性能分析">集成模型的性能分析</a></li>
<li><a href="/posts/707464b/" title="AdaBoost 算法的解释">AdaBoost 算法的解释</a></li>
<li><a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a></li>
<li><a href="/posts/48a0211a/" title="“集成学习”小结">“集成学习”小结</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前为止我们已经讲过了若干的分类器了。从它们的复杂程度可以感受到，它们有些是比较“强”的、有些是比较“弱”的。这一章我们将会阐述所谓的“强”与“弱”的定义、它们之间的联系以及阐述如何将一个“弱分类器”通过集成学习来集成出一个“强分类器”。而由于集成学习有许多种具体的方法，我
    
    </summary>
    
      <category term="集成学习" scheme="http://www.carefree0910.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="综述" scheme="http://www.carefree0910.com/tags/%E7%BB%BC%E8%BF%B0/"/>
    
      <category term="目录" scheme="http://www.carefree0910.com/tags/%E7%9B%AE%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>“决策树”小结</title>
    <link href="http://www.carefree0910.com/posts/88953f51/"/>
    <id>http://www.carefree0910.com/posts/88953f51/</id>
    <published>2017-04-23T02:43:07.000Z</published>
    <updated>2017-04-23T02:44:13.000Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>决策树是从直观上很好理解的模型，可以把它理解为一个划分规则的序列</li>
<li>决策树常用的生成算法包括：<ul>
<li>ID3 算法，它使用互信息作为信息增益的度量</li>
<li>C4.5 算法，它使用信息增益比作为信息增益的度量</li>
<li>CART 算法，它规定生成出来的决策树为二叉树、且一般使用基尼增益作为信息增益的度量</li>
</ul>
</li>
<li>决策树常用的剪枝算法有两种，它们都是为了适当地降低模型复杂度、从而期望模型在未知数据上的表现更好</li>
<li>决策树的代码实现从始到终都贯彻着递归的思想，可以说是递归的一个经典应用</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;决策树是从直观上很好理解的模型，可以把它理解为一个划分规则的序列&lt;/li&gt;
&lt;li&gt;决策树常用的生成算法包括：&lt;ul&gt;
&lt;li&gt;ID3 算法，它使用互信息作为信息增益的度量&lt;/li&gt;
&lt;li&gt;C4.5 算法，它使用信息增益比作为信息增益的度量&lt;/li&gt;
&lt;li&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="小结" scheme="http://www.carefree0910.com/tags/%E5%B0%8F%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>相关数学理论</title>
    <link href="http://www.carefree0910.com/posts/613bbb2f/"/>
    <id>http://www.carefree0910.com/posts/613bbb2f/</id>
    <published>2017-04-23T02:33:22.000Z</published>
    <updated>2017-04-23T02:42:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>这一节会叙述琴生不等式（Jensen’s Inequality）及其一个简单的应用，为此我们需要知道凸函数的概念：</p>
<ul>
<li>若函数<script type="math/tex">f(x)</script>对<script type="math/tex">\forall p\in [0,1]</script>、都满足：  <script type="math/tex; mode=display">
pf\left( x_{1} \right) + \left( 1 - p \right)f\left( x_{2} \right) \leq f\left( px_{1} + \left( 1 - p \right)x_{2} \right)</script>则称<script type="math/tex">f(x)</script>为凸函数（有时又叫上凸函数）</li>
</ul>
<p>琴生不等式是针对凸函数提出的，下面就具体说一下其内容与证明</p>
<a id="more"></a>
<h1 id="定理内容"><a href="#定理内容" class="headerlink" title="定理内容"></a>定理内容</h1><p>对于<script type="math/tex">\lbrack a,b\rbrack</script>上的凸函数，若</p>
<script type="math/tex; mode=display">
p_{1},...,p_{K} \in \left\lbrack 0,1 \right\rbrack,\ \ p_{1} + p_{2} + \ldots + p_{K} = 1</script><p>则有</p>
<script type="math/tex; mode=display">
\sum_{k = 1}^{K}{p_{i}f(x_{i})} \leq f(\sum_{k = 1}^{K}{p_{i}x_{i}})</script><p>接下来我们会利用它来证明等概率分布具有最大熵。注意到可以证明函数</p>
<script type="math/tex; mode=display">
\hat{H}\left( p \right) = - p\log p</script><p>是一个凸函数，于是熵的定义式可以写成</p>
<script type="math/tex; mode=display">
H\left( y \right) = - \sum_{k = 1}^{K}{p_{k}\log p_{k}} = \sum_{k = 1}^{K}{\hat{H}\left( p_{k} \right)}</script><p>从而</p>
<script type="math/tex; mode=display">
\frac{1}{K}H\left( y \right) = \frac{1}{K}\sum_{k = 1}^{K}{\hat{H}(p_{k})} \leq \hat{H}\left( \sum_{k = 1}^{K}{\frac{1}{K}p_{k}} \right) = \hat{H}\left( \frac{1}{K} \right) = - \frac{1}{K}\log\frac{1}{K} = \frac{1}{K}\log K</script><p>亦即</p>
<script type="math/tex; mode=display">
H\left( y \right) \leq \log K</script><p>等式当且仅当</p>
<script type="math/tex; mode=display">
p_{1} = p_{2} = \ldots = p_{K} = \frac{1}{K}</script><p>时取得</p>
<h1 id="定理证明"><a href="#定理证明" class="headerlink" title="定理证明"></a>定理证明</h1><p>应用数学归纳法可以比较简单地完成证明：</p>
<ul>
<li>当时<script type="math/tex">K=2</script>、由凸函数定义直接证毕，此为奠基</li>
<li>假设<script type="math/tex">K = n</script>时成立、考虑<script type="math/tex">K = n + 1</script>的情况，令  <script type="math/tex; mode=display">
s_{n} = \sum_{k = 1}^{n}p_{k}</script>则  <script type="math/tex; mode=display">
\sum_{k = 1}^{n + 1}{p_{k}f\left( x_{k} \right)} = s_{n}\sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} + p_{n + 1}f\left( x_{n + 1} \right)</script>注意到  <script type="math/tex; mode=display">
\sum_{k = 1}^{n}\frac{p_{k}}{s_{n}} = 1</script>从而由<script type="math/tex">K = n</script>时的琴生不等式可知  <script type="math/tex; mode=display">
\sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} \leq f\left( \sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} \right)</script>注意到  <script type="math/tex; mode=display">
s_{n} + p_{n + 1} = 1</script>从而由凸函数定义知  <script type="math/tex; mode=display">
s_{n}f\left( \sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} \right) + p_{n + 1}f\left( x_{n + 1} \right) \leq f\left( s_{n}\sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} + p_{n + 1}x_{n + 1} \right)</script>综上所述、即得  <script type="math/tex; mode=display">
\sum_{k = 1}^{n + 1}{p_{k}f(x_{k})} \leq f\left( \sum_{k = 1}^{n + 1}{p_{k}f\left( x_{k} \right)} \right)</script></li>
</ul>
<p>琴生不等式的应用非常广泛，证明等概率分布具有最大熵只是其中一个小应用。在许许多多涉及到凸问题的算法中、琴生不等式都显示出了强大的威力</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一节会叙述琴生不等式（Jensen’s Inequality）及其一个简单的应用，为此我们需要知道凸函数的概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若函数&lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;对&lt;script type=&quot;math/tex&quot;&gt;\forall p\in [0,1]&lt;/script&gt;、都满足：  &lt;script type=&quot;math/tex; mode=display&quot;&gt;
pf\left( x_{1} \right) + \left( 1 - p \right)f\left( x_{2} \right) \leq f\left( px_{1} + \left( 1 - p \right)x_{2} \right)&lt;/script&gt;则称&lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;为凸函数（有时又叫上凸函数）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;琴生不等式是针对凸函数提出的，下面就具体说一下其内容与证明&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>评估与可视化</title>
    <link href="http://www.carefree0910.com/posts/c12a819/"/>
    <id>http://www.carefree0910.com/posts/c12a819/</id>
    <published>2017-04-23T01:45:37.000Z</published>
    <updated>2017-04-23T03:08:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前我们实现了一个 Node 基类<code>CvDNode</code>和一个 Tree 基类<code>CvDBase</code>；为了评估决策树模型的表现、我们需要先在这两个基类的基础上根据不同的算法实现出各种具体的决策树。由于我们在基类里面已经完成了绝大部分工作、所以在其上进行扩展是平凡的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ID3Node</span><span class="params">(CvDNode)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></div><div class="line">        CvDNode.__init__(self, *args, **kwargs)</div><div class="line">        self.criterion = <span class="string">"ent"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">C45Node</span><span class="params">(CvDNode)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></div><div class="line">        CvDNode.__init__(self, *args, **kwargs)</div><div class="line">        self.criterion = <span class="string">"ratio"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CartNode</span><span class="params">(CvDNode)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></div><div class="line">        CvDNode.__init__(self, *args, **kwargs)</div><div class="line">        self.criterion = <span class="string">"gini"</span></div><div class="line">        self.is_cart = <span class="keyword">True</span></div></pre></td></tr></table></figure>
<p>在<code>CvDBase</code>的基础上定义三种算法对应的 Tree 结构的方法是类似的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ID3Tree</span><span class="params">(CvDBase, ID3Node, metaclass=CvDMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">C45Tree</span><span class="params">(CvDBase, C45Node, metaclass=CvDMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CartTree</span><span class="params">(CvDBase, CartNode, metaclass=CvDMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>其中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CvDMeta</span><span class="params">(type)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(mcs, *args, **kwargs)</span>:</span></div><div class="line">        name, bases, attr = args[:<span class="number">3</span>]</div><div class="line">        _, _node = bases</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, whether_continuous=None, max_depth=None, node=None, **_kwargs)</span>:</span></div><div class="line">            tmp_node = node <span class="keyword">if</span> isinstance(node, CvDNode) <span class="keyword">else</span> _node</div><div class="line">            CvDBase.__init__(self, whether_continuous, max_depth, tmp_node(**_kwargs))</div><div class="line">            self._name = name</div><div class="line"></div><div class="line">        attr[<span class="string">"__init__"</span>] = __init__</div><div class="line">        <span class="keyword">return</span> type(name, bases, attr)</div></pre></td></tr></table></figure>
<p>接下来就是具体的评估与相应的可视化</p>
<a id="more"></a>
<p>我们同样可以使用蘑菇数据集来评估决策树模型的表现，结果如下所示：</p>
<img src="/posts/c12a819/p1.png" alt="蘑菇数据集上 ID3 算法的表现" title="蘑菇数据集上 ID3 算法的表现">
<img src="/posts/c12a819/p2.png" alt="蘑菇数据集上 C4.5 算法的表现" title="蘑菇数据集上 C4.5 算法的表现">
<img src="/posts/c12a819/p3.png" alt="蘑菇数据集上 CART 算法的表现" title="蘑菇数据集上 CART 算法的表现">
<p>可以看到 CART 算法的表现相对来说要差不少，可能的原因有如下三条：</p>
<ul>
<li>CART 算法在选择划分标准时是从所有二分标准里面进行选择的，这里就会比 ID3 和 C4.5 算法多出不少倍的运算量</li>
<li>由于我们在实现 CART 剪枝算法时为了追求简洁、直接调用了标准库 copy 中的 deepcopy 方法对整颗决策树进行了深拷贝。这一步可能会连不必要的东西也进行了拷贝、从而导致了一些不必要的开销</li>
<li>CART 算法生成的是二叉决策树，所以可能生成出来的树会更深、各叶节点中的样本数可能也会分布得比较均匀、从而无论是建模过程还是预测过程都会要慢一些</li>
</ul>
<p>当然，如果结合蘑菇数据集来说的话、笔者认为最大的问题在于：CART 算法不适合应用于蘑菇数据集。一方面是因为蘑菇数据集全是离散型特征且各特征取值都挺多，另一方面是因为蘑菇数据集相对简单、有一些特征非常具有代表性（我们在说明朴素贝叶斯时也有所提及），仅仅用二分标准划分数据的话、会显得比较没有效率</p>
<p>为了更客观地评估我们模型的表现，我们可以对成熟第三方库 sklearn 中的决策树模型进行恰当的封装并看看它在蘑菇数据集上的表现：</p>
<img src="/posts/c12a819/p4.png" alt="蘑菇数据集上 sklearn 决策树的表现（criterion=entropy）" title="蘑菇数据集上 sklearn 决策树的表现（criterion=entropy）">
<img src="/posts/c12a819/p5.png" alt="蘑菇数据集上 sklearn 决策树的表现（criterion=gini）" title="蘑菇数据集上 sklearn 决策树的表现（criterion=gini）">
<p>不得不承认、成熟第三方库的效率确实要高很多（比我们的要快 5 倍左右）；这是因为虽然算法思想可能大致相同，但 sklearn 的核心实现都经过了高度优化、且（如不出意料的话）应该都是用 C 或者其它底层语言直接写的。不过正如第一章说过的，要想应用 sklearn 中的决策树、就必须先将数据数值化（即使是离散型数据）；而我们实现的决策树在处理离散型数据时却无需这一步数据预处理、可以直接应用在原始数据上（但处理混合型数据时还是要先进行数值化处理、而且将离散型数据数值化也能显著提升模型的运行速度）</p>
<p>我们在本系列的综述里面曾说过、决策树可能是从直观上最好理解的模型；事实上，我们之前画过的一些决策树示意图也确实非常直观易懂、于是我们可能自然就会希望程序能将生成类似的东西。虽然不能做到那么漂亮、不过我们确实是能在之前实现的决策树模型的基础上做出类似效果的：</p>
<img src="/posts/c12a819/p6.png" alt="蘑菇数据集上 ID3 决策树的可视化" title="蘑菇数据集上 ID3 决策树的可视化">
<img src="/posts/c12a819/p7.png" alt="蘑菇数据集上 C4.5 决策树的可视化" title="蘑菇数据集上 C4.5 决策树的可视化">
<img src="/posts/c12a819/p8.png" alt="蘑菇数据集上 CART 决策树的可视化" title="蘑菇数据集上 CART 决策树的可视化">
<p>其中，红色数字代表该 Node 作为划分标准的特征所属的维度，位于各条连线中央的字母代表着该维度特征的各个取值、加号“+”代表着“其它”，绿色字母代表类别标记。以上三张图在一定程度上验证了我们之前的很多说法，比如说 ID3 会倾向选择取值比较多的特征、C4.5 可能会倾向选择取值比较少的特征且倾向于在每个二叉分枝处留下一个小 Node 作为叶节点、CART 各个叶节点上的样本分布较均匀且生成出的决策树会比较深……等等</p>
<p>我们在说明朴素贝叶斯时曾经提过，即使只根据第 5 维的取值来进行类别的判定、最后的准确率也一定会非常高。验证这一命题的方法很简单——只需将决策树的最大深度设为 1 即可，结果如下图所示：</p>
<img src="/posts/c12a819/p9.png" alt="p9.png" title="">
<p>此时模型的表现如下图所示：</p>
<img src="/posts/c12a819/p10.png" alt="p10.png" title="">
<p>可以看到其表现确实不错。值得一提的是，单层决策树又可称为“决策树桩（Decision Stump）”、它是有特殊应用场景的（比如我们在下个系列中讲 AdaBoost 时就会用到它）</p>
<p>至今为止我们用到的数据集都是离散型数据集，为了更全面地进行评估、使用连续型混合型数据集进行评估是有必要的；同时为了增强直观、我们可以用异或数据集来进行评估。原始数据集如下图所示：</p>
<img src="/posts/c12a819/p11.png" alt="p11.png" title="">
<p>生成异或数据集（及其它二维数据集）的代码定义在之前提过 DataUtil 类中（可参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py" target="_blank" rel="external">这里</a>），读者也可以在下一章中找到相应的讲解。为使评估更具有直观性、我们可以把四种决策树（ID3、C4.5、CART 决策树和 sklearn 的决策树）在异或数据集上的表现直接画出来：</p>
<img src="/posts/c12a819/p12.png" alt="异或数据集上 ID3、CART 和 sklearn 决策树的表现" title="异或数据集上 ID3、CART 和 sklearn 决策树的表现">
<img src="/posts/c12a819/p13.png" alt="异或数据集上 C4.5 决策树的表现" title="异或数据集上 C4.5 决策树的表现">
<p>可以看到 C4.5 决策树的过拟合现象比较严重。正如我们之前所分析的一般、这很有可能是因为 C4.5 在二叉分枝时会倾向于进行“不均匀的二分”（从上图也可以大概看出）</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前我们实现了一个 Node 基类&lt;code&gt;CvDNode&lt;/code&gt;和一个 Tree 基类&lt;code&gt;CvDBase&lt;/code&gt;；为了评估决策树模型的表现、我们需要先在这两个基类的基础上根据不同的算法实现出各种具体的决策树。由于我们在基类里面已经完成了绝大部分工作、所以在其上进行扩展是平凡的：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ID3Node&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDNode)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, *args, **kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        CvDNode.__init__(self, *args, **kwargs)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.criterion = &lt;span class=&quot;string&quot;&gt;&quot;ent&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;C45Node&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDNode)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, *args, **kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        CvDNode.__init__(self, *args, **kwargs)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.criterion = &lt;span class=&quot;string&quot;&gt;&quot;ratio&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;CartNode&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDNode)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, *args, **kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        CvDNode.__init__(self, *args, **kwargs)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.criterion = &lt;span class=&quot;string&quot;&gt;&quot;gini&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.is_cart = &lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;在&lt;code&gt;CvDBase&lt;/code&gt;的基础上定义三种算法对应的 Tree 结构的方法是类似的：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ID3Tree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDBase, ID3Node, metaclass=CvDMeta)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;C45Tree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDBase, C45Node, metaclass=CvDMeta)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;CartTree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDBase, CartNode, metaclass=CvDMeta)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;其中&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;CvDMeta&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(type)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__new__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(mcs, *args, **kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        name, bases, attr = args[:&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        _, _node = bases&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, whether_continuous=None, max_depth=None, node=None, **_kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            tmp_node = node &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; isinstance(node, CvDNode) &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; _node&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            CvDBase.__init__(self, whether_continuous, max_depth, tmp_node(**_kwargs))&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            self._name = name&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        attr[&lt;span class=&quot;string&quot;&gt;&quot;__init__&quot;&lt;/span&gt;] = __init__&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; type(name, bases, attr)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;接下来就是具体的评估与相应的可视化&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>剪枝算法的实现</title>
    <link href="http://www.carefree0910.com/posts/602f7125/"/>
    <id>http://www.carefree0910.com/posts/602f7125/</id>
    <published>2017-04-23T01:25:20.000Z</published>
    <updated>2017-04-23T03:00:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py" target="_blank" rel="external">这里</a>和<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Tree.py" target="_blank" rel="external">这里</a>）</p>
<p>决策树剪枝算法的实现相对而言比较平凡，只需要把算法依次翻译成程序语言即可</p>
<a id="more"></a>
<h1 id="ID3、C4-5-剪枝算法的实现"><a href="#ID3、C4-5-剪枝算法的实现" class="headerlink" title="ID3、C4.5 剪枝算法的实现"></a>ID3、C4.5 剪枝算法的实现</h1><p>回忆算法本身，可以知道我们需要获取“从下往上”这个顺序，为此我们需要先在<code>CvDNode</code>中利用递归定义一个函数来更新 Tree 的<code>self.layers</code>属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_layers</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 根据该Node的深度、在self.layers对应位置的列表中记录自己</span></div><div class="line">    self.tree.layers[self._depth].append(self)</div><div class="line">    <span class="comment"># 遍历所有子节点、完成递归</span></div><div class="line">    <span class="keyword">for</span> _node <span class="keyword">in</span> sorted(self.children):</div><div class="line">        _node = self.children[_node]</div><div class="line">        <span class="keyword">if</span> _node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            _node.update_layers()</div></pre></td></tr></table></figure>
<p>然后、在<code>CvDBase</code>中定义一个对应的函数进行封装：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update_layers</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 根据整颗决策树的高度、在self.layers里面放相应数量的列表</span></div><div class="line">    self.layers = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.root.height)]</div><div class="line">    self.root.update_layers()</div></pre></td></tr></table></figure>
<p>同时，为了做到合理的代码重用、我们可以先在<code>CvDNode</code>中定义一个计算损失的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(self, pruned=False)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pruned:</div><div class="line">        <span class="keyword">return</span> sum([leaf[<span class="string">"chaos"</span>] * len(leaf[<span class="string">"y"</span>]) <span class="keyword">for</span> leaf <span class="keyword">in</span> self.leafs.values()])</div><div class="line">    <span class="keyword">return</span> self.chaos * len(self._y)</div></pre></td></tr></table></figure>
<p>有了以上两个函数，算法本身的实现就很直观了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_prune</span><span class="params">(self)</span>:</span></div><div class="line">    self._update_layers()</div><div class="line">    _tmp_nodes = []</div><div class="line">    <span class="comment"># 更新完决策树每一“层”的Node之后，从后往前地向 _tmp_nodes中加Node</span></div><div class="line">    <span class="keyword">for</span> _node_lst <span class="keyword">in</span> self.layers[::<span class="number">-1</span>]:</div><div class="line">        <span class="keyword">for</span> _node <span class="keyword">in</span> _node_lst[::<span class="number">-1</span>]:</div><div class="line">            <span class="keyword">if</span> _node.category <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">                _tmp_nodes.append(_node)</div><div class="line">    _old = np.array([node.cost() + self.prune_alpha * len(node.leafs)</div><div class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> _tmp_nodes])</div><div class="line">    _new = np.array([node.cost(pruned=<span class="keyword">True</span>) + self.prune_alpha</div><div class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> _tmp_nodes])</div><div class="line">    <span class="comment"># 使用 _mask变量存储 _old和 _new对应位置的大小关系</span></div><div class="line">    _mask = _old &gt;= _new</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># 若只剩根节点就退出循环体</span></div><div class="line">        <span class="keyword">if</span> self.root.height == <span class="number">1</span>:</div><div class="line">            <span class="keyword">return</span></div><div class="line">        p = np.argmax(_mask)</div><div class="line">        <span class="comment"># 如果 _new中有比 _old中对应损失小的损失、则进行局部剪枝</span></div><div class="line">        <span class="keyword">if</span> _mask[p]:</div><div class="line">            _tmp_nodes[p].prune()</div><div class="line">            <span class="comment"># 根据被影响了的Node、更新 _old、_mask对应位置的值</span></div><div class="line">            <span class="keyword">for</span> i, node <span class="keyword">in</span> enumerate(_tmp_nodes):</div><div class="line">                <span class="keyword">if</span> node.affected:</div><div class="line">                    _old[i] = node.cost() + self.prune_alpha * len(node.leafs)</div><div class="line">                    _mask[i] = _old[i] &gt;= _new[i]</div><div class="line">                    node.affected = <span class="keyword">False</span></div><div class="line">            <span class="comment"># 根据被剪掉的Node、将各个变量对应的位置除去（注意从后往前遍历）</span></div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(_tmp_nodes) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</div><div class="line">                <span class="keyword">if</span> _tmp_nodes[i].pruned:</div><div class="line">                    _tmp_nodes.pop(i)</div><div class="line">                    _old = np.delete(_old, i)</div><div class="line">                    _new = np.delete(_new, i)</div><div class="line">                    _mask = np.delete(_mask, i)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">    self.reduce_nodes()</div></pre></td></tr></table></figure>
<p>上述代码的第 25 行和第 28 行出现了 Node 的<code>affected</code>属性，这是我们之前没有进行定义的（因为若在彼时定义会显得很突兀）；不过由剪枝算法可知，这个属性的用处与其名字一致——标记一个 Node 是否是“被影响到的”Node。事实上，在一个 Node 进行了局部剪枝后，会有两类 Node “被影响到”：</p>
<ul>
<li>该 Node 的子节点、子节点的子节点……等等，它们属于被剪掉的 Node、应该要将它们在<code>_old</code>、<code>_tmp_nodes</code>中对应的位置从这些列表中除去</li>
<li>该 Node 的父节点、父节点的父节点……等等，它们存储叶节点的列表会因局部剪枝而发生改变、所以要更新<code>_old</code>和<code>_mask</code>列表中对应位置的值</li>
</ul>
<p>其中，我们之前定义的 Node 中是用<code>pruned</code>属性来标记该 Node 是否已被剪掉、且介绍了如何通过递归来更新<code>pruned</code>属性；<code>affected</code>属性和<code>pruned</code>属性的本质几乎没什么区别，所以我们同样可以通过递归来更新<code>affected</code>属性。具体而言，我们只需：</p>
<ul>
<li>在初始化时令<code>self.affected = False</code></li>
<li>在局部剪枝函数内部插入<code>_parent.affected = True</code></li>
</ul>
<p>即可，其余部分可以保持不变。</p>
<h1 id="CART-剪枝算法的实现"><a href="#CART-剪枝算法的实现" class="headerlink" title="CART 剪枝算法的实现"></a>CART 剪枝算法的实现</h1><p>同样的，为了做到合理的代码重用、我们先利用之前实现的<code>cost</code>函数、在<code>CvDNode</code>里面定义一个获取 Node 阈值的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_threshold</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> (self.cost(pruned=<span class="keyword">True</span>) - self.cost()) / (len(self.leafs) - <span class="number">1</span>)</div></pre></td></tr></table></figure>
<p>由于算法本身的实现的思想以及用到的工具都和第一种剪枝算法大同小异、所以代码写起来也差不多：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cart_prune</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 暂时将所有节点记录所属Tree的属性置为None</span></div><div class="line">    <span class="comment"># 这样做的必要性会在后文进行说明</span></div><div class="line">    self.root.cut_tree()</div><div class="line">    _tmp_nodes = [node <span class="keyword">for</span> node <span class="keyword">in</span> self.nodes <span class="keyword">if</span> node.category <span class="keyword">is</span> <span class="keyword">None</span>]</div><div class="line">    _thresholds = np.array([node.get_threshold() <span class="keyword">for</span> node <span class="keyword">in</span> _tmp_nodes])</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># 利用deepcopy对当前根节点进行深拷贝、存入self.roots列表</span></div><div class="line">        <span class="comment"># 如果前面没有把记录Tree的属性置为None，那么这里就也会对整个Tree做</span></div><div class="line">        <span class="comment"># 深拷贝。可以想象、这样会引发严重的内存问题，速度也会被拖慢非常多</span></div><div class="line">        root_copy = deepcopy(self.root)</div><div class="line">        self.roots.append(root_copy)</div><div class="line">        <span class="keyword">if</span> self.root.height == <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        p = np.argmin(_thresholds)</div><div class="line">        _tmp_nodes[p].prune()</div><div class="line">        <span class="keyword">for</span> i, node <span class="keyword">in</span> enumerate(_tmp_nodes):</div><div class="line">            <span class="comment"># 更新被影响到的Node的阈值</span></div><div class="line">            <span class="keyword">if</span> node.affected:</div><div class="line">                _thresholds[i] = node.get_threshold()</div><div class="line">                node.affected = <span class="keyword">False</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(_tmp_nodes) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</div><div class="line">            <span class="comment"># 去除掉各列表相应位置的元素</span></div><div class="line">            <span class="keyword">if</span> _tmp_nodes[i].pruned:</div><div class="line">                _tmp_nodes.pop(i)</div><div class="line">                _thresholds = np.delete(_thresholds, i)</div><div class="line">    self.reduce_nodes()</div></pre></td></tr></table></figure>
<p>代码第 4 行对根节点调用的<code>cut_tree</code>方法同样是利用递归实现的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_tree</span><span class="params">(self)</span>:</span></div><div class="line">    self.tree = <span class="keyword">None</span></div><div class="line">    <span class="keyword">for</span> child <span class="keyword">in</span> self.children.values():</div><div class="line">        <span class="keyword">if</span> child <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            child.cut_tree()</div></pre></td></tr></table></figure>
<p>然后就是最后一步、通过交叉验证选出最优树了。注意到之前我们封装生成算法时、最后一行调用了剪枝算法的封装——<code>self.prune</code>方法。由于该方法是第一个接收了交叉验证集<code>x_cv</code>和<code>y_cv</code>的方法、所以我们应该让该方法来做交叉验证。简洁起见，我们直接选用加权正确率作为交叉验证的标准：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算加权正确率的函数</span></div><div class="line"><span class="meta">@staticmethod</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">acc</span><span class="params">(y, y_pred, weights)</span>:</span></div><div class="line">    <span class="keyword">if</span> weights <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> np.sum((np.array(y) == np.array(y_pred)) * weights) / len(y)</div><div class="line">    <span class="keyword">return</span> np.sum(np.array(y) == np.array(y_pred)) / len(y)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prune</span><span class="params">(self, x_cv, y_cv, weights)</span>:</span></div><div class="line">    <span class="keyword">if</span> self.root.is_cart:</div><div class="line">        <span class="comment"># 如果该Node使用CART剪枝，那么只有在确实传入了交叉验证集的情况下</span></div><div class="line">        <span class="comment"># 才能调用相关函数、否则没有意义</span></div><div class="line">        <span class="keyword">if</span> x_cv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> y_cv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            self._cart_prune()</div><div class="line">            _arg = np.argmax([CvDBase.acc(</div><div class="line">                y_cv, tree.predict(x_cv), weights) <span class="keyword">for</span> tree <span class="keyword">in</span> self.roots])</div><div class="line">            _tar_root = self.roots[_arg]</div><div class="line">            <span class="comment"># 由于Node的feed_tree方法会递归地更新nodes属性、所以要先重置</span></div><div class="line">            self.nodes = []</div><div class="line">            _tar_root.feed_tree(self)</div><div class="line">            self.root = _tar_root</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        self._prune()</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;（本文会用到的所有代码都在&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py&quot;&gt;这里&lt;/a&gt;和&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Tree.py&quot;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;决策树剪枝算法的实现相对而言比较平凡，只需要把算法依次翻译成程序语言即可&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>决策树的剪枝算法</title>
    <link href="http://www.carefree0910.com/posts/1a7aa546/"/>
    <id>http://www.carefree0910.com/posts/1a7aa546/</id>
    <published>2017-04-22T15:33:01.000Z</published>
    <updated>2017-04-23T02:59:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>在知道怎么得到一颗决策树后、我们当然就会想知道：这样建立起来的决策树的表现究竟如何？从直观上来说，只要决策树足够深、划分标准足够细，它在训练集上的表现就能接近完美；但同时也容易想象，由于它可能把训练集的一些“特性”当做所有数据的“共性”来看待，它在未知的测试数据上的表现可能就会比较一般、亦即会出现过拟合的问题。我们知道，模型出现过拟合问题一般是因为模型太过复杂，所以决策树解决过拟合的方法是采取适当的“剪枝”、我们在上一节中也已经大量接触了这一概念。剪枝通常分为两类：“预剪枝（Pre-Pruning）”和“后剪枝（Post-Pruning）”，其中“预剪枝”的概念在生成算法中已有定义，彼时我们采取的说法是“停止条件”；而一般提起剪枝时指的都是“后剪枝”，它是指在决策树生成完毕后再对其进行修剪、把多余的节点剪掉。换句话说，后剪枝是从全局出发、通过某种标准对一些 Node 进行局部剪枝，这样就能减少决策树中 Node 的数目、从而有效地降低模型复杂度</p>
<a id="more"></a>
<p>是故问题的关键在于如何定出局部剪枝的标准。通常来说我们有两种做法：</p>
<ul>
<li>应用交叉验证的思想，若局部剪枝能够使得模型在测试集上的错误率降低、则进行局部剪枝（预剪枝中也应用了类似的思想）</li>
<li>应用正则化的思想、综合考虑不确定性和模型复杂度来定出一个新的损失（此前我们的损失只考虑了不确定性），用该损失作为一个 Node 是否进行局部剪枝的标准</li>
</ul>
<p>第二种做法又涉及到另一个关键问题：如何定量分析决策树中一个 Node 的复杂度？一个直观且合理的方法是：直接使用该 Node 下属叶节点的个数作为复杂度。基于此、第二种做法的数学描述就是：</p>
<ul>
<li>定义新损失（<script type="math/tex">T</script>代表一个 Node）  <script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = C\left( T \right) + \alpha\left| T \right|</script>其中，<script type="math/tex">C\left( T \right)</script>即是该 Node 和不确定性相关的损失、<script type="math/tex">|T|</script>则是该 Node 下属叶节点的个数。不妨设第 t 个叶节点含有<script type="math/tex">N_{t}</script>个样本且这<script type="math/tex">N_{t}</script>个样本的不确定性为<script type="math/tex">H_{t}(T)</script>，那么新损失一般可以直接定义为加权不确定性：  <script type="math/tex; mode=display">
C\left( T \right) = \sum_{t = 1}^{\left| T \right|}{N_{t}H_{t}(T)}</script></li>
</ul>
<p>我们会采取这种做法来进行实现。需要指出的是，在这种做法下、仍然可以分支出两种不同的算法：</p>
<ul>
<li>直接比较一个 Node 局部剪枝前的损失<script type="math/tex">C_{\alpha}(T)</script>和局部剪枝后的损失<script type="math/tex">C_{\alpha}(t)</script>的大小，若：  <script type="math/tex; mode=display">
C_{\alpha}\left( T \right) \geq C_{\alpha}(t)</script></li>
<li>获取一系列的剪枝阈值：<script type="math/tex">0 = \alpha_{0} < \alpha_{1} < \ldots < \alpha_{p} < + \infty</script>，在每个剪枝阈值<script type="math/tex">\alpha_{i}</script>上对相应的 Node 进行局部剪枝并将局部剪枝后得到的决策树<script type="math/tex">T_{i}</script>储存在一个列表中。在<script type="math/tex">\alpha_{p}</script>上我们会对根节点进行局部剪枝，此时剩下来的决策树<script type="math/tex">T_{p}</script>就只包含根节点这一个 Node。最后，通过交叉验证选出<script type="math/tex">T_{0},\ldots,T_{p}</script>中最好的决策树作为最终生成的决策树（注意其中的<script type="math/tex">T_{0}</script>即是没有剪过枝的原始树）</li>
</ul>
<p>第一种算法清晰易懂，第二种算法则稍显复杂；一般我们会在 ID3 和 C4.5 中应用第一种剪枝算法、在 CART 中应用第二种剪枝算法。上述这个第二种算法的说明可能有些过于简略、让人摸不着头脑；由于详细的算法叙述会在后文再次进行，所以这里只要有一个大概的直观感受即可，细节可以暂时按下、不必太过纠结</p>
<h1 id="ID3、C4-5-的剪枝算法"><a href="#ID3、C4-5-的剪枝算法" class="headerlink" title="ID3、C4.5 的剪枝算法"></a>ID3、C4.5 的剪枝算法</h1><p>首先我们来看看第一种算法的详细叙述。虽说算法本身的思想很简单，但由于其中涉及到许多中间变量、所以我们采取类似于伪代码的形式来进行叙述：</p>
<ol>
<li><strong>输入</strong>：生成算法产生的原始决策树<script type="math/tex">T</script>，惩罚因子<script type="math/tex">\alpha</script></li>
<li><strong>过程</strong>：<ol>
<li>从下往上地获取<script type="math/tex">T</script>中所有 Node，存入列表<code>_tmp_nodes</code></li>
<li>对<code>_tmp_nodes</code>中的所有 Node 计算损失，存入列表<code>_old</code></li>
<li>计算<code>_tmp_nodes</code>中所有 Node 进行局部剪枝后的损失，存入列表<code>_new</code></li>
<li>进入循环体：<ol>
<li>若<code>_new</code>中所有损失都大于<code>_old</code>中对应的损失、则退出循环体</li>
<li>否则，设 p 满足：  <script type="math/tex; mode=display">
p = \arg{\min_{p}{\text{_new}\lbrack p\rbrack \leq \text{_old}\lbrack p\rbrack}}</script>则对<code>_tmp_nodes[p]</code>进行局部剪枝</li>
<li>在完成局部剪枝后，更新<code>_old</code>、<code>_new</code>、<code>_tmp_nodes</code>等变量。具体而言，我们无需重新计算它们、只需更新“被影响到的” Node 所对应的位置的值即可</li>
</ol>
</li>
<li>最后调用<code>self.reduce_nodes</code>方法、将被剪掉的 Node 从<code>nodes</code>中除去</li>
</ol>
</li>
<li><strong>输出</strong>：修剪过后的决策树<script type="math/tex">T_{\alpha}</script></li>
</ol>
<p>我们可以在我们之前用气球数据集 1.0 根据 ID3 算法生成的决策树上过一遍剪枝算法以加深理解。由于算法顺序是从下往上、所以我们先考察最右下方的 Node（该 Node 的划分标准是“测试人员”），该 Node 所包含的数据集如下表所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>测试人员</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>不爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>从而：</p>
<ul>
<li>局部剪枝前、该 Node 的损失为：  <script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = C\left( T \right) + \alpha\left| T \right| = 0 + 2\alpha = 2\alpha</script></li>
<li>局部剪枝后、该 Node 的损失为：  <script type="math/tex; mode=display">
C_{\alpha}\left( t \right) = C\left( t \right) + \alpha\left| t \right| = C\left( t \right) + \alpha</script>其中  <script type="math/tex; mode=display">
C\left( t \right) = N_{t}H_{t} = 2 \times \left( - \frac{1}{2}\log\frac{1}{2} - \frac{1}{2}\log\frac{1}{2} \right) = 2</script>故  <script type="math/tex; mode=display">
C_{\alpha}\left( t \right) = 2 + \alpha</script></li>
</ul>
<p>回忆生成算法的实现，我们彼时将<script type="math/tex">\alpha</script>定义为了<script type="math/tex">\alpha = \frac{特征个数}{2}</script>（注意：这只是<script type="math/tex">\alpha</script>的一种朴素的定义方法，很难说它有什么合理性、只能说它从直观上有一定道理；如果想让模型表现更好、需要结合具体的问题来分析<script type="math/tex">\alpha</script>应该取何值）。由于气球数据集 1.0 一共有四个特征、所以此时<script type="math/tex">\alpha = 2</script>；结合各个公式、我们发现：</p>
<script type="math/tex; mode=display">
C_{\alpha}\left( t \right) = 2\alpha = 4 = 2 + \alpha = C_{\alpha}(t)</script><p>所以我们应该对该 Node 进行局部剪枝。局部剪枝后的决策树如下图所示：</p>
<img src="/posts/1a7aa546/p1.png" alt="p1.png" title="">
<p><strong><em>注意：进行局部剪枝后，由于该 Node 中样本只有两个、且一个样本类别为“不爆炸”一个为“爆炸”，所以给该 Node 标注为“不爆炸”、“爆炸”甚至以 50%的概率标注为“不爆炸”等做法都是合理的。为简洁，我们如上图中所做的一般、将其标注为“爆炸”</em></strong></p>
<p>然后我们需要考察最左下方的 Node（该 Node 的划分标准也是“测试人员”），易知计算过程和上述的没有区别。对其进行局部剪枝后的决策树如下图所示：</p>
<img src="/posts/1a7aa546/p2.png" alt="p2.png" title="">
<p>然后我们需要考察右下方的 Node（该 Node 的划分标准是“动作”），该 Node 所包含的数据集如下表所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用手打</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>从而：</p>
<ul>
<li><p>局部剪枝前、该 Node 的损失为：  </p>
<script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = C\left( T \right) + \alpha\left| T \right| = C\left( T \right) + 2\alpha</script><p>其中  </p>
<script type="math/tex; mode=display">
\begin{align}
C\left( T \right) = N_{}H_{} + N_{}H_{} \\

= 2 \times \left( - \frac{1}{2}\log\frac{1}{2} - \frac{1}{2}\log\frac{1}{2} \right) + 4 \times 0 = 2
\end{align}</script><p>故  </p>
<script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = 2 + 2\alpha</script></li>
<li>局部剪枝后、该 Node 的损失为：  <script type="math/tex; mode=display">
C_{\alpha}\left( t \right) = C\left( t \right) + \alpha\left| t \right| = C\left( t \right) + \alpha</script>其中  <script type="math/tex; mode=display">
C\left( t \right) = N_{t}H_{t} = 6 \times \left( - \frac{1}{6}\log\frac{1}{6} - \frac{5}{6}\log\frac{5}{6} \right) \approx 3.9</script>故  <script type="math/tex; mode=display">
C_{\alpha}\left( t \right) \approx 3.9 + \alpha</script></li>
</ul>
<p>将<script type="math/tex">\alpha = 2</script>代入、知：</p>
<script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = 2 + 2\alpha = 6 > 5.9 = 3.9 + \alpha \approx C_{\alpha}(t)</script><p>故应该对该 Node 进行局部剪枝。局部剪枝后的决策树如下图所示：</p>
<img src="/posts/1a7aa546/p3.png" alt="p3.png" title="">
<p>然后我们需要考察左下方的 Node（该 Node 的划分标准也是“动作”），易知计算过程和上述的没有区别。对其进行局部剪枝后的决策树如下图所示：</p>
<img src="/posts/1a7aa546/p4.png" alt="p4.png" title="">
<p>通过计算易知不应对根节点进行局部剪枝、所以上图所示的决策树即是当<script type="math/tex">\alpha = 2</script>时最终修剪出来的决策树</p>
<h1 id="CART-剪枝"><a href="#CART-剪枝" class="headerlink" title="CART 剪枝"></a>CART 剪枝</h1><p>第二种剪枝算法（CART 剪枝）中的许多定义可能还不是很清晰，所以我们先对相关概念进行详细一点的直观说明：</p>
<p>首先需要指出的是：关于第二种算法中出现的一系列的阈值，它们的含义其实和第一种算法中的<script type="math/tex">\alpha</script>一样、都是模型复杂度的“惩罚因子”；不同的是，第一种算法的<script type="math/tex">\alpha</script>是人为给定的、第二种算法中一系列的阈值则是算法生成出来的。其中，<script type="math/tex">\alpha_{0} = 0</script>意味着算法初始不对模型复杂度进行惩罚、此时最优树即是原始树<script type="math/tex">T_{0}</script>。然后我们设想<script type="math/tex">\alpha</script>缓慢增大、亦即缓慢增大对模型复杂度的惩罚，那么到某个阈值<script type="math/tex">\alpha_{1}</script>时，对决策树中某个 Node 进行局部剪枝就是一个更好的选择。我们将该 Node 进行局部剪枝后的决策树<script type="math/tex">T_{1}</script>存进一个列表中、然后继续缓慢增加惩罚因子<script type="math/tex">\alpha</script>，继而到某个阈值<script type="math/tex">\alpha_{2}</script>后、对某个 Node 进行局部剪枝就又会是一个更好的选择……以此类推，直到<script type="math/tex">\alpha</script>变成一个充分大的数<script type="math/tex">\alpha_{p}</script>后、只保留根节点这一个 Node 会是最好的选择，此时就终止算法并通过交叉验证从<script type="math/tex">T_{0},\ldots,T_{p}</script>中选出最好的<script type="math/tex">T_{p^{*}}</script>作为修剪后的决策树。</p>
<p>那么这个相对比较复杂的算法有什么优异之处呢？可以证明：在 CART 剪枝里得到的决策树<script type="math/tex">T_{0},\ldots,T_{p}</script>中，对<script type="math/tex">\forall i = 0,\ldots,p</script>、<script type="math/tex">T_{i}</script>都是当惩罚因子<script type="math/tex">\alpha \in \lbrack\alpha_{i},\alpha_{i + 1})</script>时的最优决策树。这条性质保证了 CART 算法最终通过交叉验证选出来的决策树<script type="math/tex">T_{p^{*}}</script>具有一定的优良性。</p>
<p>该算法的详细叙述则如下：</p>
<ol>
<li><strong>输入</strong>：在训练集上调用生成算法所产生的原始决策树<script type="math/tex">T</script>，交叉验证集</li>
<li>过程：<ol>
<li>从下往上地获取<script type="math/tex">T</script>中所有 Node，存入列表<code>_tmp_nodes</code></li>
<li>对<code>_tmp_nodes</code>中的所有 Node 计算阈值，存入列表<code>_thresholds</code>；其中，第 t 个 Node 的阈值<script type="math/tex">\alpha_{t}</script>应满足：  <script type="math/tex; mode=display">
C\left( T_{t} \right) + \alpha_{t}\left| T_{t} \right| = C_{\alpha_{t}}\left( T_{t} \right) = C_{\alpha_{t}}\left( t \right) = C\left( t \right) + \alpha_{t}</script>其中<script type="math/tex">C(t)</script>即是第 t 个 Node 自身数据的不确定性；换言之，<script type="math/tex">C_{\alpha_{t}}(T_{t})</script>代表着第 t 个 Node 进行局部剪枝前的新损失、<script type="math/tex">C_{\alpha_{t}}(t)</script>代表着局部剪枝后的新损失。由上式可求出：  <script type="math/tex; mode=display">
\alpha_{t} = \frac{C\left( t \right) - C\left( T_{t} \right)}{\left| T_{t} \right| - 1}</script>此即阈值的计算公式</li>
<li>进入循环体：<ol>
<li>将当前决策树存入列表<code>self.roots</code></li>
<li>若当前决策树中只剩根节点、则退出循环体</li>
<li>否则，取 p 满足：  <script type="math/tex; mode=display">
p = \arg{\min_{p}{\_\text{thresholds}}}</script>然后对<code>_tmp_nodes[p]</code>进行局部剪枝</li>
<li>在完成局部剪枝后，更新<code>_thresholds</code>、<code>_tmp_nodes</code>等变量。具体而言，我们无需重新计算它们、只需更新“被影响到的” Node 所对应的位置的值即可</li>
</ol>
</li>
<li>然后调用<code>self.reduce_nodes</code>方法、将被剪掉的 Node 从<code>nodes</code>中除去</li>
<li>最后利用交叉验证、从<code>self.roots</code>中选出表现最好的决策树<script type="math/tex">T_{p^{*}}</script></li>
</ol>
</li>
<li><strong>输出</strong>：修剪过后的决策树<script type="math/tex">T_{p^{*}}</script></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在知道怎么得到一颗决策树后、我们当然就会想知道：这样建立起来的决策树的表现究竟如何？从直观上来说，只要决策树足够深、划分标准足够细，它在训练集上的表现就能接近完美；但同时也容易想象，由于它可能把训练集的一些“特性”当做所有数据的“共性”来看待，它在未知的测试数据上的表现可能就会比较一般、亦即会出现过拟合的问题。我们知道，模型出现过拟合问题一般是因为模型太过复杂，所以决策树解决过拟合的方法是采取适当的“剪枝”、我们在上一节中也已经大量接触了这一概念。剪枝通常分为两类：“预剪枝（Pre-Pruning）”和“后剪枝（Post-Pruning）”，其中“预剪枝”的概念在生成算法中已有定义，彼时我们采取的说法是“停止条件”；而一般提起剪枝时指的都是“后剪枝”，它是指在决策树生成完毕后再对其进行修剪、把多余的节点剪掉。换句话说，后剪枝是从全局出发、通过某种标准对一些 Node 进行局部剪枝，这样就能减少决策树中 Node 的数目、从而有效地降低模型复杂度&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>树结构的实现</title>
    <link href="http://www.carefree0910.com/posts/b07c81ec/"/>
    <id>http://www.carefree0910.com/posts/b07c81ec/</id>
    <published>2017-04-22T15:07:52.000Z</published>
    <updated>2017-04-22T15:32:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Tree.py" target="_blank" rel="external">这里</a>）</p>
<p>由前文的诸多讨论可知，Tree 结构需要做到如下几点：</p>
<ul>
<li>定义好需要在各个 Node 上调用的“全局变量”</li>
<li>做好数据预处理的工作、保证传给 Node 的数据是合乎要求的</li>
<li>对各个 Node 进行合适的封装，做到：<ul>
<li>生成决策树时能够正确地调用它们的生成算法</li>
<li>进行后剪枝时能够正确地调用它们的局部剪枝函数</li>
</ul>
</li>
<li>定义预测函数和评估函数以供用户调用</li>
</ul>
<p>既然 Node 我们可以抽象出一个基类<code>CvDNode</code>，我们自然也能相应地对 Tree 结构抽象出一个基类<code>CvDBase</code></p>
<a id="more"></a>
<p>下面先来看看如何搭建该基类的基本框架：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</div><div class="line"><span class="comment"># 导入Node结构以进行封装</span></div><div class="line"><span class="keyword">from</span> c_CvDTree.Node <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="comment"># 定义一个足够抽象的Tree结构的基类以适应我们Node结构的基类</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CvDBase</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self.nodes：记录所有Node的列表</div><div class="line">        self.roots：主要用于CART剪枝的属性，可先按下不表</div><div class="line">（用于存储算法过程中产生的各个决策树）</div><div class="line">            self.max_depth：记录决策树最大深度的属性</div><div class="line">        self.root, self.feature_sets：根节点和记录可选特征维度的列表</div><div class="line">            self.label_dic：和朴素贝叶斯里面相应的属性意义一致、是类别的转换字典</div><div class="line">        self.prune_alpha, self.layers：主要用于ID3和C4.5剪枝的两个属性，可先按下不表</div><div class="line">（self.prune_alpha是“惩罚因子”，self.layers则记录着每一“层”的Node）</div><div class="line">        self.whether_continuous：记录着各个维度的特征是否连续的列表</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, max_depth=None, node=None)</span>:</span></div><div class="line">        self.nodes, self.layers, self.roots = [], [], []</div><div class="line">        self.max_depth = max_depth</div><div class="line">        self.root = node</div><div class="line">        self.feature_sets = []</div><div class="line">        self.label_dic = &#123;&#125;</div><div class="line">        self.prune_alpha = <span class="number">1</span></div><div class="line">        self.whether_continuous = <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> <span class="string">"CvDTree (&#123;&#125;)"</span>.format(self.root.height)</div><div class="line"></div><div class="line">    __repr__ = __str__</div></pre></td></tr></table></figure>
<p>回忆朴素贝叶斯的实现，可以知道在第二章的混合型朴素贝叶斯中、我们要求用户告诉程序哪些维度的特征是连续的；这里我们介绍一种非常简易却有一定合理性的做法、从而可以让程序在进行数据预处理时自动识别出连续特征对应的维度：</p>
<ul>
<li>将训练集中每个维度特征的所有可能的取值算出来，这一步可以用 Python 内置的数据结构<code>set</code>来完成</li>
<li>如果第 i 维可能的取值个数<script type="math/tex">S_i</script>比上训练集总样本数 N 大于某个阈值，亦即若：  <script type="math/tex; mode=display">
S_i\ge\beta N</script>那么就认为第 i 维的特征是连续型随机变量<br><script type="math/tex">\beta</script>的具体取值需要视情况而定。一般来说在样本数 N 足够大时、可以取得比较小（比如取就是个不错的选择）；但是样本数 N 比较小时，我们可能需要将取得大一些（比如取）。具体应该取什么值还是要看具体的任务和数据，毕竟这种自动识别的方法还是过于朴素了</li>
</ul>
<p>以上所叙述的数据预处理的实现如下（注：我们在朴素贝叶斯的实现里用到过的<code>quantize_data</code>方法中整合了如下代码中的核心部分）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_data</span><span class="params">(self, x, continuous_rate=<span class="number">0.2</span>)</span>:</span></div><div class="line">    <span class="comment"># 利用set获取各个维度特征的所有可能取值</span></div><div class="line">    self.feature_sets = [set(dimension) <span class="keyword">for</span> dimension <span class="keyword">in</span> x.T]</div><div class="line">    data_len, data_dim = x.shape</div><div class="line">    <span class="comment"># 判断是否连续</span></div><div class="line">    self.whether_continuous = np.array(</div><div class="line">        [len(feat) &gt;= continuous_rate * data_len <span class="keyword">for</span> feat <span class="keyword">in</span> self.feature_sets])</div><div class="line">    self.root.feats = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>])]</div><div class="line">    self.root.feed_tree(self)</div></pre></td></tr></table></figure>
<p>最后一行我们对根节点调用了<code>feed_tree</code>方法，该方法会做以下三件事：</p>
<ul>
<li>让决策树中所有的 Node 记录一下它们所属的 Tree 结构</li>
<li>将自己记录在 Tree 中记录所有 Node 的列表<code>nodes</code>里</li>
<li>根据 Tree 的相应属性更新记录连续特征的列表</li>
</ul>
<p>实现的时候同样利用上了递归：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_tree</span><span class="params">(self, tree)</span>:</span></div><div class="line">    self.tree = tree</div><div class="line">    self.tree.nodes.append(self)</div><div class="line">    self.wc = tree.whether_continuous</div><div class="line">    <span class="keyword">for</span> child <span class="keyword">in</span> self.children.values():</div><div class="line">        <span class="keyword">if</span> child <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            child.feed_tree(tree)</div></pre></td></tr></table></figure>
<p><strong><em>注意：以上代码应定义在<code>CvDNode</code>里面</em></strong></p>
<p>接下来就是对生成算法的封装了。考虑到第二节会讲到的剪枝算法、我们需要做的是：</p>
<ul>
<li>将类别向量数值化（和朴素贝叶斯里面的数值化类别向量的方法一样）</li>
<li>将数据集切分成训练集和交叉验证集、同时处理好样本权重</li>
<li>对根节点调用决策树的生成算法</li>
<li>调用自己的剪枝算法</li>
</ul>
<p>具体的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 参数alpha和剪枝有关、可按下不表</span></div><div class="line"><span class="comment"># cv_rate用于控制交叉验证集的大小，train_only则控制程序是否进行数据集的切分</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, alpha=None, sample_weight=None, eps=<span class="number">1e-8</span>,</span></span></div><div class="line">    cv_rate=<span class="number">0.2</span>, train_only=False):</div><div class="line">    <span class="comment"># 数值化类别向量</span></div><div class="line">    _dic = &#123;c: i <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(set(y))&#125;</div><div class="line">    y = np.array([_dic[yy] <span class="keyword">for</span> yy <span class="keyword">in</span> y])</div><div class="line">    self.label_dic = &#123;value: key <span class="keyword">for</span> key, value <span class="keyword">in</span> _dic.items()&#125;</div><div class="line">    x = np.array(x)</div><div class="line">    <span class="comment"># 根据特征个数定出alpha</span></div><div class="line">    self.prune_alpha = alpha <span class="keyword">if</span> alpha <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">else</span> x.shape[<span class="number">1</span>] / <span class="number">2</span></div><div class="line">    <span class="comment"># 如果需要划分数据集的话</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> train_only <span class="keyword">and</span> self.root.is_cart:</div><div class="line">        <span class="comment"># 根据cv_rate将数据集随机分成训练集和交叉验证集</span></div><div class="line">        <span class="comment"># 实现的核心思想是利用下标来进行各种切分</span></div><div class="line">        _train_num = int(len(x) * (<span class="number">1</span>-cv_rate))</div><div class="line">        _indices = np.random.permutation(np.arange(len(x)))</div><div class="line">        _train_indices = _indices[:_train_num]</div><div class="line">        _test_indices = _indices[_train_num:]</div><div class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            <span class="comment"># 注意对切分后的样本权重做归一化处理</span></div><div class="line">            _train_weights = sample_weight[_train_indices]</div><div class="line">            _test_weights = sample_weight[_test_indices]</div><div class="line">            _train_weights /= np.sum(_train_weights)</div><div class="line">            _test_weights /= np.sum(_test_weights)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            _train_weights = _test_weights = <span class="keyword">None</span></div><div class="line">        x_train, y_train = x[_train_indices], y[_train_indices]</div><div class="line">        x_cv, y_cv = x[_test_indices], y[_test_indices]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        x_train, y_train, _train_weights = x, y, sample_weight</div><div class="line">        x_cv = y_cv = _test_weights = <span class="keyword">None</span></div><div class="line">    self.feed_data(x_train)</div><div class="line">    <span class="comment"># 调用根节点的生成算法</span></div><div class="line">    self.root.fit(x_train, y_train, _train_weights, eps)</div><div class="line">    <span class="comment"># 调用对Node剪枝算法的封装</span></div><div class="line">    self.prune(x_cv, y_cv, _test_weights)</div></pre></td></tr></table></figure>
<p>这里我们用到了<code>np.random.permutation</code>方法，它其实可以看成两行代码的缩写、亦即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">_indices = np.random.permutation(np.arange(n))</div></pre></td></tr></table></figure>
<p>从效果上来说等价于</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">_indices = np.arange(n)</div><div class="line">np.random.shuffle(_indices)</div></pre></td></tr></table></figure>
<p>不过前者不仅写起来更便利、而且运行速度也要稍微快一点，是故我们选择了前一种方法来进行实现</p>
<p>除了<code>fit</code>这个函数以外，回忆 Node 中生成算法的实现过程、可知彼时我们调用了 Tree 的<code>reduce_nodes</code>方法来将被剪掉的 Node 从<code>nodes</code>中除去。该方法的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_nodes</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.nodes)<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</div><div class="line">        <span class="keyword">if</span> self.nodes[i].pruned:</div><div class="line">            self.nodes.pop(i)</div></pre></td></tr></table></figure>
<p><strong><em>注意：虽然该实现相当简单直观、不过其中却蕴含了一个具有普适意义的编程思想：如果要在遍历列表的同时进行当前列表元素的删除操作、就一定要从后往前遍历</em></strong></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;（本文会用到的所有代码都在&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Tree.py&quot;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;由前文的诸多讨论可知，Tree 结构需要做到如下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;定义好需要在各个 Node 上调用的“全局变量”&lt;/li&gt;
&lt;li&gt;做好数据预处理的工作、保证传给 Node 的数据是合乎要求的&lt;/li&gt;
&lt;li&gt;对各个 Node 进行合适的封装，做到：&lt;ul&gt;
&lt;li&gt;生成决策树时能够正确地调用它们的生成算法&lt;/li&gt;
&lt;li&gt;进行后剪枝时能够正确地调用它们的局部剪枝函数&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;定义预测函数和评估函数以供用户调用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;既然 Node 我们可以抽象出一个基类&lt;code&gt;CvDNode&lt;/code&gt;，我们自然也能相应地对 Tree 结构抽象出一个基类&lt;code&gt;CvDBase&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>节点结构的实现</title>
    <link href="http://www.carefree0910.com/posts/41abb98b/"/>
    <id>http://www.carefree0910.com/posts/41abb98b/</id>
    <published>2017-04-22T14:17:38.000Z</published>
    <updated>2017-04-23T00:45:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py" target="_blank" rel="external">这里</a>）</p>
<p>在实现完计算各种信息量的函数之后，我们就可以着手实现决策树本身了。由前文的讨论可知、组成决策树主体的是一个个的 Node，所以我们接下来首先要实现的就是 Node 这个结构。而且由于我们所关心的 ID3、C4.5 和 CART 分类树的 Node 在大多数情况下表现一致、只有少数几个地方有所不同，因此我们可以写一个统一的 Node 结构的基类<code>CvDNode</code>来囊括我们所有关心的决策树生成算法，该基类需要实现如下功能：</p>
<ul>
<li>根据离散型特征划分数据（ID3、C4.5、CART）</li>
<li>根据连续型特征划分数据（C4.5、CART）</li>
<li>根据当前的数据判定所属的类别</li>
</ul>
<p>虽然说起来显得轻巧，但这之中的抽象还是比较繁琐的</p>
<a id="more"></a>
<p>我们先看看这个基类的基本框架该如何搭建：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment"># 导入之前定义的Cluster类以计算各种信息量</span></div><div class="line"><span class="keyword">from</span> c_Tree.Cluster <span class="keyword">import</span> Cluster</div><div class="line"></div><div class="line"><span class="comment"># 定义一个足够抽象的基类以囊括所有我们关心的算法</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CvDNode</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._x, self._y：记录数据集的变量</div><div class="line">        self.base, self.chaos：记录对数的底和当前的不确定性</div><div class="line">        self.criterion, self.category：记录该Node计算信息增益的方法和所属的类别</div><div class="line">        self.left_child, self.right_child：针对连续型特征和CART、记录该Node的左右子节点</div><div class="line">        self._children, self.leafs：记录该Node的所有子节点和所有下属的叶节点</div><div class="line">        self.sample_weight：记录样本权重</div><div class="line">        self.wc：记录着各个维度的特征是否连续的列表（whether continuous的缩写）</div><div class="line">        self.tree：记录着该Node所属的Tree</div><div class="line">        self.feature_dim, self.tar, self.feats：记录该Node划分标准的相关信息。具体而言：</div><div class="line">            self.feature_dim：记录着作为划分标准的特征所对应的维度</div><div class="line">            self.tar：针对连续型特征和CART、记录二分标准</div><div class="line">            self.feats：记录该Node能进行选择的、作为划分标准的特征的维度</div><div class="line">        self.parent, self.is_root：记录该Node的父节点以及该Node是否为根节点</div><div class="line">        self._depth, self.prev_feat：记录Node的深度和其父节点的划分标准</div><div class="line">        self.is_cart：记录该Node是否使用了CART算法</div><div class="line">        self.is_continuous：记录该Node选择的划分标准对应的特征是否连续</div><div class="line">        self.pruned：记录该Node是否已被剪掉，后面实现局部剪枝算法时会用到</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tree=None, base=<span class="number">2</span>, chaos=None,</span></span></div><div class="line">               depth=<span class="number">0</span>, parent=None, is_root=True, prev_feat=<span class="string">"Root"</span>):</div><div class="line">        self._x = self._y = <span class="keyword">None</span></div><div class="line">        self.base, self.chaos = base, chaos</div><div class="line">        self.criterion = self.category = <span class="keyword">None</span></div><div class="line">        self.left_child = self.right_child = <span class="keyword">None</span></div><div class="line">        self._children, self.leafs = &#123;&#125;, &#123;&#125;</div><div class="line">        self.sample_weight = <span class="keyword">None</span></div><div class="line">        self.wc = <span class="keyword">None</span></div><div class="line">        self.tree = tree</div><div class="line">        <span class="comment"># 如果传入了Tree的话就进行相应的初始化</span></div><div class="line">        <span class="keyword">if</span> tree <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            <span class="comment"># 由于数据预处理是由Tree完成的</span></div><div class="line">            <span class="comment"># 所以各个维度的特征是否是连续型随机变量也是由Tree记录的</span></div><div class="line">            self.wc = tree.whether_continuous</div><div class="line">            <span class="comment"># 这里的nodes变量是Tree中记录所有Node的列表</span></div><div class="line">            tree.nodes.append(self)</div><div class="line">        self.feature_dim, self.tar, self.feats = <span class="keyword">None</span>, <span class="keyword">None</span>, []</div><div class="line">        self.parent, self.is_root = parent, is_root</div><div class="line">        self._depth, self.prev_feat = depth, prev_feat</div><div class="line">        self.is_cart = self.is_continuous = self.pruned = <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, item)</span>:</span></div><div class="line">        <span class="keyword">if</span> isinstance(item, str):</div><div class="line">            <span class="keyword">return</span> getattr(self, <span class="string">"_"</span> + item)</div><div class="line"></div><div class="line">    <span class="comment"># 重载 __lt__ 方法，使得Node之间可以比较谁更小、进而方便调试和可视化</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, other)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.prev_feat &lt; other.prev_feat</div><div class="line">    </div><div class="line">    <span class="comment"># 重载 __str__ 和 __repr__ 方法，同样是为了方便调试和可视化</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">if</span> self.category <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">return</span> <span class="string">"CvDNode (&#123;&#125;) (&#123;&#125; -&gt; &#123;&#125;)"</span>.format(</div><div class="line">                self._depth, self.prev_feat, self.feature_dim)</div><div class="line">        <span class="keyword">return</span> <span class="string">"CvDNode (&#123;&#125;) (&#123;&#125; -&gt; class: &#123;&#125;)"</span>.format(</div><div class="line">            self._depth, self.prev_feat, self.tree.label_dic[self.category])</div><div class="line">    __repr__ = __str__</div></pre></td></tr></table></figure>
<p>可以看到，除了重载 <strong>getitem</strong> 方法以外、我们还重载 <strong>lt</strong>、<strong>str</strong> 和 <strong>repr</strong> 方法。这是因为决策树模型的结构比起朴素贝叶斯模型而言要复杂一些，为了开发过程中的调试和最后的可视化更加便利、通常来说最好让我们模型的表现更贴近内置类型的表现</p>
<p>然后我们需要定义几个 property 以使开发过程变得便利：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义children属性，主要是区分开连续+CART的情况和其余情况</span></div><div class="line"><span class="comment"># 有了该属性后，想要获得所有子节点时就不用分情况讨论了</span></div><div class="line"><span class="meta">@property</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">children</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> &#123;</div><div class="line">        <span class="string">"left"</span>: self.left_child, <span class="string">"right"</span>: self.right_child</div><div class="line">    &#125; <span class="keyword">if</span> (self.is_cart <span class="keyword">or</span> self.is_continuous) <span class="keyword">else</span> self._children</div><div class="line"></div><div class="line"><span class="comment"># 递归定义height（高度）属性：</span></div><div class="line"><span class="comment"># 叶节点高度都定义为1、其余节点的高度定义为最高的子节点的高度+1</span></div><div class="line"><span class="meta">@property</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">height</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">if</span> self.category <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span> + max([_child.height <span class="keyword">if</span> _child <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">else</span> <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> _child <span class="keyword">in</span> self.children.values()])</div><div class="line"></div><div class="line"><span class="comment"># 定义info_dic（信息字典）属性，它记录了该Node的主要信息</span></div><div class="line"><span class="comment"># 在更新各个Node的叶节点时，被记录进各个self.leafs属性的就是该字典</span></div><div class="line"><span class="meta">@property</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">info_dic</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> &#123;<span class="string">"chaos"</span>: self.chaos, <span class="string">"y"</span>: self._y&#125;</div></pre></td></tr></table></figure>
<p>以上就是<code>CvDNode</code>的基本框架，接下来就可以在这个框架的基础上实现决策树的各种生成算法了。首先需要指出的是，由于 Node 结构是会被 Tree 结构封装的、所以我们应该将数据预处理操作交给 Tree 来做。其次，由于我们实现的是抽象程度比较高的基类，所以我们要做比较完备的分情况讨论</p>
<p><strong><em>注意：把 ID3、C4.5 和 CART 这三种算法分开实现是可行且高效的、此时各个部分的代码都会显得更加简洁可读一些；但这样做的话，整体的代码量就会不可避免地骤增。具体应当选择何种实现方案需要看具体的需求</em></strong></p>
<p>我们先来看看实现生成算法所需要做的一些准备工作，比如定义停止继续生成的准则、定义停止后该 Node 的行为等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义第一种停止准则：当特征维度为0或当前Node的数据的不确定性小于阈值时停止</span></div><div class="line"><span class="comment"># 同时，如果用户指定了决策树的最大深度，那么当该Node的深度太深时也停止</span></div><div class="line"><span class="comment"># 若满足了停止条件，该函数会返回True、否则会返回False</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop1</span><span class="params">(self, eps)</span>:</span></div><div class="line">    <span class="keyword">if</span> (</div><div class="line">        self._x.shape[<span class="number">1</span>] == <span class="number">0</span> <span class="keyword">or</span> (self.chaos <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self.chaos &lt;= eps)</div><div class="line">        <span class="keyword">or</span> (self.tree.max_depth <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self._depth &gt;= self.tree.max_depth)</div><div class="line">    ):</div><div class="line">        <span class="comment"># 调用处理停止情况的方法</span></div><div class="line">        self._handle_terminate()</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># 定义第二种停止准则，当最大信息增益仍然小于阈值时停止</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop2</span><span class="params">(self, max_gain, eps)</span>:</span></div><div class="line">    <span class="keyword">if</span> max_gain &lt;= eps:</div><div class="line">        self._handle_terminate()</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># 利用bincount方法定义根据数据生成该Node所属类别的方法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_category</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.argmax(np.bincount(self._y))</div><div class="line"></div><div class="line"><span class="comment"># 定义处理停止情况的方法，核心思想就是把该Node转化为一个叶节点</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_handle_terminate</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 首先要生成该Node所属的类别</span></div><div class="line">    self.category = self.get_category()</div><div class="line">    <span class="comment"># 然后一路回溯、更新父节点、父节点的父节点、……等等记录叶节点的属性leafs</span></div><div class="line">    _parent = self.parent</div><div class="line">    <span class="keyword">while</span> _parent <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        _parent.leafs[id(self)] = self.info_dic</div><div class="line">        _parent = _parent.parent</div></pre></td></tr></table></figure>
<p>接下来就要实现生成算法的核心了，我们可以将它分成三步以使逻辑清晰：</p>
<ul>
<li>定义一个方法使其能将一个有子节点的 Node 转化为叶节点（局部剪枝）</li>
<li>定义一个方法使其能挑选出最好的划分标准</li>
<li>定义一个方法使其能根据划分标准进行生成</li>
</ul>
<p>我们先来看看如何进行局部剪枝：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prune</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 调用相应方法进行计算该Node所属类别</span></div><div class="line">    self.category = self.get_category()</div><div class="line">    <span class="comment"># 记录由于该Node转化为叶节点而被剪去的、下属的叶节点</span></div><div class="line">    _pop_lst = [key <span class="keyword">for</span> key <span class="keyword">in</span> self.leafs]</div><div class="line">    <span class="comment"># 然后一路回溯、更新各个parent的属性leafs（使用id作为key以避免重复）</span></div><div class="line">    _parent = self.parent</div><div class="line">    <span class="keyword">while</span> _parent <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">for</span> _k <span class="keyword">in</span> _pop_lst:</div><div class="line">            <span class="comment"># 删去由于局部剪枝而被剪掉的叶节点</span></div><div class="line">            _parent.leafs.pop(_k)</div><div class="line">        _parent.leafs[id(self)] = self.info_dic</div><div class="line">        _parent = _parent.parent</div><div class="line">    <span class="comment"># 调用mark_pruned方法将自己所有子节点、子节点的子节点……</span></div><div class="line">    <span class="comment"># 的pruned属性置为True，因为它们都被“剪掉”了</span></div><div class="line">    self.mark_pruned()</div><div class="line">    <span class="comment"># 重置各个属性</span></div><div class="line">    self.feature_dim = <span class="keyword">None</span></div><div class="line">    self.left_child = self.right_child = <span class="keyword">None</span></div><div class="line">    self._children = &#123;&#125;</div><div class="line">    self.leafs = &#123;&#125;</div></pre></td></tr></table></figure>
<p>第 16 行的 mark_pruned 方法用于给各个被局部剪枝剪掉的 Node 打上一个标记、从而今后 Tree 可以根据这些标记将被剪掉的 Node 从它记录所有 Node 的列表<code>nodes</code>中删去。该方法同样是通过递归实现的，代码十分简洁：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mark_pruned</span><span class="params">(self)</span>:</span></div><div class="line">    self.pruned = <span class="keyword">True</span></div><div class="line">    <span class="comment"># 遍历各个子节点</span></div><div class="line">    <span class="keyword">for</span> _child <span class="keyword">in</span> self.children.values():</div><div class="line">        <span class="comment"># 如果当前的子节点不是None的话、递归调用mark_pruned方法</span></div><div class="line">        <span class="comment">#（连续型特征和CART算法有可能导致children中出现None，</span></div><div class="line">        <span class="comment"># 因为此时children由left_child和right_child组成，它们有可能是None）</span></div><div class="line">        <span class="keyword">if</span> _child <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            _child.mark_pruned()</div></pre></td></tr></table></figure>
<p>有了能够进行局部剪枝的方法后，我们就能实现拿来挑选最佳划分标准的方法了。开发时需要时刻注意分清楚二分（连续 / CART）和多分（其它）的情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight, eps=<span class="number">1e-8</span>)</span>:</span></div><div class="line">    self._x, self._y = np.atleast_2d(x), np.array(y)</div><div class="line">    self.sample_weight = sample_weight</div><div class="line">    <span class="comment"># 若满足第一种停止准则、则退出函数体</span></div><div class="line">    <span class="keyword">if</span> self.stop1(eps):</div><div class="line">        <span class="keyword">return</span></div><div class="line">    <span class="comment"># 用该Node的数据实例化Cluster类以计算各种信息量</span></div><div class="line">    _cluster = Cluster(self._x, self._y, sample_weight, self.base)</div><div class="line">    <span class="comment"># 对于根节点、我们需要额外算一下其数据的不确定性</span></div><div class="line">    <span class="keyword">if</span> self.is_root:</div><div class="line">        <span class="keyword">if</span> self.criterion == <span class="string">"gini"</span>:</div><div class="line">            self.chaos = _cluster.gini()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.chaos = _cluster.ent()</div><div class="line">    _max_gain, _chaos_lst = <span class="number">0</span>, []</div><div class="line">    _max_feature = _max_tar = <span class="keyword">None</span></div><div class="line">    <span class="comment"># 遍历还能选择的特征</span></div><div class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> self.feats:</div><div class="line">        <span class="comment"># 如果是连续型特征或是CART算法，需要额外计算二分标准的取值集合</span></div><div class="line">        <span class="keyword">if</span> self.wc[feat]:</div><div class="line">            _samples = np.sort(self._x.T[feat])</div><div class="line">            _set = (_samples[:<span class="number">-1</span>] + _samples[<span class="number">1</span>:]) * <span class="number">0.5</span></div><div class="line">        <span class="keyword">elif</span> self.is_cart:</div><div class="line">            _set = self.tree.feature_sets[feat]</div><div class="line">        <span class="comment"># 然后遍历这些二分标准并调用二类问题相关的、计算信息量的方法</span></div><div class="line">        <span class="keyword">if</span> self.is_cart <span class="keyword">or</span> self.wc[feat]:</div><div class="line">            <span class="keyword">for</span> tar <span class="keyword">in</span> _set:</div><div class="line">                _tmp_gain, _tmp_chaos_lst = _cluster.bin_info_gain(</div><div class="line">                    feat, tar, criterion=self.criterion,</div><div class="line">                    get_chaos_lst=<span class="keyword">True</span>, continuous=self.wc[feat])</div><div class="line">                <span class="keyword">if</span> _tmp_gain &gt; _max_gain:</div><div class="line">                    (_max_gain, _chaos_lst), _max_feature, _max_tar = (</div><div class="line">                        _tmp_gain, _tmp_chaos_lst), feat, tar</div><div class="line">        <span class="comment"># 对于离散型特征的ID3和C4.5算法，调用普通的计算信息量的方法</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            _tmp_gain, _tmp_chaos_lst = _cluster.info_gain(</div><div class="line">                feat, self.criterion, <span class="keyword">True</span>, self.tree.feature_sets[feat])</div><div class="line">            <span class="keyword">if</span> _tmp_gain &gt; _max_gain:</div><div class="line">                (_max_gain, _chaos_lst), _max_feature = (</div><div class="line">                    _tmp_gain, _tmp_chaos_lst), feat</div><div class="line">    <span class="comment"># 若满足第二种停止准则、则退出函数体</span></div><div class="line">    <span class="keyword">if</span> self.stop2(_max_gain, eps):</div><div class="line">        <span class="keyword">return</span></div><div class="line">    <span class="comment"># 更新相关的属性</span></div><div class="line">    self.feature_dim = _max_feature</div><div class="line">    <span class="keyword">if</span> self.is_cart <span class="keyword">or</span> self.wc[_max_feature]:</div><div class="line">        self.tar = _max_tar</div><div class="line">        <span class="comment"># 调用根据划分标准进行生成的方法</span></div><div class="line">        self._gen_children(_chaos_lst)</div><div class="line">        <span class="comment"># 如果该Node的左子节点和右子节点都是叶节点且所属类别一样</span></div><div class="line">        <span class="comment"># 那么就将它们合并、亦即进行局部剪枝</span></div><div class="line">        <span class="keyword">if</span> (self.left_child.category <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span></div><div class="line">                self.left_child.category == self.right_child.category):</div><div class="line">            self.prune()</div><div class="line">            <span class="comment"># 调用Tree的相关方法，将被剪掉的、该Node的左右子节点</span></div><div class="line">            <span class="comment"># 从Tree的记录所有Node的列表nodes中除去</span></div><div class="line">            self.tree.reduce_nodes()</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># 调用根据划分标准进行生成的方法</span></div><div class="line">        self._gen_children(_chaos_lst)</div></pre></td></tr></table></figure>
<p>根据划分标准进行生成的方法相当冗长、因为需要进行相当多的分情况讨论。它的实现用到了递归的思想，真正写起来就会发现其实并不困难、只不过会有些繁琐。囿于篇幅、我们略去它的实现细节，其算法描述则如下：</p>
<ul>
<li>根据划分标准将数据划分成若干份</li>
<li>依次用这若干份数据实例化新 Node（新 Node 即是当前 Node 的子节点），同时将当前 Node 的相关信息传给新 Node。这里需要注意的是，如果划分标准是离散型特征的话：<ul>
<li>若算法是 ID3 或 C4.5，需将该特征对应的维度从新 Node 的<code>self.feats</code>属性中除去</li>
<li>若算法是 CART，需要将二分标准从新 Node 的二分标准取值集合中除去</li>
</ul>
</li>
<li>最后对新 Node 调用<code>fit</code>方法、完成递归</li>
</ul>
<p>我个人实现的版本可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py#L171" target="_blank" rel="external">这里</a></p>
<p>以上我们就实现了 Node 结构并在其上实现了决策树的生成算法，接下来我们要做的就是实现 Tree 结构来将各个 Node 封装起来</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;（本文会用到的所有代码都在&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py&quot;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;在实现完计算各种信息量的函数之后，我们就可以着手实现决策树本身了。由前文的讨论可知、组成决策树主体的是一个个的 Node，所以我们接下来首先要实现的就是 Node 这个结构。而且由于我们所关心的 ID3、C4.5 和 CART 分类树的 Node 在大多数情况下表现一致、只有少数几个地方有所不同，因此我们可以写一个统一的 Node 结构的基类&lt;code&gt;CvDNode&lt;/code&gt;来囊括我们所有关心的决策树生成算法，该基类需要实现如下功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据离散型特征划分数据（ID3、C4.5、CART）&lt;/li&gt;
&lt;li&gt;根据连续型特征划分数据（C4.5、CART）&lt;/li&gt;
&lt;li&gt;根据当前的数据判定所属的类别&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;虽然说起来显得轻巧，但这之中的抽象还是比较繁琐的&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>信息量计算的实现</title>
    <link href="http://www.carefree0910.com/posts/e0705aab/"/>
    <id>http://www.carefree0910.com/posts/e0705aab/</id>
    <published>2017-04-22T14:07:57.000Z</published>
    <updated>2017-04-22T14:20:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Cluster.py" target="_blank" rel="external">这里</a>）</p>
<p>由于决策树的生成算法中会用到各种定义下的信息量的计算，所以我们应该先把这些计算信息量相关的算法实现出来。注意到这些算法同样是在不断地进行计数工作，所以我们同样需要尽量尝试利用好上一章讲述过的 bincount 方法。由于我们是在决策树模型中调用这些算法的，所以数据预处理应该交由决策树来做、这里就只需要专注于算法本身。值得一提的是，这一套算法不仅能够应用在决策树中，在遇到任何其它需要计算信息量的场合时都能够进行应用</p>
<a id="more"></a>
<p>首先实现其基本结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cluster</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._x, self._y：记录数据集的变量</div><div class="line">        self._counters：类别向量的计数器，记录第i类数据的个数</div><div class="line">            self._sample_weight：记录样本权重的属性</div><div class="line">        self._con_chaos_cache, self._ent_cache, self._gini_cache：记录中间结果的属性</div><div class="line">            self._base：记录对数的底的属性</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x, y, sample_weight=None, base=<span class="number">2</span>)</span>:</span></div><div class="line">        <span class="comment"># 这里我们要求输入的是Numpy向量（矩阵）</span></div><div class="line">        self._x, self._y = x.T, y</div><div class="line">        <span class="comment"># 利用样本权重对类别向量y进行计数</span></div><div class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            self._counters = np.bincount(self._y)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self._counters = np.bincount(self._y,</div><div class="line">                weights=sample_weight*len(sample_weight))</div><div class="line">        self._sample_weight = sample_weight</div><div class="line">        self._con_chaos_cache = self._ent_cache = self._gini_cache = <span class="keyword">None</span></div><div class="line">        self._base = base</div></pre></td></tr></table></figure>
<p>接下来就需要定义计算不确定性的两个函数。由于一个 Cluster 只接受一份数据，所以其实总的不确定性只用计算一次：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算信息熵的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ent</span><span class="params">(self, ent=None, eps=<span class="number">1e-12</span>)</span>:</span></div><div class="line">    <span class="comment"># 如果已经计算过且调用时没有额外给各类别样本的个数、就直接调用结果</span></div><div class="line">    <span class="keyword">if</span> self._ent_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> ent <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> self._ent_cache</div><div class="line">    _len = len(self._y)</div><div class="line">    <span class="comment"># 如果调用时没有给各类别样本的个数，就利用结构本身的计数器来获取相应个数</span></div><div class="line">    <span class="keyword">if</span> ent <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        ent = self._counters</div><div class="line">    <span class="comment"># 使用eps来让算法的数值稳定性更好</span></div><div class="line">    _ent_cache = max(eps, -sum(</div><div class="line">        [_c / _len * math.log(_c / _len, self._base) <span class="keyword">if</span> _c != <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> _c <span class="keyword">in</span> ent]))</div><div class="line">    <span class="comment"># 如果调用时没有给各类别样本的个数、就将计算好的信息熵储存下来</span></div><div class="line">    <span class="keyword">if</span> ent <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        self._ent_cache = _ent_cache</div><div class="line">    <span class="keyword">return</span> _ent_cache</div><div class="line"></div><div class="line"><span class="comment"># 定义计算基尼系数的函数，和计算信息熵的函数很类似、所以略去注释</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gini</span><span class="params">(self, p=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> self._gini_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> p <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> self._gini_cache</div><div class="line">    <span class="keyword">if</span> p <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        p = self._counters</div><div class="line">    _gini_cache = <span class="number">1</span> - np.sum((p / len(self._y)) ** <span class="number">2</span>)</div><div class="line">    <span class="keyword">if</span> p <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        self._gini_cache = _gini_cache</div><div class="line">    <span class="keyword">return</span> _gini_cache</div></pre></td></tr></table></figure>
<p>然后就需要定义计算<script type="math/tex">H(y|A)</script>和<script type="math/tex">\text{Gini}(y|A)</script>的函数。从算法公式可以看出它们具有形式一致性，所以我们可以把它们的实现整合在一起：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算和的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">con_chaos</span><span class="params">(self, idx, criterion=<span class="string">"ent"</span>, features=None)</span>:</span></div><div class="line">    <span class="comment"># 根据不同的准则、调用不同的方法</span></div><div class="line">    <span class="keyword">if</span> criterion == <span class="string">"ent"</span>:</div><div class="line">        _method = <span class="keyword">lambda</span> cluster: cluster.ent()</div><div class="line">    <span class="keyword">elif</span> criterion == <span class="string">"gini"</span>:</div><div class="line">        _method = <span class="keyword">lambda</span> cluster: cluster.gini()</div><div class="line">    <span class="comment"># 根据输入获取相应维度的向量</span></div><div class="line">    data = self._x[idx]</div><div class="line">    <span class="comment"># 如果调用时没有给该维度的取值空间features、就调用set方法获得该取值空间</span></div><div class="line">    <span class="comment"># 由于调用set方法比较耗时，在决策树实现时应努力将features传入</span></div><div class="line">    <span class="keyword">if</span> features <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        features = set(data)</div><div class="line">    <span class="comment"># 获得该维度特征各取值所对应的数据的下标</span></div><div class="line">    <span class="comment"># 用self._con_chaos_cache记录下相应结果以加速后面定义的相关函数</span></div><div class="line">    tmp_labels = [data == feature <span class="keyword">for</span> feature <span class="keyword">in</span> features]</div><div class="line">    self._con_chaos_cache = [np.sum(_label) <span class="keyword">for</span> _label <span class="keyword">in</span> tmp_labels]</div><div class="line">    <span class="comment"># 利用下标获取相应的类别向量</span></div><div class="line">    label_lst = [self._y[label] <span class="keyword">for</span> label <span class="keyword">in</span> tmp_labels]</div><div class="line">    rs, chaos_lst = <span class="number">0</span>, []</div><div class="line">    <span class="comment"># 遍历各下标和对应的类别向量</span></div><div class="line">    <span class="keyword">for</span> data_label, tar_label <span class="keyword">in</span> zip(tmp_labels, label_lst):</div><div class="line">        <span class="comment"># 获取相应的数据</span></div><div class="line">        tmp_data = self._x.T[data_label]</div><div class="line">        <span class="comment"># 根据相应数据、类别向量和样本权重计算出不确定性</span></div><div class="line">        <span class="keyword">if</span> self._sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            _chaos = _method(Cluster(tmp_data, tar_label, base=self._base))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            _new_weights = self._sample_weight[data_label]</div><div class="line">            _chaos = _method(Cluster(tmp_data, tar_label, _new_weights / np.sum(</div><div class="line">                _new_weights), base=self._base))</div><div class="line">        <span class="comment"># 依概率加权，同时把各个初始条件不确定性记录下来</span></div><div class="line">        rs += len(tmp_data) / len(data) * _chaos</div><div class="line">        chaos_lst.append(_chaos)</div><div class="line">    <span class="keyword">return</span> rs, chaos_lst</div></pre></td></tr></table></figure>
<p><strong><em>注意：如果仅仅是为了获得总的条件不确定性、是不用将划分后数据的各个部分的条件不确定记录下来的；之所以我们把它记录下来、是因为在决策树生成的过程里会用到这个中间变量。我们会在后面讲解决策树结构时进行相应的说明</em></strong></p>
<p>最后需要定义计算信息增益的函数。我们将会实现涉及过的三种定义方法，而且由于它们同样具有形式一致性、所以它们的实现同样可以整合在一起：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算信息增益的函数，参数get_chaos_lst用于控制输出</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">info_gain</span><span class="params">(self, idx, criterion=<span class="string">"ent"</span>, get_chaos_lst=False, features=None)</span>:</span></div><div class="line">    <span class="comment"># 根据不同的准则、获取相应的“条件不确定性”</span></div><div class="line">    <span class="keyword">if</span> criterion <span class="keyword">in</span> (<span class="string">"ent"</span>, <span class="string">"ratio"</span>):</div><div class="line">        _con_chaos, _chaos_lst = self.con_chaos(idx, <span class="string">"ent"</span>, features)</div><div class="line">        _gain = self.ent() - _con_chaos</div><div class="line">        <span class="keyword">if</span> criterion == <span class="string">"ratio"</span>:</div><div class="line">            _gain /= self.ent(self._con_chaos_cache)</div><div class="line">    <span class="keyword">elif</span> criterion == <span class="string">"gini"</span>:</div><div class="line">        _con_chaos, _chaos_lst = self.con_chaos(idx, <span class="string">"gini"</span>, features)</div><div class="line">        _gain = self.gini() - _con_chaos</div><div class="line">    <span class="keyword">return</span> (_gain, _chaos_lst) <span class="keyword">if</span> get_chaos_lst <span class="keyword">else</span> _gain</div></pre></td></tr></table></figure>
<p>考虑到二类问题的特殊性，我们需要定义专门处理二类问题的、计算信息增益相关的函数。它们大部分和以上定义的函数没有区别、代码也有大量重复，只是它会多传进一个代表二分标准的参数。为简洁，我们略去上文已经给出过的注释</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算二类问题条件不确定性的函数</span></div><div class="line"><span class="comment"># 参数tar即是二分标准，参数continuous则告诉我们该维度的特征是否连续</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bin_con_chaos</span><span class="params">(self, idx, tar, criterion=<span class="string">"gini"</span>, continuous=False)</span>:</span></div><div class="line">    <span class="keyword">if</span> criterion == <span class="string">"ent"</span>:</div><div class="line">        _method = <span class="keyword">lambda</span> cluster: cluster.ent()</div><div class="line">    <span class="keyword">elif</span> criterion == <span class="string">"gini"</span>:</div><div class="line">        _method = <span class="keyword">lambda</span> cluster: cluster.gini()</div><div class="line">    data = self._x[idx]</div><div class="line">    <span class="comment"># 根据二分标准划分数据，注意要分离散和连续两种情况讨论</span></div><div class="line">    tar = data == tar <span class="keyword">if</span> <span class="keyword">not</span> continuous <span class="keyword">else</span> data &lt; tar</div><div class="line">    tmp_labels = [tar, ~tar]</div><div class="line">    self._con_chaos_cache = [np.sum(_label) <span class="keyword">for</span> _label <span class="keyword">in</span> tmp_labels]</div><div class="line">    label_lst = [self._y[label] <span class="keyword">for</span> label <span class="keyword">in</span> tmp_labels]</div><div class="line">    rs, chaos_lst = <span class="number">0</span>, []</div><div class="line">    <span class="keyword">for</span> data_label, tar_label <span class="keyword">in</span> zip(tmp_labels, label_lst):</div><div class="line">        tmp_data = self._x.T[data_label]</div><div class="line">        <span class="keyword">if</span> self._sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            _chaos = _method(Cluster(tmp_data, tar_label, base=self._base))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            _new_weights = self._sample_weight[data_label]</div><div class="line">            _chaos = _method(Cluster(tmp_data, tar_label, _new_weights / np.sum(</div><div class="line">                _new_weights), base=self._base))</div><div class="line">        rs += len(tmp_data) / len(data) * _chaos</div><div class="line">        chaos_lst.append(_chaos)</div><div class="line">    <span class="keyword">return</span> rs, chaos_lst</div></pre></td></tr></table></figure>
<p>定义计算二类问题信息增益的函数时，只需将之前定义过的、计算信息增益的函数中计算条件不确定性的函数替换成计算二类问题条件不确定性的函数即可</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;（本文会用到的所有代码都在&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Cluster.py&quot;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;由于决策树的生成算法中会用到各种定义下的信息量的计算，所以我们应该先把这些计算信息量相关的算法实现出来。注意到这些算法同样是在不断地进行计数工作，所以我们同样需要尽量尝试利用好上一章讲述过的 bincount 方法。由于我们是在决策树模型中调用这些算法的，所以数据预处理应该交由决策树来做、这里就只需要专注于算法本身。值得一提的是，这一套算法不仅能够应用在决策树中，在遇到任何其它需要计算信息量的场合时都能够进行应用&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>决策树的生成算法</title>
    <link href="http://www.carefree0910.com/posts/c6faa205/"/>
    <id>http://www.carefree0910.com/posts/c6faa205/</id>
    <published>2017-04-22T12:11:18.000Z</published>
    <updated>2017-04-23T02:58:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>虽然我们之前已经用了许多文字来描述决策树、但可能还是显得过于抽象。为了能有直观的认知，在此援引维基百科上一张很好的图来进行说明：</p>
<img src="/posts/c6faa205/p1.png" alt="p1.png" title="">
<p>这张图基本蕴含了决策树中所有的关键结构，下面我们分开来分析它们</p>
<a id="more"></a>
<h1 id="决策树的相关术语"><a href="#决策树的相关术语" class="headerlink" title="决策树的相关术语"></a>决策树的相关术语</h1><p>首先是之前分析过的、“划分标准”这个概念。在上图中，被菱形橙色方框框起来的就是作为划分标准的特征，被长方形橙色方框框起来的就是对应特征的各个取值</p>
<p>然后是“节点”（Node）的概念。如果读者有学过数据结构，那么想必在提起“树”（Tree）的同时会自然而然地联想到 Node。考虑到可能有读者没有学过相关知识，这里就简要说一些相关的定义。决策树，顾名思义，确实是一个 Tree 模型。在上图中，我们可以直观地把整张图想象成一棵 Tree、把被黑色边框框起来的部分理解为一个 Node，这些 Node 是 Tree 的组成部分、Tree 本身可以协助 Node 之间的数据传输和参数调用。最上方的 Node 酷似整棵树的根，我们一般称其为“根节点”（Root）；用黑色方框（亦即四个角是直角而不是圆弧）框起来的 Node 是整棵树“生长的终点”、酷似树的叶子，我们一般称其为“叶节点”（Leaf）。比如，第三排的所有 Node 都是叶节点</p>
<p>通常来说，我们还会称一个非叶节点有一些“下属的叶节点”。比如，所有的叶节点都是根节点下属的叶节点，第三排左数第一、第二个叶节点是第二排左数第一个 Node 下属的叶节点</p>
<p>决策树中的叶节点还有一个有趣的特性：每个叶节点都对应着原样本空间的一个子空间，这些叶节点对应的子空间彼此不会相交、且并起来的话就会恰好构成完整的样本空间。换句话说、决策树的行为可以概括为如下两步：</p>
<ul>
<li>将样本空间划分为若干个互不相交的子空间</li>
<li>给每个子空间贴一个类别标签</li>
</ul>
<p>此外、我们在上图中还可以看到许多箭头，这些箭头代表着树的“生长方向”。我们一般习惯称箭头的起点是终点的“父节点”（parent）、终点是起点的“子节点”（child）；而当子节点只有两个时，通常把他们称作“左子节点”和“右子节点”。比如说，根节点是第二排所有 Node 的父节点，第二排所有 Node 都是根节点的子节点；第三排左数第一、第二个 Node 是第二排左数第一个 Node 的左、右子节点</p>
<p>对决策树有一个直观认知后，我们关心的就是怎样去生成这么一个结构了</p>
<h1 id="决策树的生成算法"><a href="#决策树的生成算法" class="headerlink" title="决策树的生成算法"></a>决策树的生成算法</h1><p>决策树的生成算法发展至今已经有许多变种，想要全面介绍它们不是短短一篇文章所能做到的。本文拟介绍其中三个上一节有所提及的、相对而言比较基本的算法：ID3、C4.5 和 CART。它们本身存在着某种递进关系：</p>
<ul>
<li>ID3 算法可说是“最朴素”的决策树算法，它给出了对离散型数据分类的解决方案</li>
<li>C4.5 算法在其上进一步发展、给出了对混合型数据分类的解决方案</li>
<li>CART 算法则更进一步、给出了对数据回归的解决方案</li>
</ul>
<p>虽说它们的功能越来越强大，但正如第一小节所言、它们的核心思想都是一致的：算法通过不断划分数据集来生成决策树，其中每一步的划分能够使当前的信息增益达到最大</p>
<p>值得一提的是，该核心思想的背后其实也有着机器学习的一些普适性的思想。我们可以这样来看待决策树：模型的损失就是数据集的不确定性，模型的算法就是最小化该不确定性；同时，和许多其它模型一样，想要从整个参数空间中选出模型的最优参数是个 NP 完全问题，所以我们（和许多其它算法一样）采用启发式的方法、近似求解这个最优化问题。具体而言，我们每次会选取一个局部最优解（每次选取一个特征对数据集进行划分使得信息增益最大化）、并把这些局部解合成最终解（合成一个划分规则的序列）</p>
<p>可以这样直观地去想一个决策树的生成过程：</p>
<ul>
<li>向根节点输入数据</li>
<li>根据信息增益的度量、选择数据的某个特征来把数据划分成（互不相交的）好几份并分别喂给一个新 Node</li>
<li>如果分完数据后发现：<ul>
<li>某份数据的不确定较小、亦即其中某一类别的样本已经占了大多数，此时就不再对这份数据继续进行划分、将对应的 Node 转化为叶节点</li>
<li>某份数据的不确定性仍然较大，那么这份数据就要继续分割下去（转第 2 步）</li>
</ul>
</li>
</ul>
<p><strong><em>注意：虽然划分的规则是根据数据定出的，但是划分本身其实是针对整个输入空间进行划分的</em></strong></p>
<p>从上述过程可知，决策树的生成过程就是根据某个度量从数据集中训练出一系列的划分规则、使得这些规则能够在数据集有好的表现。事实上，上文说的3种不同算法在分类问题上的区别亦仅表现在度量信息增益和划分数据的方法的不同上</p>
<h2 id="ID3（Interactive-Dichotomizer-3）"><a href="#ID3（Interactive-Dichotomizer-3）" class="headerlink" title="ID3（Interactive Dichotomizer-3）"></a>ID3（Interactive Dichotomizer-3）</h2><p>ID3 可以译为“交互式二分法”，虽说这个名字里面带了个“二分”，但该方法完全适用于“多分”的情况。它选择互信息作为信息增益的度量、针对离散型数据进行划分。其算法叙述如下：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,(x_{N},y_{N})\}</script></li>
<li><strong>过程</strong>：<ol>
<li>将数据集<script type="math/tex">D</script>喂给一个 Node</li>
<li>若<script type="math/tex">D</script>中的所有样本同属于类别<script type="math/tex">c_{k}</script>，则该 Node 不再继续生成、并将其类别标记为<script type="math/tex">c_{k}</script>类</li>
<li>若<script type="math/tex">x_{i}</script>已经是 0 维向量、亦即已没有可选特征，则将此时<script type="math/tex">D</script>中样本个数最多的类别<script type="math/tex">c_{k}</script>作为该 Node 的类别</li>
<li>否则，按照互信息定义的信息增益：  <script type="math/tex; mode=display">
g\left( y,x^{\left( j \right)} \right) = H\left( y \right) - H(y|x^{\left( j \right)})</script>来计算第 j 维特征的信息增益，然后选择使得信息增益最大的特征<script type="math/tex">x^{\left( j^{*} \right)}</script>作为划分标准，亦即：  <script type="math/tex; mode=display">
j^{*} = \arg{\max_{j}{g\left( y,x^{\left( j \right)} \right)}}</script></li>
<li>若<script type="math/tex">x^{\left( j^{*} \right)}</script>满足停止条件、则不再继续生成并则将此时<script type="math/tex">D</script>中样本个数最多的类别<script type="math/tex">c_{k}</script>作为类别标记</li>
<li>否则，依<script type="math/tex">x^{\left( j^{*} \right)}</script>的所有可能取值<script type="math/tex">\{ a_{1},\ldots,a_{m}\}</script>将数据集<script type="math/tex">D</script>划分为<script type="math/tex">{\{ D}_{1},\ldots,D_{m}\}</script>、使得：  <script type="math/tex; mode=display">
{(x}_{i},y_{i}) \in D_{j} \Leftrightarrow x_{i}^{\left( j^{*} \right)} = a_{j},\forall i = 1,\ldots,N</script>同时，将<script type="math/tex">x_{1},\ldots,x_{N}</script>的第<script type="math/tex">j^{*}</script>维去掉、使它们成为<script type="math/tex">n - 1</script>维的特征向量</li>
<li>对每个<script type="math/tex">D_{j}</script>从 2.1 开始调用算法</li>
</ol>
</li>
<li><strong>输出</strong>：原始数据对应的 Node（亦即根节点）</li>
</ol>
<p>其中算法第 2.5 步的“停止条件”（也可称为“预剪枝”；有关剪枝的讨论会放在<a href="/posts/1a7aa546/" title="决策树的剪枝算法">决策树的剪枝算法</a>）有许多种提法，常用的是如下两种：</p>
<ul>
<li>若选择<script type="math/tex">x^{\left( j^{*} \right)}</script>作为特征时信息增益<script type="math/tex">g(y,x^{\left( j^{*} \right)})</script>仍然很小（通常会传入一个参数<script type="math/tex">\epsilon</script>作为阈值）、则停止</li>
<li>事先把数据集分为训练集与测试集（交叉验证的思想），若由训练集得到的<script type="math/tex">x^{\left( j^{*} \right)}</script>并不能使得决策树在测试集上的错误率更小、则停止</li>
</ul>
<p>这两种停止条件的提法通用于 C4.5 和 CART，后文将不再赘述。同时，正如本章一开始有所提及的、决策树会在许多地方应用到递归的思想，上述算法中的第 2.6 步正是经典的递归</p>
<p>我们可以对气球数据集 1.0 过一遍 ID3 算法以加深理解。由算法可知，因为每个 Node 的信息熵是确定的、所以选择互信息最大的特征等价于选择条件熵最小的特征，是故我们只需要在每个 Node 上计算各个可选特征的条件熵。易知在根节点上：</p>
<script type="math/tex; mode=display">
\begin{align}
H\left( 不爆炸\middle| 颜色\right) &= - p_{11}\log p_{11} - p_{12}\log p_{12} = 1 \\

H\left( 不爆炸\middle| 大小\right) &= - p_{21}\log p_{21} - p_{22}\log p_{22} \approx 0.65 \\

H\left( 不爆炸\middle| 人员\right) &= - p_{31}\log p_{31} - p_{32}\log p_{32} \approx 0.92 \\

H\left( 不爆炸\middle| 动作\right) &= - p_{41}\log p_{41} - p_{42}\log p_{42} \approx 0.65
\end{align}</script><p>其中</p>
<script type="math/tex; mode=display">p_{11} \triangleq p\left( 不爆炸\middle| 黄色\right) = \frac{1}{2},\ \ p_{12} \triangleq p\left( 不爆炸\middle| 紫色\right) = \frac{1}{2}</script><script type="math/tex; mode=display">p_{21} \triangleq p\left( 不爆炸\middle| 小\right) = \frac{5}{6},\ \ p_{22} \triangleq p\left( 不爆炸\middle| 大\right) = \frac{1}{6}</script><script type="math/tex; mode=display">p_{31} \triangleq p\left( 不爆炸\middle| 成人\right) = \frac{1}{3},\ \ p_{32} \triangleq p\left( 不爆炸\middle| 小孩\right) = \frac{2}{3}</script><script type="math/tex; mode=display">p_{41} \triangleq p\left( 不爆炸\middle| 手打\right) = \frac{5}{6},\ \ p_{42} \triangleq p\left( 不爆炸\middle| 脚踩\right) = \frac{1}{6}</script><p>且易知</p>
<script type="math/tex; mode=display">H\left( 不爆炸\middle| A \right) = H\left( 爆炸\middle| A \right),\ \ \forall A \in \{ 颜色, 大小, 人员, 动作\}</script><p>从而可知应选测试动作或者气球大小作为根节点的划分标准。不妨选择气球大小作为划分标准，此时的决策树如下图所示（图片是使用 ProcessOn 在线绘制而成的）：</p>
<img src="/posts/c6faa205/p2.png" alt="p2.png" title="">
<p>图中 Node A 和 Node B 所对应的数据集分别如下面两张表所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用手打</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>注意此时气球大小已经不是可选特征了。我们接下来要分别对 Node A 和 Node B 调用 ID3 算法，计算过程和根节点上的计算过程大同小异。以此类推、最终我们可以得到如下图所示的决策树：</p>
<img src="/posts/c6faa205/p3.png" alt="p3.png" title="">
<p>可知该决策树在气球数据集 1.0 上的正确率为 100%、且它做的决策都很符合直观</p>
<h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><p>C4.5 使用信息增益比作为信息增益的度量，从而缓解了 ID3 算法会倾向于选择 m 比较大的特征<script type="math/tex">A</script>作为划分依据这个问题；也正因如此，C4.5 算法可以处理 ID3 算法比较难处理的混合型数据。我们先来看看它在离散型数据上的算法（仅展示和 ID3 算法中不同的部分）：</p>
<h3 id="算法-2-4-步"><a href="#算法-2-4-步" class="headerlink" title="算法 2.4 步"></a>算法 2.4 步</h3><p>否则，按照信息增益比定义的信息增益：</p>
<script type="math/tex; mode=display">
g_{R}\left( y,x^{\left( j \right)} \right) = \frac{g(y,x^{\left( j \right)})}{H_{x^{\left( j \right)}}(y)}</script><p>来计算第 j 维特征的信息增益，然后选择使得信息增益最大的特征<script type="math/tex">x^{\left( j^{*} \right)}</script>作为划分标准，亦即：</p>
<script type="math/tex; mode=display">
j^{*} = \arg{\max_{j}{g_{R}\left( y,x^{\left( j \right)} \right)}}</script><p><strong><em>注意：C4.5 算法虽然不会倾向于选择 m 比较大的特征、但有可能会倾向于选择 m 比较小的特征。针对这个问题，Quinlan 在 1993 年提出了这么一个启发式的方法：先选出互信息比平均互信息要高的特征、然后从这些特征中选出信息增益比最高的</em></strong></p>
<p>混合型数据的处理方法大同小异，本书拟介绍一种常用且符合直观的、同时亦是 C4.5 所采用的方法：使用二类问题的解决方案处理连续型特征。具体而言，当二类问题和决策树结合起来时，在连续的情况下、我们通常可以把它转述为：</p>
<script type="math/tex; mode=display">
Y_{1} = \left\{ y:y^{A} < a_{1} \right\},Y_{2} = \{ y:y^{A} \geq a_{1}\}</script><p>相对应的，我们同样可以用处理二类问题的思想来处理离散型特征，此时：</p>
<script type="math/tex; mode=display">
A \in \left\{ a_{1},a_{2} \right\};\ \ Y_{1} = \left\{ y:y^{A} = a_{1} \right\},\ Y_{2} = \{ y:y^{A} = a_{2}\}</script><p>更进一步、我们通常会将它表示为：</p>
<script type="math/tex; mode=display">
Y_{1} = \left\{ y:y^{A} = a_{1} \right\},Y_{2} = \{ y:y^{A} \neq a_{1}\}</script><p>我们通常称以上各式中的<script type="math/tex">a_{1}</script>为“二分标准”。一般而言，如何处理连续型特征这个问题会归结于如何选择“二分标准”这个问题。一个比较容易想到的做法是：</p>
<ul>
<li>若在当前数据集中有 m 个取值，不妨假设它们为<script type="math/tex">u_1,...,u_m</script>；不失一般性、再不妨假设它们满足<script type="math/tex">u_1<...<u_m</script>（若不然，进行一次排序操作即可），那么依次选择<script type="math/tex">v_1,...,v_p</script>作为二分标准并决出最好的一个，其中<script type="math/tex">v_1,...,v_p</script>构成等差数列、且:  <script type="math/tex; mode=display">
v_{1} = u_{1},v_{p} = u_{m}</script>p 的选取则视情况而定，一般而言会取 p 反比于“深度”。这意味着当数据越分越细时、对特征的划分会越分越粗，从直观上来说这有益于防止过拟合</li>
</ul>
<p>但这样可能会产生许多“冗余”的二分标准。试想如果这些取值满足：</p>
<script type="math/tex; mode=display">
u_{1} = 0,u_{2} = 100,u_{3} = 101,u_{4} = 102,\ldots</script><p>那么我们就会在<script type="math/tex">u_{1}</script>和<script type="math/tex">u_{2}</script>之间尝试大量的划分标准、但显然这些划分标准算出来的结果都是一样的。为了处理类似于这种不合理的情况，C4.5 采用如下做法：</p>
<ul>
<li>依次选择<script type="math/tex">v_{1} = \frac{u_{1} + u_{2}}{2},\ldots,v_{m - 1} = \frac{u_{m - 1} + u_{m}}{2}</script>作为二分标准、计算它们的信息增益比、从而决出最好的二分标准来划分数据</li>
</ul>
<p>在这之上还有另一种可行的做法：</p>
<ul>
<li>设<script type="math/tex">u_1,...,u_m</script>所对应的类别是<script type="math/tex">y_1,...,y_m</script>，那么在<script type="math/tex">v_1,...,v_{m-1}</script>中只选取使得：  <script type="math/tex; mode=display">
y_{i} \neq y_{i + 1},\ \ (i = 1,\ldots,m - 1)</script>的<script type="math/tex">v_i</script>作为二分标准、计算它们的信息增益比、从而决出最好的二分标准来划分数据</li>
</ul>
<p>这种做法在某些情况下会表现得更好、但在某些情况下也会显得不合理。鉴于此，本书会采用更稳定的上一种做法来进行实现</p>
<p><strong><em>注意：从以上讨论可知，我们完全可以把 ID3 算法推广成可以处理连续型特征的算法。只不过如果数据集是混合型数据集的话、ID3 就会倾向于选择离散型特征作为划分标准而已。如果数据集的所有特征都是连续型特征、那么 ID3 和 C4.5 之间孰优孰劣是难有定论的</em></strong></p>
<p>这里需要特别指出的是、C4.5 也是有一个比较糟糕的性质的：由信息增益比的定义可知，如果是二分的话、它会倾向于把数据集分成很不均匀的两份；因为此时<script type="math/tex">H_{A}(y)</script>将非常小、导致<script type="math/tex">g_{R}(y,A)</script>很大（即使<script type="math/tex">g(y,A)</script>比较小）。举个例子：如果当前划分标准为连续特征、那么 C4.5 可能会倾向于直接选择<script type="math/tex">v_{1},v_{2},v_{3}</script>等作为二分标准</p>
<p>之所以说该性质比较糟糕、是因为它将直接导致如下结果：当 C4.5 进行二叉分枝时、它可能总会直接分出一个比较小的 Node 作为叶节点、然后剩下一个大的 Node 继续进行生成。这种行为会导致决策树倾向于往深处发展、从而导致很容易产生过拟合现象，这并不是我们期望的结果</p>
<h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><p>CART 的全称是 Classification and Regression Tree（“分类与回归树”）。顾名思义，它既可做分类亦可做回归。囿于篇幅，本书会介绍分类问题的算法和实现、对于回归问题则只叙述算法，感兴趣的观众老爷们可以尝试触类旁通地实现它</p>
<p>CART 算法一般会使用基尼增益作为信息增益的度量（当然也可以使用互信息和信息增益比作为度量，需要视具体场合而定），其一大特色就是它假设了最终生成的决策树为二叉树、亦即它在处理离散型特征时也会通过决出二分标准来划分数据：</p>
<h3 id="算法-2-4-步-1"><a href="#算法-2-4-步-1" class="headerlink" title="算法 2.4 步"></a>算法 2.4 步</h3><p>否则，不妨设<script type="math/tex">x^{\left( j \right)}</script>在当前数据集中有<script type="math/tex">S_{j}</script>个取值<script type="math/tex">u_{1}^{\left( j \right)},\ldots,u_{S_{j}}^{\left( j \right)}</script>且它们满足<script type="math/tex">u_{1}^{\left( j \right)} < \ldots < u_{S_{j}}^{\left( j \right)}</script>，则：</p>
<ul>
<li>若<script type="math/tex">x^{\left( j \right)}</script>是离散型的，则依次选取<script type="math/tex">u_{1}^{\left( j \right)},\ldots,u_{S_{j}}^{\left( j \right)}</script>作为二分标准<script type="math/tex">a_{p}</script>，此时：  <script type="math/tex; mode=display">
A_{jp} = \{ x^{\left( j \right)} = a_{p},x^{\left( j \right)} \neq a_{p}\}</script></li>
<li>若<script type="math/tex">x^{\left( j \right)}</script>是连续型的，则依次选取<script type="math/tex">\frac{u_{1}^{\left( j \right)} + u_{2}^{\left( j \right)}}{2},\ldots,\frac{u_{S_{j} - 1}^{\left( j \right)} + u_{S_{j}}^{\left( j \right)}}{2}</script>作为二分标准<script type="math/tex">a_{p}</script>，此时：  <script type="math/tex; mode=display">
A_{jp} = \{ x^{\left( j \right)} < a_{p},x^{\left( j \right)} \geq a_{p}\}</script>按照基尼系数定义的信息增益：  <script type="math/tex; mode=display">
g_{\text{Gini}}\left( y,A_{jp} \right) = \text{Gini}\left( y \right) - \text{Gini}(y|A_{jp})</script>来计算第 j 维特征在这些二分标准下的信息增益，然后选择使得信息增益最大的特征<script type="math/tex">x^{\left( j^{*} \right)}</script>和相应的二分标准<script type="math/tex">u_{p^{*}}^{\left( j^{*} \right)}</script>作为划分标准，亦即：  <script type="math/tex; mode=display">
{(j}^{*},p^{*}) = \arg{\max_{j,p}{g_{\text{Gini}}\left( y,A_{jp} \right)}}</script></li>
</ul>
<p>从分类问题到回归问题不是一个不平凡的问题，它们的区别仅在于：回归问题除了特征是连续型的以外、“类别”也是连续型的，此时我们一般把“类别向量”改称为“输出向量”。正如前文所提及，决策树可以转化为最小化损失的问题。我们之前讨论的分类问题中的损失都是数据的不确定性，而在回归问题中、一种常见的做法就是将损失定义为平方损失：</p>
<script type="math/tex; mode=display">
L(D) = \sum_{i = 1}^{N}{I\left( y_{i} \neq f\left( x_{i} \right) \right)\left\lbrack y_{i} - f\left( x_{i} \right) \right\rbrack^{2}}</script><p>这里的<script type="math/tex">I</script>是示性函数，<script type="math/tex">f</script>是我们的模型、<script type="math/tex">f(x_{i})</script>是<script type="math/tex">x_{i}</script>在我们模型下的预测输出、<script type="math/tex">y_{i}</script>是真实输出。平方损失其实就是我们熟悉的“（欧式）距离”（预测向量和输出向量之间的距离），我们会在许多分类、回归问题中见到它的身影。在损失为平方损失时，一般称此时生成的回归决策树为最小二乘回归树</p>
<p>在分类问题中决策树是一个划分规则的序列、在回归问题中也差不多。具体而言，假设该序列一共会将输入空间划分为<script type="math/tex">R_{1},\ldots,R_{M}</script>（这 M 个子空间彼此不相交）、那么：</p>
<ul>
<li>对于分类问题，模型可表示为：  <script type="math/tex; mode=display">
f\left( x_{i} \right) = \sum_{m = 1}^{M}{y_{m}I(x_{i} \in R_{m})}</script></li>
<li>对于回归问题，模型可表示为：  <script type="math/tex; mode=display">
f\left( x_{i} \right) = \sum_{m = 1}^{M}{c_{m}I\left( x_{i} \in R_{m} \right)}</script>这里<script type="math/tex">c_{m} = \arg{\min_{c}{L_{m}\left( c \right)}} \triangleq \arg{\min_{c}{\sum_{\left( x_{i},y_{i} \right) \in R_{m}}^{}\left( y_{i} - c \right)^{2}}}</script>。那么由一阶条件：  <script type="math/tex; mode=display">
\frac{\partial L_{m}\left( c \right)}{\partial c} = 0 \Leftrightarrow - 2\sum_{\left( x_{i},y_{i} \right) \in R_{m}}^{}\left( y_{i} - c_{m} \right) = 0</script>可解得<script type="math/tex">c_{m} = \text{avg}\left( y_{i}|(x_{i},y_{i}) \in R_{m} \right) \triangleq \frac{1}{\left| R_{m} \right|}\sum_{\left( x_{i},y_{i} \right) \in R_{m}}^{}y_{i}</script></li>
</ul>
<p>最小二乘回归树的算法和 CART 做分类时的算法几乎完全一样，区别只在于：</p>
<ul>
<li>解决分类问题时，我们会在特征和二分标准选好后，通过求解：  <script type="math/tex; mode=display">
{(j}^{*},p^{*}) = \arg{\max_{j,p}{g_{\text{Gini}}\left( y,A_{jp} \right)}}</script>来选取划分标准</li>
<li>解决回归问题时，我们会在特征和二分标准选好后，通过求解：  <script type="math/tex; mode=display">
\left( j^{*},p^{*} \right) = \arg{\min_{j,p}{\lbrack\sum_{x_{i} < p}^{}{\left( y_{i} - c_{jp}^{\left( 1 \right)} \right)^{2} + \sum_{x_{i} \geq p}^{}\left( y_{i} - c_{jp}^{\left( 2 \right)} \right)^{2}}\rbrack}}</script>来选取划分标准，其中<script type="math/tex">c_{jp}^{\left( 1 \right)} = \text{avg}(y_{i}|x_{i} < p)</script>、<script type="math/tex">c_{jp}^{\left( 2 \right)} = \text{avg}(y_{i}|x_{i} \geq p)</script>，p（切分点）的选取则视情况而定（可以模仿分类问题中二分标准的选取方法）</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽然我们之前已经用了许多文字来描述决策树、但可能还是显得过于抽象。为了能有直观的认知，在此援引维基百科上一张很好的图来进行说明：&lt;/p&gt;
&lt;img src=&quot;/posts/c6faa205/p1.png&quot; alt=&quot;p1.png&quot; title=&quot;&quot;&gt;
&lt;p&gt;这张图基本蕴含了决策树中所有的关键结构，下面我们分开来分析它们&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>数据的信息</title>
    <link href="http://www.carefree0910.com/posts/2ce87ace/"/>
    <id>http://www.carefree0910.com/posts/2ce87ace/</id>
    <published>2017-04-22T11:29:24.000Z</published>
    <updated>2017-04-25T12:17:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文首先简要地说明一下决策树生成算法背后的数学基础和思想、然后再叙述具体的算法。往大了说、决策树的生成可以算是信息论的一个应用，但它其实只用到了信息论中一小部分的思想。不过，先对信息论有个概括性的认知还是有必要的、因为这样我们就可以有个更宽的视野</p>
<a id="more"></a>
<h1 id="信息论简介"><a href="#信息论简介" class="headerlink" title="信息论简介"></a>信息论简介</h1><p>（注：本节有许多内容节选、修改、总结自维基百科）<br>被誉为信息论创始人的是克劳德·艾尔伍德·香农（Claude Elwood Shannon，1916.4.30－2001.2.26），他是美国数学家、电子工程师和密码学家，是密歇根大学学士、麻省理工学院博士。他在 1948 年发表的划时代的论文——“通信的数学原理”奠定了现代信息论的基础</p>
<p>信息论（Information Theory）涉及的领域相当多，包括但不限于信息的量化、存储和通信、统计推断、自然语言处理、密码学等等。信息论的主要内容可以类比人类最广泛的交流手段——语言来阐述。一种简洁的语言（以英语为例）通常有如下两个重要特点：</p>
<ul>
<li>最常用的一些词汇（比如“a”、“the”、“I”）应该要比相对而言不太常用的词（比如“Python”、“Machine”、“Learning”）要短一些</li>
<li>如果句子的某一部分被漏听或者由于噪声干扰（比如身处闹市）而被误听，听者应该仍然可以抓住句子的大概意思</li>
</ul>
<p>其中第二点被称作为“鲁棒性（Robustness）”。如果把电子通信系统比作一种语言的话，这种鲁棒性（Robustness）不可或缺。信息论的基本研究课题是信源编码和信道编码（通俗一点来讲就是怎么发出信息和怎么传递信息），将鲁棒性引入通信正是通过其中的信道编码来完成的，由此可见信息论的重要性</p>
<p>注意这些内容同消息的重要性之间是毫不相干的。例如，像“你好；再见”这样的话语和像“救命”这样的紧急请求，在说起来或写起来所花的时间是差不多的，然而明显后者更重要也更有意义。信息论却不会考虑一段消息的重要性或内在意义，因为这些属于信息的质量的问题而不是信息量和可读性方面上的问题，后者只是由概率这一因素单独决定的</p>
<p>既然我们我们关注的是信息量，我们就需要有一个度量方法。决策树生成算法背后的思想正是利用该度量方法来衡量一种“数据划分”的优劣、从而生成一个“判定序列”。具体而言，它会不断地寻找数据的划分方法、使得在该划分下我们能够获得的信息量最大（更详细的叙述会在后文给出）</p>
<h1 id="不确定性"><a href="#不确定性" class="headerlink" title="不确定性"></a>不确定性</h1><p>在决策树的生成中，获得的信息量的度量方法是从反方向来定义的：若一种划分能使数据的“不确定性”减少得越多、就意味着该划分能获得越多信息。这是很符合直观的，关键问题就在于应该如何度量数据的不确定性（或说不纯度，Impurity）。常见的度量标准有两个：信息熵（Entropy）和基尼系数（Gini Index），接下来我们就说说它们的定义和性质</p>
<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>先来看看它的公式：</p>
<script type="math/tex; mode=display">
H\left( y \right) = - \sum_{k = 1}^{K}{p_{k}\log p_{k}}</script><p>对于具体的、随机变量<script type="math/tex">y</script>生成的数据集<script type="math/tex">D = \{ y_{1},\ldots,y_{N}\}</script>而言，在实际操作中通常会利用经验熵来估计真正的信息熵：</p>
<script type="math/tex; mode=display">
H\left( y \right) = H\left( D \right) = - \sum_{k = 1}^{K}{\frac{|C_{k}|}{|D|}\log\frac{|C_{k}|}{|D|}}</script><p>这里假设随机变量<script type="math/tex">y</script>的取值空间为<script type="math/tex">\{ c_{1},\ldots,c_{K}\}</script>，<script type="math/tex">p_{k}</script>表示<script type="math/tex">y</script>取<script type="math/tex">c_{k}</script>的概率：<script type="math/tex">p_{k} = p(y = c_{k})</script>；<script type="math/tex">|C_{k}|</script>代表由随机变量<script type="math/tex">y</script>中类别为<script type="math/tex">c_{k}</script>的样本的个数、<script type="math/tex">|D|</script>代表<script type="math/tex">D</script>的总样本个数（亦即<script type="math/tex">\left| D \right| = N</script>）。可以看到，经验公式背后的思想其实就是“频率估计概率”</p>
<p>通常来说，公式中对数的底会取为 2、此时信息熵<script type="math/tex">H(y)</script>的单位叫作比特（bit）；如果把底取为<script type="math/tex">e</script>（亦即取自然对数）的话，<script type="math/tex">H(y)</script>的单位就称作纳特（nat）</p>
<p>接下来说明为何上式能够度量数据的不确定性。可以证明（详细推导可参见<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>），当：</p>
<script type="math/tex; mode=display">
p_{1} = p_{2} = \ldots = p_{K} = \frac{1}{K}</script><p>时，<script type="math/tex">H(y)</script>达到最大值<script type="math/tex">- \log\frac{1}{K}</script>、亦即<script type="math/tex">\log K</script>。由于<script type="math/tex">p_{k} = p(y = c_{k})</script>，上式即意味着随机变量<script type="math/tex">y</script>取每一个类的概率都是一样的、亦即<script type="math/tex">y</script>完全没有规律可循，想要预测它的取值只能靠运气。换句话说，由<script type="math/tex">y</script>生成出来的数据<script type="math/tex">\{ y_{1},\ldots,y_{N}\}</script>的不确定性是在取值空间为<script type="math/tex">\{ c_{1},\ldots,c_{K}\}</script>、样本数为 N 的数据中最大的（想象预测 N 次正 K 面体骰子的结果）</p>
<p>我们的目的是想让<script type="math/tex">y</script>的不确定性减小、亦即想让<script type="math/tex">y</script>变得有规律以方便我们预测。稍微严谨地来说，就是<script type="math/tex">y</script>取某个类的概率特别大、取其它类的概率都特别小。极端的例子自然就是存在某个<script type="math/tex">k^{*}</script>、使得<script type="math/tex">p\left( {y = c}_{k^{*}} \right) = 1</script>、<script type="math/tex">p\left( y = c_{k} \right) = 0,\forall k \neq k^{*}</script>、亦即<em>y</em>生成的样本总属于<script type="math/tex">c_{k^{*}}</script>类。带入<script type="math/tex">H(y)</script>的定义式，可以发现此时<script type="math/tex">H\left( y \right) = 0</script>、亦即<script type="math/tex">y</script>生成的样本没有不确定性</p>
<p><strong><em>注意：由于<script type="math/tex">p\log p \rightarrow 0\ (p \rightarrow 0)</script>、所以认为<script type="math/tex">0\log 0 = 0</script></em></strong></p>
<p>特殊的情况就是二类问题、亦即<script type="math/tex">K = 2</script>的情况。先不妨设<script type="math/tex">y</script>只取 0、1 二值，再设：</p>
<script type="math/tex; mode=display">
p\left( y = 0 \right) = p,\ \ p\left( y = 1 \right) = 1 - p,\ \ 0 \leq p \leq 1</script><p>那么此时的信息熵<script type="math/tex">H(y)</script>即为：</p>
<script type="math/tex; mode=display">
H\left( y \right) = - \operatorname{plog}p - \left( 1 - p \right)\log{(1 - p)}</script><p>由此可得<script type="math/tex">H(y)</script>随<script type="math/tex">p</script>变化的函数曲线。底为 2 时函数图像如下图所示：</p>
<img src="/posts/2ce87ace/p1.png" alt="p1.png" title="">
<p>如前文所述，在<script type="math/tex">p = 0.5</script>时<script type="math/tex">H(y)</script>取得最大值 1。底为<script type="math/tex">e</script>时函数图像则如下图所示：</p>
<img src="/posts/2ce87ace/p2.png" alt="p2.png" title="">
<p>虽然最大值仍在<script type="math/tex">p = 0.5</script>时取得，但是此时<script type="math/tex">H(y)</script>仅有 0.693（<script type="math/tex">\ln 2</script>）左右</p>
<p>如果对上述二类问题稍作推广：<script type="math/tex">y \in \{ Y_{1},Y_{2}\}</script>、其中<script type="math/tex">Y_{1}</script>、<script type="math/tex">Y_{2}</script>都是一个集合，那么此时信息熵的定义式即为：</p>
<script type="math/tex; mode=display">
H\left( y \right) = - p\left( y \in Y_{1} \right)\log{p\left( y \in Y_{1} \right)} - p\left( y \in Y_{2} \right)\log{p(y \in Y_{2})}</script><p>且易知：</p>
<script type="math/tex; mode=display">
p\left( y \in Y_{1} \right) + p\left( y \in Y_{2} \right) = 1</script><p>如无特殊说明，今后谈及二类问题时讨论的范围都包括推广后的二类问题。</p>
<p>以上的叙述说明了，<script type="math/tex">y</script>越乱意味着<script type="math/tex">H(y)</script>越大、<script type="math/tex">y</script>越有规律意味着<script type="math/tex">H(y)</script>越小，亦即<script type="math/tex">H(y)</script>确实可以作为不确定性的度量标准</p>
<h2 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h2><p>基尼系数的定义会更简洁一些：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \right) = \sum_{k = 1}^{K}{p_{k}(1 - p_{k})} = 1 - \sum_{k = 1}^{K}p_{k}^{2}</script><p>同样可以利用经验基尼系数来进行估计：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \right) = \text{Gini}\left( D \right) = 1 - \sum_{k = 1}^{K}\left( \frac{\left| C_{k} \right|}{\left| D \right|} \right)^{2}</script><p>以及同样可以证明，当</p>
<script type="math/tex; mode=display">
p_{1} = p_{2} = \ldots = p_{K} = \frac{1}{K}</script><p>时，<script type="math/tex">\text{Gini}(y)</script>取得最大值<script type="math/tex">1 - \frac{1}{K}</script>；当存在<script type="math/tex">k^{*}</script>使得<script type="math/tex">p_{k^{*}} = 1</script>时、<script type="math/tex">\text{Gini}\left( y \right) = 0</script>。特别地、当<script type="math/tex">K = 2</script>时，可以导出：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \right) = 2p(1 - p)</script><p>此时<script type="math/tex">\text{Gini}(y)</script>的函数图像如下图所示：</p>
<img src="/posts/2ce87ace/p3.png" alt="p3.png" title="">
<p>虽然最大值仍在<script type="math/tex">p = 0.5</script>时取得，但是此时<script type="math/tex">\text{Gini}(y)</script>仅有 0.5。我们同样可以对二类问题进行推广、此时有：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \right) = 1 - p^{2}\left( y \in Y_{1} \right) - p^{2}(y \in Y_{2})</script><p>且</p>
<script type="math/tex; mode=display">
p\left( y \in Y_{1} \right) + p\left( y \in Y_{2} \right) = 1</script><p>以上的叙述说明了<script type="math/tex">\text{Gini}(y)</script>也可以用来度量不确定性</p>
<h1 id="信息的增益"><a href="#信息的增益" class="headerlink" title="信息的增益"></a>信息的增益</h1><p>在定义完不确定性的度量标准之后，我们就可以看看什么叫“获得信息”、亦即信息的增益了。从直观上来说，信息的增益是针对随机变量<script type="math/tex">y</script>和描述该变量的特征来定义的，此时数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,(x_{N},y_{N})\}</script>，其中<script type="math/tex">x_{i} = \left( x_{i}^{\left( 1 \right)},\ldots,x_{i}^{\left( n \right)} \right)^{T}</script>是描述<script type="math/tex">y_{i}</script>的特征向量、n 则是特征个数。我们可以先研究单一特征的情况（<script type="math/tex">n = 1</script>）：不妨设该特征叫<script type="math/tex">A</script>、数据集<script type="math/tex">D = \{\left( A_{1},y_{1} \right),\ldots,(A_{N},y_{N})\}</script>；此时所谓信息的增益，反映的就是特征<script type="math/tex">A</script>所能给我们带来的关于<script type="math/tex">y</script>的“信息量”的大小</p>
<p>可以引入条件熵<script type="math/tex">H(y|A)</script>的概念来定义信息的增益，它同样有着比较好的直观：</p>
<ul>
<li>所谓条件熵，就是根据特征<script type="math/tex">A</script>的不同取值<script type="math/tex">\{ a_{1},\ldots,a_{m}\}</script>对<script type="math/tex">y</script>进行限制后，先对这些被限制的<script type="math/tex">y</script>分别计算信息熵、再把这些信息熵（一共有 m 个）根据特征取值本身的概率加权求和、从而得到总的条件熵。换句话说，条件熵是由被<script type="math/tex">A</script>不同取值限制的各个部分的<script type="math/tex">y</script>的不确定性以取值本身的概率作为权重加总得到的</li>
</ul>
<p>所以，条件熵<script type="math/tex">H(y|A)</script>越小、意味着<script type="math/tex">y</script>被<script type="math/tex">A</script>限制后的总的不确定性越小、从而意味着<em>A</em>更能够帮助我们做出决策</p>
<p>接下来就是数学定义：</p>
<script type="math/tex; mode=display">
H\left( y \middle| A \right) = \sum_{j = 1}^{m}{p\left( A = a_{j} \right)H(y|A = a_{j})}</script><p>其中</p>
<script type="math/tex; mode=display">
H\left( y \middle| A = a_{j} \right) = - \sum_{k = 1}^{K}{p\left( y = c_{k} \middle| A = a_{j} \right)\log{p(y = c_{k}|A = a_{j})}}</script><p>同样可以用经验条件熵来估计真正的条件熵：</p>
<script type="math/tex; mode=display">
H\left( y \middle| A \right) = H\left( y \middle| D \right) = \sum_{j = 1}^{m}{\frac{\left| D_{j} \right|}{\left| D \right|}\sum_{k = 1}^{K}{\frac{|D_{\text{jk}}|}{|D_{j}|}\log\frac{|D_{\text{jk}}|}{|D_{j}|}}}</script><p>这里的<script type="math/tex">D_{j}</script>表示在<script type="math/tex">A = a_{j}</script>限制下的数据集。通常可以记<script type="math/tex">D_{j}</script>中的样本<script type="math/tex">y_{i}</script>满足<script type="math/tex">y_{i}^{A} = a_{j}</script>，亦即：</p>
<script type="math/tex; mode=display">
y_{i}^{A} = a_{j} \Leftrightarrow \left( A_{i},y_{i} \right) \in Y_{j} \Leftrightarrow A_{i} = a_{j}</script><p>而公式中的<script type="math/tex">|D_{\text{jk}}|</script>则代表着<script type="math/tex">D_{j}</script>中第 k 类样本的个数</p>
<p>从条件熵的直观含义，信息的增益就可以自然地定义为：</p>
<script type="math/tex; mode=display">
g\left( y,A \right) = H\left( y \right) - H(y|A)</script><p>这里的<script type="math/tex">g(y,A)</script>常被称为互信息（Mutual<br>Information），决策树中的 ID3 算法即是利用它来作为特征选取的标准的（相关定义会在后文给出）。但是，如果简单地以<script type="math/tex">g(y,A)</script>作为标准的话，会存在偏向于选择取值较多的特征、也就是 m 比较大的特征的问题。我们仍然可以从直观上去理解为什么会偏向于选取 m 较大的特征以及为什么这样做是不尽合理的：</p>
<ul>
<li>我们希望得到的决策树应该是比较深（又不会太深）的决策树，从而它可以基于多个方面而不是片面地根据某些特征来判断</li>
<li>如果单纯以<script type="math/tex">g(y,A)</script>作为标准，由于<script type="math/tex">g(y,A)</script>的直观意义是<script type="math/tex">y</script>被<script type="math/tex">A</script>划分后不确定性的减少量，可想而知，当<script type="math/tex">A</script>的取值很多时，<script type="math/tex">y</script>会被<script type="math/tex">A</script>划分成很多份、于是其不确定性自然会减少很多、从而 ID3 算法会倾向于选择<script type="math/tex">A</script>作为划分依据。但如果这样做的话，可以想象、我们最终得到的决策树将会是一颗很胖很矮的决策树，这并不是我们想要的</li>
</ul>
<p>为解决该问题、我们可以给 m 一个惩罚，由此我们可以得到信息增益比（Information<br>Gain Ratio）的概念，该概念对应着 C4.5 算法：</p>
<script type="math/tex; mode=display">
g_{R}(y,A) = \frac{g(y,A)}{H_{A}(y)}</script><p>其中<script type="math/tex">H_{A}(y)</script>是<script type="math/tex">y</script>关于<script type="math/tex">A</script>的熵，它的定义为：</p>
<script type="math/tex; mode=display">
H_{A}\left( y \right) = - \sum_{j = 1}^{m}{p\left( y^{A} = a_{j} \right)\log{p(y^{A} = a_{j})}}</script><p>同样可以用经验熵来进行估计：</p>
<script type="math/tex; mode=display">
H_{A}\left( y \right) = H_{A}\left( D \right) = - \sum_{j = 1}^{m}{\frac{\left| D_{j} \right|}{\left| D \right|}\log\frac{\left| D_{j} \right|}{\left| D \right|}}</script><p>该定义式和信息熵的定义式很像，它们的性质也有相通之处</p>
<p>需要指出的是，只需要类比上述的过程、我们同样可以使用基尼系数来定义信息的增益。具体而言，我们可以先定义条件基尼系数：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \middle| A \right) = \sum_{j = 1}^{m}{p\left( A = a_{j} \right)\text{Gini}(y|A = a_{j})}</script><p>其中</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \middle| A = a_{j} \right) = 1 - \sum_{k = 1}^{K}{p^{2}\left( y = c_{k} \middle| A = a_{j} \right)}</script><p>同样可以用经验条件基尼系数来进行估计：</p>
<script type="math/tex; mode=display">
{\text{Gini}\left( y \middle| A \right) = \text{Gini}(y|D) = \sum_{j = 1}^{m}{\frac{\left| D_{i} \right|}{\left| D \right|}\left\lbrack 1 - \sum_{k = 1}^{K}\left( \frac{\left| D_{\text{jk}} \right|}{\left| D_{j} \right|} \right)^{2} \right\rbrack}\backslash n}{= 1 - \sum_{j = 1}^{m}\frac{\left| D_{j} \right|}{\left| D \right|}\sum_{k = 1}^{K}\left( \frac{\left| D_{\text{Jk}} \right|}{\left| D_{j} \right|} \right)^{2}}</script><p>信息的增益则自然地定义为（不妨称之为“基尼增益”）：</p>
<script type="math/tex; mode=display">
g_{\text{Gini}}\left( y,A \right) = \text{Gini}\left( y \right) - \text{Gini}\left( y \middle| A \right)</script><p>决策树算法中的CART算法通常会应用这种定义</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文首先简要地说明一下决策树生成算法背后的数学基础和思想、然后再叙述具体的算法。往大了说、决策树的生成可以算是信息论的一个应用，但它其实只用到了信息论中一小部分的思想。不过，先对信息论有个概括性的认知还是有必要的、因为这样我们就可以有个更宽的视野&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>决策树综述</title>
    <link href="http://www.carefree0910.com/posts/dd35ec8b/"/>
    <id>http://www.carefree0910.com/posts/dd35ec8b/</id>
    <published>2017-04-22T11:09:59.000Z</published>
    <updated>2017-04-22T11:35:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>上个系列讲的朴素贝叶斯模型的理论基础大部分是数理统计和概率论相关的东西，可能从直观上不太好理解。这一章我们会讲解一种可以说是从直观上最好理解的模型——决策树。决策树是听上去比较厉害且又相对简单的模型，虽然它用到的数学知识确实不怎么多、但是在实现它的过程中可能可以获得对编程本身更深的理解，尤其是对递归的利用这一块可能会有更深的体会</p>
<p>以下是目录：</p>
<ul>
<li><a href="/posts/2ce87ace/" title="数据的信息">数据的信息</a></li>
<li><a href="/posts/c6faa205/" title="决策树的生成算法">决策树的生成算法</a></li>
<li><a href="/posts/e0705aab/" title="信息量计算的实现">信息量计算的实现</a></li>
<li><a href="/posts/41abb98b/" title="节点结构的实现">节点结构的实现</a></li>
<li><a href="/posts/b07c81ec/" title="树结构的实现">树结构的实现</a></li>
<li><a href="/posts/1a7aa546/" title="决策树的剪枝算法">决策树的剪枝算法</a></li>
<li><a href="/posts/602f7125/" title="剪枝算法的实现">剪枝算法的实现</a></li>
<li><a href="/posts/c12a819/" title="评估与可视化">评估与可视化</a></li>
<li><a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a></li>
<li><a href="/posts/88953f51/" title="“决策树”小结">“决策树”小结</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上个系列讲的朴素贝叶斯模型的理论基础大部分是数理统计和概率论相关的东西，可能从直观上不太好理解。这一章我们会讲解一种可以说是从直观上最好理解的模型——决策树。决策树是听上去比较厉害且又相对简单的模型，虽然它用到的数学知识确实不怎么多、但是在实现它的过程中可能可以获得对编程本
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="综述" scheme="http://www.carefree0910.com/tags/%E7%BB%BC%E8%BF%B0/"/>
    
      <category term="目录" scheme="http://www.carefree0910.com/tags/%E7%9B%AE%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>“朴素贝叶斯”小结</title>
    <link href="http://www.carefree0910.com/posts/a75c0d1b/"/>
    <id>http://www.carefree0910.com/posts/a75c0d1b/</id>
    <published>2017-04-20T12:52:08.000Z</published>
    <updated>2017-04-20T13:23:52.000Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>贝叶斯学派强调概率的“主观性”、而频率学派则强调“自然属性”</li>
<li>常见的参数估计有 ML 估计和 MAP 估计两种，其中 MAP 估计比 ML 估计多了对数先验概率这一项，体现了贝叶斯学派的思想</li>
<li>朴素贝叶斯算法下的模型一般分为三类：离散型、连续型和混合型。其中，离散型朴素贝叶斯不但能够进行对离散型数据进行分类、还能进行特征提取和可视化</li>
<li>朴素贝叶斯是简单而高效的算法，它是损失函数为 0-1 函数下的贝叶斯决策。朴素贝叶斯的基本假设是条件独立性假设，该假设一般来说太过苛刻，视情况可以通过另外两种贝叶斯分类器算法——半朴素贝叶斯和贝叶斯网来弱化</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;贝叶斯学派强调概率的“主观性”、而频率学派则强调“自然属性”&lt;/li&gt;
&lt;li&gt;常见的参数估计有 ML 估计和 MAP 估计两种，其中 MAP 估计比 ML 估计多了对数先验概率这一项，体现了贝叶斯学派的思想&lt;/li&gt;
&lt;li&gt;朴素贝叶斯算法下的模型一般分为三类
    
    </summary>
    
      <category term="朴素贝叶斯" scheme="http://www.carefree0910.com/categories/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
      <category term="小结" scheme="http://www.carefree0910.com/tags/%E5%B0%8F%E7%BB%93/"/>
    
  </entry>
  
</feed>
