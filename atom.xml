<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Python 与机器学习</title>
  <subtitle>Python &amp; Machine Learning</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.carefree0910.com/"/>
  <updated>2017-04-23T02:44:13.000Z</updated>
  <id>http://www.carefree0910.com/</id>
  
  <author>
    <name>射命丸咲</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>“决策树”小结</title>
    <link href="http://www.carefree0910.com/posts/88953f51/"/>
    <id>http://www.carefree0910.com/posts/88953f51/</id>
    <published>2017-04-23T02:43:07.000Z</published>
    <updated>2017-04-23T02:44:13.000Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>决策树是从直观上很好理解的模型，可以把它理解为一个划分规则的序列</li>
<li>决策树常用的生成算法包括：<ul>
<li>ID3 算法，它使用互信息作为信息增益的度量</li>
<li>C4.5 算法，它使用信息增益比作为信息增益的度量</li>
<li>CART 算法，它规定生成出来的决策树为二叉树、且一般使用基尼增益作为信息增益的度量</li>
</ul>
</li>
<li>决策树常用的剪枝算法有两种，它们都是为了适当地降低模型复杂度、从而期望模型在未知数据上的表现更好</li>
<li>决策树的代码实现从始到终都贯彻着递归的思想，可以说是递归的一个经典应用</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;决策树是从直观上很好理解的模型，可以把它理解为一个划分规则的序列&lt;/li&gt;
&lt;li&gt;决策树常用的生成算法包括：&lt;ul&gt;
&lt;li&gt;ID3 算法，它使用互信息作为信息增益的度量&lt;/li&gt;
&lt;li&gt;C4.5 算法，它使用信息增益比作为信息增益的度量&lt;/li&gt;
&lt;li&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="小结" scheme="http://www.carefree0910.com/tags/%E5%B0%8F%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>相关数学理论</title>
    <link href="http://www.carefree0910.com/posts/613bbb2f/"/>
    <id>http://www.carefree0910.com/posts/613bbb2f/</id>
    <published>2017-04-23T02:33:22.000Z</published>
    <updated>2017-04-23T02:42:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>这一节会叙述琴生不等式（Jensen’s Inequality）及其一个简单的应用，为此我们需要知道凸函数的概念：</p>
<ul>
<li>若函数<script type="math/tex">f(x)</script>对<script type="math/tex">\forall p\in [0,1]</script>、都满足：  <script type="math/tex; mode=display">
pf\left( x_{1} \right) + \left( 1 - p \right)f\left( x_{2} \right) \leq f\left( px_{1} + \left( 1 - p \right)x_{2} \right)</script>则称<script type="math/tex">f(x)</script>为凸函数（有时又叫上凸函数）</li>
</ul>
<p>琴生不等式是针对凸函数提出的，下面就具体说一下其内容与证明</p>
<a id="more"></a>
<h1 id="定理内容"><a href="#定理内容" class="headerlink" title="定理内容"></a>定理内容</h1><p>对于<script type="math/tex">\lbrack a,b\rbrack</script>上的凸函数，若</p>
<script type="math/tex; mode=display">
p_{1},...,p_{K} \in \left\lbrack 0,1 \right\rbrack,\ \ p_{1} + p_{2} + \ldots + p_{K} = 1</script><p>则有</p>
<script type="math/tex; mode=display">
\sum_{k = 1}^{K}{p_{i}f(x_{i})} \leq f(\sum_{k = 1}^{K}{p_{i}x_{i}})</script><p>接下来我们会利用它来证明等概率分布具有最大熵。注意到可以证明函数</p>
<script type="math/tex; mode=display">
\hat{H}\left( p \right) = - p\log p</script><p>是一个凸函数，于是熵的定义式可以写成</p>
<script type="math/tex; mode=display">
H\left( y \right) = - \sum_{k = 1}^{K}{p_{k}\log p_{k}} = \sum_{k = 1}^{K}{\hat{H}\left( p_{k} \right)}</script><p>从而</p>
<script type="math/tex; mode=display">
\frac{1}{K}H\left( y \right) = \frac{1}{K}\sum_{k = 1}^{K}{\hat{H}(p_{k})} \leq \hat{H}\left( \sum_{k = 1}^{K}{\frac{1}{K}p_{k}} \right) = \hat{H}\left( \frac{1}{K} \right) = - \frac{1}{K}\log\frac{1}{K} = \frac{1}{K}\log K</script><p>亦即</p>
<script type="math/tex; mode=display">
H\left( y \right) \leq \log K</script><p>等式当且仅当</p>
<script type="math/tex; mode=display">
p_{1} = p_{2} = \ldots = p_{K} = \frac{1}{K}</script><p>时取得</p>
<h1 id="定理证明"><a href="#定理证明" class="headerlink" title="定理证明"></a>定理证明</h1><p>应用数学归纳法可以比较简单地完成证明：</p>
<ul>
<li>当时<script type="math/tex">K=2</script>、由凸函数定义直接证毕，此为奠基</li>
<li>假设<script type="math/tex">K = n</script>时成立、考虑<script type="math/tex">K = n + 1</script>的情况，令  <script type="math/tex; mode=display">
s_{n} = \sum_{k = 1}^{n}p_{k}</script>则  <script type="math/tex; mode=display">
\sum_{k = 1}^{n + 1}{p_{k}f\left( x_{k} \right)} = s_{n}\sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} + p_{n + 1}f\left( x_{n + 1} \right)</script>注意到  <script type="math/tex; mode=display">
\sum_{k = 1}^{n}\frac{p_{k}}{s_{n}} = 1</script>从而由<script type="math/tex">K = n</script>时的琴生不等式可知  <script type="math/tex; mode=display">
\sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} \leq f\left( \sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} \right)</script>注意到  <script type="math/tex; mode=display">
s_{n} + p_{n + 1} = 1</script>从而由凸函数定义知  <script type="math/tex; mode=display">
s_{n}f\left( \sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} \right) + p_{n + 1}f\left( x_{n + 1} \right) \leq f\left( s_{n}\sum_{k = 1}^{n}{\frac{p_{k}}{s_{n}}f\left( x_{k} \right)} + p_{n + 1}x_{n + 1} \right)</script>综上所述、即得  <script type="math/tex; mode=display">
\sum_{k = 1}^{n + 1}{p_{k}f(x_{k})} \leq f\left( \sum_{k = 1}^{n + 1}{p_{k}f\left( x_{k} \right)} \right)</script></li>
</ul>
<p>琴生不等式的应用非常广泛，证明等概率分布具有最大熵只是其中一个小应用。在许许多多涉及到凸问题的算法中、琴生不等式都显示出了强大的威力</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一节会叙述琴生不等式（Jensen’s Inequality）及其一个简单的应用，为此我们需要知道凸函数的概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若函数&lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;对&lt;script type=&quot;math/tex&quot;&gt;\forall p\in [0,1]&lt;/script&gt;、都满足：  &lt;script type=&quot;math/tex; mode=display&quot;&gt;
pf\left( x_{1} \right) + \left( 1 - p \right)f\left( x_{2} \right) \leq f\left( px_{1} + \left( 1 - p \right)x_{2} \right)&lt;/script&gt;则称&lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;为凸函数（有时又叫上凸函数）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;琴生不等式是针对凸函数提出的，下面就具体说一下其内容与证明&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>评估与可视化</title>
    <link href="http://www.carefree0910.com/posts/c12a819/"/>
    <id>http://www.carefree0910.com/posts/c12a819/</id>
    <published>2017-04-23T01:45:37.000Z</published>
    <updated>2017-04-23T03:08:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前我们实现了一个 Node 基类<code>CvDNode</code>和一个 Tree 基类<code>CvDBase</code>；为了评估决策树模型的表现、我们需要先在这两个基类的基础上根据不同的算法实现出各种具体的决策树。由于我们在基类里面已经完成了绝大部分工作、所以在其上进行扩展是平凡的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ID3Node</span><span class="params">(CvDNode)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></div><div class="line">        CvDNode.__init__(self, *args, **kwargs)</div><div class="line">        self.criterion = <span class="string">"ent"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">C45Node</span><span class="params">(CvDNode)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></div><div class="line">        CvDNode.__init__(self, *args, **kwargs)</div><div class="line">        self.criterion = <span class="string">"ratio"</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CartNode</span><span class="params">(CvDNode)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *args, **kwargs)</span>:</span></div><div class="line">        CvDNode.__init__(self, *args, **kwargs)</div><div class="line">        self.criterion = <span class="string">"gini"</span></div><div class="line">        self.is_cart = <span class="keyword">True</span></div></pre></td></tr></table></figure>
<p>在<code>CvDBase</code>的基础上定义三种算法对应的 Tree 结构的方法是类似的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">ID3Tree</span><span class="params">(CvDBase, ID3Node, metaclass=CvDMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">C45Tree</span><span class="params">(CvDBase, C45Node, metaclass=CvDMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CartTree</span><span class="params">(CvDBase, CartNode, metaclass=CvDMeta)</span>:</span></div><div class="line">    <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>其中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CvDMeta</span><span class="params">(type)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(mcs, *args, **kwargs)</span>:</span></div><div class="line">        name, bases, attr = args[:<span class="number">3</span>]</div><div class="line">        _, _node = bases</div><div class="line"></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, whether_continuous=None, max_depth=None, node=None, **_kwargs)</span>:</span></div><div class="line">            tmp_node = node <span class="keyword">if</span> isinstance(node, CvDNode) <span class="keyword">else</span> _node</div><div class="line">            CvDBase.__init__(self, whether_continuous, max_depth, tmp_node(**_kwargs))</div><div class="line">            self._name = name</div><div class="line"></div><div class="line">        attr[<span class="string">"__init__"</span>] = __init__</div><div class="line">        <span class="keyword">return</span> type(name, bases, attr)</div></pre></td></tr></table></figure>
<p>接下来就是具体的评估与相应的可视化</p>
<a id="more"></a>
<p>我们同样可以使用蘑菇数据集来评估决策树模型的表现，结果如下所示：</p>
<img src="/posts/c12a819/p1.png" alt="蘑菇数据集上 ID3 算法的表现" title="蘑菇数据集上 ID3 算法的表现">
<img src="/posts/c12a819/p2.png" alt="蘑菇数据集上 C4.5 算法的表现" title="蘑菇数据集上 C4.5 算法的表现">
<img src="/posts/c12a819/p3.png" alt="蘑菇数据集上 CART 算法的表现" title="蘑菇数据集上 CART 算法的表现">
<p>可以看到 CART 算法的表现相对来说要差不少，可能的原因有如下三条：</p>
<ul>
<li>CART 算法在选择划分标准时是从所有二分标准里面进行选择的，这里就会比 ID3 和 C4.5 算法多出不少倍的运算量</li>
<li>由于我们在实现 CART 剪枝算法时为了追求简洁、直接调用了标准库 copy 中的 deepcopy 方法对整颗决策树进行了深拷贝。这一步可能会连不必要的东西也进行了拷贝、从而导致了一些不必要的开销</li>
<li>CART 算法生成的是二叉决策树，所以可能生成出来的树会更深、各叶节点中的样本数可能也会分布得比较均匀、从而无论是建模过程还是预测过程都会要慢一些</li>
</ul>
<p>当然，如果结合蘑菇数据集来说的话、笔者认为最大的问题在于：CART 算法不适合应用于蘑菇数据集。一方面是因为蘑菇数据集全是离散型特征且各特征取值都挺多，另一方面是因为蘑菇数据集相对简单、有一些特征非常具有代表性（我们在说明朴素贝叶斯时也有所提及），仅仅用二分标准划分数据的话、会显得比较没有效率</p>
<p>为了更客观地评估我们模型的表现，我们可以对成熟第三方库 sklearn 中的决策树模型进行恰当的封装并看看它在蘑菇数据集上的表现：</p>
<img src="/posts/c12a819/p4.png" alt="蘑菇数据集上 sklearn 决策树的表现（criterion=entropy）" title="蘑菇数据集上 sklearn 决策树的表现（criterion=entropy）">
<img src="/posts/c12a819/p5.png" alt="蘑菇数据集上 sklearn 决策树的表现（criterion=gini）" title="蘑菇数据集上 sklearn 决策树的表现（criterion=gini）">
<p>不得不承认、成熟第三方库的效率确实要高很多（比我们的要快 5 倍左右）；这是因为虽然算法思想可能大致相同，但 sklearn 的核心实现都经过了高度优化、且（如不出意料的话）应该都是用 C 或者其它底层语言直接写的。不过正如第一章说过的，要想应用 sklearn 中的决策树、就必须先将数据数值化（即使是离散型数据）；而我们实现的决策树在处理离散型数据时却无需这一步数据预处理、可以直接应用在原始数据上（但处理混合型数据时还是要先进行数值化处理、而且将离散型数据数值化也能显著提升模型的运行速度）</p>
<p>我们在本系列的综述里面曾说过、决策树可能是从直观上最好理解的模型；事实上，我们之前画过的一些决策树示意图也确实非常直观易懂、于是我们可能自然就会希望程序能将生成类似的东西。虽然不能做到那么漂亮、不过我们确实是能在之前实现的决策树模型的基础上做出类似效果的：</p>
<img src="/posts/c12a819/p6.png" alt="蘑菇数据集上 ID3 决策树的可视化" title="蘑菇数据集上 ID3 决策树的可视化">
<img src="/posts/c12a819/p7.png" alt="蘑菇数据集上 C4.5 决策树的可视化" title="蘑菇数据集上 C4.5 决策树的可视化">
<img src="/posts/c12a819/p8.png" alt="蘑菇数据集上 CART 决策树的可视化" title="蘑菇数据集上 CART 决策树的可视化">
<p>其中，红色数字代表该 Node 作为划分标准的特征所属的维度，位于各条连线中央的字母代表着该维度特征的各个取值、加号“+”代表着“其它”，绿色字母代表类别标记。以上三张图在一定程度上验证了我们之前的很多说法，比如说 ID3 会倾向选择取值比较多的特征、C4.5 可能会倾向选择取值比较少的特征且倾向于在每个二叉分枝处留下一个小 Node 作为叶节点、CART 各个叶节点上的样本分布较均匀且生成出的决策树会比较深……等等</p>
<p>我们在说明朴素贝叶斯时曾经提过，即使只根据第 5 维的取值来进行类别的判定、最后的准确率也一定会非常高。验证这一命题的方法很简单——只需将决策树的最大深度设为 1 即可，结果如下图所示：</p>
<img src="/posts/c12a819/p9.png" alt="p9.png" title="">
<p>此时模型的表现如下图所示：</p>
<img src="/posts/c12a819/p10.png" alt="p10.png" title="">
<p>可以看到其表现确实不错。值得一提的是，单层决策树又可称为“决策树桩（Decision Stump）”、它是有特殊应用场景的（比如我们在下个系列中讲 AdaBoost 时就会用到它）</p>
<p>至今为止我们用到的数据集都是离散型数据集，为了更全面地进行评估、使用连续型混合型数据集进行评估是有必要的；同时为了增强直观、我们可以用异或数据集来进行评估。原始数据集如下图所示：</p>
<img src="/posts/c12a819/p11.png" alt="p11.png" title="">
<p>生成异或数据集（及其它二维数据集）的代码定义在之前提过 DataUtil 类中（可参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py" target="_blank" rel="external">这里</a>），读者也可以在下一章中找到相应的讲解。为使评估更具有直观性、我们可以把四种决策树（ID3、C4.5、CART 决策树和 sklearn 的决策树）在异或数据集上的表现直接画出来：</p>
<img src="/posts/c12a819/p12.png" alt="异或数据集上 ID3、CART 和 sklearn 决策树的表现" title="异或数据集上 ID3、CART 和 sklearn 决策树的表现">
<img src="/posts/c12a819/p13.png" alt="异或数据集上 C4.5 决策树的表现" title="异或数据集上 C4.5 决策树的表现">
<p>可以看到 C4.5 决策树的过拟合现象比较严重。正如我们之前所分析的一般、这很有可能是因为 C4.5 在二叉分枝时会倾向于进行“不均匀的二分”（从上图也可以大概看出）</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前我们实现了一个 Node 基类&lt;code&gt;CvDNode&lt;/code&gt;和一个 Tree 基类&lt;code&gt;CvDBase&lt;/code&gt;；为了评估决策树模型的表现、我们需要先在这两个基类的基础上根据不同的算法实现出各种具体的决策树。由于我们在基类里面已经完成了绝大部分工作、所以在其上进行扩展是平凡的：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ID3Node&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDNode)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, *args, **kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        CvDNode.__init__(self, *args, **kwargs)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.criterion = &lt;span class=&quot;string&quot;&gt;&quot;ent&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;C45Node&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDNode)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, *args, **kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        CvDNode.__init__(self, *args, **kwargs)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.criterion = &lt;span class=&quot;string&quot;&gt;&quot;ratio&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;CartNode&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDNode)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, *args, **kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        CvDNode.__init__(self, *args, **kwargs)&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.criterion = &lt;span class=&quot;string&quot;&gt;&quot;gini&quot;&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.is_cart = &lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;在&lt;code&gt;CvDBase&lt;/code&gt;的基础上定义三种算法对应的 Tree 结构的方法是类似的：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;ID3Tree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDBase, ID3Node, metaclass=CvDMeta)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;C45Tree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDBase, C45Node, metaclass=CvDMeta)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;CartTree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(CvDBase, CartNode, metaclass=CvDMeta)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;其中&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;CvDMeta&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(type)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__new__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(mcs, *args, **kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        name, bases, attr = args[:&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;]&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        _, _node = bases&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, whether_continuous=None, max_depth=None, node=None, **_kwargs)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            tmp_node = node &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; isinstance(node, CvDNode) &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; _node&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            CvDBase.__init__(self, whether_continuous, max_depth, tmp_node(**_kwargs))&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;            self._name = name&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        attr[&lt;span class=&quot;string&quot;&gt;&quot;__init__&quot;&lt;/span&gt;] = __init__&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; type(name, bases, attr)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;接下来就是具体的评估与相应的可视化&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>剪枝算法的实现</title>
    <link href="http://www.carefree0910.com/posts/602f7125/"/>
    <id>http://www.carefree0910.com/posts/602f7125/</id>
    <published>2017-04-23T01:25:20.000Z</published>
    <updated>2017-04-23T03:00:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py" target="_blank" rel="external">这里</a>和<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Tree.py" target="_blank" rel="external">这里</a>）</p>
<p>决策树剪枝算法的实现相对而言比较平凡，只需要把算法依次翻译成程序语言即可</p>
<a id="more"></a>
<h1 id="ID3、C4-5-剪枝算法的实现"><a href="#ID3、C4-5-剪枝算法的实现" class="headerlink" title="ID3、C4.5 剪枝算法的实现"></a>ID3、C4.5 剪枝算法的实现</h1><p>回忆算法本身，可以知道我们需要获取“从下往上”这个顺序，为此我们需要先在<code>CvDNode</code>中利用递归定义一个函数来更新 Tree 的<code>self.layers</code>属性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_layers</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 根据该Node的深度、在self.layers对应位置的列表中记录自己</span></div><div class="line">    self.tree.layers[self._depth].append(self)</div><div class="line">    <span class="comment"># 遍历所有子节点、完成递归</span></div><div class="line">    <span class="keyword">for</span> _node <span class="keyword">in</span> sorted(self.children):</div><div class="line">        _node = self.children[_node]</div><div class="line">        <span class="keyword">if</span> _node <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            _node.update_layers()</div></pre></td></tr></table></figure>
<p>然后、在<code>CvDBase</code>中定义一个对应的函数进行封装：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update_layers</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 根据整颗决策树的高度、在self.layers里面放相应数量的列表</span></div><div class="line">    self.layers = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.root.height)]</div><div class="line">    self.root.update_layers()</div></pre></td></tr></table></figure>
<p>同时，为了做到合理的代码重用、我们可以先在<code>CvDNode</code>中定义一个计算损失的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(self, pruned=False)</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pruned:</div><div class="line">        <span class="keyword">return</span> sum([leaf[<span class="string">"chaos"</span>] * len(leaf[<span class="string">"y"</span>]) <span class="keyword">for</span> leaf <span class="keyword">in</span> self.leafs.values()])</div><div class="line">    <span class="keyword">return</span> self.chaos * len(self._y)</div></pre></td></tr></table></figure>
<p>有了以上两个函数，算法本身的实现就很直观了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_prune</span><span class="params">(self)</span>:</span></div><div class="line">    self._update_layers()</div><div class="line">    _tmp_nodes = []</div><div class="line">    <span class="comment"># 更新完决策树每一“层”的Node之后，从后往前地向 _tmp_nodes中加Node</span></div><div class="line">    <span class="keyword">for</span> _node_lst <span class="keyword">in</span> self.layers[::<span class="number">-1</span>]:</div><div class="line">        <span class="keyword">for</span> _node <span class="keyword">in</span> _node_lst[::<span class="number">-1</span>]:</div><div class="line">            <span class="keyword">if</span> _node.category <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">                _tmp_nodes.append(_node)</div><div class="line">    _old = np.array([node.cost() + self.prune_alpha * len(node.leafs)</div><div class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> _tmp_nodes])</div><div class="line">    _new = np.array([node.cost(pruned=<span class="keyword">True</span>) + self.prune_alpha</div><div class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> _tmp_nodes])</div><div class="line">    <span class="comment"># 使用 _mask变量存储 _old和 _new对应位置的大小关系</span></div><div class="line">    _mask = _old &gt;= _new</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># 若只剩根节点就退出循环体</span></div><div class="line">        <span class="keyword">if</span> self.root.height == <span class="number">1</span>:</div><div class="line">            <span class="keyword">return</span></div><div class="line">        p = np.argmax(_mask)</div><div class="line">        <span class="comment"># 如果 _new中有比 _old中对应损失小的损失、则进行局部剪枝</span></div><div class="line">        <span class="keyword">if</span> _mask[p]:</div><div class="line">            _tmp_nodes[p].prune()</div><div class="line">            <span class="comment"># 根据被影响了的Node、更新 _old、_mask对应位置的值</span></div><div class="line">            <span class="keyword">for</span> i, node <span class="keyword">in</span> enumerate(_tmp_nodes):</div><div class="line">                <span class="keyword">if</span> node.affected:</div><div class="line">                    _old[i] = node.cost() + self.prune_alpha * len(node.leafs)</div><div class="line">                    _mask[i] = _old[i] &gt;= _new[i]</div><div class="line">                    node.affected = <span class="keyword">False</span></div><div class="line">            <span class="comment"># 根据被剪掉的Node、将各个变量对应的位置除去（注意从后往前遍历）</span></div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(_tmp_nodes) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</div><div class="line">                <span class="keyword">if</span> _tmp_nodes[i].pruned:</div><div class="line">                    _tmp_nodes.pop(i)</div><div class="line">                    _old = np.delete(_old, i)</div><div class="line">                    _new = np.delete(_new, i)</div><div class="line">                    _mask = np.delete(_mask, i)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">    self.reduce_nodes()</div></pre></td></tr></table></figure>
<p>上述代码的第 25 行和第 28 行出现了 Node 的<code>affected</code>属性，这是我们之前没有进行定义的（因为若在彼时定义会显得很突兀）；不过由剪枝算法可知，这个属性的用处与其名字一致——标记一个 Node 是否是“被影响到的”Node。事实上，在一个 Node 进行了局部剪枝后，会有两类 Node “被影响到”：</p>
<ul>
<li>该 Node 的子节点、子节点的子节点……等等，它们属于被剪掉的 Node、应该要将它们在<code>_old</code>、<code>_tmp_nodes</code>中对应的位置从这些列表中除去</li>
<li>该 Node 的父节点、父节点的父节点……等等，它们存储叶节点的列表会因局部剪枝而发生改变、所以要更新<code>_old</code>和<code>_mask</code>列表中对应位置的值</li>
</ul>
<p>其中，我们之前定义的 Node 中是用<code>pruned</code>属性来标记该 Node 是否已被剪掉、且介绍了如何通过递归来更新<code>pruned</code>属性；<code>affected</code>属性和<code>pruned</code>属性的本质几乎没什么区别，所以我们同样可以通过递归来更新<code>affected</code>属性。具体而言，我们只需：</p>
<ul>
<li>在初始化时令<code>self.affected = False</code></li>
<li>在局部剪枝函数内部插入<code>_parent.affected = True</code></li>
</ul>
<p>即可，其余部分可以保持不变。</p>
<h1 id="CART-剪枝算法的实现"><a href="#CART-剪枝算法的实现" class="headerlink" title="CART 剪枝算法的实现"></a>CART 剪枝算法的实现</h1><p>同样的，为了做到合理的代码重用、我们先利用之前实现的<code>cost</code>函数、在<code>CvDNode</code>里面定义一个获取 Node 阈值的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_threshold</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> (self.cost(pruned=<span class="keyword">True</span>) - self.cost()) / (len(self.leafs) - <span class="number">1</span>)</div></pre></td></tr></table></figure>
<p>由于算法本身的实现的思想以及用到的工具都和第一种剪枝算法大同小异、所以代码写起来也差不多：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_cart_prune</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 暂时将所有节点记录所属Tree的属性置为None</span></div><div class="line">    <span class="comment"># 这样做的必要性会在后文进行说明</span></div><div class="line">    self.root.cut_tree()</div><div class="line">    _tmp_nodes = [node <span class="keyword">for</span> node <span class="keyword">in</span> self.nodes <span class="keyword">if</span> node.category <span class="keyword">is</span> <span class="keyword">None</span>]</div><div class="line">    _thresholds = np.array([node.get_threshold() <span class="keyword">for</span> node <span class="keyword">in</span> _tmp_nodes])</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="comment"># 利用deepcopy对当前根节点进行深拷贝、存入self.roots列表</span></div><div class="line">        <span class="comment"># 如果前面没有把记录Tree的属性置为None，那么这里就也会对整个Tree做</span></div><div class="line">        <span class="comment"># 深拷贝。可以想象、这样会引发严重的内存问题，速度也会被拖慢非常多</span></div><div class="line">        root_copy = deepcopy(self.root)</div><div class="line">        self.roots.append(root_copy)</div><div class="line">        <span class="keyword">if</span> self.root.height == <span class="number">1</span>:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        p = np.argmin(_thresholds)</div><div class="line">        _tmp_nodes[p].prune()</div><div class="line">        <span class="keyword">for</span> i, node <span class="keyword">in</span> enumerate(_tmp_nodes):</div><div class="line">            <span class="comment"># 更新被影响到的Node的阈值</span></div><div class="line">            <span class="keyword">if</span> node.affected:</div><div class="line">                _thresholds[i] = node.get_threshold()</div><div class="line">                node.affected = <span class="keyword">False</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(_tmp_nodes) - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</div><div class="line">            <span class="comment"># 去除掉各列表相应位置的元素</span></div><div class="line">            <span class="keyword">if</span> _tmp_nodes[i].pruned:</div><div class="line">                _tmp_nodes.pop(i)</div><div class="line">                _thresholds = np.delete(_thresholds, i)</div><div class="line">    self.reduce_nodes()</div></pre></td></tr></table></figure>
<p>代码第 4 行对根节点调用的<code>cut_tree</code>方法同样是利用递归实现的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_tree</span><span class="params">(self)</span>:</span></div><div class="line">    self.tree = <span class="keyword">None</span></div><div class="line">    <span class="keyword">for</span> child <span class="keyword">in</span> self.children.values():</div><div class="line">        <span class="keyword">if</span> child <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            child.cut_tree()</div></pre></td></tr></table></figure>
<p>然后就是最后一步、通过交叉验证选出最优树了。注意到之前我们封装生成算法时、最后一行调用了剪枝算法的封装——<code>self.prune</code>方法。由于该方法是第一个接收了交叉验证集<code>x_cv</code>和<code>y_cv</code>的方法、所以我们应该让该方法来做交叉验证。简洁起见，我们直接选用加权正确率作为交叉验证的标准：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算加权正确率的函数</span></div><div class="line"><span class="meta">@staticmethod</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">acc</span><span class="params">(y, y_pred, weights)</span>:</span></div><div class="line">    <span class="keyword">if</span> weights <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> np.sum((np.array(y) == np.array(y_pred)) * weights) / len(y)</div><div class="line">    <span class="keyword">return</span> np.sum(np.array(y) == np.array(y_pred)) / len(y)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prune</span><span class="params">(self, x_cv, y_cv, weights)</span>:</span></div><div class="line">    <span class="keyword">if</span> self.root.is_cart:</div><div class="line">        <span class="comment"># 如果该Node使用CART剪枝，那么只有在确实传入了交叉验证集的情况下</span></div><div class="line">        <span class="comment"># 才能调用相关函数、否则没有意义</span></div><div class="line">        <span class="keyword">if</span> x_cv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> y_cv <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            self._cart_prune()</div><div class="line">            _arg = np.argmax([CvDBase.acc(</div><div class="line">                y_cv, tree.predict(x_cv), weights) <span class="keyword">for</span> tree <span class="keyword">in</span> self.roots])</div><div class="line">            _tar_root = self.roots[_arg]</div><div class="line">            <span class="comment"># 由于Node的feed_tree方法会递归地更新nodes属性、所以要先重置</span></div><div class="line">            self.nodes = []</div><div class="line">            _tar_root.feed_tree(self)</div><div class="line">            self.root = _tar_root</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        self._prune()</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;（本文会用到的所有代码都在&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py&quot;&gt;这里&lt;/a&gt;和&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Tree.py&quot;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;决策树剪枝算法的实现相对而言比较平凡，只需要把算法依次翻译成程序语言即可&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>决策树的剪枝算法</title>
    <link href="http://www.carefree0910.com/posts/1a7aa546/"/>
    <id>http://www.carefree0910.com/posts/1a7aa546/</id>
    <published>2017-04-22T15:33:01.000Z</published>
    <updated>2017-04-23T02:59:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>在知道怎么得到一颗决策树后、我们当然就会想知道：这样建立起来的决策树的表现究竟如何？从直观上来说，只要决策树足够深、划分标准足够细，它在训练集上的表现就能接近完美；但同时也容易想象，由于它可能把训练集的一些“特性”当做所有数据的“共性”来看待，它在未知的测试数据上的表现可能就会比较一般、亦即会出现过拟合的问题。我们知道，模型出现过拟合问题一般是因为模型太过复杂，所以决策树解决过拟合的方法是采取适当的“剪枝”、我们在上一节中也已经大量接触了这一概念。剪枝通常分为两类：“预剪枝（Pre-Pruning）”和“后剪枝（Post-Pruning）”，其中“预剪枝”的概念在生成算法中已有定义，彼时我们采取的说法是“停止条件”；而一般提起剪枝时指的都是“后剪枝”，它是指在决策树生成完毕后再对其进行修剪、把多余的节点剪掉。换句话说，后剪枝是从全局出发、通过某种标准对一些 Node 进行局部剪枝，这样就能减少决策树中 Node 的数目、从而有效地降低模型复杂度</p>
<a id="more"></a>
<p>是故问题的关键在于如何定出局部剪枝的标准。通常来说我们有两种做法：</p>
<ul>
<li>应用交叉验证的思想，若局部剪枝能够使得模型在测试集上的错误率降低、则进行局部剪枝（预剪枝中也应用了类似的思想）</li>
<li>应用正则化的思想、综合考虑不确定性和模型复杂度来定出一个新的损失（此前我们的损失只考虑了不确定性），用该损失作为一个 Node 是否进行局部剪枝的标准</li>
</ul>
<p>第二种做法又涉及到另一个关键问题：如何定量分析决策树中一个 Node 的复杂度？一个直观且合理的方法是：直接使用该 Node 下属叶节点的个数作为复杂度。基于此、第二种做法的数学描述就是：</p>
<ul>
<li>定义新损失（<script type="math/tex">T</script>代表一个 Node）  <script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = C\left( T \right) + \alpha\left| T \right|</script>其中，<script type="math/tex">C\left( T \right)</script>即是该 Node 和不确定性相关的损失、<script type="math/tex">|T|</script>则是该 Node 下属叶节点的个数。不妨设第 t 个叶节点含有<script type="math/tex">N_{t}</script>个样本且这<script type="math/tex">N_{t}</script>个样本的不确定性为<script type="math/tex">H_{t}(T)</script>，那么新损失一般可以直接定义为加权不确定性：  <script type="math/tex; mode=display">
C\left( T \right) = \sum_{t = 1}^{\left| T \right|}{N_{t}H_{t}(T)}</script></li>
</ul>
<p>我们会采取这种做法来进行实现。需要指出的是，在这种做法下、仍然可以分支出两种不同的算法：</p>
<ul>
<li>直接比较一个 Node 局部剪枝前的损失<script type="math/tex">C_{\alpha}(T)</script>和局部剪枝后的损失<script type="math/tex">C_{\alpha}(t)</script>的大小，若：  <script type="math/tex; mode=display">
C_{\alpha}\left( T \right) \geq C_{\alpha}(t)</script></li>
<li>获取一系列的剪枝阈值：<script type="math/tex">0 = \alpha_{0} < \alpha_{1} < \ldots < \alpha_{p} < + \infty</script>，在每个剪枝阈值<script type="math/tex">\alpha_{i}</script>上对相应的 Node 进行局部剪枝并将局部剪枝后得到的决策树<script type="math/tex">T_{i}</script>储存在一个列表中。在<script type="math/tex">\alpha_{p}</script>上我们会对根节点进行局部剪枝，此时剩下来的决策树<script type="math/tex">T_{p}</script>就只包含根节点这一个 Node。最后，通过交叉验证选出<script type="math/tex">T_{0},\ldots,T_{p}</script>中最好的决策树作为最终生成的决策树（注意其中的<script type="math/tex">T_{0}</script>即是没有剪过枝的原始树）</li>
</ul>
<p>第一种算法清晰易懂，第二种算法则稍显复杂；一般我们会在 ID3 和 C4.5 中应用第一种剪枝算法、在 CART 中应用第二种剪枝算法。上述这个第二种算法的说明可能有些过于简略、让人摸不着头脑；由于详细的算法叙述会在后文再次进行，所以这里只要有一个大概的直观感受即可，细节可以暂时按下、不必太过纠结</p>
<h1 id="ID3、C4-5-的剪枝算法"><a href="#ID3、C4-5-的剪枝算法" class="headerlink" title="ID3、C4.5 的剪枝算法"></a>ID3、C4.5 的剪枝算法</h1><p>首先我们来看看第一种算法的详细叙述。虽说算法本身的思想很简单，但由于其中涉及到许多中间变量、所以我们采取类似于伪代码的形式来进行叙述：</p>
<ol>
<li><strong>输入</strong>：生成算法产生的原始决策树<script type="math/tex">T</script>，惩罚因子<script type="math/tex">\alpha</script></li>
<li><strong>过程</strong>：<ol>
<li>从下往上地获取<script type="math/tex">T</script>中所有 Node，存入列表<code>_tmp_nodes</code></li>
<li>对<code>_tmp_nodes</code>中的所有 Node 计算损失，存入列表<code>_old</code></li>
<li>计算<code>_tmp_nodes</code>中所有 Node 进行局部剪枝后的损失，存入列表<code>_new</code></li>
<li>进入循环体：<ol>
<li>若<code>_new</code>中所有损失都大于<code>_old</code>中对应的损失、则退出循环体</li>
<li>否则，设 p 满足：  <script type="math/tex; mode=display">
p = \arg{\min_{p}{\text{_new}\lbrack p\rbrack \leq \text{_old}\lbrack p\rbrack}}</script>则对<code>_tmp_nodes[p]</code>进行局部剪枝</li>
<li>在完成局部剪枝后，更新<code>_old</code>、<code>_new</code>、<code>_tmp_nodes</code>等变量。具体而言，我们无需重新计算它们、只需更新“被影响到的” Node 所对应的位置的值即可</li>
</ol>
</li>
<li>最后调用<code>self.reduce_nodes</code>方法、将被剪掉的 Node 从<code>nodes</code>中除去</li>
</ol>
</li>
<li><strong>输出</strong>：修剪过后的决策树<script type="math/tex">T_{\alpha}</script></li>
</ol>
<p>我们可以在我们之前用气球数据集 1.0 根据 ID3 算法生成的决策树上过一遍剪枝算法以加深理解。由于算法顺序是从下往上、所以我们先考察最右下方的 Node（该 Node 的划分标准是“测试人员”），该 Node 所包含的数据集如下表所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>测试人员</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>不爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>从而：</p>
<ul>
<li>局部剪枝前、该 Node 的损失为：  <script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = C\left( T \right) + \alpha\left| T \right| = 0 + 2\alpha = 2\alpha</script></li>
<li>局部剪枝后、该 Node 的损失为：  <script type="math/tex; mode=display">
C_{\alpha}\left( t \right) = C\left( t \right) + \alpha\left| t \right| = C\left( t \right) + \alpha</script>其中  <script type="math/tex; mode=display">
C\left( t \right) = N_{t}H_{t} = 2 \times \left( - \frac{1}{2}\log\frac{1}{2} - \frac{1}{2}\log\frac{1}{2} \right) = 2</script>故  <script type="math/tex; mode=display">
C_{\alpha}\left( t \right) = 2 + \alpha</script></li>
</ul>
<p>回忆生成算法的实现，我们彼时将<script type="math/tex">\alpha</script>定义为了<script type="math/tex">\alpha = \frac{特征个数}{2}</script>（注意：这只是<script type="math/tex">\alpha</script>的一种朴素的定义方法，很难说它有什么合理性、只能说它从直观上有一定道理；如果想让模型表现更好、需要结合具体的问题来分析<script type="math/tex">\alpha</script>应该取何值）。由于气球数据集 1.0 一共有四个特征、所以此时<script type="math/tex">\alpha = 2</script>；结合各个公式、我们发现：</p>
<script type="math/tex; mode=display">
C_{\alpha}\left( t \right) = 2\alpha = 4 = 2 + \alpha = C_{\alpha}(t)</script><p>所以我们应该对该 Node 进行局部剪枝。局部剪枝后的决策树如下图所示：</p>
<img src="/posts/1a7aa546/p1.png" alt="p1.png" title="">
<p><strong><em>注意：进行局部剪枝后，由于该 Node 中样本只有两个、且一个样本类别为“不爆炸”一个为“爆炸”，所以给该 Node 标注为“不爆炸”、“爆炸”甚至以 50%的概率标注为“不爆炸”等做法都是合理的。为简洁，我们如上图中所做的一般、将其标注为“爆炸”</em></strong></p>
<p>然后我们需要考察最左下方的 Node（该 Node 的划分标准也是“测试人员”），易知计算过程和上述的没有区别。对其进行局部剪枝后的决策树如下图所示：</p>
<img src="/posts/1a7aa546/p2.png" alt="p2.png" title="">
<p>然后我们需要考察右下方的 Node（该 Node 的划分标准是“动作”），该 Node 所包含的数据集如下表所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用手打</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>从而：</p>
<ul>
<li><p>局部剪枝前、该 Node 的损失为：  </p>
<script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = C\left( T \right) + \alpha\left| T \right| = C\left( T \right) + 2\alpha</script><p>其中  </p>
<script type="math/tex; mode=display">
\begin{align}
C\left( T \right) = N_{}H_{} + N_{}H_{} \\

= 2 \times \left( - \frac{1}{2}\log\frac{1}{2} - \frac{1}{2}\log\frac{1}{2} \right) + 4 \times 0 = 2
\end{align}</script><p>故  </p>
<script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = 2 + 2\alpha</script></li>
<li>局部剪枝后、该 Node 的损失为：  <script type="math/tex; mode=display">
C_{\alpha}\left( t \right) = C\left( t \right) + \alpha\left| t \right| = C\left( t \right) + \alpha</script>其中  <script type="math/tex; mode=display">
C\left( t \right) = N_{t}H_{t} = 6 \times \left( - \frac{1}{6}\log\frac{1}{6} - \frac{5}{6}\log\frac{5}{6} \right) \approx 3.9</script>故  <script type="math/tex; mode=display">
C_{\alpha}\left( t \right) \approx 3.9 + \alpha</script></li>
</ul>
<p>将<script type="math/tex">\alpha = 2</script>代入、知：</p>
<script type="math/tex; mode=display">
C_{\alpha}\left( T \right) = 2 + 2\alpha = 6 > 5.9 = 3.9 + \alpha \approx C_{\alpha}(t)</script><p>故应该对该 Node 进行局部剪枝。局部剪枝后的决策树如下图所示：</p>
<img src="/posts/1a7aa546/p3.png" alt="p3.png" title="">
<p>然后我们需要考察左下方的 Node（该 Node 的划分标准也是“动作”），易知计算过程和上述的没有区别。对其进行局部剪枝后的决策树如下图所示：</p>
<img src="/posts/1a7aa546/p4.png" alt="p4.png" title="">
<p>通过计算易知不应对根节点进行局部剪枝、所以上图所示的决策树即是当<script type="math/tex">\alpha = 2</script>时最终修剪出来的决策树</p>
<h1 id="CART-剪枝"><a href="#CART-剪枝" class="headerlink" title="CART 剪枝"></a>CART 剪枝</h1><p>第二种剪枝算法（CART 剪枝）中的许多定义可能还不是很清晰，所以我们先对相关概念进行详细一点的直观说明：</p>
<p>首先需要指出的是：关于第二种算法中出现的一系列的阈值，它们的含义其实和第一种算法中的<script type="math/tex">\alpha</script>一样、都是模型复杂度的“惩罚因子”；不同的是，第一种算法的<script type="math/tex">\alpha</script>是人为给定的、第二种算法中一系列的阈值则是算法生成出来的。其中，<script type="math/tex">\alpha_{0} = 0</script>意味着算法初始不对模型复杂度进行惩罚、此时最优树即是原始树<script type="math/tex">T_{0}</script>。然后我们设想<script type="math/tex">\alpha</script>缓慢增大、亦即缓慢增大对模型复杂度的惩罚，那么到某个阈值<script type="math/tex">\alpha_{1}</script>时，对决策树中某个 Node 进行局部剪枝就是一个更好的选择。我们将该 Node 进行局部剪枝后的决策树<script type="math/tex">T_{1}</script>存进一个列表中、然后继续缓慢增加惩罚因子<script type="math/tex">\alpha</script>，继而到某个阈值<script type="math/tex">\alpha_{2}</script>后、对某个 Node 进行局部剪枝就又会是一个更好的选择……以此类推，直到<script type="math/tex">\alpha</script>变成一个充分大的数<script type="math/tex">\alpha_{p}</script>后、只保留根节点这一个 Node 会是最好的选择，此时就终止算法并通过交叉验证从<script type="math/tex">T_{0},\ldots,T_{p}</script>中选出最好的<script type="math/tex">T_{p^{*}}</script>作为修剪后的决策树。</p>
<p>那么这个相对比较复杂的算法有什么优异之处呢？可以证明：在 CART 剪枝里得到的决策树<script type="math/tex">T_{0},\ldots,T_{p}</script>中，对<script type="math/tex">\forall i = 0,\ldots,p</script>、<script type="math/tex">T_{i}</script>都是当惩罚因子<script type="math/tex">\alpha \in \lbrack\alpha_{i},\alpha_{i + 1})</script>时的最优决策树。这条性质保证了 CART 算法最终通过交叉验证选出来的决策树<script type="math/tex">T_{p^{*}}</script>具有一定的优良性。</p>
<p>该算法的详细叙述则如下：</p>
<ol>
<li><strong>输入</strong>：在训练集上调用生成算法所产生的原始决策树<script type="math/tex">T</script>，交叉验证集</li>
<li>过程：<ol>
<li>从下往上地获取<script type="math/tex">T</script>中所有 Node，存入列表<code>_tmp_nodes</code></li>
<li>对<code>_tmp_nodes</code>中的所有 Node 计算阈值，存入列表<code>_thresholds</code>；其中，第 t 个 Node 的阈值<script type="math/tex">\alpha_{t}</script>应满足：  <script type="math/tex; mode=display">
C\left( T_{t} \right) + \alpha_{t}\left| T_{t} \right| = C_{\alpha_{t}}\left( T_{t} \right) = C_{\alpha_{t}}\left( t \right) = C\left( t \right) + \alpha_{t}</script>其中<script type="math/tex">C(t)</script>即是第 t 个 Node 自身数据的不确定性；换言之，<script type="math/tex">C_{\alpha_{t}}(T_{t})</script>代表着第 t 个 Node 进行局部剪枝前的新损失、<script type="math/tex">C_{\alpha_{t}}(t)</script>代表着局部剪枝后的新损失。由上式可求出：  <script type="math/tex; mode=display">
\alpha_{t} = \frac{C\left( t \right) - C\left( T_{t} \right)}{\left| T_{t} \right| - 1}</script>此即阈值的计算公式</li>
<li>进入循环体：<ol>
<li>将当前决策树存入列表<code>self.roots</code></li>
<li>若当前决策树中只剩根节点、则退出循环体</li>
<li>否则，取 p 满足：  <script type="math/tex; mode=display">
p = \arg{\min_{p}{\_\text{thresholds}}}</script>然后对<code>_tmp_nodes[p]</code>进行局部剪枝</li>
<li>在完成局部剪枝后，更新<code>_thresholds</code>、<code>_tmp_nodes</code>等变量。具体而言，我们无需重新计算它们、只需更新“被影响到的” Node 所对应的位置的值即可</li>
</ol>
</li>
<li>然后调用<code>self.reduce_nodes</code>方法、将被剪掉的 Node 从<code>nodes</code>中除去</li>
<li>最后利用交叉验证、从<code>self.roots</code>中选出表现最好的决策树<script type="math/tex">T_{p^{*}}</script></li>
</ol>
</li>
<li><strong>输出</strong>：修剪过后的决策树<script type="math/tex">T_{p^{*}}</script></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在知道怎么得到一颗决策树后、我们当然就会想知道：这样建立起来的决策树的表现究竟如何？从直观上来说，只要决策树足够深、划分标准足够细，它在训练集上的表现就能接近完美；但同时也容易想象，由于它可能把训练集的一些“特性”当做所有数据的“共性”来看待，它在未知的测试数据上的表现可能就会比较一般、亦即会出现过拟合的问题。我们知道，模型出现过拟合问题一般是因为模型太过复杂，所以决策树解决过拟合的方法是采取适当的“剪枝”、我们在上一节中也已经大量接触了这一概念。剪枝通常分为两类：“预剪枝（Pre-Pruning）”和“后剪枝（Post-Pruning）”，其中“预剪枝”的概念在生成算法中已有定义，彼时我们采取的说法是“停止条件”；而一般提起剪枝时指的都是“后剪枝”，它是指在决策树生成完毕后再对其进行修剪、把多余的节点剪掉。换句话说，后剪枝是从全局出发、通过某种标准对一些 Node 进行局部剪枝，这样就能减少决策树中 Node 的数目、从而有效地降低模型复杂度&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>树结构的实现</title>
    <link href="http://www.carefree0910.com/posts/b07c81ec/"/>
    <id>http://www.carefree0910.com/posts/b07c81ec/</id>
    <published>2017-04-22T15:07:52.000Z</published>
    <updated>2017-04-22T15:32:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Tree.py" target="_blank" rel="external">这里</a>）</p>
<p>由前文的诸多讨论可知，Tree 结构需要做到如下几点：</p>
<ul>
<li>定义好需要在各个 Node 上调用的“全局变量”</li>
<li>做好数据预处理的工作、保证传给 Node 的数据是合乎要求的</li>
<li>对各个 Node 进行合适的封装，做到：<ul>
<li>生成决策树时能够正确地调用它们的生成算法</li>
<li>进行后剪枝时能够正确地调用它们的局部剪枝函数</li>
</ul>
</li>
<li>定义预测函数和评估函数以供用户调用</li>
</ul>
<p>既然 Node 我们可以抽象出一个基类<code>CvDNode</code>，我们自然也能相应地对 Tree 结构抽象出一个基类<code>CvDBase</code></p>
<a id="more"></a>
<p>下面先来看看如何搭建该基类的基本框架：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</div><div class="line"><span class="comment"># 导入Node结构以进行封装</span></div><div class="line"><span class="keyword">from</span> c_CvDTree.Node <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="comment"># 定义一个足够抽象的Tree结构的基类以适应我们Node结构的基类</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CvDBase</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self.nodes：记录所有Node的列表</div><div class="line">        self.roots：主要用于CART剪枝的属性，可先按下不表</div><div class="line">（用于存储算法过程中产生的各个决策树）</div><div class="line">            self.max_depth：记录决策树最大深度的属性</div><div class="line">        self.root, self.feature_sets：根节点和记录可选特征维度的列表</div><div class="line">            self.label_dic：和朴素贝叶斯里面相应的属性意义一致、是类别的转换字典</div><div class="line">        self.prune_alpha, self.layers：主要用于ID3和C4.5剪枝的两个属性，可先按下不表</div><div class="line">（self.prune_alpha是“惩罚因子”，self.layers则记录着每一“层”的Node）</div><div class="line">        self.whether_continuous：记录着各个维度的特征是否连续的列表</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, max_depth=None, node=None)</span>:</span></div><div class="line">        self.nodes, self.layers, self.roots = [], [], []</div><div class="line">        self.max_depth = max_depth</div><div class="line">        self.root = node</div><div class="line">        self.feature_sets = []</div><div class="line">        self.label_dic = &#123;&#125;</div><div class="line">        self.prune_alpha = <span class="number">1</span></div><div class="line">        self.whether_continuous = <span class="keyword">None</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> <span class="string">"CvDTree (&#123;&#125;)"</span>.format(self.root.height)</div><div class="line"></div><div class="line">    __repr__ = __str__</div></pre></td></tr></table></figure>
<p>回忆朴素贝叶斯的实现，可以知道在第二章的混合型朴素贝叶斯中、我们要求用户告诉程序哪些维度的特征是连续的；这里我们介绍一种非常简易却有一定合理性的做法、从而可以让程序在进行数据预处理时自动识别出连续特征对应的维度：</p>
<ul>
<li>将训练集中每个维度特征的所有可能的取值算出来，这一步可以用 Python 内置的数据结构<code>set</code>来完成</li>
<li>如果第 i 维可能的取值个数<script type="math/tex">S_i</script>比上训练集总样本数 N 大于某个阈值，亦即若：  <script type="math/tex; mode=display">
S_i\ge\beta N</script>那么就认为第 i 维的特征是连续型随机变量<br><script type="math/tex">\beta</script>的具体取值需要视情况而定。一般来说在样本数 N 足够大时、可以取得比较小（比如取就是个不错的选择）；但是样本数 N 比较小时，我们可能需要将取得大一些（比如取）。具体应该取什么值还是要看具体的任务和数据，毕竟这种自动识别的方法还是过于朴素了</li>
</ul>
<p>以上所叙述的数据预处理的实现如下（注：我们在朴素贝叶斯的实现里用到过的<code>quantize_data</code>方法中整合了如下代码中的核心部分）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_data</span><span class="params">(self, x, continuous_rate=<span class="number">0.2</span>)</span>:</span></div><div class="line">    <span class="comment"># 利用set获取各个维度特征的所有可能取值</span></div><div class="line">    self.feature_sets = [set(dimension) <span class="keyword">for</span> dimension <span class="keyword">in</span> x.T]</div><div class="line">    data_len, data_dim = x.shape</div><div class="line">    <span class="comment"># 判断是否连续</span></div><div class="line">    self.whether_continuous = np.array(</div><div class="line">        [len(feat) &gt;= continuous_rate * data_len <span class="keyword">for</span> feat <span class="keyword">in</span> self.feature_sets])</div><div class="line">    self.root.feats = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>])]</div><div class="line">    self.root.feed_tree(self)</div></pre></td></tr></table></figure>
<p>最后一行我们对根节点调用了<code>feed_tree</code>方法，该方法会做以下三件事：</p>
<ul>
<li>让决策树中所有的 Node 记录一下它们所属的 Tree 结构</li>
<li>将自己记录在 Tree 中记录所有 Node 的列表<code>nodes</code>里</li>
<li>根据 Tree 的相应属性更新记录连续特征的列表</li>
</ul>
<p>实现的时候同样利用上了递归：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_tree</span><span class="params">(self, tree)</span>:</span></div><div class="line">    self.tree = tree</div><div class="line">    self.tree.nodes.append(self)</div><div class="line">    self.wc = tree.whether_continuous</div><div class="line">    <span class="keyword">for</span> child <span class="keyword">in</span> self.children.values():</div><div class="line">        <span class="keyword">if</span> child <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            child.feed_tree(tree)</div></pre></td></tr></table></figure>
<p><strong><em>注意：以上代码应定义在<code>CvDNode</code>里面</em></strong></p>
<p>接下来就是对生成算法的封装了。考虑到第二节会讲到的剪枝算法、我们需要做的是：</p>
<ul>
<li>将类别向量数值化（和朴素贝叶斯里面的数值化类别向量的方法一样）</li>
<li>将数据集切分成训练集和交叉验证集、同时处理好样本权重</li>
<li>对根节点调用决策树的生成算法</li>
<li>调用自己的剪枝算法</li>
</ul>
<p>具体的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 参数alpha和剪枝有关、可按下不表</span></div><div class="line"><span class="comment"># cv_rate用于控制交叉验证集的大小，train_only则控制程序是否进行数据集的切分</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, alpha=None, sample_weight=None, eps=<span class="number">1e-8</span>,</span></span></div><div class="line">    cv_rate=<span class="number">0.2</span>, train_only=False):</div><div class="line">    <span class="comment"># 数值化类别向量</span></div><div class="line">    _dic = &#123;c: i <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(set(y))&#125;</div><div class="line">    y = np.array([_dic[yy] <span class="keyword">for</span> yy <span class="keyword">in</span> y])</div><div class="line">    self.label_dic = &#123;value: key <span class="keyword">for</span> key, value <span class="keyword">in</span> _dic.items()&#125;</div><div class="line">    x = np.array(x)</div><div class="line">    <span class="comment"># 根据特征个数定出alpha</span></div><div class="line">    self.prune_alpha = alpha <span class="keyword">if</span> alpha <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">else</span> x.shape[<span class="number">1</span>] / <span class="number">2</span></div><div class="line">    <span class="comment"># 如果需要划分数据集的话</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> train_only <span class="keyword">and</span> self.root.is_cart:</div><div class="line">        <span class="comment"># 根据cv_rate将数据集随机分成训练集和交叉验证集</span></div><div class="line">        <span class="comment"># 实现的核心思想是利用下标来进行各种切分</span></div><div class="line">        _train_num = int(len(x) * (<span class="number">1</span>-cv_rate))</div><div class="line">        _indices = np.random.permutation(np.arange(len(x)))</div><div class="line">        _train_indices = _indices[:_train_num]</div><div class="line">        _test_indices = _indices[_train_num:]</div><div class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            <span class="comment"># 注意对切分后的样本权重做归一化处理</span></div><div class="line">            _train_weights = sample_weight[_train_indices]</div><div class="line">            _test_weights = sample_weight[_test_indices]</div><div class="line">            _train_weights /= np.sum(_train_weights)</div><div class="line">            _test_weights /= np.sum(_test_weights)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            _train_weights = _test_weights = <span class="keyword">None</span></div><div class="line">        x_train, y_train = x[_train_indices], y[_train_indices]</div><div class="line">        x_cv, y_cv = x[_test_indices], y[_test_indices]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        x_train, y_train, _train_weights = x, y, sample_weight</div><div class="line">        x_cv = y_cv = _test_weights = <span class="keyword">None</span></div><div class="line">    self.feed_data(x_train)</div><div class="line">    <span class="comment"># 调用根节点的生成算法</span></div><div class="line">    self.root.fit(x_train, y_train, _train_weights, eps)</div><div class="line">    <span class="comment"># 调用对Node剪枝算法的封装</span></div><div class="line">    self.prune(x_cv, y_cv, _test_weights)</div></pre></td></tr></table></figure>
<p>这里我们用到了<code>np.random.permutation</code>方法，它其实可以看成两行代码的缩写、亦即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">_indices = np.random.permutation(np.arange(n))</div></pre></td></tr></table></figure>
<p>从效果上来说等价于</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">_indices = np.arange(n)</div><div class="line">np.random.shuffle(_indices)</div></pre></td></tr></table></figure>
<p>不过前者不仅写起来更便利、而且运行速度也要稍微快一点，是故我们选择了前一种方法来进行实现</p>
<p>除了<code>fit</code>这个函数以外，回忆 Node 中生成算法的实现过程、可知彼时我们调用了 Tree 的<code>reduce_nodes</code>方法来将被剪掉的 Node 从<code>nodes</code>中除去。该方法的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_nodes</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.nodes)<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</div><div class="line">        <span class="keyword">if</span> self.nodes[i].pruned:</div><div class="line">            self.nodes.pop(i)</div></pre></td></tr></table></figure>
<p><strong><em>注意：虽然该实现相当简单直观、不过其中却蕴含了一个具有普适意义的编程思想：如果要在遍历列表的同时进行当前列表元素的删除操作、就一定要从后往前遍历</em></strong></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;（本文会用到的所有代码都在&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Tree.py&quot;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;由前文的诸多讨论可知，Tree 结构需要做到如下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;定义好需要在各个 Node 上调用的“全局变量”&lt;/li&gt;
&lt;li&gt;做好数据预处理的工作、保证传给 Node 的数据是合乎要求的&lt;/li&gt;
&lt;li&gt;对各个 Node 进行合适的封装，做到：&lt;ul&gt;
&lt;li&gt;生成决策树时能够正确地调用它们的生成算法&lt;/li&gt;
&lt;li&gt;进行后剪枝时能够正确地调用它们的局部剪枝函数&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;定义预测函数和评估函数以供用户调用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;既然 Node 我们可以抽象出一个基类&lt;code&gt;CvDNode&lt;/code&gt;，我们自然也能相应地对 Tree 结构抽象出一个基类&lt;code&gt;CvDBase&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>节点结构的实现</title>
    <link href="http://www.carefree0910.com/posts/41abb98b/"/>
    <id>http://www.carefree0910.com/posts/41abb98b/</id>
    <published>2017-04-22T14:17:38.000Z</published>
    <updated>2017-04-23T00:45:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py" target="_blank" rel="external">这里</a>）</p>
<p>在实现完计算各种信息量的函数之后，我们就可以着手实现决策树本身了。由前文的讨论可知、组成决策树主体的是一个个的 Node，所以我们接下来首先要实现的就是 Node 这个结构。而且由于我们所关心的 ID3、C4.5 和 CART 分类树的 Node 在大多数情况下表现一致、只有少数几个地方有所不同，因此我们可以写一个统一的 Node 结构的基类<code>CvDNode</code>来囊括我们所有关心的决策树生成算法，该基类需要实现如下功能：</p>
<ul>
<li>根据离散型特征划分数据（ID3、C4.5、CART）</li>
<li>根据连续型特征划分数据（C4.5、CART）</li>
<li>根据当前的数据判定所属的类别</li>
</ul>
<p>虽然说起来显得轻巧，但这之中的抽象还是比较繁琐的</p>
<a id="more"></a>
<p>我们先看看这个基类的基本框架该如何搭建：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="comment"># 导入之前定义的Cluster类以计算各种信息量</span></div><div class="line"><span class="keyword">from</span> c_Tree.Cluster <span class="keyword">import</span> Cluster</div><div class="line"></div><div class="line"><span class="comment"># 定义一个足够抽象的基类以囊括所有我们关心的算法</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">CvDNode</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._x, self._y：记录数据集的变量</div><div class="line">        self.base, self.chaos：记录对数的底和当前的不确定性</div><div class="line">        self.criterion, self.category：记录该Node计算信息增益的方法和所属的类别</div><div class="line">        self.left_child, self.right_child：针对连续型特征和CART、记录该Node的左右子节点</div><div class="line">        self._children, self.leafs：记录该Node的所有子节点和所有下属的叶节点</div><div class="line">        self.sample_weight：记录样本权重</div><div class="line">        self.wc：记录着各个维度的特征是否连续的列表（whether continuous的缩写）</div><div class="line">        self.tree：记录着该Node所属的Tree</div><div class="line">        self.feature_dim, self.tar, self.feats：记录该Node划分标准的相关信息。具体而言：</div><div class="line">            self.feature_dim：记录着作为划分标准的特征所对应的维度</div><div class="line">            self.tar：针对连续型特征和CART、记录二分标准</div><div class="line">            self.feats：记录该Node能进行选择的、作为划分标准的特征的维度</div><div class="line">        self.parent, self.is_root：记录该Node的父节点以及该Node是否为根节点</div><div class="line">        self._depth, self.prev_feat：记录Node的深度和其父节点的划分标准</div><div class="line">        self.is_cart：记录该Node是否使用了CART算法</div><div class="line">        self.is_continuous：记录该Node选择的划分标准对应的特征是否连续</div><div class="line">        self.pruned：记录该Node是否已被剪掉，后面实现局部剪枝算法时会用到</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tree=None, base=<span class="number">2</span>, chaos=None,</span></span></div><div class="line">               depth=<span class="number">0</span>, parent=None, is_root=True, prev_feat=<span class="string">"Root"</span>):</div><div class="line">        self._x = self._y = <span class="keyword">None</span></div><div class="line">        self.base, self.chaos = base, chaos</div><div class="line">        self.criterion = self.category = <span class="keyword">None</span></div><div class="line">        self.left_child = self.right_child = <span class="keyword">None</span></div><div class="line">        self._children, self.leafs = &#123;&#125;, &#123;&#125;</div><div class="line">        self.sample_weight = <span class="keyword">None</span></div><div class="line">        self.wc = <span class="keyword">None</span></div><div class="line">        self.tree = tree</div><div class="line">        <span class="comment"># 如果传入了Tree的话就进行相应的初始化</span></div><div class="line">        <span class="keyword">if</span> tree <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            <span class="comment"># 由于数据预处理是由Tree完成的</span></div><div class="line">            <span class="comment"># 所以各个维度的特征是否是连续型随机变量也是由Tree记录的</span></div><div class="line">            self.wc = tree.whether_continuous</div><div class="line">            <span class="comment"># 这里的nodes变量是Tree中记录所有Node的列表</span></div><div class="line">            tree.nodes.append(self)</div><div class="line">        self.feature_dim, self.tar, self.feats = <span class="keyword">None</span>, <span class="keyword">None</span>, []</div><div class="line">        self.parent, self.is_root = parent, is_root</div><div class="line">        self._depth, self.prev_feat = depth, prev_feat</div><div class="line">        self.is_cart = self.is_continuous = self.pruned = <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, item)</span>:</span></div><div class="line">        <span class="keyword">if</span> isinstance(item, str):</div><div class="line">            <span class="keyword">return</span> getattr(self, <span class="string">"_"</span> + item)</div><div class="line"></div><div class="line">    <span class="comment"># 重载 __lt__ 方法，使得Node之间可以比较谁更小、进而方便调试和可视化</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, other)</span>:</span></div><div class="line">        <span class="keyword">return</span> self.prev_feat &lt; other.prev_feat</div><div class="line">    </div><div class="line">    <span class="comment"># 重载 __str__ 和 __repr__ 方法，同样是为了方便调试和可视化</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">if</span> self.category <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">return</span> <span class="string">"CvDNode (&#123;&#125;) (&#123;&#125; -&gt; &#123;&#125;)"</span>.format(</div><div class="line">                self._depth, self.prev_feat, self.feature_dim)</div><div class="line">        <span class="keyword">return</span> <span class="string">"CvDNode (&#123;&#125;) (&#123;&#125; -&gt; class: &#123;&#125;)"</span>.format(</div><div class="line">            self._depth, self.prev_feat, self.tree.label_dic[self.category])</div><div class="line">    __repr__ = __str__</div></pre></td></tr></table></figure>
<p>可以看到，除了重载 <strong>getitem</strong> 方法以外、我们还重载 <strong>lt</strong>、<strong>str</strong> 和 <strong>repr</strong> 方法。这是因为决策树模型的结构比起朴素贝叶斯模型而言要复杂一些，为了开发过程中的调试和最后的可视化更加便利、通常来说最好让我们模型的表现更贴近内置类型的表现</p>
<p>然后我们需要定义几个 property 以使开发过程变得便利：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义children属性，主要是区分开连续+CART的情况和其余情况</span></div><div class="line"><span class="comment"># 有了该属性后，想要获得所有子节点时就不用分情况讨论了</span></div><div class="line"><span class="meta">@property</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">children</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> &#123;</div><div class="line">        <span class="string">"left"</span>: self.left_child, <span class="string">"right"</span>: self.right_child</div><div class="line">    &#125; <span class="keyword">if</span> (self.is_cart <span class="keyword">or</span> self.is_continuous) <span class="keyword">else</span> self._children</div><div class="line"></div><div class="line"><span class="comment"># 递归定义height（高度）属性：</span></div><div class="line"><span class="comment"># 叶节点高度都定义为1、其余节点的高度定义为最高的子节点的高度+1</span></div><div class="line"><span class="meta">@property</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">height</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">if</span> self.category <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span> + max([_child.height <span class="keyword">if</span> _child <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">else</span> <span class="number">0</span></div><div class="line">        <span class="keyword">for</span> _child <span class="keyword">in</span> self.children.values()])</div><div class="line"></div><div class="line"><span class="comment"># 定义info_dic（信息字典）属性，它记录了该Node的主要信息</span></div><div class="line"><span class="comment"># 在更新各个Node的叶节点时，被记录进各个self.leafs属性的就是该字典</span></div><div class="line"><span class="meta">@property</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">info_dic</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> &#123;<span class="string">"chaos"</span>: self.chaos, <span class="string">"y"</span>: self._y&#125;</div></pre></td></tr></table></figure>
<p>以上就是<code>CvDNode</code>的基本框架，接下来就可以在这个框架的基础上实现决策树的各种生成算法了。首先需要指出的是，由于 Node 结构是会被 Tree 结构封装的、所以我们应该将数据预处理操作交给 Tree 来做。其次，由于我们实现的是抽象程度比较高的基类，所以我们要做比较完备的分情况讨论</p>
<p><strong><em>注意：把 ID3、C4.5 和 CART 这三种算法分开实现是可行且高效的、此时各个部分的代码都会显得更加简洁可读一些；但这样做的话，整体的代码量就会不可避免地骤增。具体应当选择何种实现方案需要看具体的需求</em></strong></p>
<p>我们先来看看实现生成算法所需要做的一些准备工作，比如定义停止继续生成的准则、定义停止后该 Node 的行为等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义第一种停止准则：当特征维度为0或当前Node的数据的不确定性小于阈值时停止</span></div><div class="line"><span class="comment"># 同时，如果用户指定了决策树的最大深度，那么当该Node的深度太深时也停止</span></div><div class="line"><span class="comment"># 若满足了停止条件，该函数会返回True、否则会返回False</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop1</span><span class="params">(self, eps)</span>:</span></div><div class="line">    <span class="keyword">if</span> (</div><div class="line">        self._x.shape[<span class="number">1</span>] == <span class="number">0</span> <span class="keyword">or</span> (self.chaos <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self.chaos &lt;= eps)</div><div class="line">        <span class="keyword">or</span> (self.tree.max_depth <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> self._depth &gt;= self.tree.max_depth)</div><div class="line">    ):</div><div class="line">        <span class="comment"># 调用处理停止情况的方法</span></div><div class="line">        self._handle_terminate()</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># 定义第二种停止准则，当最大信息增益仍然小于阈值时停止</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">stop2</span><span class="params">(self, max_gain, eps)</span>:</span></div><div class="line">    <span class="keyword">if</span> max_gain &lt;= eps:</div><div class="line">        self._handle_terminate()</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div><div class="line">    <span class="keyword">return</span> <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># 利用bincount方法定义根据数据生成该Node所属类别的方法</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_category</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.argmax(np.bincount(self._y))</div><div class="line"></div><div class="line"><span class="comment"># 定义处理停止情况的方法，核心思想就是把该Node转化为一个叶节点</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_handle_terminate</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 首先要生成该Node所属的类别</span></div><div class="line">    self.category = self.get_category()</div><div class="line">    <span class="comment"># 然后一路回溯、更新父节点、父节点的父节点、……等等记录叶节点的属性leafs</span></div><div class="line">    _parent = self.parent</div><div class="line">    <span class="keyword">while</span> _parent <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        _parent.leafs[id(self)] = self.info_dic</div><div class="line">        _parent = _parent.parent</div></pre></td></tr></table></figure>
<p>接下来就要实现生成算法的核心了，我们可以将它分成三步以使逻辑清晰：</p>
<ul>
<li>定义一个方法使其能将一个有子节点的 Node 转化为叶节点（局部剪枝）</li>
<li>定义一个方法使其能挑选出最好的划分标准</li>
<li>定义一个方法使其能根据划分标准进行生成</li>
</ul>
<p>我们先来看看如何进行局部剪枝：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prune</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="comment"># 调用相应方法进行计算该Node所属类别</span></div><div class="line">    self.category = self.get_category()</div><div class="line">    <span class="comment"># 记录由于该Node转化为叶节点而被剪去的、下属的叶节点</span></div><div class="line">    _pop_lst = [key <span class="keyword">for</span> key <span class="keyword">in</span> self.leafs]</div><div class="line">    <span class="comment"># 然后一路回溯、更新各个parent的属性leafs（使用id作为key以避免重复）</span></div><div class="line">    _parent = self.parent</div><div class="line">    <span class="keyword">while</span> _parent <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">for</span> _k <span class="keyword">in</span> _pop_lst:</div><div class="line">            <span class="comment"># 删去由于局部剪枝而被剪掉的叶节点</span></div><div class="line">            _parent.leafs.pop(_k)</div><div class="line">        _parent.leafs[id(self)] = self.info_dic</div><div class="line">        _parent = _parent.parent</div><div class="line">    <span class="comment"># 调用mark_pruned方法将自己所有子节点、子节点的子节点……</span></div><div class="line">    <span class="comment"># 的pruned属性置为True，因为它们都被“剪掉”了</span></div><div class="line">    self.mark_pruned()</div><div class="line">    <span class="comment"># 重置各个属性</span></div><div class="line">    self.feature_dim = <span class="keyword">None</span></div><div class="line">    self.left_child = self.right_child = <span class="keyword">None</span></div><div class="line">    self._children = &#123;&#125;</div><div class="line">    self.leafs = &#123;&#125;</div></pre></td></tr></table></figure>
<p>第 16 行的 mark_pruned 方法用于给各个被局部剪枝剪掉的 Node 打上一个标记、从而今后 Tree 可以根据这些标记将被剪掉的 Node 从它记录所有 Node 的列表<code>nodes</code>中删去。该方法同样是通过递归实现的，代码十分简洁：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mark_pruned</span><span class="params">(self)</span>:</span></div><div class="line">    self.pruned = <span class="keyword">True</span></div><div class="line">    <span class="comment"># 遍历各个子节点</span></div><div class="line">    <span class="keyword">for</span> _child <span class="keyword">in</span> self.children.values():</div><div class="line">        <span class="comment"># 如果当前的子节点不是None的话、递归调用mark_pruned方法</span></div><div class="line">        <span class="comment">#（连续型特征和CART算法有可能导致children中出现None，</span></div><div class="line">        <span class="comment"># 因为此时children由left_child和right_child组成，它们有可能是None）</span></div><div class="line">        <span class="keyword">if</span> _child <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">            _child.mark_pruned()</div></pre></td></tr></table></figure>
<p>有了能够进行局部剪枝的方法后，我们就能实现拿来挑选最佳划分标准的方法了。开发时需要时刻注意分清楚二分（连续 / CART）和多分（其它）的情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y, sample_weight, eps=<span class="number">1e-8</span>)</span>:</span></div><div class="line">    self._x, self._y = np.atleast_2d(x), np.array(y)</div><div class="line">    self.sample_weight = sample_weight</div><div class="line">    <span class="comment"># 若满足第一种停止准则、则退出函数体</span></div><div class="line">    <span class="keyword">if</span> self.stop1(eps):</div><div class="line">        <span class="keyword">return</span></div><div class="line">    <span class="comment"># 用该Node的数据实例化Cluster类以计算各种信息量</span></div><div class="line">    _cluster = Cluster(self._x, self._y, sample_weight, self.base)</div><div class="line">    <span class="comment"># 对于根节点、我们需要额外算一下其数据的不确定性</span></div><div class="line">    <span class="keyword">if</span> self.is_root:</div><div class="line">        <span class="keyword">if</span> self.criterion == <span class="string">"gini"</span>:</div><div class="line">            self.chaos = _cluster.gini()</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.chaos = _cluster.ent()</div><div class="line">    _max_gain, _chaos_lst = <span class="number">0</span>, []</div><div class="line">    _max_feature = _max_tar = <span class="keyword">None</span></div><div class="line">    <span class="comment"># 遍历还能选择的特征</span></div><div class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> self.feats:</div><div class="line">        <span class="comment"># 如果是连续型特征或是CART算法，需要额外计算二分标准的取值集合</span></div><div class="line">        <span class="keyword">if</span> self.wc[feat]:</div><div class="line">            _samples = np.sort(self._x.T[feat])</div><div class="line">            _set = (_samples[:<span class="number">-1</span>] + _samples[<span class="number">1</span>:]) * <span class="number">0.5</span></div><div class="line">        <span class="keyword">elif</span> self.is_cart:</div><div class="line">            _set = self.tree.feature_sets[feat]</div><div class="line">        <span class="comment"># 然后遍历这些二分标准并调用二类问题相关的、计算信息量的方法</span></div><div class="line">        <span class="keyword">if</span> self.is_cart <span class="keyword">or</span> self.wc[feat]:</div><div class="line">            <span class="keyword">for</span> tar <span class="keyword">in</span> _set:</div><div class="line">                _tmp_gain, _tmp_chaos_lst = _cluster.bin_info_gain(</div><div class="line">                    feat, tar, criterion=self.criterion,</div><div class="line">                    get_chaos_lst=<span class="keyword">True</span>, continuous=self.wc[feat])</div><div class="line">                <span class="keyword">if</span> _tmp_gain &gt; _max_gain:</div><div class="line">                    (_max_gain, _chaos_lst), _max_feature, _max_tar = (</div><div class="line">                        _tmp_gain, _tmp_chaos_lst), feat, tar</div><div class="line">        <span class="comment"># 对于离散型特征的ID3和C4.5算法，调用普通的计算信息量的方法</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            _tmp_gain, _tmp_chaos_lst = _cluster.info_gain(</div><div class="line">                feat, self.criterion, <span class="keyword">True</span>, self.tree.feature_sets[feat])</div><div class="line">            <span class="keyword">if</span> _tmp_gain &gt; _max_gain:</div><div class="line">                (_max_gain, _chaos_lst), _max_feature = (</div><div class="line">                    _tmp_gain, _tmp_chaos_lst), feat</div><div class="line">    <span class="comment"># 若满足第二种停止准则、则退出函数体</span></div><div class="line">    <span class="keyword">if</span> self.stop2(_max_gain, eps):</div><div class="line">        <span class="keyword">return</span></div><div class="line">    <span class="comment"># 更新相关的属性</span></div><div class="line">    self.feature_dim = _max_feature</div><div class="line">    <span class="keyword">if</span> self.is_cart <span class="keyword">or</span> self.wc[_max_feature]:</div><div class="line">        self.tar = _max_tar</div><div class="line">        <span class="comment"># 调用根据划分标准进行生成的方法</span></div><div class="line">        self._gen_children(_chaos_lst)</div><div class="line">        <span class="comment"># 如果该Node的左子节点和右子节点都是叶节点且所属类别一样</span></div><div class="line">        <span class="comment"># 那么就将它们合并、亦即进行局部剪枝</span></div><div class="line">        <span class="keyword">if</span> (self.left_child.category <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span></div><div class="line">                self.left_child.category == self.right_child.category):</div><div class="line">            self.prune()</div><div class="line">            <span class="comment"># 调用Tree的相关方法，将被剪掉的、该Node的左右子节点</span></div><div class="line">            <span class="comment"># 从Tree的记录所有Node的列表nodes中除去</span></div><div class="line">            self.tree.reduce_nodes()</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="comment"># 调用根据划分标准进行生成的方法</span></div><div class="line">        self._gen_children(_chaos_lst)</div></pre></td></tr></table></figure>
<p>根据划分标准进行生成的方法相当冗长、因为需要进行相当多的分情况讨论。它的实现用到了递归的思想，真正写起来就会发现其实并不困难、只不过会有些繁琐。囿于篇幅、我们略去它的实现细节，其算法描述则如下：</p>
<ul>
<li>根据划分标准将数据划分成若干份</li>
<li>依次用这若干份数据实例化新 Node（新 Node 即是当前 Node 的子节点），同时将当前 Node 的相关信息传给新 Node。这里需要注意的是，如果划分标准是离散型特征的话：<ul>
<li>若算法是 ID3 或 C4.5，需将该特征对应的维度从新 Node 的<code>self.feats</code>属性中除去</li>
<li>若算法是 CART，需要将二分标准从新 Node 的二分标准取值集合中除去</li>
</ul>
</li>
<li>最后对新 Node 调用<code>fit</code>方法、完成递归</li>
</ul>
<p>我个人实现的版本可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py#L171" target="_blank" rel="external">这里</a></p>
<p>以上我们就实现了 Node 结构并在其上实现了决策树的生成算法，接下来我们要做的就是实现 Tree 结构来将各个 Node 封装起来</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;（本文会用到的所有代码都在&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Node.py&quot;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;在实现完计算各种信息量的函数之后，我们就可以着手实现决策树本身了。由前文的讨论可知、组成决策树主体的是一个个的 Node，所以我们接下来首先要实现的就是 Node 这个结构。而且由于我们所关心的 ID3、C4.5 和 CART 分类树的 Node 在大多数情况下表现一致、只有少数几个地方有所不同，因此我们可以写一个统一的 Node 结构的基类&lt;code&gt;CvDNode&lt;/code&gt;来囊括我们所有关心的决策树生成算法，该基类需要实现如下功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根据离散型特征划分数据（ID3、C4.5、CART）&lt;/li&gt;
&lt;li&gt;根据连续型特征划分数据（C4.5、CART）&lt;/li&gt;
&lt;li&gt;根据当前的数据判定所属的类别&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;虽然说起来显得轻巧，但这之中的抽象还是比较繁琐的&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>信息量计算的实现</title>
    <link href="http://www.carefree0910.com/posts/e0705aab/"/>
    <id>http://www.carefree0910.com/posts/e0705aab/</id>
    <published>2017-04-22T14:07:57.000Z</published>
    <updated>2017-04-22T14:20:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Cluster.py" target="_blank" rel="external">这里</a>）</p>
<p>由于决策树的生成算法中会用到各种定义下的信息量的计算，所以我们应该先把这些计算信息量相关的算法实现出来。注意到这些算法同样是在不断地进行计数工作，所以我们同样需要尽量尝试利用好上一章讲述过的 bincount 方法。由于我们是在决策树模型中调用这些算法的，所以数据预处理应该交由决策树来做、这里就只需要专注于算法本身。值得一提的是，这一套算法不仅能够应用在决策树中，在遇到任何其它需要计算信息量的场合时都能够进行应用</p>
<a id="more"></a>
<p>首先实现其基本结构：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> math</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Cluster</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._x, self._y：记录数据集的变量</div><div class="line">        self._counters：类别向量的计数器，记录第i类数据的个数</div><div class="line">            self._sample_weight：记录样本权重的属性</div><div class="line">        self._con_chaos_cache, self._ent_cache, self._gini_cache：记录中间结果的属性</div><div class="line">            self._base：记录对数的底的属性</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x, y, sample_weight=None, base=<span class="number">2</span>)</span>:</span></div><div class="line">        <span class="comment"># 这里我们要求输入的是Numpy向量（矩阵）</span></div><div class="line">        self._x, self._y = x.T, y</div><div class="line">        <span class="comment"># 利用样本权重对类别向量y进行计数</span></div><div class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            self._counters = np.bincount(self._y)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self._counters = np.bincount(self._y,</div><div class="line">                weights=sample_weight*len(sample_weight))</div><div class="line">        self._sample_weight = sample_weight</div><div class="line">        self._con_chaos_cache = self._ent_cache = self._gini_cache = <span class="keyword">None</span></div><div class="line">        self._base = base</div></pre></td></tr></table></figure>
<p>接下来就需要定义计算不确定性的两个函数。由于一个 Cluster 只接受一份数据，所以其实总的不确定性只用计算一次：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算信息熵的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">ent</span><span class="params">(self, ent=None, eps=<span class="number">1e-12</span>)</span>:</span></div><div class="line">    <span class="comment"># 如果已经计算过且调用时没有额外给各类别样本的个数、就直接调用结果</span></div><div class="line">    <span class="keyword">if</span> self._ent_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> ent <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> self._ent_cache</div><div class="line">    _len = len(self._y)</div><div class="line">    <span class="comment"># 如果调用时没有给各类别样本的个数，就利用结构本身的计数器来获取相应个数</span></div><div class="line">    <span class="keyword">if</span> ent <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        ent = self._counters</div><div class="line">    <span class="comment"># 使用eps来让算法的数值稳定性更好</span></div><div class="line">    _ent_cache = max(eps, -sum(</div><div class="line">        [_c / _len * math.log(_c / _len, self._base) <span class="keyword">if</span> _c != <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> _c <span class="keyword">in</span> ent]))</div><div class="line">    <span class="comment"># 如果调用时没有给各类别样本的个数、就将计算好的信息熵储存下来</span></div><div class="line">    <span class="keyword">if</span> ent <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        self._ent_cache = _ent_cache</div><div class="line">    <span class="keyword">return</span> _ent_cache</div><div class="line"></div><div class="line"><span class="comment"># 定义计算基尼系数的函数，和计算信息熵的函数很类似、所以略去注释</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gini</span><span class="params">(self, p=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> self._gini_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> p <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        <span class="keyword">return</span> self._gini_cache</div><div class="line">    <span class="keyword">if</span> p <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        p = self._counters</div><div class="line">    _gini_cache = <span class="number">1</span> - np.sum((p / len(self._y)) ** <span class="number">2</span>)</div><div class="line">    <span class="keyword">if</span> p <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        self._gini_cache = _gini_cache</div><div class="line">    <span class="keyword">return</span> _gini_cache</div></pre></td></tr></table></figure>
<p>然后就需要定义计算<script type="math/tex">H(y|A)</script>和<script type="math/tex">\text{Gini}(y|A)</script>的函数。从算法公式可以看出它们具有形式一致性，所以我们可以把它们的实现整合在一起：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算和的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">con_chaos</span><span class="params">(self, idx, criterion=<span class="string">"ent"</span>, features=None)</span>:</span></div><div class="line">    <span class="comment"># 根据不同的准则、调用不同的方法</span></div><div class="line">    <span class="keyword">if</span> criterion == <span class="string">"ent"</span>:</div><div class="line">        _method = <span class="keyword">lambda</span> cluster: cluster.ent()</div><div class="line">    <span class="keyword">elif</span> criterion == <span class="string">"gini"</span>:</div><div class="line">        _method = <span class="keyword">lambda</span> cluster: cluster.gini()</div><div class="line">    <span class="comment"># 根据输入获取相应维度的向量</span></div><div class="line">    data = self._x[idx]</div><div class="line">    <span class="comment"># 如果调用时没有给该维度的取值空间features、就调用set方法获得该取值空间</span></div><div class="line">    <span class="comment"># 由于调用set方法比较耗时，在决策树实现时应努力将features传入</span></div><div class="line">    <span class="keyword">if</span> features <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        features = set(data)</div><div class="line">    <span class="comment"># 获得该维度特征各取值所对应的数据的下标</span></div><div class="line">    <span class="comment"># 用self._con_chaos_cache记录下相应结果以加速后面定义的相关函数</span></div><div class="line">    tmp_labels = [data == feature <span class="keyword">for</span> feature <span class="keyword">in</span> features]</div><div class="line">    self._con_chaos_cache = [np.sum(_label) <span class="keyword">for</span> _label <span class="keyword">in</span> tmp_labels]</div><div class="line">    <span class="comment"># 利用下标获取相应的类别向量</span></div><div class="line">    label_lst = [self._y[label] <span class="keyword">for</span> label <span class="keyword">in</span> tmp_labels]</div><div class="line">    rs, chaos_lst = <span class="number">0</span>, []</div><div class="line">    <span class="comment"># 遍历各下标和对应的类别向量</span></div><div class="line">    <span class="keyword">for</span> data_label, tar_label <span class="keyword">in</span> zip(tmp_labels, label_lst):</div><div class="line">        <span class="comment"># 获取相应的数据</span></div><div class="line">        tmp_data = self._x.T[data_label]</div><div class="line">        <span class="comment"># 根据相应数据、类别向量和样本权重计算出不确定性</span></div><div class="line">        <span class="keyword">if</span> self._sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            _chaos = _method(Cluster(tmp_data, tar_label, base=self._base))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            _new_weights = self._sample_weight[data_label]</div><div class="line">            _chaos = _method(Cluster(tmp_data, tar_label, _new_weights / np.sum(</div><div class="line">                _new_weights), base=self._base))</div><div class="line">        <span class="comment"># 依概率加权，同时把各个初始条件不确定性记录下来</span></div><div class="line">        rs += len(tmp_data) / len(data) * _chaos</div><div class="line">        chaos_lst.append(_chaos)</div><div class="line">    <span class="keyword">return</span> rs, chaos_lst</div></pre></td></tr></table></figure>
<p><strong><em>注意：如果仅仅是为了获得总的条件不确定性、是不用将划分后数据的各个部分的条件不确定记录下来的；之所以我们把它记录下来、是因为在决策树生成的过程里会用到这个中间变量。我们会在后面讲解决策树结构时进行相应的说明</em></strong></p>
<p>最后需要定义计算信息增益的函数。我们将会实现涉及过的三种定义方法，而且由于它们同样具有形式一致性、所以它们的实现同样可以整合在一起：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算信息增益的函数，参数get_chaos_lst用于控制输出</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">info_gain</span><span class="params">(self, idx, criterion=<span class="string">"ent"</span>, get_chaos_lst=False, features=None)</span>:</span></div><div class="line">    <span class="comment"># 根据不同的准则、获取相应的“条件不确定性”</span></div><div class="line">    <span class="keyword">if</span> criterion <span class="keyword">in</span> (<span class="string">"ent"</span>, <span class="string">"ratio"</span>):</div><div class="line">        _con_chaos, _chaos_lst = self.con_chaos(idx, <span class="string">"ent"</span>, features)</div><div class="line">        _gain = self.ent() - _con_chaos</div><div class="line">        <span class="keyword">if</span> criterion == <span class="string">"ratio"</span>:</div><div class="line">            _gain /= self.ent(self._con_chaos_cache)</div><div class="line">    <span class="keyword">elif</span> criterion == <span class="string">"gini"</span>:</div><div class="line">        _con_chaos, _chaos_lst = self.con_chaos(idx, <span class="string">"gini"</span>, features)</div><div class="line">        _gain = self.gini() - _con_chaos</div><div class="line">    <span class="keyword">return</span> (_gain, _chaos_lst) <span class="keyword">if</span> get_chaos_lst <span class="keyword">else</span> _gain</div></pre></td></tr></table></figure>
<p>考虑到二类问题的特殊性，我们需要定义专门处理二类问题的、计算信息增益相关的函数。它们大部分和以上定义的函数没有区别、代码也有大量重复，只是它会多传进一个代表二分标准的参数。为简洁，我们略去上文已经给出过的注释</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义计算二类问题条件不确定性的函数</span></div><div class="line"><span class="comment"># 参数tar即是二分标准，参数continuous则告诉我们该维度的特征是否连续</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">bin_con_chaos</span><span class="params">(self, idx, tar, criterion=<span class="string">"gini"</span>, continuous=False)</span>:</span></div><div class="line">    <span class="keyword">if</span> criterion == <span class="string">"ent"</span>:</div><div class="line">        _method = <span class="keyword">lambda</span> cluster: cluster.ent()</div><div class="line">    <span class="keyword">elif</span> criterion == <span class="string">"gini"</span>:</div><div class="line">        _method = <span class="keyword">lambda</span> cluster: cluster.gini()</div><div class="line">    data = self._x[idx]</div><div class="line">    <span class="comment"># 根据二分标准划分数据，注意要分离散和连续两种情况讨论</span></div><div class="line">    tar = data == tar <span class="keyword">if</span> <span class="keyword">not</span> continuous <span class="keyword">else</span> data &lt; tar</div><div class="line">    tmp_labels = [tar, ~tar]</div><div class="line">    self._con_chaos_cache = [np.sum(_label) <span class="keyword">for</span> _label <span class="keyword">in</span> tmp_labels]</div><div class="line">    label_lst = [self._y[label] <span class="keyword">for</span> label <span class="keyword">in</span> tmp_labels]</div><div class="line">    rs, chaos_lst = <span class="number">0</span>, []</div><div class="line">    <span class="keyword">for</span> data_label, tar_label <span class="keyword">in</span> zip(tmp_labels, label_lst):</div><div class="line">        tmp_data = self._x.T[data_label]</div><div class="line">        <span class="keyword">if</span> self._sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            _chaos = _method(Cluster(tmp_data, tar_label, base=self._base))</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            _new_weights = self._sample_weight[data_label]</div><div class="line">            _chaos = _method(Cluster(tmp_data, tar_label, _new_weights / np.sum(</div><div class="line">                _new_weights), base=self._base))</div><div class="line">        rs += len(tmp_data) / len(data) * _chaos</div><div class="line">        chaos_lst.append(_chaos)</div><div class="line">    <span class="keyword">return</span> rs, chaos_lst</div></pre></td></tr></table></figure>
<p>定义计算二类问题信息增益的函数时，只需将之前定义过的、计算信息增益的函数中计算条件不确定性的函数替换成计算二类问题条件不确定性的函数即可</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;（本文会用到的所有代码都在&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/c_CvDTree/Cluster.py&quot;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;由于决策树的生成算法中会用到各种定义下的信息量的计算，所以我们应该先把这些计算信息量相关的算法实现出来。注意到这些算法同样是在不断地进行计数工作，所以我们同样需要尽量尝试利用好上一章讲述过的 bincount 方法。由于我们是在决策树模型中调用这些算法的，所以数据预处理应该交由决策树来做、这里就只需要专注于算法本身。值得一提的是，这一套算法不仅能够应用在决策树中，在遇到任何其它需要计算信息量的场合时都能够进行应用&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>决策树的生成算法</title>
    <link href="http://www.carefree0910.com/posts/c6faa205/"/>
    <id>http://www.carefree0910.com/posts/c6faa205/</id>
    <published>2017-04-22T12:11:18.000Z</published>
    <updated>2017-04-23T02:58:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>虽然我们之前已经用了许多文字来描述决策树、但可能还是显得过于抽象。为了能有直观的认知，在此援引维基百科上一张很好的图来进行说明：</p>
<img src="/posts/c6faa205/p1.png" alt="p1.png" title="">
<p>这张图基本蕴含了决策树中所有的关键结构，下面我们分开来分析它们</p>
<a id="more"></a>
<h1 id="决策树的相关术语"><a href="#决策树的相关术语" class="headerlink" title="决策树的相关术语"></a>决策树的相关术语</h1><p>首先是之前分析过的、“划分标准”这个概念。在上图中，被菱形橙色方框框起来的就是作为划分标准的特征，被长方形橙色方框框起来的就是对应特征的各个取值</p>
<p>然后是“节点”（Node）的概念。如果读者有学过数据结构，那么想必在提起“树”（Tree）的同时会自然而然地联想到 Node。考虑到可能有读者没有学过相关知识，这里就简要说一些相关的定义。决策树，顾名思义，确实是一个 Tree 模型。在上图中，我们可以直观地把整张图想象成一棵 Tree、把被黑色边框框起来的部分理解为一个 Node，这些 Node 是 Tree 的组成部分、Tree 本身可以协助 Node 之间的数据传输和参数调用。最上方的 Node 酷似整棵树的根，我们一般称其为“根节点”（Root）；用黑色方框（亦即四个角是直角而不是圆弧）框起来的 Node 是整棵树“生长的终点”、酷似树的叶子，我们一般称其为“叶节点”（Leaf）。比如，第三排的所有 Node 都是叶节点</p>
<p>通常来说，我们还会称一个非叶节点有一些“下属的叶节点”。比如，所有的叶节点都是根节点下属的叶节点，第三排左数第一、第二个叶节点是第二排左数第一个 Node 下属的叶节点</p>
<p>决策树中的叶节点还有一个有趣的特性：每个叶节点都对应着原样本空间的一个子空间，这些叶节点对应的子空间彼此不会相交、且并起来的话就会恰好构成完整的样本空间。换句话说、决策树的行为可以概括为如下两步：</p>
<ul>
<li>将样本空间划分为若干个互不相交的子空间</li>
<li>给每个子空间贴一个类别标签</li>
</ul>
<p>此外、我们在上图中还可以看到许多箭头，这些箭头代表着树的“生长方向”。我们一般习惯称箭头的起点是终点的“父节点”（parent）、终点是起点的“子节点”（child）；而当子节点只有两个时，通常把他们称作“左子节点”和“右子节点”。比如说，根节点是第二排所有 Node 的父节点，第二排所有 Node 都是根节点的子节点；第三排左数第一、第二个 Node 是第二排左数第一个 Node 的左、右子节点</p>
<p>对决策树有一个直观认知后，我们关心的就是怎样去生成这么一个结构了</p>
<h1 id="决策树的生成算法"><a href="#决策树的生成算法" class="headerlink" title="决策树的生成算法"></a>决策树的生成算法</h1><p>决策树的生成算法发展至今已经有许多变种，想要全面介绍它们不是短短一篇文章所能做到的。本文拟介绍其中三个上一节有所提及的、相对而言比较基本的算法：ID3、C4.5 和 CART。它们本身存在着某种递进关系：</p>
<ul>
<li>ID3 算法可说是“最朴素”的决策树算法，它给出了对离散型数据分类的解决方案</li>
<li>C4.5 算法在其上进一步发展、给出了对混合型数据分类的解决方案</li>
<li>CART 算法则更进一步、给出了对数据回归的解决方案</li>
</ul>
<p>虽说它们的功能越来越强大，但正如第一小节所言、它们的核心思想都是一致的：算法通过不断划分数据集来生成决策树，其中每一步的划分能够使当前的信息增益达到最大</p>
<p>值得一提的是，该核心思想的背后其实也有着机器学习的一些普适性的思想。我们可以这样来看待决策树：模型的损失就是数据集的不确定性，模型的算法就是最小化该不确定性；同时，和许多其它模型一样，想要从整个参数空间中选出模型的最优参数是个 NP 完全问题，所以我们（和许多其它算法一样）采用启发式的方法、近似求解这个最优化问题。具体而言，我们每次会选取一个局部最优解（每次选取一个特征对数据集进行划分使得信息增益最大化）、并把这些局部解合成最终解（合成一个划分规则的序列）</p>
<p>可以这样直观地去想一个决策树的生成过程：</p>
<ul>
<li>向根节点输入数据</li>
<li>根据信息增益的度量、选择数据的某个特征来把数据划分成（互不相交的）好几份并分别喂给一个新 Node</li>
<li>如果分完数据后发现：<ul>
<li>某份数据的不确定较小、亦即其中某一类别的样本已经占了大多数，此时就不再对这份数据继续进行划分、将对应的 Node 转化为叶节点</li>
<li>某份数据的不确定性仍然较大，那么这份数据就要继续分割下去（转第 2 步）</li>
</ul>
</li>
</ul>
<p><strong><em>注意：虽然划分的规则是根据数据定出的，但是划分本身其实是针对整个输入空间进行划分的</em></strong></p>
<p>从上述过程可知，决策树的生成过程就是根据某个度量从数据集中训练出一系列的划分规则、使得这些规则能够在数据集有好的表现。事实上，上文说的3种不同算法在分类问题上的区别亦仅表现在度量信息增益和划分数据的方法的不同上</p>
<h2 id="ID3（Interactive-Dichotomizer-3）"><a href="#ID3（Interactive-Dichotomizer-3）" class="headerlink" title="ID3（Interactive Dichotomizer-3）"></a>ID3（Interactive Dichotomizer-3）</h2><p>ID3 可以译为“交互式二分法”，虽说这个名字里面带了个“二分”，但该方法完全适用于“多分”的情况。它选择互信息作为信息增益的度量、针对离散型数据进行划分。其算法叙述如下：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,(x_{N},y_{N})\}</script></li>
<li><strong>过程</strong>：<ol>
<li>将数据集<script type="math/tex">D</script>喂给一个 Node</li>
<li>若<script type="math/tex">D</script>中的所有样本同属于类别<script type="math/tex">c_{k}</script>，则该 Node 不再继续生成、并将其类别标记为<script type="math/tex">c_{k}</script>类</li>
<li>若<script type="math/tex">x_{i}</script>已经是 0 维向量、亦即已没有可选特征，则将此时<script type="math/tex">D</script>中样本个数最多的类别<script type="math/tex">c_{k}</script>作为该 Node 的类别</li>
<li>否则，按照互信息定义的信息增益：  <script type="math/tex; mode=display">
g\left( y,x^{\left( j \right)} \right) = H\left( y \right) - H(y|x^{\left( j \right)})</script>来计算第 j 维特征的信息增益，然后选择使得信息增益最大的特征<script type="math/tex">x^{\left( j^{*} \right)}</script>作为划分标准，亦即：  <script type="math/tex; mode=display">
j^{*} = \arg{\max_{j}{g\left( y,x^{\left( j \right)} \right)}}</script></li>
<li>若<script type="math/tex">x^{\left( j^{*} \right)}</script>满足停止条件、则不再继续生成并则将此时<script type="math/tex">D</script>中样本个数最多的类别<script type="math/tex">c_{k}</script>作为类别标记</li>
<li>否则，依<script type="math/tex">x^{\left( j^{*} \right)}</script>的所有可能取值<script type="math/tex">\{ a_{1},\ldots,a_{m}\}</script>将数据集<script type="math/tex">D</script>划分为<script type="math/tex">{\{ D}_{1},\ldots,D_{m}\}</script>、使得：  <script type="math/tex; mode=display">
{(x}_{i},y_{i}) \in D_{j} \Leftrightarrow x_{i}^{\left( j^{*} \right)} = a_{j},\forall i = 1,\ldots,N</script>同时，将<script type="math/tex">x_{1},\ldots,x_{N}</script>的第<script type="math/tex">j^{*}</script>维去掉、使它们成为<script type="math/tex">n - 1</script>维的特征向量</li>
<li>对每个<script type="math/tex">D_{j}</script>从 2.1 开始调用算法</li>
</ol>
</li>
<li><strong>输出</strong>：原始数据对应的 Node（亦即根节点）</li>
</ol>
<p>其中算法第 2.5 步的“停止条件”（也可称为“预剪枝”；有关剪枝的讨论会放在<a href="/posts/1a7aa546/" title="决策树的剪枝算法">决策树的剪枝算法</a>）有许多种提法，常用的是如下两种：</p>
<ul>
<li>若选择<script type="math/tex">x^{\left( j^{*} \right)}</script>作为特征时信息增益<script type="math/tex">g(y,x^{\left( j^{*} \right)})</script>仍然很小（通常会传入一个参数<script type="math/tex">\epsilon</script>作为阈值）、则停止</li>
<li>事先把数据集分为训练集与测试集（交叉验证的思想），若由训练集得到的<script type="math/tex">x^{\left( j^{*} \right)}</script>并不能使得决策树在测试集上的错误率更小、则停止</li>
</ul>
<p>这两种停止条件的提法通用于 C4.5 和 CART，后文将不再赘述。同时，正如本章一开始有所提及的、决策树会在许多地方应用到递归的思想，上述算法中的第 2.6 步正是经典的递归</p>
<p>我们可以对气球数据集 1.0 过一遍 ID3 算法以加深理解。由算法可知，因为每个 Node 的信息熵是确定的、所以选择互信息最大的特征等价于选择条件熵最小的特征，是故我们只需要在每个 Node 上计算各个可选特征的条件熵。易知在根节点上：</p>
<script type="math/tex; mode=display">
\begin{align}
H\left( 不爆炸\middle| 颜色\right) &= - p_{11}\log p_{11} - p_{12}\log p_{12} = 1 \\

H\left( 不爆炸\middle| 大小\right) &= - p_{21}\log p_{21} - p_{22}\log p_{22} \approx 0.65 \\

H\left( 不爆炸\middle| 人员\right) &= - p_{31}\log p_{31} - p_{32}\log p_{32} \approx 0.92 \\

H\left( 不爆炸\middle| 动作\right) &= - p_{41}\log p_{41} - p_{42}\log p_{42} \approx 0.65
\end{align}</script><p>其中</p>
<script type="math/tex; mode=display">p_{11} \triangleq p\left( 不爆炸\middle| 黄色\right) = \frac{1}{2},\ \ p_{12} \triangleq p\left( 不爆炸\middle| 紫色\right) = \frac{1}{2}</script><script type="math/tex; mode=display">p_{21} \triangleq p\left( 不爆炸\middle| 小\right) = \frac{5}{6},\ \ p_{22} \triangleq p\left( 不爆炸\middle| 大\right) = \frac{1}{6}</script><script type="math/tex; mode=display">p_{31} \triangleq p\left( 不爆炸\middle| 成人\right) = \frac{1}{3},\ \ p_{32} \triangleq p\left( 不爆炸\middle| 小孩\right) = \frac{2}{3}</script><script type="math/tex; mode=display">p_{41} \triangleq p\left( 不爆炸\middle| 手打\right) = \frac{5}{6},\ \ p_{42} \triangleq p\left( 不爆炸\middle| 脚踩\right) = \frac{1}{6}</script><p>且易知</p>
<script type="math/tex; mode=display">H\left( 不爆炸\middle| A \right) = H\left( 爆炸\middle| A \right),\ \ \forall A \in \{ 颜色, 大小, 人员, 动作\}</script><p>从而可知应选测试动作或者气球大小作为根节点的划分标准。不妨选择气球大小作为划分标准，此时的决策树如下图所示（图片是使用 ProcessOn 在线绘制而成的）：</p>
<img src="/posts/c6faa205/p2.png" alt="p2.png" title="">
<p>图中 Node A 和 Node B 所对应的数据集分别如下面两张表所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用手打</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>注意此时气球大小已经不是可选特征了。我们接下来要分别对 Node A 和 Node B 调用 ID3 算法，计算过程和根节点上的计算过程大同小异。以此类推、最终我们可以得到如下图所示的决策树：</p>
<img src="/posts/c6faa205/p3.png" alt="p3.png" title="">
<p>可知该决策树在气球数据集 1.0 上的正确率为 100%、且它做的决策都很符合直观</p>
<h2 id="C4-5"><a href="#C4-5" class="headerlink" title="C4.5"></a>C4.5</h2><p>C4.5 使用信息增益比作为信息增益的度量，从而缓解了 ID3 算法会倾向于选择 m 比较大的特征<script type="math/tex">A</script>作为划分依据这个问题；也正因如此，C4.5 算法可以处理 ID3 算法比较难处理的混合型数据。我们先来看看它在离散型数据上的算法（仅展示和 ID3 算法中不同的部分）：</p>
<h3 id="算法-2-4-步"><a href="#算法-2-4-步" class="headerlink" title="算法 2.4 步"></a>算法 2.4 步</h3><p>否则，按照信息增益比定义的信息增益：</p>
<script type="math/tex; mode=display">
g_{R}\left( y,x^{\left( j \right)} \right) = \frac{g(y,x^{\left( j \right)})}{H_{x^{\left( j \right)}}(y)}</script><p>来计算第 j 维特征的信息增益，然后选择使得信息增益最大的特征<script type="math/tex">x^{\left( j^{*} \right)}</script>作为划分标准，亦即：</p>
<script type="math/tex; mode=display">
j^{*} = \arg{\max_{j}{g_{R}\left( y,x^{\left( j \right)} \right)}}</script><p><strong><em>注意：C4.5 算法虽然不会倾向于选择 m 比较大的特征、但有可能会倾向于选择 m 比较小的特征。针对这个问题，Quinlan 在 1993 年提出了这么一个启发式的方法：先选出互信息比平均互信息要高的特征、然后从这些特征中选出信息增益比最高的</em></strong></p>
<p>混合型数据的处理方法大同小异，本书拟介绍一种常用且符合直观的、同时亦是 C4.5 所采用的方法：使用二类问题的解决方案处理连续型特征。具体而言，当二类问题和决策树结合起来时，在连续的情况下、我们通常可以把它转述为：</p>
<script type="math/tex; mode=display">
Y_{1} = \left\{ y:y^{A} < a_{1} \right\},Y_{2} = \{ y:y^{A} \geq a_{1}\}</script><p>相对应的，我们同样可以用处理二类问题的思想来处理离散型特征，此时：</p>
<script type="math/tex; mode=display">
A \in \left\{ a_{1},a_{2} \right\};\ \ Y_{1} = \left\{ y:y^{A} = a_{1} \right\},\ Y_{2} = \{ y:y^{A} = a_{2}\}</script><p>更进一步、我们通常会将它表示为：</p>
<script type="math/tex; mode=display">
Y_{1} = \left\{ y:y^{A} = a_{1} \right\},Y_{2} = \{ y:y^{A} \neq a_{1}\}</script><p>我们通常称以上各式中的<script type="math/tex">a_{1}</script>为“二分标准”。一般而言，如何处理连续型特征这个问题会归结于如何选择“二分标准”这个问题。一个比较容易想到的做法是：</p>
<ul>
<li>若在当前数据集中有 m 个取值，不妨假设它们为<script type="math/tex">u_1,...,u_m</script>；不失一般性、再不妨假设它们满足<script type="math/tex">u_1<...<u_m</script>（若不然，进行一次排序操作即可），那么依次选择<script type="math/tex">v_1,...,v_p</script>作为二分标准并决出最好的一个，其中<script type="math/tex">v_1,...,v_p</script>构成等差数列、且:  <script type="math/tex; mode=display">
v_{1} = u_{1},v_{p} = u_{m}</script>p 的选取则视情况而定，一般而言会取 p 反比于“深度”。这意味着当数据越分越细时、对特征的划分会越分越粗，从直观上来说这有益于防止过拟合</li>
</ul>
<p>但这样可能会产生许多“冗余”的二分标准。试想如果这些取值满足：</p>
<script type="math/tex; mode=display">
u_{1} = 0,u_{2} = 100,u_{3} = 101,u_{4} = 102,\ldots</script><p>那么我们就会在<script type="math/tex">u_{1}</script>和<script type="math/tex">u_{2}</script>之间尝试大量的划分标准、但显然这些划分标准算出来的结果都是一样的。为了处理类似于这种不合理的情况，C4.5 采用如下做法：</p>
<ul>
<li>依次选择<script type="math/tex">v_{1} = \frac{u_{1} + u_{2}}{2},\ldots,v_{m - 1} = \frac{u_{m - 1} + u_{m}}{2}</script>作为二分标准、计算它们的信息增益比、从而决出最好的二分标准来划分数据</li>
</ul>
<p>在这之上还有另一种可行的做法：</p>
<ul>
<li>设<script type="math/tex">u_1,...,u_m</script>所对应的类别是<script type="math/tex">y_1,...,y_m</script>，那么在<script type="math/tex">v_1,...,v_{m-1}</script>中只选取使得：  <script type="math/tex; mode=display">
y_{i} \neq y_{i + 1},\ \ (i = 1,\ldots,m - 1)</script>的<script type="math/tex">v_i</script>作为二分标准、计算它们的信息增益比、从而决出最好的二分标准来划分数据</li>
</ul>
<p>这种做法在某些情况下会表现得更好、但在某些情况下也会显得不合理。鉴于此，本书会采用更稳定的上一种做法来进行实现</p>
<p><strong><em>注意：从以上讨论可知，我们完全可以把 ID3 算法推广成可以处理连续型特征的算法。只不过如果数据集是混合型数据集的话、ID3 就会倾向于选择离散型特征作为划分标准而已。如果数据集的所有特征都是连续型特征、那么 ID3 和 C4.5 之间孰优孰劣是难有定论的</em></strong></p>
<p>这里需要特别指出的是、C4.5 也是有一个比较糟糕的性质的：由信息增益比的定义可知，如果是二分的话、它会倾向于把数据集分成很不均匀的两份；因为此时<script type="math/tex">H_{A}(y)</script>将非常小、导致<script type="math/tex">g_{R}(y,A)</script>很大（即使<script type="math/tex">g(y,A)</script>比较小）。举个例子：如果当前划分标准为连续特征、那么 C4.5 可能会倾向于直接选择<script type="math/tex">v_{1},v_{2},v_{3}</script>等作为二分标准</p>
<p>之所以说该性质比较糟糕、是因为它将直接导致如下结果：当 C4.5 进行二叉分枝时、它可能总会直接分出一个比较小的 Node 作为叶节点、然后剩下一个大的 Node 继续进行生成。这种行为会导致决策树倾向于往深处发展、从而导致很容易产生过拟合现象，这并不是我们期望的结果</p>
<h2 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h2><p>CART 的全称是 Classification and Regression Tree（“分类与回归树”）。顾名思义，它既可做分类亦可做回归。囿于篇幅，本书会介绍分类问题的算法和实现、对于回归问题则只叙述算法，感兴趣的观众老爷们可以尝试触类旁通地实现它</p>
<p>CART 算法一般会使用基尼增益作为信息增益的度量（当然也可以使用互信息和信息增益比作为度量，需要视具体场合而定），其一大特色就是它假设了最终生成的决策树为二叉树、亦即它在处理离散型特征时也会通过决出二分标准来划分数据：</p>
<h3 id="算法-2-4-步-1"><a href="#算法-2-4-步-1" class="headerlink" title="算法 2.4 步"></a>算法 2.4 步</h3><p>否则，不妨设<script type="math/tex">x^{\left( j \right)}</script>在当前数据集中有<script type="math/tex">S_{j}</script>个取值<script type="math/tex">u_{1}^{\left( j \right)},\ldots,u_{S_{j}}^{\left( j \right)}</script>且它们满足<script type="math/tex">u_{1}^{\left( j \right)} < \ldots < u_{S_{j}}^{\left( j \right)}</script>，则：</p>
<ul>
<li>若<script type="math/tex">x^{\left( j \right)}</script>是离散型的，则依次选取<script type="math/tex">u_{1}^{\left( j \right)},\ldots,u_{S_{j}}^{\left( j \right)}</script>作为二分标准<script type="math/tex">a_{p}</script>，此时：  <script type="math/tex; mode=display">
A_{jp} = \{ x^{\left( j \right)} = a_{p},x^{\left( j \right)} \neq a_{p}\}</script></li>
<li>若<script type="math/tex">x^{\left( j \right)}</script>是连续型的，则依次选取<script type="math/tex">\frac{u_{1}^{\left( j \right)} + u_{2}^{\left( j \right)}}{2},\ldots,\frac{u_{S_{j} - 1}^{\left( j \right)} + u_{S_{j}}^{\left( j \right)}}{2}</script>作为二分标准<script type="math/tex">a_{p}</script>，此时：  <script type="math/tex; mode=display">
A_{jp} = \{ x^{\left( j \right)} < a_{p},x^{\left( j \right)} \geq a_{p}\}</script>按照基尼系数定义的信息增益：  <script type="math/tex; mode=display">
g_{\text{Gini}}\left( y,A_{jp} \right) = \text{Gini}\left( y \right) - \text{Gini}(y|A_{jp})</script>来计算第 j 维特征在这些二分标准下的信息增益，然后选择使得信息增益最大的特征<script type="math/tex">x^{\left( j^{*} \right)}</script>和相应的二分标准<script type="math/tex">u_{p^{*}}^{\left( j^{*} \right)}</script>作为划分标准，亦即：  <script type="math/tex; mode=display">
{(j}^{*},p^{*}) = \arg{\max_{j,p}{g_{\text{Gini}}\left( y,A_{jp} \right)}}</script></li>
</ul>
<p>从分类问题到回归问题不是一个不平凡的问题，它们的区别仅在于：回归问题除了特征是连续型的以外、“类别”也是连续型的，此时我们一般把“类别向量”改称为“输出向量”。正如前文所提及，决策树可以转化为最小化损失的问题。我们之前讨论的分类问题中的损失都是数据的不确定性，而在回归问题中、一种常见的做法就是将损失定义为平方损失：</p>
<script type="math/tex; mode=display">
L(D) = \sum_{i = 1}^{N}{I\left( y_{i} \neq f\left( x_{i} \right) \right)\left\lbrack y_{i} - f\left( x_{i} \right) \right\rbrack^{2}}</script><p>这里的<script type="math/tex">I</script>是示性函数，<script type="math/tex">f</script>是我们的模型、<script type="math/tex">f(x_{i})</script>是<script type="math/tex">x_{i}</script>在我们模型下的预测输出、<script type="math/tex">y_{i}</script>是真实输出。平方损失其实就是我们熟悉的“（欧式）距离”（预测向量和输出向量之间的距离），我们会在许多分类、回归问题中见到它的身影。在损失为平方损失时，一般称此时生成的回归决策树为最小二乘回归树</p>
<p>在分类问题中决策树是一个划分规则的序列、在回归问题中也差不多。具体而言，假设该序列一共会将输入空间划分为<script type="math/tex">R_{1},\ldots,R_{M}</script>（这 M 个子空间彼此不相交）、那么：</p>
<ul>
<li>对于分类问题，模型可表示为：  <script type="math/tex; mode=display">
f\left( x_{i} \right) = \sum_{m = 1}^{M}{y_{m}I(x_{i} \in R_{m})}</script></li>
<li>对于回归问题，模型可表示为：  <script type="math/tex; mode=display">
f\left( x_{i} \right) = \sum_{m = 1}^{M}{c_{m}I\left( x_{i} \in R_{m} \right)}</script>这里<script type="math/tex">c_{m} = \arg{\min_{c}{L_{m}\left( c \right)}} \triangleq \arg{\min_{c}{\sum_{\left( x_{i},y_{i} \right) \in R_{m}}^{}\left( y_{i} - c \right)^{2}}}</script>。那么由一阶条件：  <script type="math/tex; mode=display">
\frac{\partial L_{m}\left( c \right)}{\partial c} = 0 \Leftrightarrow - 2\sum_{\left( x_{i},y_{i} \right) \in R_{m}}^{}\left( y_{i} - c_{m} \right) = 0</script>可解得<script type="math/tex">c_{m} = \text{avg}\left( y_{i}|(x_{i},y_{i}) \in R_{m} \right) \triangleq \frac{1}{\left| R_{m} \right|}\sum_{\left( x_{i},y_{i} \right) \in R_{m}}^{}y_{i}</script></li>
</ul>
<p>最小二乘回归树的算法和 CART 做分类时的算法几乎完全一样，区别只在于：</p>
<ul>
<li>解决分类问题时，我们会在特征和二分标准选好后，通过求解：  <script type="math/tex; mode=display">
{(j}^{*},p^{*}) = \arg{\max_{j,p}{g_{\text{Gini}}\left( y,A_{jp} \right)}}</script>来选取划分标准</li>
<li>解决回归问题时，我们会在特征和二分标准选好后，通过求解：  <script type="math/tex; mode=display">
\left( j^{*},p^{*} \right) = \arg{\min_{j,p}{\lbrack\sum_{x_{i} < p}^{}{\left( y_{i} - c_{jp}^{\left( 1 \right)} \right)^{2} + \sum_{x_{i} \geq p}^{}\left( y_{i} - c_{jp}^{\left( 2 \right)} \right)^{2}}\rbrack}}</script>来选取划分标准，其中<script type="math/tex">c_{jp}^{\left( 1 \right)} = \text{avg}(y_{i}|x_{i} < p)</script>、<script type="math/tex">c_{jp}^{\left( 2 \right)} = \text{avg}(y_{i}|x_{i} \geq p)</script>，p（切分点）的选取则视情况而定（可以模仿分类问题中二分标准的选取方法）</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽然我们之前已经用了许多文字来描述决策树、但可能还是显得过于抽象。为了能有直观的认知，在此援引维基百科上一张很好的图来进行说明：&lt;/p&gt;
&lt;img src=&quot;/posts/c6faa205/p1.png&quot; alt=&quot;p1.png&quot; title=&quot;&quot;&gt;
&lt;p&gt;这张图基本蕴含了决策树中所有的关键结构，下面我们分开来分析它们&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>数据的信息</title>
    <link href="http://www.carefree0910.com/posts/2ce87ace/"/>
    <id>http://www.carefree0910.com/posts/2ce87ace/</id>
    <published>2017-04-22T11:29:24.000Z</published>
    <updated>2017-04-23T02:54:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文首先简要地说明一下决策树生成算法背后的数学基础和思想、然后再叙述具体的算法。往大了说、决策树的生成可以算是信息论的一个应用，但它其实只用到了信息论中一小部分的思想。不过，先对信息论有个概括性的认知还是有必要的、因为这样我们就可以有个更宽的视野</p>
<a id="more"></a>
<h1 id="信息论简介"><a href="#信息论简介" class="headerlink" title="信息论简介"></a>信息论简介</h1><p><strong>（注：本节有许多内容节选、修改、总结自维基百科）</strong></p>
<p>被誉为信息论创始人的是克劳德·艾尔伍德·香农（Claude Elwood Shannon，1916.4.30－2001.2.26），他是美国数学家、电子工程师和密码学家，是密歇根大学学士、麻省理工学院博士。他在 1948 年发表的划时代的论文——“通信的数学原理”奠定了现代信息论的基础</p>
<p>信息论（Information Theory）涉及的领域相当多，包括但不限于信息的量化、存储和通信、统计推断、自然语言处理、密码学等等。信息论的主要内容可以类比人类最广泛的交流手段——语言来阐述。一种简洁的语言（以英语为例）通常有如下两个重要特点：</p>
<ul>
<li>最常用的一些词汇（比如“a”、“the”、“I”）应该要比相对而言不太常用的词（比如“Python”、“Machine”、“Learning”）要短一些</li>
<li>如果句子的某一部分被漏听或者由于噪声干扰（比如身处闹市）而被误听，听者应该仍然可以抓住句子的大概意思</li>
</ul>
<p>其中第二点被称作为“鲁棒性（Robustness）”。如果把电子通信系统比作一种语言的话，这种鲁棒性（Robustness）不可或缺。信息论的基本研究课题是信源编码和信道编码（通俗一点来讲就是怎么发出信息和怎么传递信息），将鲁棒性引入通信正是通过其中的信道编码来完成的，由此可见信息论的重要性</p>
<p>注意这些内容同消息的重要性之间是毫不相干的。例如，像“你好；再见”这样的话语和像“救命”这样的紧急请求，在说起来或写起来所花的时间是差不多的，然而明显后者更重要也更有意义。信息论却不会考虑一段消息的重要性或内在意义，因为这些属于信息的质量的问题而不是信息量和可读性方面上的问题，后者只是由概率这一因素单独决定的</p>
<p>既然我们我们关注的是信息量，我们就需要有一个度量方法。决策树生成算法背后的思想正是利用该度量方法来衡量一种“数据划分”的优劣、从而生成一个“判定序列”。具体而言，它会不断地寻找数据的划分方法、使得在该划分下我们能够获得的信息量最大（更详细的叙述会在后文给出）</p>
<h1 id="不确定性"><a href="#不确定性" class="headerlink" title="不确定性"></a>不确定性</h1><p>在决策树的生成中，获得的信息量的度量方法是从反方向来定义的：若一种划分能使数据的“不确定性”减少得越多、就意味着该划分能获得越多信息。这是很符合直观的，关键问题就在于应该如何度量数据的不确定性（或说不纯度，Impurity）。常见的度量标准有两个：信息熵（Entropy）和基尼系数（Gini Index），接下来我们就说说它们的定义和性质</p>
<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>先来看看它的公式：</p>
<script type="math/tex; mode=display">
H\left( y \right) = - \sum_{k = 1}^{K}{p_{k}\log p_{k}}</script><p>对于具体的、随机变量<script type="math/tex">y</script>生成的数据集<script type="math/tex">D = \{ y_{1},\ldots,y_{N}\}</script>而言，在实际操作中通常会利用经验熵来估计真正的信息熵：</p>
<script type="math/tex; mode=display">
H\left( y \right) = H\left( D \right) = - \sum_{k = 1}^{K}{\frac{|C_{k}|}{|D|}\log\frac{|C_{k}|}{|D|}}</script><p>这里假设随机变量<script type="math/tex">y</script>的取值空间为<script type="math/tex">\{ c_{1},\ldots,c_{K}\}</script>，<script type="math/tex">p_{k}</script>表示<script type="math/tex">y</script>取<script type="math/tex">c_{k}</script>的概率：<script type="math/tex">p_{k} = p(y = c_{k})</script>；<script type="math/tex">|C_{k}|</script>代表由随机变量<script type="math/tex">y</script>中类别为<script type="math/tex">c_{k}</script>的样本的个数、<script type="math/tex">|D|</script>代表<script type="math/tex">D</script>的总样本个数（亦即<script type="math/tex">\left| D \right| = N</script>）。可以看到，经验公式背后的思想其实就是“频率估计概率”</p>
<p>通常来说，公式中对数的底会取为2、此时信息熵<script type="math/tex">H(y)</script>的单位叫作比特（bit）；如果把底取为<script type="math/tex">e</script>（亦即取自然对数）的话，<script type="math/tex">H(y)</script>的单位就称作纳特（nat）</p>
<p>接下来说明为何上式能够度量数据的不确定性。可以证明（详细推导可参见<a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a>），当：</p>
<script type="math/tex; mode=display">
p_{1} = p_{2} = \ldots = p_{K} = \frac{1}{K}</script><p>时，<script type="math/tex">H(y)</script>达到最大值<script type="math/tex">- \log\frac{1}{K}</script>、亦即<script type="math/tex">\log K</script>。由于<script type="math/tex">p_{k} = p(y = c_{k})</script>，上式即意味着随机变量<script type="math/tex">y</script>取每一个类的概率都是一样的、亦即<script type="math/tex">y</script>完全没有规律可循，想要预测它的取值只能靠运气。换句话说，由<script type="math/tex">y</script>生成出来的数据<script type="math/tex">\{ y_{1},\ldots,y_{N}\}</script>的不确定性是在取值空间为<script type="math/tex">\{ c_{1},\ldots,c_{K}\}</script>、样本数为 N 的数据中最大的（想象预测 N 次正 K 面体骰子的结果）</p>
<p>我们的目的是想让<script type="math/tex">y</script>的不确定性减小、亦即想让<script type="math/tex">y</script>变得有规律以方便我们预测。稍微严谨地来说，就是<script type="math/tex">y</script>取某个类的概率特别大、取其它类的概率都特别小。极端的例子自然就是存在某个<script type="math/tex">k^{*}</script>、使得<script type="math/tex">p\left( {y = c}_{k^{*}} \right) = 1</script>、<script type="math/tex">p\left( y = c_{k} \right) = 0,\forall k \neq k^{*}</script>、亦即<script type="math/tex">y</script>生成的样本总属于<script type="math/tex">c_{k^{*}}</script>类。带入<script type="math/tex">H(y)</script>的定义式，可以发现此时<script type="math/tex">H\left( y \right) = 0</script>、亦即<script type="math/tex">y</script>生成的样本没有不确定性</p>
<p><strong><em>注意：由于<script type="math/tex">p\log p \rightarrow 0\ (p \rightarrow 0)</script>、所以认为<script type="math/tex">0\log 0 = 0</script></em></strong></p>
<p>特殊的情况就是二类问题、亦即<script type="math/tex">K = 2</script>的情况。先不妨设<script type="math/tex">y</script>只取 0、1 二值，再设：</p>
<script type="math/tex; mode=display">
p\left( y = 0 \right) = p,\ \ p\left( y = 1 \right) = 1 - p,\ \ 0 \leq p \leq 1</script><p>那么此时的信息熵<script type="math/tex">H(y)</script>即为：</p>
<script type="math/tex; mode=display">
H\left( y \right) = - \operatorname{plog}p - \left( 1 - p \right)\log{(1 - p)}</script><p>由此可得<script type="math/tex">H(y)</script>随<script type="math/tex">p</script>变化的函数曲线。底为 2 时函数图像如下图所示：</p>
<img src="/posts/2ce87ace/p1.png" alt="p1.png" title="">
<p>如前文所述，在<script type="math/tex">p = 0.5</script>时<script type="math/tex">H(y)</script>取得最大值 1。底为<script type="math/tex">e</script>时函数图像则如下图所示：</p>
<img src="/posts/2ce87ace/p2.png" alt="p2.png" title="">
<p>虽然最大值仍在<script type="math/tex">p = 0.5</script>时取得，但是此时<script type="math/tex">H(y)</script>仅有 0.693（<script type="math/tex">\ln 2</script>）左右</p>
<p>如果对上述二类问题稍作推广：<script type="math/tex">y \in \{ Y_{1},Y_{2}\}</script>、其中<script type="math/tex">Y_{1}</script>、<script type="math/tex">Y_{2}</script>都是一个集合，那么此时信息熵的定义式即为：</p>
<script type="math/tex; mode=display">
H\left( y \right) = - p\left( y \in Y_{1} \right)\log{p\left( y \in Y_{1} \right)} - p\left( y \in Y_{2} \right)\log{p(y \in Y_{2})}</script><p>且易知：</p>
<script type="math/tex; mode=display">
p\left( y \in Y_{1} \right) + p\left( y \in Y_{2} \right) = 1</script><p>如无特殊说明，今后谈及二类问题时讨论的范围都包括推广后的二类问题</p>
<p>以上的叙述说明了，<script type="math/tex">y</script>越乱意味着<script type="math/tex">H(y)</script>越大、<script type="math/tex">y</script>越有规律意味着<script type="math/tex">H(y)</script>越小，亦即<script type="math/tex">H(y)</script>确实可以作为不确定性的度量标准</p>
<h2 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h2><p>基尼系数的定义会更简洁一些：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \right) = \sum_{k = 1}^{K}{p_{k}(1 - p_{k})} = 1 - \sum_{k = 1}^{K}p_{k}^{2}</script><p>同样可以利用经验基尼系数来进行估计：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \right) = \text{Gini}\left( D \right) = 1 - \sum_{k = 1}^{K}\left( \frac{\left| C_{k} \right|}{\left| D \right|} \right)^{2}</script><p>以及同样可以证明，当</p>
<script type="math/tex; mode=display">
p_{1} = p_{2} = \ldots = p_{K} = \frac{1}{K}</script><p>时，<script type="math/tex">\text{Gini}(y)</script>取得最大值<script type="math/tex">1 - \frac{1}{K}</script>；当存在<script type="math/tex">k^{*}</script>使得<script type="math/tex">p_{k^{*}} = 1</script>时、<script type="math/tex">\text{Gini}\left( y \right) = 0</script>。特别地、当<script type="math/tex">K = 2</script>时，可以导出：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \right) = 2p(1 - p)</script><p>此时<script type="math/tex">\text{Gini}(y)</script>的函数图像如下图所示：</p>
<img src="/posts/2ce87ace/p3.png" alt="p3.png" title="">
<p>虽然最大值仍在<script type="math/tex">p = 0.5</script>时取得，但是此时<script type="math/tex">\text{Gini}(y)</script>仅有 0.5。我们同样可以对二类问题进行推广、此时有：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \right) = 1 - p^{2}\left( y \in Y_{1} \right) - p^{2}(y \in Y_{2})</script><p>且</p>
<script type="math/tex; mode=display">
p\left( y \in Y_{1} \right) + p\left( y \in Y_{2} \right) = 1</script><p>以上的叙述说明了<script type="math/tex">\text{Gini}(y)</script>也可以用来度量不确定性</p>
<h1 id="信息的增益"><a href="#信息的增益" class="headerlink" title="信息的增益"></a>信息的增益</h1><p>在定义完不确定性的度量标准之后，我们就可以看看什么叫“获得信息”、亦即信息的增益了。从直观上来说，信息的增益是针对随机变量<script type="math/tex">y</script>和描述该变量的特征来定义的，此时数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,(x_{N},y_{N})\}</script>，其中<script type="math/tex">x_{i} = \left( x_{i}^{\left( 1 \right)},\ldots,x_{i}^{\left( n \right)} \right)^{T}</script>是描述<script type="math/tex">y_{i}</script>的特征向量、n 则是特征个数。我们可以先研究单一特征的情况（<script type="math/tex">n = 1</script>）：不妨设该特征叫<script type="math/tex">A</script>、数据集<script type="math/tex">D = \{\left( A_{1},y_{1} \right),\ldots,(A_{N},y_{N})\}</script>；此时所谓信息的增益，反映的就是特征<script type="math/tex">A</script>所能给我们带来的关于<script type="math/tex">y</script>的“信息量”的大小</p>
<p>可以引入条件熵<script type="math/tex">H(y|A)</script>的概念来定义信息的增益，它同样有着比较好的直观：</p>
<ul>
<li>所谓条件熵，就是根据特征<script type="math/tex">A</script>的不同取值<script type="math/tex">\{ a_{1},\ldots,a_{m}\}</script>对<script type="math/tex">y</script>进行限制后，先对这些被限制的<script type="math/tex">y</script>分别计算信息熵、再把这些信息熵（一共有 m 个）根据特征取值本身的概率加权求和、从而得到总的条件熵。换句话说，条件熵是由被<script type="math/tex">A</script>不同取值限制的各个部分的<script type="math/tex">y</script>的不确定性以取值本身的概率作为权重加总得到的</li>
</ul>
<p>所以，条件熵<script type="math/tex">H(y|A)</script>越小、意味着<script type="math/tex">y</script>被<script type="math/tex">A</script>限制后的总的不确定性越小、从而意味着<script type="math/tex">A</script>更能够帮助我们做出决策</p>
<p>接下来就是数学定义：</p>
<script type="math/tex; mode=display">
H\left( y \middle| A \right) = \sum_{j = 1}^{m}{p\left( A = a_{j} \right)H(y|A = a_{j})}</script><p>其中</p>
<script type="math/tex; mode=display">
H\left( y \middle| A = a_{j} \right) = - \sum_{k = 1}^{K}{p\left( y = c_{k} \middle| A = a_{j} \right)\log{p(y = c_{k}|A = a_{j})}}</script><p>同样可以用经验条件熵来估计真正的条件熵：</p>
<script type="math/tex; mode=display">
H\left( y \middle| A \right) = H\left( y \middle| D \right) = \sum_{j = 1}^{m}{\frac{\left| D_{j} \right|}{\left| D \right|}\sum_{k = 1}^{K}{\frac{|D_{jk}|}{|D_{j}|}\log\frac{|D_{jk}|}{|D_{j}|}}}</script><p>这里的<script type="math/tex">D_{j}</script>表示在<script type="math/tex">A = a_{j}</script>限制下的数据集。通常可以记<script type="math/tex">D_{j}</script>中的样本<script type="math/tex">y_{i}</script>满足<script type="math/tex">y_{i}^{A} = a_{j}</script>，亦即：</p>
<script type="math/tex; mode=display">
y_{i}^{A} = a_{j} \Leftrightarrow \left( A_{i},y_{i} \right) \in Y_{j} \Leftrightarrow A_{i} = a_{j}</script><p>而公式中的<script type="math/tex">|D_{jk}|</script>则代表着<script type="math/tex">D_{j}</script>中第 k 类样本的个数。</p>
<p>从条件熵的直观含义，信息的增益就可以自然地定义为：</p>
<script type="math/tex; mode=display">
g\left( y,A \right) = H\left( y \right) - H(y|A)</script><p>这里的<script type="math/tex">g(y,A)</script>常被称为互信息（Mutual Information），决策树中的 ID3 算法即是利用它来作为特征选取的标准的（相关定义会在后文给出）。但是，如果简单地以<script type="math/tex">g(y,A)</script>作为标准的话，会存在偏向于选择取值较多的特征、也就是 m 比较大的特征的问题。我们仍然可以从直观上去理解为什么会偏向于选取 m 较大的特征以及为什么这样做是不尽合理的：</p>
<ul>
<li>我们希望得到的决策树应该是比较深（又不会太深）的决策树，从而它可以基于多个方面而不是片面地根据某些特征来判断</li>
<li>如果单纯以<script type="math/tex">g(y,A)</script>作为标准，由于<script type="math/tex">g(y,A)</script>的直观意义是<script type="math/tex">y</script>被<script type="math/tex">A</script>划分后不确定性的减少量，可想而知，当<script type="math/tex">A</script>的取值很多时，<script type="math/tex">y</script>会被<script type="math/tex">A</script>划分成很多份、于是其不确定性自然会减少很多、从而 ID3 算法会倾向于选择<script type="math/tex">A</script>作为划分依据。但如果这样做的话，可以想象、我们最终得到的决策树将会是一颗很胖很矮的决策树，这并不是我们想要的</li>
</ul>
<p>为解决该问题、我们可以给 m 一个惩罚，由此我们可以得到信息增益比（Information Gain Ratio）的概念，该概念对应着 C4.5 算法：</p>
<script type="math/tex; mode=display">
g_{R}(y,A) = \frac{g(y,A)}{H_{A}(y)}</script><p>其中<script type="math/tex">H_{A}(y)</script>是<script type="math/tex">y</script>关于<script type="math/tex">A</script>的熵，它的定义为：</p>
<script type="math/tex; mode=display">
H_{A}\left( y \right) = - \sum_{j = 1}^{m}{p\left( y^{A} = a_{j} \right)\log{p(y^{A} = a_{j})}}</script><p>同样可以用经验熵来进行估计：</p>
<script type="math/tex; mode=display">
H_{A}\left( y \right) = H_{A}\left( D \right) = - \sum_{j = 1}^{m}{\frac{\left| D_{j} \right|}{\left| D \right|}\log\frac{\left| D_{j} \right|}{\left| D \right|}}</script><p>该定义式和信息熵的定义式很像，它们的性质也有相通之处</p>
<p>需要指出的是，只需要类比上述的过程、我们同样可以使用基尼系数来定义信息的增益。具体而言，我们可以先定义条件基尼系数：</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \middle| A \right) = \sum_{j = 1}^{m}{p\left( A = a_{j} \right)\text{Gini}(y|A = a_{j})}</script><p>其中</p>
<script type="math/tex; mode=display">
\text{Gini}\left( y \middle| A = a_{j} \right) = 1 - \sum_{k = 1}^{K}{p^{2}\left( y = c_{k} \middle| A = a_{j} \right)}</script><p>同样可以用经验条件基尼系数来进行估计：</p>
<script type="math/tex; mode=display">
\begin{align}
\text{Gini}\left( y \middle| A \right) = \text{Gini}(y|D) &= \sum_{j = 1}^{m}{\frac{\left| D_{i} \right|}{\left| D \right|}\left\lbrack 1 - \sum_{k = 1}^{K}\left( \frac{\left| D_{jk} \right|}{\left| D_{j} \right|} \right)^{2} \right\rbrack} \\

&= 1 - \sum_{j = 1}^{m}\frac{\left| D_{j} \right|}{\left| D \right|}\sum_{k = 1}^{K}\left( \frac{\left| D_{jk} \right|}{\left| D_{j} \right|} \right)^{2}
\end{align}</script><p>信息的增益则自然地定义为（不妨称之为“基尼增益”）：</p>
<script type="math/tex; mode=display">
g_{\text{Gini}}\left( y,A \right) = \text{Gini}\left( y \right) - \text{Gini}\left( y \middle| A \right)</script><p>决策树算法中的 CART 算法通常会应用这种定义</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文首先简要地说明一下决策树生成算法背后的数学基础和思想、然后再叙述具体的算法。往大了说、决策树的生成可以算是信息论的一个应用，但它其实只用到了信息论中一小部分的思想。不过，先对信息论有个概括性的认知还是有必要的、因为这样我们就可以有个更宽的视野&lt;/p&gt;
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>决策树综述</title>
    <link href="http://www.carefree0910.com/posts/dd35ec8b/"/>
    <id>http://www.carefree0910.com/posts/dd35ec8b/</id>
    <published>2017-04-22T11:09:59.000Z</published>
    <updated>2017-04-22T11:35:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>上个系列讲的朴素贝叶斯模型的理论基础大部分是数理统计和概率论相关的东西，可能从直观上不太好理解。这一章我们会讲解一种可以说是从直观上最好理解的模型——决策树。决策树是听上去比较厉害且又相对简单的模型，虽然它用到的数学知识确实不怎么多、但是在实现它的过程中可能可以获得对编程本身更深的理解，尤其是对递归的利用这一块可能会有更深的体会</p>
<p>以下是目录：</p>
<ul>
<li><a href="/posts/2ce87ace/" title="数据的信息">数据的信息</a></li>
<li><a href="/posts/c6faa205/" title="决策树的生成算法">决策树的生成算法</a></li>
<li><a href="/posts/e0705aab/" title="信息量计算的实现">信息量计算的实现</a></li>
<li><a href="/posts/41abb98b/" title="节点结构的实现">节点结构的实现</a></li>
<li><a href="/posts/b07c81ec/" title="树结构的实现">树结构的实现</a></li>
<li><a href="/posts/1a7aa546/" title="决策树的剪枝算法">决策树的剪枝算法</a></li>
<li><a href="/posts/602f7125/" title="剪枝算法的实现">剪枝算法的实现</a></li>
<li><a href="/posts/c12a819/" title="评估与可视化">评估与可视化</a></li>
<li><a href="/posts/613bbb2f/" title="相关数学理论">相关数学理论</a></li>
<li><a href="/posts/88953f51/" title="“决策树”小结">“决策树”小结</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上个系列讲的朴素贝叶斯模型的理论基础大部分是数理统计和概率论相关的东西，可能从直观上不太好理解。这一章我们会讲解一种可以说是从直观上最好理解的模型——决策树。决策树是听上去比较厉害且又相对简单的模型，虽然它用到的数学知识确实不怎么多、但是在实现它的过程中可能可以获得对编程本
    
    </summary>
    
      <category term="决策树" scheme="http://www.carefree0910.com/categories/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
    
      <category term="综述" scheme="http://www.carefree0910.com/tags/%E7%BB%BC%E8%BF%B0/"/>
    
      <category term="目录" scheme="http://www.carefree0910.com/tags/%E7%9B%AE%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>“朴素贝叶斯”小结</title>
    <link href="http://www.carefree0910.com/posts/a75c0d1b/"/>
    <id>http://www.carefree0910.com/posts/a75c0d1b/</id>
    <published>2017-04-20T12:52:08.000Z</published>
    <updated>2017-04-20T13:23:52.000Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>贝叶斯学派强调概率的“主观性”、而频率学派则强调“自然属性”</li>
<li>常见的参数估计有 ML 估计和 MAP 估计两种，其中 MAP 估计比 ML 估计多了对数先验概率这一项，体现了贝叶斯学派的思想</li>
<li>朴素贝叶斯算法下的模型一般分为三类：离散型、连续型和混合型。其中，离散型朴素贝叶斯不但能够进行对离散型数据进行分类、还能进行特征提取和可视化</li>
<li>朴素贝叶斯是简单而高效的算法，它是损失函数为 0-1 函数下的贝叶斯决策。朴素贝叶斯的基本假设是条件独立性假设，该假设一般来说太过苛刻，视情况可以通过另外两种贝叶斯分类器算法——半朴素贝叶斯和贝叶斯网来弱化</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;贝叶斯学派强调概率的“主观性”、而频率学派则强调“自然属性”&lt;/li&gt;
&lt;li&gt;常见的参数估计有 ML 估计和 MAP 估计两种，其中 MAP 估计比 ML 估计多了对数先验概率这一项，体现了贝叶斯学派的思想&lt;/li&gt;
&lt;li&gt;朴素贝叶斯算法下的模型一般分为三类
    
    </summary>
    
      <category term="朴素贝叶斯" scheme="http://www.carefree0910.com/categories/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
      <category term="小结" scheme="http://www.carefree0910.com/tags/%E5%B0%8F%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>推导与推广</title>
    <link href="http://www.carefree0910.com/posts/e312d61a/"/>
    <id>http://www.carefree0910.com/posts/e312d61a/</id>
    <published>2017-04-20T11:10:17.000Z</published>
    <updated>2017-04-20T13:51:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文旨在解决如下两个问题：</p>
<ul>
<li>为何后验概率最大化是贝叶斯决策？</li>
<li>如何导出离散型朴素贝叶斯的算法？</li>
</ul>
<p>以及旨在叙述一些朴素贝叶斯的推广。具体而言、我们会简要介绍：</p>
<ul>
<li>半朴素贝叶斯</li>
<li>贝叶斯网</li>
</ul>
<a id="more"></a>
<h1 id="朴素贝叶斯与贝叶斯决策"><a href="#朴素贝叶斯与贝叶斯决策" class="headerlink" title="朴素贝叶斯与贝叶斯决策"></a>朴素贝叶斯与贝叶斯决策</h1><p>可以证明，应用朴素贝叶斯算法得到的模型所做的决策就是 0-1 损失函数下的贝叶斯决策。这里先说一个直观：在损失函数为 0-1 损失函数的情况下，决策风险、亦即训练集的损失的期望就是示性函数某种线性组合的期望、从而就是相对应的概率；朴素贝叶斯本身就是运用相应的概率做决策、所以可以想象它们很有可能等价</p>
<p>下给出推导过程，首先我们要叙述一个定理：令<script type="math/tex">\rho(x_1,...,x_n)</script>满足：</p>
<script type="math/tex; mode=display">
\rho\left( x_{1},\ldots,x_{n} \right) = \inf_{a\in A}{\int_{\Theta}^{}{L\left( \theta,a \right)\xi\left( \theta \middle| x_{1},\ldots,x_{n} \right)\text{d}\theta}}</script><p>亦即<script type="math/tex">\rho(x_1,...,x_n)</script>是已知训练集<script type="math/tex">\tilde X=(x_1,...,x_n)</script>的最小后验期望损失。那么如果一个决策<script type="math/tex">\delta^*(x_1,...,x_n)</script>能能使任意一个含有 n 个样本的训练集的后验期望损失达到最小、亦即：</p>
<script type="math/tex; mode=display">
\int_{\Theta}^{}{L\left( \theta,\delta^{*}\left( x_{1},\ldots,x_{n} \right) \right)\xi\left( \theta \middle| x_{1},\ldots,x_{n} \right)d\theta = \rho\left( x_{1},\ldots,x_{n} \right)}\ (\forall x_{1},\ldots,x_{n})</script><p>的话，那么<script type="math/tex">\delta^*</script>就是贝叶斯决策。该定理的数学证明要用到比较深的数学知识、这里从略，但从直观上来说是可以理解的</p>
<p>是故如果我们想证明朴素贝叶斯算法能导出贝叶斯决策、我们只需证明它能使任一个训练集<script type="math/tex">\tilde X</script>上的后验期望损失<script type="math/tex">R\left( \theta,\delta(\tilde X)\right)</script>最小即可。为此，需要先计算<script type="math/tex">R\left( \theta,\delta(\tilde X)\right)</script>：</p>
<p><strong><em>注意：这里的期望是对联合分布取的，所以可以取成条件期望</em></strong></p>
<script type="math/tex; mode=display">
R\left( \theta,\delta(\tilde{X}) \right) = EL\left( \theta,\delta\left( \tilde{X} \right) \right) = E_{X}\sum_{k = 1}^{K}{\tilde{L}(c_{k},f\left( X \right))p(c_{k}|X)}</script><p>为了使上式达到最小，我们只需逐个对<script type="math/tex">X=x</script>最小化，从而有：</p>
<script type="math/tex; mode=display">
\begin{align}
  f\left( x \right) &= \arg{\min_{y\in S}{\sum_{k = 1}^{K}{\tilde{L}\left( c_{k},y \right)p\left( c_{k} \middle| X = x \right)}}} \\

  &= \arg{\min_{y\in S}{\sum_{k = 1}^{K}{p\left( y \neq c_{k} \middle| X = x \right)}}} \\

  &= \arg{\min_{c_k}\left\lbrack 1 - p\left( c_{k} \middle| X = x \right) \right\rbrack} \\

  &= \arg{\max_{c_k}{p\left( c_{k} \middle| X = x \right)}}
\end{align}</script><p>此即后验概率最大化准则、也就是朴素贝叶斯所采用的原理</p>
<h1 id="离散型朴素贝叶斯算法的推导"><a href="#离散型朴素贝叶斯算法的推导" class="headerlink" title="离散型朴素贝叶斯算法的推导"></a>离散型朴素贝叶斯算法的推导</h1><p>离散型朴素贝叶斯算法的推导相对简单但比较繁琐，核心的思想是利用示性函数将对数似然函数写成比较“整齐”的形式、再运用拉格朗日方法进行求解</p>
<p>在正式推导前，我们先说明一下符号约定：</p>
<ul>
<li>记已有的数据为<script type="math/tex">\tilde X=(x_1,x_2,...,x_N)</script>，其中：  <script type="math/tex; mode=display">
x_{i} = \left( x_{i}^{\left( 1 \right)},x_{i}^{\left( 2 \right)},\cdots,x_{i}^{\left( n \right)} \right)^{T}\ (i = 1,2,\cdots,N)</script></li>
<li><script type="math/tex">X^{\left( j \right)}</script>表示生成数据<script type="math/tex">x^{\left( j \right)}</script>的随机变量</li>
<li>随机变量<script type="math/tex">X^{\left( j \right)}</script>的取值限制在集合<script type="math/tex">K_{j} = \{ a_{j1},a_{j2},\ldots,a_{jS_j}\}\ (j = 1,2,\cdots,n)</script>中</li>
<li>类别<script type="math/tex">Y</script>的可能取值集合为<script type="math/tex">S = \{ c_{1},c_{2},\ldots,c_{K}\}</script></li>
<li>用<script type="math/tex">\theta^{c_{k}}(k = 1,2,\ldots,K)</script>表示先验概率<script type="math/tex">p(Y = c_{k})</script></li>
<li>用<script type="math/tex">\theta_{j,a_{jl}}^{c_{k}}</script>表示条件概率<script type="math/tex">p(X^{\left( j \right)} = a_{jl}|Y = c_{k})\ (j \in \left\{ 1,\ldots,n \right\},l \in \{ 1,\ldots,S_{j}\},k \in \{ 1,\ldots,K\}</script></li>
</ul>
<p>接下来就可以开始算法推导了：</p>
<h2 id="计算对数似然函数"><a href="#计算对数似然函数" class="headerlink" title="计算对数似然函数"></a>计算对数似然函数</h2><script type="math/tex; mode=display">
\begin{align}
  \ln L &= \ln{\prod_{i = 1}^{N}\left( \theta^{y_{i}} \cdot \prod_{j = 1}^{n}{\theta_{j,x_{i}^{\left( j \right)}}^{y_{i}}\ } \right)} \\

  &= \sum_{k = 1}^{K}{n_{k}\ln{\theta^{k} + \sum_{j = 1}^{n}{\sum_{k = 1}^{K}{\sum_{l = 1}^{S_{j}}{n_{j,l}^{k}\ln\theta_{j,a_{jl}}^{c_{k}}}\ }}}}
\end{align}</script><p>其中</p>
<script type="math/tex; mode=display">
\begin{align}
  n_{s} &= \sum_{i = 1}^{N}{I\left( y_{i} = c_{s} \right)} \\

  n_{j,l}^{k} &= \sum_{i = 1}^{N}{I\left( x_{i}^{\left( j \right)} = a_{jl}{,y}_{i} = c_{k} \right)}
\end{align}</script><h2 id="极大化似然函数"><a href="#极大化似然函数" class="headerlink" title="极大化似然函数"></a>极大化似然函数</h2><p>为此，只需分别最大化</p>
<script type="math/tex; mode=display">
f_{1} = \sum_{k = 1}^{K}{n_{k}\ln\theta^{k}}</script><p>和</p>
<script type="math/tex; mode=display">
f_{2} = \sum_{j = 1}^{n}{\sum_{k = 1}^{K}{\sum_{l = 1}^{S_{j}}{n_{j,l}^{k}\ln\theta_{j,a_{jl}}^{c_{k}}}\ }}</script><p>对于后者，由条件独立性假设可知、我们只需要对<script type="math/tex">j=1,2,...,n</script>分别最大化：</p>
<script type="math/tex; mode=display">f_{2}^{\left( j \right)} = \sum_{k = 1}^{K}{\sum_{l = 1}^{S_{j}}{n_{j,l}^{k}\ln\theta_{j,a_{jl}}^{c_{k}}}\ }</script><p>即可。我们利用拉格朗日方法来求解该问题，用到的约束条件是：</p>
<script type="math/tex; mode=display">
\begin{align}
  \sum_{k = 1}^{K}\theta^{k} &= \sum_{k = 1}^{K}{p\left( Y = c_{k} \right)} = 1 \\

  \sum_{l = 1}^{S_{j}}{\theta_{j,l}^{k}} &= \sum_{l = 1}^{S_{j}}{p\left( X^{\left( j \right)} = a_{jl} \middle| Y = c_{k} \right) = 1\ \left( \forall k \in \left\{ 1,\ldots,K \right\},j \in \left\{ 1,\ldots,n \right\} \right)}
\end{align}</script><p>从而可知</p>
<script type="math/tex; mode=display">
L_{1} = \sum_{k = 1}^{K}{n_{k}\ln{\theta^{k} + \alpha\left( \sum_{k = 1}^{K}{\theta^{k} - 1} \right)}}</script><p>由一阶条件</p>
<script type="math/tex; mode=display">
\frac{\partial L_{1}}{\partial\theta_{k}} = \frac{\partial L_{1}}{\partial\alpha} = 0</script><p>可以解得</p>
<script type="math/tex; mode=display">
p\left( Y = c_{k} \right) = \theta^{k} = \frac{n_{k}}{N} = \frac{\sum_{i = 1}^{N}{I(y_{i} = c_{k})}}{N}</script><p>同理，对<script type="math/tex">f_2^{(j)}</script>应用拉格朗日方法，可以得到</p>
<script type="math/tex; mode=display">
p\left( X^{\left( j \right)} = a_{jl} \middle| Y = c_{k} \right) = \theta_{j,l}^{k} = \frac{n_{j,l}^{k}}{\sum_{i = 1}^{N}{I(y_{i} = c_{k})}} = \frac{\sum_{i = 1}^{N}{I(x_{i}^{\left( j \right)} = a_{jl},y_{i} = c_{k})}}{\sum_{i = 1}^{N}{I(y_{i} = c_{k})}}</script><p>以上，我们完成了离散型朴素贝叶斯算法的推导</p>
<h1 id="半朴素贝叶斯"><a href="#半朴素贝叶斯" class="headerlink" title="半朴素贝叶斯"></a>半朴素贝叶斯</h1><p>由于提出条件独立性假设的原因正是联合概率难以求解，所以在弱化假设的时候同样应该避免引入过多的联合概率，这也正是半朴素贝叶斯的基本想法。比较常见的半朴素贝叶斯算法有如下三种：</p>
<h2 id="ODE-算法（One-Dependent-Estimator，可译为“独依赖估计”）"><a href="#ODE-算法（One-Dependent-Estimator，可译为“独依赖估计”）" class="headerlink" title="ODE 算法（One-Dependent Estimator，可译为“独依赖估计”）"></a>ODE 算法（One-Dependent Estimator，可译为“独依赖估计”）</h2><p><del>（常微分方程：？？？）</del><br>顾名思义，在该算法中、各个维度的特征至多依赖一个其它维度的特征。从公式上来说，它在描述条件概率时会多出一个条件：</p>
<script type="math/tex; mode=display">
p\left( c_{k} \middle| X = x \right) = p\left( y = c_{k} \right)\prod_{i = 1}^{n}{p\left( X^{\left( j \right)} = x^{\left( j \right)} \middle| Y = c_{k},X^{\left( pa_{j} \right)} = x^{\left( pa_{j} \right)} \right)}</script><p>这里的<script type="math/tex">\text{pa}_{j}</script>代表着维度 j 所“独依赖”的维度</p>
<h2 id="SPODE-算法（Super-Parent-ODE，可译为“超父独依赖估计”）"><a href="#SPODE-算法（Super-Parent-ODE，可译为“超父独依赖估计”）" class="headerlink" title="SPODE 算法（Super-Parent ODE，可译为“超父独依赖估计”）"></a>SPODE 算法（Super-Parent ODE，可译为“超父独依赖估计”）</h2><p>这是 ODE 算法的一个特例。在该算法中，所有维度的特征都独依赖于同一个维度的特征，这个被共同依赖的特征就叫“超父（Super-Parent）”。若它的维度是第 pa 维，知：</p>
<script type="math/tex; mode=display">
p\left( c_{k} \middle| X = x \right) = p\left( y = c_{k} \right)\prod_{i = 1}^{n}{p\left( X^{\left( j \right)} = x^{\left( j \right)} \middle| Y = c_{k},X^{\left( \text{pa} \right)} = x^{\left( \text{pa} \right)} \right)}</script><p>一般而言，会选择通过交叉验证来选择超父</p>
<h2 id="AODE-算法（Averaged-One-Dependent-Estimator，可译为“集成独依赖估计”）"><a href="#AODE-算法（Averaged-One-Dependent-Estimator，可译为“集成独依赖估计”）" class="headerlink" title="AODE 算法（Averaged One-Dependent Estimator，可译为“集成独依赖估计”）"></a>AODE 算法（Averaged One-Dependent Estimator，可译为“集成独依赖估计”）</h2><p>这种算法背后有提升方法的思想。AODE 算法会利用 SPODE 算法并尝试把许多个训练后的、有足够的训练数据量支撑的SPODE模型集成在一起来构建最终的模型。一般来说，AODE 会以所有维度的特征作为超父训练 n 个 SPODE 模型、然后线性组合出最终的模型</p>
<h1 id="贝叶斯网"><a href="#贝叶斯网" class="headerlink" title="贝叶斯网"></a>贝叶斯网</h1><p>贝叶斯网又称“信念网（Belief Network）”，比起朴素贝叶斯来说、它背后还蕴含了图论的思想。贝叶斯网有许多奇妙的性质，详细的讨论不可避免地要使用到图论的术语，这里仅拟对其做一个直观的介绍。 贝叶斯网既然带了“网”字，它的结构自然可以直观地想成是一张网络，其中：</p>
<ul>
<li>网络的节点就是单一样本的各个维度上的随机变量<script type="math/tex">X^{(1)},...,X^{(n)}</script> </li>
<li>连接节点的边就是节点之间的依赖关系</li>
</ul>
<p>需要注意的是，贝叶斯网一般要求这些边是“有方向的”、同时整张网络中不能出现“环”。无向的贝叶斯网通常是由有向贝叶斯网无向化得到的，此时它被称为 moral graph（除了把所有有向边改成无向边以外，moral graph 还需要将有向网络中不相互独立的随机变量之间连上一条无向边，细节不表），基于它能够非常直观、迅速地看出变量间的条件独立性</p>
<p>显然，有了代表各个维度随机变量的节点和代表这些节点之间的依赖关系的边之后，各个随机变量之间的条件依赖关系都可以通过这张网络表示出来。类似的东西在条件随机场中也有用到，可以说是一个适用范围非常宽泛的思想</p>
<p>贝叶斯网的学习在网络结构已经确定的情况下相对简单，其思想和朴素贝叶斯相去无多：只需要对训练集相应的条件进行“计数”即可，所以贝叶斯网的学习任务主要归结于如何找到最恰当的网络结构。常见的做法是定义一个用来打分的函数并基于该函数通过某种搜索手段来决定结构，但正如同很多最优化算法一样、在所有可能的结构空间中搜索最优结构是一个 NP 问题、无法在合理的时间内求解，所以一般会使用替代的方法求近似最优解。常见的方法有两种，一种是贪心法、比如：先定下一个初始的网络结构并从该结构出发，每次增添一条边、删去一条边或调整一条边的方向，期望通过这些手段能够使评分函数的值变大；另一种是直接限定假设空间、比如假设要求的贝叶斯网一定是一个树形的结构</p>
<p>相比起学习方法来说，贝叶斯网的决策方法相对来说显得比较不平凡。虽说最理想的情况是直接根据贝叶斯网的结构所决定的联合概率密度来计算后验概率，但是这样的计算被证明了是 NP 问题 [Cooper, 1990]。换句话说，只要贝叶斯网稍微复杂一点，这种精确的计算就无法在合理的时间内做完。所以我们同样要借助近似法求解，一种常见的做法是吉布斯采样（Gibbs Sampling），它的定义涉及到马尔科夫链相关的<del>（我还没有学的）</del>知识，这里就不详细展开了</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文旨在解决如下两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为何后验概率最大化是贝叶斯决策？&lt;/li&gt;
&lt;li&gt;如何导出离散型朴素贝叶斯的算法？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以及旨在叙述一些朴素贝叶斯的推广。具体而言、我们会简要介绍：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;半朴素贝叶斯&lt;/li&gt;
&lt;li&gt;贝叶斯网&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="朴素贝叶斯" scheme="http://www.carefree0910.com/categories/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>MergedNB 的实现</title>
    <link href="http://www.carefree0910.com/posts/7c13f69c/"/>
    <id>http://www.carefree0910.com/posts/7c13f69c/</id>
    <published>2017-04-20T09:02:35.000Z</published>
    <updated>2017-04-20T13:30:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/b_NaiveBayes/Vectorized/MergedNB.py" target="_blank" rel="external">这里</a>）</p>
<p>本文主要介绍混合型朴素贝叶斯—— MergedNB 的实现。（我知道的）混合型朴素贝叶斯算法主要有两种提法：</p>
<ul>
<li>用某种分布的密度函数算出训练集中各个样本连续型特征相应维度的密度之后，根据这些密度的情况将该维度离散化、最后再训练离散型朴素贝叶斯模型</li>
<li>直接结合离散型朴素贝叶斯和连续型朴素贝叶斯：  <script type="math/tex; mode=display">
y = f(x^{*}) = \arg{\max_{c_k}{p\left( y = c_{k} \right)\prod_{j \in S_{1}}^{}{p(X^{\left( j \right)} = {x^{*}}^{\left( j \right)}|y = c_{k})}\prod_{j \in S_{2}}^{}{p(X^{j} = {x^{*}}^{\left( j \right)}|y = c_{k})}}}</script></li>
</ul>
<p>从直观可以看出、第二种提法可能会比第一种提法要“激进”一些，因为如果某个连续型维度采用的分布特别“大起大落”的话、该维度可能就会直接“主导”整个决策。但是考虑到实现的简洁和直观（……），我们还是演示第二种提法的实现。感兴趣的观众老爷可以尝试实现第一种提法，思路和过程都是没有太本质的区别的、只是会繁琐不少</p>
<a id="more"></a>
<p>我们可以对气球数据集 1.0 稍作变动、将“气球大小”这个特征改成“气球直径”，然后我们再手动做一次分类以加深对混合型朴素贝叶斯算法的理解。新数据集如下表所示（不妨称之为气球数据集 2.0）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>直径</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>10</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>15</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>9</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>9</td>
<td>小孩</td>
<td>用脚踩</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>19</td>
<td>成人</td>
<td>用手打</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>21</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>16</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>22</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>10</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>12</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>22</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>21</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>该数据集的电子版本可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/_Data/balloon2.0.txt" target="_blank" rel="external">这里</a>。我们想预测的是样本：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>大小</th>
<th>测试人员</th>
<th>测试动作</th>
</tr>
</thead>
<tbody>
<tr>
<td>  紫色</td>
<td>10</td>
<td>小孩</td>
<td>用脚踩</td>
</tr>
</tbody>
</table>
</div>
<p>除了“大小”变成了“直径”、其余特征都一点未变，所以我们只需再计算直径的条件概率（密度）即可。由 GaussianNB 的算法可知：</p>
<script type="math/tex; mode=display">{\hat{\mu}}_{不爆炸} = \frac{10 + 9 + 9 + 16 + 10 + 12}{6} = 11</script><script type="math/tex; mode=display">{\hat{\mu}}_{爆炸} = \frac{15 + 19 + 21 + 22 + 22 + 21}{6} = 20</script><script type="math/tex; mode=display">{\hat{\sigma}}_{不爆炸} = \frac{1}{6}\left\lbrack \left( 10 - {\hat{\mu}}_{不爆炸} \right)^{2} + \ldots + \left( 12 - {\hat{\mu}}_{不爆炸} \right)^{2} \right\rbrack = 6</script><script type="math/tex; mode=display">{\hat{\sigma}}_{爆炸} = \frac{1}{6}\left\lbrack \left( 15 - {\hat{\mu}}_{爆炸} \right)^{2} + \ldots + \left( 21 - {\hat{\mu}}_{爆炸} \right)^{2} \right\rbrack = 6</script><p>从而</p>
<script type="math/tex; mode=display">
\hat{p}\left( 不爆炸\right) = \frac{1}{\sqrt{2\pi}{\hat{\sigma}}_{不爆炸}}e^{- \frac{\left( 10 - {\hat{\mu}}_{不爆炸} \right)^{2}}{2{\hat{\sigma}}^{2}_{不爆炸}}} \times p\left( 小孩\middle| 不爆炸\right) \times p\left( 用脚踩\middle| 不爆炸\right) \approx 0.0073</script><script type="math/tex; mode=display">
\hat{p}\left( 爆炸\right) = \frac{1}{\sqrt{2\pi}{\hat{\sigma}}_{爆炸}}e^{- \frac{\left( 10 - {\hat{\mu}}_{爆炸} \right)^{2}}{2{\hat{\sigma}}^{2}_{爆炸}}} \times p\left( 小孩\middle| 爆炸\right) \times p\left( 用脚踩\middle| 爆炸\right) \approx 0.0046</script><p>因此我们应该认为给定样本所导致的结果是“不爆炸”，这和直观大体相符。接下来看看具体应该如何进行实现，首先是初始化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> b_NaiveBayes.Original.Basic <span class="keyword">import</span> *</div><div class="line"><span class="keyword">from</span> b_NaiveBayes.Original.MultinomialNB <span class="keyword">import</span> MultinomialNB</div><div class="line"><span class="keyword">from</span> b_NaiveBayes.Original.GaussianNB <span class="keyword">import</span> GaussianNB</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MergedNB</span><span class="params">(NaiveBayes)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._whether_discrete：记录各个维度的变量是否是离散型变量</div><div class="line">        self._whether_continuous：记录各个维度的变量是否是连续型变量</div><div class="line">        self._multinomial、self._gaussian：离散型、连续型朴素贝叶斯模型</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, whether_continuous)</span>:</span></div><div class="line">        self._multinomial, self._gaussian = (</div><div class="line">            MultinomialNB(), GaussianNB()</div><div class="line">        <span class="keyword">if</span> whether_continuous <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            self._whether_discrete = self._whether_continuous = <span class="keyword">None</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self._whether_continuous = np.array(whether_continuous)</div><div class="line">            self._whether_discrete = ~self._whether_continuous</div></pre></td></tr></table></figure>
<p>然后是和模型的训练相关的实现，这一块将会大量重用之前在 MultinomialNB 和 GaussianNB 里面写过的东西：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_data</span><span class="params">(self, x, y, sample_weight=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        sample_weight = np.array(sample_weight)</div><div class="line">    x, y, wc, features, feat_dics, label_dic = DataUtil.quantize_data(</div><div class="line">        x, y, wc=self._whether_continuous, separate=<span class="keyword">True</span>)</div><div class="line">    <span class="comment"># 若没有指定哪些维度连续，则用 quantize_data 中朴素的方法判定哪些维度连续</span></div><div class="line">    <span class="keyword">if</span> self._whether_continuous <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        self._whether_continuous = wc</div><div class="line">        <span class="comment"># 通过Numpy中对逻辑非的支持进行快速运算</span></div><div class="line">        self._whether_discrete = ~self._whether_continuous</div><div class="line">    <span class="comment"># 计算通用变量</span></div><div class="line">    self.label_dic = label_dic</div><div class="line">    discrete_x, continuous_x = x</div><div class="line">    cat_counter = np.bincount(y)</div><div class="line">    self._cat_counter = cat_counter</div><div class="line">    labels = [y == value <span class="keyword">for</span> value <span class="keyword">in</span> range(len(cat_counter))]</div><div class="line">    <span class="comment"># 训练离散型朴素贝叶斯</span></div><div class="line">    labelled_x = [discrete_x[ci].T <span class="keyword">for</span> ci <span class="keyword">in</span> labels]</div><div class="line">    self._multinomial._x, self._multinomial._y = x, y</div><div class="line">    self._multinomial._labelled_x, self._multinomial._label_zip = (</div><div class="line">        labelled_x, list(zip(labels, labelled_x)))</div><div class="line">    self._multinomial._cat_counter = cat_counter</div><div class="line">    self._multinomial._feat_dics = [_dic</div><div class="line">        <span class="keyword">for</span> i, _dic <span class="keyword">in</span> enumerate(feat_dics) <span class="keyword">if</span> self._whether_discrete[i]]</div><div class="line">    self._multinomial._n_possibilities = [len(feats)</div><div class="line">        <span class="keyword">for</span> i, feats <span class="keyword">in</span> enumerate(features) <span class="keyword">if</span> self._whether_discrete[i]]</div><div class="line">    self._multinomial.label_dic = label_dic</div><div class="line">    <span class="comment"># 训练连续型朴素贝叶斯</span></div><div class="line">    labelled_x = [continuous_x[label].T <span class="keyword">for</span> label <span class="keyword">in</span> labels]</div><div class="line">    self._gaussian._x, self._gaussian._y = continuous_x.T, y</div><div class="line">    self._gaussian._labelled_x, self._gaussian._label_zip = labelled_x, labels</div><div class="line">    self._gaussian._cat_counter, self._gaussian.label_dic = cat_counter, label_dic</div><div class="line">    <span class="comment"># 处理样本权重</span></div><div class="line">    self._feed_sample_weight(sample_weight)</div><div class="line"></div><div class="line"><span class="comment"># 分别利用 MultinomialNB 和 GaussianNB 处理样本权重的方法来处理样本权重</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_sample_weight</span><span class="params">(self, sample_weight=None)</span>:</span></div><div class="line">    self._multinomial.feed_sample_weight(sample_weight)</div><div class="line">    self._gaussian.feed_sample_weight(sample_weight)</div><div class="line"></div><div class="line"><span class="comment"># 分别利用 MultinomialNB 和 GaussianNB 的训练函数来进行训练</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_fit</span><span class="params">(self, lb)</span>:</span></div><div class="line">    self._multinomial.fit()</div><div class="line">    self._gaussian.fit()</div><div class="line">    p_category = self._multinomial.get_prior_probability(lb)</div><div class="line">    discrete_func, continuous_func = (</div><div class="line">        self._multinomial[<span class="string">"func"</span>], self._gaussian[<span class="string">"func"</span>])</div><div class="line">    <span class="comment"># 将 MultinomialNB 和 GaussianNB 的决策函数直接合成最终决策函数</span></div><div class="line">    <span class="comment"># 由于这两个决策函数都乘了先验概率、我们需要除掉一个先验概率</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(input_x, tar_category)</span>:</span></div><div class="line">        input_x = np.array(input_x)</div><div class="line">        <span class="keyword">return</span> discrete_func(</div><div class="line">            input_x[self._whether_discrete].astype(</div><div class="line">                np.int), tar_category) * continuous_func(</div><div class="line">            input_x[self._whether_continuous], tar_category) / p_category[tar_category]</div><div class="line">    <span class="keyword">return</span> func</div></pre></td></tr></table></figure>
<p><del>（又臭又长啊喂)</del></p>
<p>上述实现有一个显而易见的可以优化的地方：我们一共在代码中重复计算了三次先验概率、但其实只用计算一次就可以。考虑到这一点不是性能瓶颈，为了代码的连贯性和可读性、我们就没有进行这个优化<del>（？？？）</del></p>
<p>数据转换函数则相对而言要复杂一点，因为我们需要跳过连续维度、将离散维度挑出来进行数值化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 实现转换混合型数据的方法，要注意利用 MultinomialNB 的相应变量</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_transfer_x</span><span class="params">(self, x)</span>:</span></div><div class="line">    _feat_dics = self._multinomial[<span class="string">"feat_dics"</span>]</div><div class="line">    idx = <span class="number">0</span></div><div class="line">    <span class="keyword">for</span> d, discrete <span class="keyword">in</span> enumerate(self._whether_discrete):</div><div class="line">        <span class="comment"># 如果是连续维度，直接调用 float 方法将其转为浮点数</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> discrete:</div><div class="line">            x[d] = float(x[d])</div><div class="line">        <span class="comment"># 如果是离散维度，利用转换字典进行数值化</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            x[d] = _feat_dics[idx][x[d]]</div><div class="line">        <span class="keyword">if</span> discrete:</div><div class="line">            idx += <span class="number">1</span></div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure>
<p>至此，混合型朴素贝叶斯模型就搭建完毕了。为了比较合理地对它进行评估，我们不妨采用 UCI 上一个我认为有些病态的数据集进行测试。问题的描述大概可以概括如下：</p>
<p>“训练数据包含了某银行一项业务的目标客户的信息、电话销售记录以及后来他是否购买了这项业务的信息。我们希望做到：根据客户的基本信息和历史联系记录，预测他是否会购买这项业务”。UCI 上的原问题描述则如下图所示：</p>
<img src="/posts/7c13f69c/p1.png" alt="p1.png" title="">
<p>概括其主要内容、就是它是一个有 17 个属性的二类分类问题。之所以我认为它是病态的，是因为我发现即使是 17 个属性几乎完全一样的两个人，他们选择是否购买业务的结果也会截然相反。事实上从心理学的角度来说，想要很好地预测人的行为确实是一项非常困难的事情、尤其是当该行为直接牵扯到较大的利益时</p>
<p>完整的数据集可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/_Data/bank1.0.txt" target="_blank" rel="external">这里</a>（最后一列数据是类别）。按照数据的特性、我们可以通过和之前用来评估 MultinomialNB 的代码差不多的代码（注意额外定义一个记录离散型维度的数组即可）得出如下图所示的结果：</p>
<img src="/posts/7c13f69c/p2.png" alt="p2.png" title="">
<p>虽然准确率达到了 89%左右，但其实该问题不应该用准确率作为评判的标准。因为如果我们观察数据就会发现、数据存在着严重的非均衡现象。事实上，88%的客户最终都是没有购买这个业务的、但我们更关心的是那一小部分购买了业务的客户，这种情况我们通常会用 F1-score 来衡量模型的好坏。此外，该问题非常需要人为进行数据清洗、因为其原始数据非常杂乱。此外，我们可以对该问题中的各个离散维度进行可视化。该数据共 9 个离散维度，我们可以将它们合并在同一个图中以方便获得该数据离散部分的直观（如下图所示；由于各个特征的各个取值通常比较长（比如”manager”之类的），为整洁、我们直接将横坐标置为等差数列而没有进行转换）：</p>
<img src="/posts/7c13f69c/p3.png" alt="p3.png" title="">
<p>其中天蓝色代表类别 yes、亦即购买了业务；橙色则代表 no、亦即没有购买业务。可以看到、所有离散维度的特征都是前面所说的“无足轻重”的特征</p>
<p>连续维度的可视化是几乎同理的，唯一的差别在于它不是柱状图而是正态分布密度函数的函数曲线。具体的代码实现从略、感兴趣的观众老爷们可以尝试动手实现一下，这里仅放出程序运行的结果。该数据共 7 个连续维度，我们同样把它们放在同一个图中：</p>
<img src="/posts/7c13f69c/p4.png" alt="p4.png" title="">
<p>其中，天蓝色曲线代表类别 yes、橙色曲线代表类别 no。可以看到，两种类别的数据在各个维度上的正态分布的均值、方差都几乎一致</p>
<p>从以上的分析已经可以比较直观地感受到、该问题确实相当病态。特别地，考虑到朴素贝叶斯的算法、不难想象此时的混合型朴素贝叶斯模型基本就只是根据各类别的先验概率来进行分类决策</p>
<p>至此，朴素贝叶斯算法的理论、实现就差不多都说了一遍。作为收尾，下篇文章会补上之前没有展开叙述的一些细节、同时也会简要地介绍一下其余的贝叶斯分类器</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;（本文会用到的所有代码都在&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/b_NaiveBayes/Vectorized/MergedNB.py&quot;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;本文主要介绍混合型朴素贝叶斯—— MergedNB 的实现。（我知道的）混合型朴素贝叶斯算法主要有两种提法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用某种分布的密度函数算出训练集中各个样本连续型特征相应维度的密度之后，根据这些密度的情况将该维度离散化、最后再训练离散型朴素贝叶斯模型&lt;/li&gt;
&lt;li&gt;直接结合离散型朴素贝叶斯和连续型朴素贝叶斯：  &lt;script type=&quot;math/tex; mode=display&quot;&gt;
y = f(x^{*}) = \arg{\max_{c_k}{p\left( y = c_{k} \right)\prod_{j \in S_{1}}^{}{p(X^{\left( j \right)} = {x^{*}}^{\left( j \right)}|y = c_{k})}\prod_{j \in S_{2}}^{}{p(X^{j} = {x^{*}}^{\left( j \right)}|y = c_{k})}}}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从直观可以看出、第二种提法可能会比第一种提法要“激进”一些，因为如果某个连续型维度采用的分布特别“大起大落”的话、该维度可能就会直接“主导”整个决策。但是考虑到实现的简洁和直观（……），我们还是演示第二种提法的实现。感兴趣的观众老爷可以尝试实现第一种提法，思路和过程都是没有太本质的区别的、只是会繁琐不少&lt;/p&gt;
    
    </summary>
    
      <category term="朴素贝叶斯" scheme="http://www.carefree0910.com/categories/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>GaussianNB 的实现</title>
    <link href="http://www.carefree0910.com/posts/c836ba35/"/>
    <id>http://www.carefree0910.com/posts/c836ba35/</id>
    <published>2017-04-20T07:34:35.000Z</published>
    <updated>2017-04-20T13:51:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/b_NaiveBayes/Vectorized/GaussianNB.py" target="_blank" rel="external">这里</a>）</p>
<p>本文主要介绍连续型朴素贝叶斯——GaussianNB 的实现。在有了实现离散型朴素贝叶斯的经验后，实现连续型朴素贝叶斯模型其实只是个触类旁通的活了</p>
<a id="more"></a>
<p>不过在介绍实现之前，我们还是需要先要知道连续型朴素贝叶斯的算法是怎样的。处理连续型变量有一个最直观的方法：使用小区间切割、直接使其离散化。由于这种方法较难控制小区间的大小、而且对训练集质量的要求比较高，所以我们选用第二种方法：假设该变量服从正态分布（或称高斯分布，Gaussian Distribution）、再利用极大似然估计来计算该变量的“条件概率”。具体而言、GaussianNB 通过如下公式计算“条件概率”<script type="math/tex">p(X^{\left( j \right)} = a_{jl}|y = c_{k})</script>：</p>
<script type="math/tex; mode=display">
p\left( X^{\left( j \right)} = a_{jl} \middle| y = c_{k} \right) = \frac{1}{\sqrt{2\pi}\sigma_{jk}}e^{- \frac{\left( a_{jl} - \mu_{jk} \right)^{2}}{2\sigma_{jk}^{2}}}</script><p>这里有两个参数：<script type="math/tex">\mu_{jk}</script>、<script type="math/tex">\sigma_{jk}</script>，它们可以用极大似然估计法定出：</p>
<script type="math/tex; mode=display">
{\hat{\sigma}}_{jk}^{2} = \frac{1}{N_{k}}\sum_{i = 1}^{N}{\left( x_{i}^{\left( j \right)} - \mu_{jk} \right)^{2}I(y_{i} = c_{k})}</script><script type="math/tex; mode=display">
{\hat{\mu}}_{jk} = \frac{1}{N_{k}}\sum_{i = 1}^{N}{x_{i}^{\left( j \right)}I(y_{i} = c_{k})}</script><p>其中，<script type="math/tex">N_{k} = \sum_{i = 1}^{N}{I(y_{i} = c_{k})}</script>是类别<script type="math/tex">c_{k}</script>的样本数。需要注意的是，这里的“条件概率”其实是“条件概率密度”，真正的条件概率其实是 0（因为连续型变量单点概率为 0）。这样做的合理性涉及到了比较深的概率论知识，此处不表<del>（其实我想表也表不出来）</del></p>
<p>所以在实现 GaussianNB 之前、我们需要先实现一个能够计算正态分布密度和进行正态分布极大似然估计的类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> math <span class="keyword">import</span> pi, exp</div><div class="line"></div><div class="line"><span class="comment"># 记录常量以避免重复运算</span></div><div class="line">sqrt_pi = (<span class="number">2</span> * pi) ** <span class="number">0.5</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">NBFunctions</span>:</span></div><div class="line">    <span class="comment"># 定义正态分布的密度函数</span></div><div class="line"><span class="meta">    @staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gaussian</span><span class="params">(x, mu, sigma)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.exp(</div><div class="line">            -(x - mu) ** <span class="number">2</span> / (<span class="number">2</span> * sigma)) / (sqrt_pi * sigma ** <span class="number">0.5</span>)</div><div class="line"></div><div class="line">    <span class="comment"># 定义进行极大似然估计的函数</span></div><div class="line">    <span class="comment"># 它能返回一个存储着计算条件概率密度的函数的列表</span></div><div class="line"><span class="meta">    @staticmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gaussian_maximum_likelihood</span><span class="params">(labelled_x, n_category, dim)</span>:</span></div><div class="line">        mu = [np.sum(</div><div class="line">            labelled_x[c][dim]) / </div><div class="line">            len(labelled_x[c][dim]) <span class="keyword">for</span> c <span class="keyword">in</span> range(n_category)]</div><div class="line">        sigma = [np.sum(</div><div class="line">            (labelled_x[c][dim]-mu[c])**<span class="number">2</span>) / </div><div class="line">            len(labelled_x[c][dim]) <span class="keyword">for</span> c <span class="keyword">in</span> range(n_category)]</div><div class="line">        <span class="comment"># 利用极大似然估计得到的和、定义生成计算条件概率密度的函数的函数 func</span></div><div class="line">        <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(_c)</span>:</span></div><div class="line">            <span class="function"><span class="keyword">def</span> <span class="title">sub</span><span class="params">(xx)</span>:</span></div><div class="line">                <span class="keyword">return</span> NBFunctions.gaussian(xx, mu[_c], sigma[_c])</div><div class="line">            <span class="keyword">return</span> sub</div><div class="line">        <span class="comment"># 利用 func 返回目标列表</span></div><div class="line">        <span class="keyword">return</span> [func(_c=c) <span class="keyword">for</span> c <span class="keyword">in</span> range(n_category)]</div></pre></td></tr></table></figure>
<p>对于 GaussianNB 本身，由于算法中只有条件概率相关的定义变了、所以只需要将相关的函数重新定义即可。此外，由于输入数据肯定是数值数据、所以数据预处理会简单不少（至少不用因为要对输入进行特殊的数值化处理而记录其转换字典了）。考虑到上一章说明 MultinomialNB 的实现时已经基本把我们框架的思想都说明清楚了，在接下来的 GaussianNB 的代码实现中、我们会适当地减少注释以提高阅读流畅度<del>（其实主要还是为了偷懒）</del>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> b_NaiveBayes.Original.Basic <span class="keyword">import</span> *</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">GaussianNB</span><span class="params">(NaiveBayes)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feed_data</span><span class="params">(self, x, y, sample_weight=None)</span>:</span></div><div class="line">        <span class="comment"># 简单地调用 Python 自带的 float 方法将输入数据数值化</span></div><div class="line">        x = np.array([list(map(</div><div class="line">            <span class="keyword">lambda</span> c: float(c), sample)) <span class="keyword">for</span> sample <span class="keyword">in</span> x])</div><div class="line">        <span class="comment"># 数值化类别向量</span></div><div class="line">        labels = list(set(y))</div><div class="line">        label_dic = &#123;label: i <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels)&#125;</div><div class="line">        y = np.array([label_dic[yy] <span class="keyword">for</span> yy <span class="keyword">in</span> y])</div><div class="line">        cat_counter = np.bincount(y)</div><div class="line">        labels = [y == value <span class="keyword">for</span> value <span class="keyword">in</span> range(len(cat_counter))]</div><div class="line">        labelled_x = [x[label].T <span class="keyword">for</span> label <span class="keyword">in</span> labels]</div><div class="line">        <span class="comment"># 更新模型的各个属性</span></div><div class="line">        self._x, self._y = x.T, y</div><div class="line">        self._labelled_x, self._label_zip = labelled_x, labels</div><div class="line">        self._cat_counter, self.label_dic = (</div><div class="line">            cat_counter, &#123;i: _l <span class="keyword">for</span> _l, i <span class="keyword">in</span> label_dic.items()&#125;</div><div class="line">        self.feed_sample_weight(sample_weight)</div></pre></td></tr></table></figure>
<p>可以看到，数据预处理这一步确实要轻松很多。接下来只需要再定义训练用的代码就行，它们和 MultinomialNB 中的实现也大同小异： </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 定义处理样本权重的函数</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_sample_weight</span><span class="params">(self, sample_weight=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        local_weights = sample_weight * len(sample_weight)</div><div class="line">        <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(self._label_zip):</div><div class="line">            self._labelled_x[i] *= local_weights[label]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_fit</span><span class="params">(self, lb)</span>:</span></div><div class="line">    n_category = len(self._cat_counter)</div><div class="line">    p_category = self.get_prior_probability(lb)</div><div class="line">    <span class="comment"># 利用极大似然估计获得计算条件概率的函数、使用数组变量 data 进行存储</span></div><div class="line">    data = [</div><div class="line">        NBFunctions.gaussian_maximum_likelihood(</div><div class="line">            self._labelled_x, n_category, dim)</div><div class="line">                <span class="keyword">for</span> dim <span class="keyword">in</span> range(len(self._x))]</div><div class="line">    self._data = data</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(input_x, tar_category)</span>:</span></div><div class="line">        <span class="comment"># 将输入转换成二维数组（矩阵）</span></div><div class="line">        input_x = np.atleast_2d(input_x).T</div><div class="line">        rs = np.ones(input_x.shape[<span class="number">1</span>])</div><div class="line">        <span class="keyword">for</span> d, xx <span class="keyword">in</span> enumerate(input_x):</div><div class="line">            rs *= data[d][tar_category](xx)</div><div class="line">        <span class="keyword">return</span> rs * p_category[tar_category]</div><div class="line"></div><div class="line"><span class="comment"># 由于数据本身就是数值的，所以数据转换函数只需直接返回输入值即可</span></div><div class="line"><span class="meta">@staticmethod</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_transfer_x</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure>
<p>至此，连续型朴素贝叶斯模型就搭建完毕了</p>
<p>连续型朴素贝叶斯同样能够进行和离散型朴素贝叶斯类似的可视化，不过由于我们接下来就要实现适用范围最广的朴素贝叶斯模型：混合型朴素贝叶斯了，所以我们这里不打算进行 GaussianNB 合理的评估、而打算把它归结到对混合型朴素贝叶斯的评估中</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;（本文会用到的所有代码都在&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/b_NaiveBayes/Vectorized/GaussianNB.py&quot;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;本文主要介绍连续型朴素贝叶斯——GaussianNB 的实现。在有了实现离散型朴素贝叶斯的经验后，实现连续型朴素贝叶斯模型其实只是个触类旁通的活了&lt;/p&gt;
    
    </summary>
    
      <category term="朴素贝叶斯" scheme="http://www.carefree0910.com/categories/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>MultinomialNB 的实现</title>
    <link href="http://www.carefree0910.com/posts/74647589/"/>
    <id>http://www.carefree0910.com/posts/74647589/</id>
    <published>2017-04-20T06:50:30.000Z</published>
    <updated>2017-04-20T13:53:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/b_NaiveBayes/Vectorized/MultinomialNB.py" target="_blank" rel="external">这里</a>）</p>
<p>本文主要介绍离散型朴素贝叶斯——MultinomialNB 的实现。对于离散型朴素贝叶斯模型的实现，由于核心算法都是在进行“计数”工作、所以问题的关键就转换为了如何进行计数。幸运的是、Numpy 中的一个方法：<code>bincount</code>就是专门用来计数的，它能够非常快速地数出一个数组中各个数字出现的频率；而且由于它是 Numpy 自带的方法，其速度比 Python 标准库<code>collections</code>中的计数器<code>Counter</code>还要快上非常多。不幸的是、该方法有如下两个缺点：</p>
<ul>
<li>只能处理非负整数型中数组</li>
<li>向量中的最大值即为返回的数组的长度，换句话说，如果用<code>bincount</code>方法对一个长度为 1、元素为 1000 的数组计数的话，返回的结果就是 999 个 0 加 1 个 1</li>
</ul>
<p>所以我们做数据预处理时就要充分考虑到这两点</p>
<a id="more"></a>
<p>在综述中我们曾经提到过在<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py#L189" target="_blank" rel="external">这里</a>可以找到将数据进行数值化的具体实现，该数据数值化的方法其实可以说是为<code>bincount</code>方法“量身定做”的。举个栗子，当原始数据形如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">x, s, n, t, p, f</div><div class="line">x, s, y, t, a, f</div><div class="line">b, s, w, t, l, f</div><div class="line">x, y, w, t, p, f</div><div class="line">x, s, g, f, n, f</div></pre></td></tr></table></figure>
<p>时，调用上述数值化数据的方法将会把数据数值化为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">0, 0, 0, 0, 0, 0</div><div class="line">0, 0, 1, 0, 1, 0</div><div class="line">1, 0, 2, 0, 2, 0</div><div class="line">0, 1, 2, 0, 0, 0</div><div class="line">0, 0, 3, 1, 3, 0</div></pre></td></tr></table></figure>
<p>单就实现这个功能而言、实现是平凡的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">quantize_data</span><span class="params">(x)</span>:</span></div><div class="line">    features = [set(feat) <span class="keyword">for</span> feat <span class="keyword">in</span> xt]</div><div class="line">    feat_dics = [&#123;</div><div class="line">        _l: i <span class="keyword">for</span> i, _l <span class="keyword">in</span> enumerate(feats)</div><div class="line">    &#125; <span class="keyword">if</span> <span class="keyword">not</span> wc[i] <span class="keyword">else</span> <span class="keyword">None</span> <span class="keyword">for</span> i, feats <span class="keyword">in</span> enumerate(features)]</div><div class="line">    x = np.array([[</div><div class="line">        feat_dics[i][_l] <span class="keyword">for</span> i, _l <span class="keyword">in</span> enumerate(sample)]</div><div class="line">            <span class="keyword">for</span> sample <span class="keyword">in</span> x])</div><div class="line">    <span class="keyword">return</span> x, feat_dics</div></pre></td></tr></table></figure>
<p>不过考虑到离散型朴素贝叶斯需要的东西比这要多很多，所以才有了<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py#L189" target="_blank" rel="external">这里</a>所实现的、相对而言繁复很多的版本。建议观众老爷们在看接下来的实现之前先把那个<code>quantize_data</code>函数的完整版看一遍、因为我接下来会直接用<del>（那你很棒棒哦）</del></p>
<p>当然，考虑到朴素贝叶斯的相关理论还是比较多的、我就不把实现一股脑扔出来了，那样估计大部分人<del>（包括我自己在内）</del>都看不懂……所以我决定把离散型朴素贝叶斯算法和对应的实现进行逐一讲解 ( σ’ω’)σ</p>
<h1 id="计算先验概率"><a href="#计算先验概率" class="headerlink" title="计算先验概率"></a>计算先验概率</h1><p>这倒是在将框架时就已经讲过了、但我还是打算重讲一遍以加深理解。首先把实现放出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_prior_probability</span><span class="params">(self, lb=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="keyword">return</span> [(_c_num + lb) / (len(self._y) + lb * len(self._cat_counter))</div><div class="line">        <span class="keyword">for</span> _c_num <span class="keyword">in</span> self._cat_counter]</div></pre></td></tr></table></figure>
<p>其中的<code>lb</code>为平滑系数（默认为 1、亦即拉普拉斯平滑），这对应的公式其实是带平滑项的、先验概率的极大似然估计：</p>
<script type="math/tex; mode=display">
p_\lambda(y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda},k=1,2,...,K</script><p>所以代码中的<code>self._cat_counter</code>的意义就很明确了——它存储着 K 个<script type="math/tex">\sum_{i=1}^NI(y_i=c_k)</script></p>
<p>（cat counter 是 category counter 的简称）<del>（我知道我命名很差所以不要打我）</del></p>
<h1 id="计算条件概率"><a href="#计算条件概率" class="headerlink" title="计算条件概率"></a>计算条件概率</h1><p>同样先看核心实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">data = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_dim)]</div><div class="line"><span class="keyword">for</span> dim, n_possibilities <span class="keyword">in</span> enumerate(self._n_possibilities):</div><div class="line">    data[dim] = [[</div><div class="line">        (self._con_counter[dim][c][p] + lb) / (</div><div class="line">            self._cat_counter[c] + lb * n_possibilities)</div><div class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> range(n_possibilities)] <span class="keyword">for</span> c <span class="keyword">in</span> range(n_category)]</div><div class="line">self._data = [np.array(dim_info) <span class="keyword">for</span> dim_info <span class="keyword">in</span> data]</div></pre></td></tr></table></figure>
<p>这对应的公式其实就是带平滑项（<code>lb</code>）的条件概率的极大似然估计：</p>
<script type="math/tex; mode=display">
p_{\lambda}\left( X^{\left( j \right)} = a_{jl} \middle| y = c_{k} \right) = \frac{\sum_{i = 1}^{N}{I\left( x_{i}^{\left( j \right)} = a_{jl},y_{i} = c_{k} \right) + \lambda}}{\sum_{i = 1}^{N}{I(y_{i} = c_{k})} + S_{j}\lambda}</script><p>其中</p>
<script type="math/tex; mode=display">
k=1,...,K;\ \ j=1,...,n;\ \ l=1,...,S_j</script><p>可以看到我们利用到了<code>self._cat_counter</code>属性来计算<script type="math/tex">\sum_{i=1}^NI(y_i=c_k)</script>。同时可以看出：</p>
<ul>
<li><code>n_category</code>即为 K </li>
<li><code>self._n_possibilities</code>储存着 n 个<script type="math/tex">S_j</script></li>
<li><code>self._con_counter</code>储存的即是各个<script type="math/tex">\sum_{i=1}^NI(x_i^{(j)}=a_{jl}, y_i=c_k)</script>的值。具体而言：  <script type="math/tex; mode=display">
\text{self._con_counter[d][c][p]}=p_\lambda(X^{(d)}=p|y=c)</script></li>
</ul>
<p>至于<code>self._data</code>、就只是为了向量化算法而存在的一个变量而已，它将<code>data</code>中的每一个列表都转成了 Numpy 数组、以便在计算后验概率时利用 Numpy 数组的 Fancy Indexing 来加速算法</p>
<p>聪明的观众老爷可能已经发现、其实<code>self._con_counter</code>才是计算条件概率的关键，事实上这里也正是<code>bincount</code>大放异彩的地方。以下为计算<code>self._con_counter</code>的函数的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_sample_weight</span><span class="params">(self, sample_weight=None)</span>:</span></div><div class="line">    self._con_counter = []</div><div class="line">    <span class="keyword">for</span> dim, _p <span class="keyword">in</span> enumerate(self._n_possibilities):</div><div class="line">        <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            self._con_counter.append([</div><div class="line">                np.bincount(xx[dim], minlength=_p) <span class="keyword">for</span> xx <span class="keyword">in</span></div><div class="line">                    self._labelled_x])</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self._con_counter.append([</div><div class="line">                np.bincount(xx[dim], weights=sample_weight[</div><div class="line">                    label] / sample_weight[label].mean(), minlength=_p)</div><div class="line">                <span class="keyword">for</span> label, xx <span class="keyword">in</span> self._label_zip])</div></pre></td></tr></table></figure>
<p>可以看到、<code>bincount</code>方法甚至能帮我们处理样本权重的问题</p>
<p>代码中有两个我们还没进行说明的属性：<code>self._labelled_x</code>和<code>self._label_zip</code>，不过从代码上下文不难推断出、它们储存的是应该是不同类别所对应的数据。具体而言：</p>
<ul>
<li><code>self._labelled_x</code>：记录按类别分开后的、输入数据的数组</li>
<li><code>self._label_zip</code>：比<code>self._labelled_x</code>多记录了个各个类别的数据所对应的下标</li>
</ul>
<p>这里就提前将它们的实现放出来以帮助理解吧：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 获得各类别数据的下标</span></div><div class="line">labels = [y == value <span class="keyword">for</span> value <span class="keyword">in</span> range(len(cat_counter))]</div><div class="line"><span class="comment"># 利用下标获取记录按类别分开后的输入数据的数组</span></div><div class="line">labelled_x = [x[ci].T <span class="keyword">for</span> ci <span class="keyword">in</span> labels]</div><div class="line">self._labelled_x, self._label_zip = labelled_x, list(zip(labels, labelled_x))</div></pre></td></tr></table></figure>
<h1 id="计算后验概率"><a href="#计算后验概率" class="headerlink" title="计算后验概率"></a>计算后验概率</h1><p>仍然先看核心实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(input_x, tar_category)</span>:</span></div><div class="line">    input_x = np.atleast_2d(input_x).T</div><div class="line">    rs = np.ones(input_x.shape[<span class="number">1</span>])</div><div class="line">    <span class="keyword">for</span> d, xx <span class="keyword">in</span> enumerate(input_x):</div><div class="line">        rs *= self._data[d][tar_category][xx]</div><div class="line">    <span class="keyword">return</span> rs * p_category[tar_category]</div></pre></td></tr></table></figure>
<p>这对应的公式其实就是决策公式：</p>
<script type="math/tex; mode=display">
y=f(x^*)=\arg\max_{c_k}\hat p(y=c_k)\prod_{i=1}^n\hat p(x^{(i)}=x^{*(i)}|y=c_k)</script><p>所以不难看出代码中的<code>p_category</code>存储着 K 个<script type="math/tex">\hat p(y=c_k)</script></p>
<h1 id="整合封装模型"><a href="#整合封装模型" class="headerlink" title="整合封装模型"></a>整合封装模型</h1><p>最后要做的、无非就是把上述三个步骤进行封装而已，首先是数据预处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">feed_data</span><span class="params">(self, x, y, sample_weight=None)</span>:</span></div><div class="line">    <span class="keyword">if</span> sample_weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        sample_weight = np.array(sample_weight)</div><div class="line">    <span class="comment"># 调用 quantize_data 函数获得诸多信息</span></div><div class="line">    x, y, _, features, feat_dics, label_dic = DataUtil.quantize_data(</div><div class="line">        x, y, wc=np.array([<span class="keyword">False</span>] * len(x[<span class="number">0</span>])))</div><div class="line">    <span class="comment"># 利用 bincount 函数直接获得 self._cat_counter</span></div><div class="line">    cat_counter = np.bincount(y)</div><div class="line">    <span class="comment"># 利用 features 变量获取各个维度的特征个数 Sj</span></div><div class="line">    n_possibilities = [len(feats) <span class="keyword">for</span> feats <span class="keyword">in</span> features]</div><div class="line">    <span class="comment"># 获得各类别数据的下标</span></div><div class="line">    labels = [y == value <span class="keyword">for</span> value <span class="keyword">in</span> range(len(cat_counter))]</div><div class="line">    <span class="comment"># 利用下标获取记录按类别分开后的输入数据的数组</span></div><div class="line">    labelled_x = [x[ci].T <span class="keyword">for</span> ci <span class="keyword">in</span> labels]</div><div class="line">    <span class="comment"># 更新模型的各个属性</span></div><div class="line">    self._x, self._y = x, y</div><div class="line">    self._labelled_x, self._label_zip = labelled_x, list(</div><div class="line">        zip(labels, labelled_x))</div><div class="line">    self._cat_counter, self._feat_dics, self._n_possibilities = cat_counter, feat_dics, n_possibilities</div><div class="line">    self.label_dic = label_dic</div><div class="line">    self.feed_sample_weight(sample_weight)</div></pre></td></tr></table></figure>
<p>然后利用上一章我们定义的框架的话、只需定义核心训练函数即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_fit</span><span class="params">(self, lb)</span>:</span></div><div class="line">    n_dim = len(self._n_possibilities)</div><div class="line">    n_category = len(self._cat_counter)</div><div class="line">    p_category = self.get_prior_probability(lb)</div><div class="line"></div><div class="line">    data = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_dim)]</div><div class="line">    <span class="keyword">for</span> dim, n_possibilities <span class="keyword">in</span> enumerate(self._n_possibilities):</div><div class="line">        data[dim] = [[</div><div class="line">            (self._con_counter[dim][c][p] + lb) / (</div><div class="line">                self._cat_counter[c] + lb * n_possibilities)</div><div class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> range(n_possibilities)] <span class="keyword">for</span> c <span class="keyword">in</span> range(n_category)]</div><div class="line">    self._data = [np.array(dim_info) <span class="keyword">for</span> dim_info <span class="keyword">in</span> data]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(input_x, tar_category)</span>:</span></div><div class="line">        input_x = np.atleast_2d(input_x).T</div><div class="line">        rs = np.ones(input_x.shape[<span class="number">1</span>])</div><div class="line">        <span class="keyword">for</span> d, xx <span class="keyword">in</span> enumerate(input_x):</div><div class="line">            rs *= self._data[d][tar_category][xx]</div><div class="line">        <span class="keyword">return</span> rs * p_category[tar_category]</div><div class="line">    <span class="keyword">return</span> func</div></pre></td></tr></table></figure>
<p>最后，我们需要定义一个将测试数据转化为模型所需的、数值化数据的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">_transfer_x</span><span class="params">(self, x)</span>:</span></div><div class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(x):</div><div class="line">        <span class="keyword">for</span> j, char <span class="keyword">in</span> enumerate(sample):</div><div class="line">            x[i][j] = self._feat_dics[j][char]</div><div class="line">    <span class="keyword">return</span> x</div></pre></td></tr></table></figure>
<p>至此，离散型朴素贝叶斯就全部实现完毕了<del>（鼓掌！）</del></p>
<h1 id="评估与可视化"><a href="#评估与可视化" class="headerlink" title="评估与可视化"></a>评估与可视化</h1><p>我们可以先拿之前的气球数据集 1.0、1.5 来简单地评估一下我们的模型。首先我们要定义一个能够将文件中的数据转化为 Python 数组的类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataUtil</span>:</span></div><div class="line">    <span class="comment"># 定义一个方法使其能从文件中读取数据</span></div><div class="line">    <span class="comment"># 该方法接受五个参数：</span></div><div class="line">        数据集的名字、数据集的路径、训练样本数、类别所在列、是否打乱数据</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span><span class="params">(name, path, train_num=None, tar_idx=None, shuffle=True)</span>:</span></div><div class="line">        x = []</div><div class="line">        <span class="comment"># 将编码设为utf8以便读入中文等特殊字符</span></div><div class="line">        <span class="keyword">with</span> open(path, <span class="string">"r"</span>, encoding=<span class="string">"utf8"</span>) <span class="keyword">as</span> file:</div><div class="line">            <span class="comment"># 如果是气球数据集的话、直接依逗号分割数据即可</span></div><div class="line">            <span class="keyword">if</span> <span class="string">"balloon"</span> <span class="keyword">in</span> name:</div><div class="line">                <span class="keyword">for</span> sample <span class="keyword">in</span> file:</div><div class="line">                    x.append(sample.strip().split(<span class="string">","</span>))</div><div class="line">        <span class="comment"># 默认打乱数据</span></div><div class="line">        <span class="keyword">if</span> shuffle:</div><div class="line">            np.random.shuffle(x)</div><div class="line">        <span class="comment"># 默认类别在最后一列</span></div><div class="line">        tar_idx = <span class="number">-1</span> <span class="keyword">if</span> tar_idx <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> tar_idx</div><div class="line">        y = np.array([xx.pop(tar_idx) <span class="keyword">for</span> xx <span class="keyword">in</span> x])</div><div class="line">        x = np.array(x)</div><div class="line">        <span class="comment"># 默认全都是训练样本</span></div><div class="line">        <span class="keyword">if</span> train_num <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">            <span class="keyword">return</span> x, y</div><div class="line">        <span class="comment"># 若传入了训练样本数，则依之将数据集切分为训练集和测试集</span></div><div class="line">        <span class="keyword">return</span> (x[:train_num], y[:train_num]), (x[train_num:], y[train_num:])</div></pre></td></tr></table></figure>
<p>需要指出的是，今后获取各种数据的过程都会放在上述<code>DataUtil</code>中的这个<code>get_dataset</code>方法中，其完整版本可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/Util/Util.py#L94" target="_blank" rel="external">这里</a>。下面就放出 MultinomialNB 的评估用代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    <span class="comment"># 导入标准库time以计时，导入DataUtil类以获取数据</span></div><div class="line">    <span class="keyword">import</span> time</div><div class="line">    <span class="keyword">from</span> Util <span class="keyword">import</span> DataUtil</div><div class="line">    <span class="comment"># 遍历1.0、1.5两个版本的气球数据集</span></div><div class="line">    <span class="keyword">for</span> dataset <span class="keyword">in</span> (<span class="string">"balloon1.0"</span>, <span class="string">"balloon1.5"</span>):</div><div class="line">        <span class="comment"># 读入数据</span></div><div class="line">        _x, _y = DataUtil.get_dataset(dataset, <span class="string">"../../_Data/&#123;&#125;.txt"</span>.format(dataset))</div><div class="line">        <span class="comment"># 实例化模型并进行训练、同时记录整个过程花费的时间</span></div><div class="line">        learning_time = time.time()</div><div class="line">        nb = MultinomialNB()</div><div class="line">        nb.fit(_x, _y)</div><div class="line">        learning_time = time.time() - learning_time</div><div class="line">        <span class="comment"># 评估模型的表现，同时记录评估过程花费的时间</span></div><div class="line">        estimation_time = time.time()</div><div class="line">        nb.evaluate(_x, _y)</div><div class="line">        estimation_time = time.time() - estimation_time</div><div class="line">        <span class="comment"># 将记录下来的耗时输出</span></div><div class="line">        print(</div><div class="line">            <span class="string">"Model building  : &#123;:12.6&#125; s\n"</span></div><div class="line">            <span class="string">"Estimation      : &#123;:12.6&#125; s\n"</span></div><div class="line">            <span class="string">"Total           : &#123;:12.6&#125; s"</span>.format(</div><div class="line">                learning_time, estimation_time,</div><div class="line">                learning_time + estimation_time</div><div class="line">            )</div><div class="line">        )</div></pre></td></tr></table></figure>
<p>上面这段代码的运行结果如下图所示：</p>
<img src="/posts/74647589/p4.png" alt="p4.png" title="">
<p>由于数据量太少、所以建模和评估的过程耗费的时间已是可以忽略不计的程度。同时正如前文所提及的，气球数据集1.5是“不太均衡”的数据集，所以朴素贝叶斯在其上的表现会比较差</p>
<p>仅仅在虚构的数据集上进行评估可能不太有说服力，我们可以拿 UCI 上一个比较出名<del>（简单）</del>的“蘑菇数据集（Mushroom Data Set）”来评估一下我们的模型。该数据集的大致描述如下：它有 8124 个样本、22 个属性，类别取值有两个：“能吃”或“有毒”；该数据每个单一样本都占一行、属性之间使用逗号隔开。选择该数据集的原因是它无需进行额外的数据预处理、样本量和属性量都相对合适、二类分类问题也相对来说具有代表性。更重要的是，它所有维度的特征取值都是离散的、从而非常适合用来测试我们的 MultinomialNB 模型</p>
<p>完整的数据集可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/_Data/mushroom.txt" target="_blank" rel="external">这里</a>（第一列数据是类别），我们的模型在其上的表现则如下图所示： </p>
<img src="/posts/74647589/p1.png" alt="p1.png" title="">
<p>其中第一、二行分别是训练集、测试集上的准确率，接下来三行则分别是建立模型、评估模型和总花费时间的记录</p>
<p>当然，仅仅看一个结果没有什么意思、也完全无法知道模型到底干了什么。为了获得更好的直观，我们可以进行一定的可视化，比如说将极大似然估计法得到的条件概率画出（如综述所示的那样）。可视化的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入 matplotlib 库以进行可视化</span></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="comment"># 进行一些设置使得 matplotlib 能够显示中文</span></div><div class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> mpl</div><div class="line"><span class="comment"># 将字体设为“仿宋”</span></div><div class="line">mpl.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'FangSong'</span>]</div><div class="line">mpl.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="keyword">False</span></div><div class="line"><span class="comment"># 利用 MultinomialNB 搭建过程中记录的变量获取条件概率</span></div><div class="line">data = nb[<span class="string">"data"</span>]</div><div class="line"><span class="comment"># 定义颜色字典，将类别 e（能吃）设为天蓝色、类别 p（有毒）设为橙色</span></div><div class="line">colors = &#123;<span class="string">"e"</span>: <span class="string">"lightSkyBlue"</span>, <span class="string">"p"</span>: <span class="string">"orange"</span>&#125;</div><div class="line"><span class="comment"># 利用转换字典定义其“反字典”，后面可视化会用上</span></div><div class="line">_rev_feat_dics = [&#123;_val: _key <span class="keyword">for</span> _key, _val <span class="keyword">in</span> _feat_dic.items()&#125; </div><div class="line">    <span class="keyword">for</span> _feat_dic <span class="keyword">in</span> self._feat_dics]</div><div class="line"><span class="comment"># 遍历各维度进行可视化</span></div><div class="line"><span class="comment"># 利用 MultinomialNB 搭建过程中记录的变量，获取画图所需的信息</span></div><div class="line"><span class="keyword">for</span> _j <span class="keyword">in</span> range(nb[<span class="string">"x"</span>].shape[<span class="number">1</span>]):</div><div class="line">    sj = nb[<span class="string">"n_possibilities"</span>][_j]</div><div class="line">    tmp_x = np.arange(<span class="number">1</span>, sj+<span class="number">1</span>)</div><div class="line">    <span class="comment"># 利用 matplotlib 对 LaTeX 的支持来写标题，两个 $ 之间的即是 LaTeX 语句</span></div><div class="line">    title = <span class="string">"$j = &#123;&#125;; S_j = &#123;&#125;$"</span>.format(_j+<span class="number">1</span>, sj)</div><div class="line">    plt.figure()</div><div class="line">    plt.title(title)</div><div class="line">    <span class="comment"># 根据条件概率的大小画出柱状图</span></div><div class="line">    <span class="keyword">for</span> _c <span class="keyword">in</span> range(len(nb.label_dic)):</div><div class="line">        plt.bar(tmp_x<span class="number">-0.35</span>*_c, data[_j][_c, :], width=<span class="number">0.35</span>,</div><div class="line">                facecolor=colors[nb.label_dic[_c]], edgecolor=<span class="string">"white"</span>, </div><div class="line">                label=<span class="string">"class: &#123;&#125;"</span>.format(nb.label_dic[_c]))</div><div class="line">    <span class="comment"># 利用上文定义的“反字典”将横坐标转换成特征的各个取值</span></div><div class="line">    plt.xticks([i <span class="keyword">for</span> i <span class="keyword">in</span> range(sj + <span class="number">2</span>)], [<span class="string">""</span>] + [_rev_dic[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(sj)] + [<span class="string">""</span>])</div><div class="line">    plt.ylim(<span class="number">0</span>, <span class="number">1.0</span>)</div><div class="line">    plt.legend()</div><div class="line">    <span class="comment"># 保存画好的图像</span></div><div class="line">    plt.savefig(<span class="string">"d&#123;&#125;"</span>.format(j+<span class="number">1</span>))</div></pre></td></tr></table></figure>
<p>由于蘑菇数据一共有 22 维，所以上述代码会生成 22 张图，从这些图可以非常清晰地看出训练数据集各维度特征的分布。下选出几组有代表性的图片进行说明</p>
<p>一般来说，一组数据特征中会有相对“重要”的特征和相对“无足轻重”的特征，通过以上实现的可视化可以比较轻松地辨析出在离散型朴素贝叶斯中这两者的区别。比如说，在离散型朴素贝叶斯里、相对重要的特征的表现会如下图所示（左图对应第 5 维、右图对应第 19 维）：</p>
<img src="/posts/74647589/p2.png" alt="“优秀”的特征" title="“优秀”的特征">
<p>可以看出，蘑菇数据集在第 19 维上两个类别各自的“优势特征”都非常明显、第 5 维上两个类别各自特征的取值更是基本没有交集。可以想象，即使只根据第 5 维的取值来进行类别的判定、最后的准确率也一定会非常高</p>
<p>那么与之相反的、在 MultinomialNB 中相对没那么重要的特征的表现则会形如下图所示（左图对应第 3 维、右图对应第 16 维）：</p>
<img src="/posts/74647589/p3.png" alt="“无用”的特征" title="“无用”的特征">
<p>可以看出，蘑菇数据集在第 3 维上两个类的特征取值基本没有什么差异、第 16 维数据更是似乎完全没有存在的价值。像这样的数据就可以考虑直接剔除掉</p>
<p>看到这里的观众老爷如果再回过头去看上一篇文章所讲的框架、想必会有些新的体会吧 ( σ’ω’)σ</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;（本文会用到的所有代码都在&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/b_NaiveBayes/Vectorized/MultinomialNB.py&quot;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;本文主要介绍离散型朴素贝叶斯——MultinomialNB 的实现。对于离散型朴素贝叶斯模型的实现，由于核心算法都是在进行“计数”工作、所以问题的关键就转换为了如何进行计数。幸运的是、Numpy 中的一个方法：&lt;code&gt;bincount&lt;/code&gt;就是专门用来计数的，它能够非常快速地数出一个数组中各个数字出现的频率；而且由于它是 Numpy 自带的方法，其速度比 Python 标准库&lt;code&gt;collections&lt;/code&gt;中的计数器&lt;code&gt;Counter&lt;/code&gt;还要快上非常多。不幸的是、该方法有如下两个缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只能处理非负整数型中数组&lt;/li&gt;
&lt;li&gt;向量中的最大值即为返回的数组的长度，换句话说，如果用&lt;code&gt;bincount&lt;/code&gt;方法对一个长度为 1、元素为 1000 的数组计数的话，返回的结果就是 999 个 0 加 1 个 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以我们做数据预处理时就要充分考虑到这两点&lt;/p&gt;
    
    </summary>
    
      <category term="朴素贝叶斯" scheme="http://www.carefree0910.com/categories/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>框架的实现</title>
    <link href="http://www.carefree0910.com/posts/fa51e28/"/>
    <id>http://www.carefree0910.com/posts/fa51e28/</id>
    <published>2017-04-20T06:31:53.000Z</published>
    <updated>2017-04-20T12:50:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>（本文会用到的所有代码都在<a href="https://github.com/carefree0910/MachineLearning/blob/master/b_NaiveBayes/Vectorized/Basic.py" target="_blank" rel="external">这里</a>）</p>
<p>对于我个人而言、光看这么一个框架是非常容易摸不着头脑的<br>毕竟之前花了许多时间在数学部分讲的那些算法完全没有体现在这个框架中、取而代之的是一些我抽象出来的和算法无关的结构性部分……<br>虽然从逻辑上来说应该先说明如何搭建这个框架，但从容易理解的角度来说、个人建议先不看这章的内容而是先看后续的实现具体算法的章节<br>然后如果那时有不懂的定义、再对照这一章的相关部分来看<br>不过如果是对朴素贝叶斯算法非常熟悉的观众老爷的话、直接看本章的抽象会引起一些共鸣也说不定 ( σ’ω’)σ</p>
<a id="more"></a>
<p>所谓的框架、自然是指三种朴素贝叶斯模型（离散、连续、混合）共性的抽象了。由于贝叶斯决策论就摆在那里、不难知道如下功能是通用的：</p>
<ul>
<li>计算类别的先验概率</li>
<li>训练出一个能输出后验概率的决策函数</li>
<li>利用该决策函数进行预测和评估</li>
</ul>
<p>虽说朴素贝叶斯大体上来说只是简单的计数、但是想以比较高的效率做好这件事却比想象中的要麻烦不少<del>（说实话麻烦到我有些不想讲的程度了）</del></p>
<p>总之先来看看这个框架的初始化步骤吧<del>（前方高能）</del></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">NaiveBayes</span><span class="params">(ClassifierBase)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">        初始化结构</div><div class="line">        self._x, self._y：记录训练集的变量</div><div class="line">        self._data：核心数组，存储实际使用的条件概率的相关信息</div><div class="line">        self._func：模型核心——决策函数，能够根据输入的x、y输出对应的后验概率</div><div class="line">        self._n_possibilities：记录各个维度特征取值个数的数组</div><div class="line">        self._labelled_x：记录按类别分开后的输入数据的数组</div><div class="line">        self._label_zip：记录类别相关信息的数组，视具体算法、定义会有所不同</div><div class="line">        self._cat_counter：核心数组，记录第i类数据的个数（cat是category的缩写）</div><div class="line">        self._con_counter：核心数组，用于记录数据条件概率的原始极大似然估计</div><div class="line">        self.label_dic：核心字典，用于记录数值化类别时的转换关系</div><div class="line">        self._feat_dics：核心字典，用于记录数值化各维度特征（feat）时的转换关系</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self._x = self._y = <span class="keyword">None</span></div><div class="line">        self._data = self._func = <span class="keyword">None</span></div><div class="line">        self._n_possibilities = <span class="keyword">None</span></div><div class="line">        self._labelled_x = self._label_zip = <span class="keyword">None</span></div><div class="line">        self._cat_counter = self._con_counter = <span class="keyword">None</span></div><div class="line">        self.label_dic = self._feat_dics = <span class="keyword">None</span></div></pre></td></tr></table></figure>
<p>其中、<code>self._con_counter[d][c][p]</code><script type="math/tex">=\hat p(x^{(d)}=p|y=c)</script>（con 是 conditional 的缩写）</p>
<img src="/posts/fa51e28/p1.jpg" alt="注释比代码还多是想闹哪样？？？(╯‵□′)╯︵┻━┻" title="注释比代码还多是想闹哪样？？？(╯‵□′)╯︵┻━┻">
<p>总之和我一样陷入了茫然的观众老爷们可以先不太在意这一坨是什么玩意儿，毕竟这些东西是抽象程度比较高的属性……等结合具体算法时、这些属性的意义可能就会明确得多</p>
<p>下面进入正题……首先来看怎么计算先验概率（直接利用上面的<code>self._cat_counter</code>属性即可）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_prior_probability</span><span class="params">(self, lb=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="keyword">return</span> [(_c_num + lb) / (len(self._y) + lb * len(self._cat_counter))</div><div class="line">            <span class="keyword">for</span> _c_num <span class="keyword">in</span> self._cat_counter]</div></pre></td></tr></table></figure>
<p>其中参数<code>lb</code>即为平滑项，默认为 1 意味着默认使用拉普拉斯平滑 </p>
<p>然后看看训练步骤能如何进行抽象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x=None, y=None, sample_weight=None, lb=<span class="number">1</span>)</span>:</span></div><div class="line">    <span class="keyword">if</span> x <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> y <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        self.feed_data(x, y, sample_weight)</div><div class="line">    self._func = self._fit(lb)</div></pre></td></tr></table></figure>
<p><del>（岂可修不就只是调用了一下<code>feed_data</code>方法而已嘛还说成抽象什么的行不行啊）</del></p>
<p>其中用到的<code>feed_data</code>方法是留给各个子类定义的、进行数据预处理的方法；然后<code>self._fit</code>可说是核心训练函数、它会返回我们的决策函数<code>self._func</code></p>
<p>最后看看怎样利用<code>self._func</code>来预测未知数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x, get_raw_result=False)</span>:</span></div><div class="line">    <span class="comment"># 调用相应方法进行数据预处理（这在离散型朴素贝叶斯中尤为重要）</span></div><div class="line">    x = self._transfer_x(x)</div><div class="line">    <span class="comment"># 只有将算法进行向量化之后才能做以下的步骤</span></div><div class="line">    m_arg, m_probability = np.zeros(len(x), dtype=np.int8), np.zeros(len(x))</div><div class="line">    <span class="comment"># len(self._cat_counter) 其实就是类别个数</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self._cat_counter)):</div><div class="line">        <span class="comment"># 注意这里的 x 其实是矩阵、p 是对应的“后验概率矩阵”：p = p(y=i|x)</span></div><div class="line">        <span class="comment"># 这意味着决策函数 self._func 需要支持矩阵运算</span></div><div class="line">        p = self._func(x, i)</div><div class="line">        <span class="comment"># 利用 Numpy 进行向量化操作</span></div><div class="line">        _mask = p &gt; m_probability</div><div class="line">        m_arg[_mask], m_probability[_mask] = i, p[_mask]</div><div class="line">    <span class="comment"># 利用转换字典 self.label_dic 输出决策</span></div><div class="line">    <span class="comment"># 参数 get_raw_result 控制该函数是输出预测的类别还是输出相应的后验概率</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> get_raw_result:</div><div class="line">        <span class="keyword">return</span> np.array([self.label_dic[arg] <span class="keyword">for</span> arg <span class="keyword">in</span> m_arg])</div><div class="line">    <span class="keyword">return</span> m_probability</div></pre></td></tr></table></figure>
<p>其中<code>self.label_dic</code>大概是这个德性的：比如训练集的类别空间为 {red, green, blue} 然后第一个样本的类别是 red 且第二个样本的类别是 blue、那么就有</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">self.label_dic = np.array([<span class="string">"red"</span>, <span class="string">"blue"</span>, <span class="string">"green"</span>])</div></pre></td></tr></table></figure>
<p>以上就是朴素贝叶斯模型框架的搭建，下一篇文章则会在该框架的基础上实现离散型朴素贝叶斯模型</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;（本文会用到的所有代码都在&lt;a href=&quot;https://github.com/carefree0910/MachineLearning/blob/master/b_NaiveBayes/Vectorized/Basic.py&quot;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;对于我个人而言、光看这么一个框架是非常容易摸不着头脑的&lt;br&gt;毕竟之前花了许多时间在数学部分讲的那些算法完全没有体现在这个框架中、取而代之的是一些我抽象出来的和算法无关的结构性部分……&lt;br&gt;虽然从逻辑上来说应该先说明如何搭建这个框架，但从容易理解的角度来说、个人建议先不看这章的内容而是先看后续的实现具体算法的章节&lt;br&gt;然后如果那时有不懂的定义、再对照这一章的相关部分来看&lt;br&gt;不过如果是对朴素贝叶斯算法非常熟悉的观众老爷的话、直接看本章的抽象会引起一些共鸣也说不定 ( σ’ω’)σ&lt;/p&gt;
    
    </summary>
    
      <category term="朴素贝叶斯" scheme="http://www.carefree0910.com/categories/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
      <category term="Python" scheme="http://www.carefree0910.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯算法</title>
    <link href="http://www.carefree0910.com/posts/ea9b7d09/"/>
    <id>http://www.carefree0910.com/posts/ea9b7d09/</id>
    <published>2017-04-20T04:15:21.000Z</published>
    <updated>2017-04-22T12:53:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>首先要叙述朴素贝叶斯算法的基本假设：</p>
<ul>
<li><strong>独立性假设</strong>：假设单一样本<script type="math/tex">X_i</script>的 n 个维度<script type="math/tex">X_i^{(1)},...,X_i^{(n)}</script>彼此之间在各种意义上相互独立</li>
</ul>
<p>这当然是很强的假设，在现实任务中也大多无法满足该假设。由此会衍生出所谓的半朴素贝叶斯和贝叶斯网，这里先按下不表</p>
<p>然后就是算法。我们打算先只叙述它的基本思想和各个公式，相关的定义和证明会放在后面的文章中。不过其实仅对着接下来的公式敲代码的话、就已经可以实现一个朴素贝叶斯模型了：</p>
<ul>
<li>基本思想：<strong>后验概率最大化</strong>、然后通过贝叶斯公式转换成先验概率乘条件概率最大化</li>
<li>各个公式（假设输入有 N 个、单个样本是 n 维的、一共有 K 类：<script type="math/tex">c_1,...,c_K</script>）<ul>
<li>计算先验概率的极大似然估计：  <script type="math/tex; mode=display">
\hat p(y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N},k=1,2,...,K</script></li>
<li>计算条件概率的极大似然估计：  <script type="math/tex; mode=display">
\hat p(x^{(j)}=a_{jl}|y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}</script>其中样本<script type="math/tex">x_i</script>第 j 维<script type="math/tex">x_i^{(j)}</script>的取值集合为<script type="math/tex">\{a_{j1},...,a_{jS_j}\}</script></li>
</ul>
</li>
<li>得到最终的分类器：  <script type="math/tex; mode=display">
y=f(x^*)=\arg\max_{c_k}\hat p(y=c_k)\prod_{i=1}^n\hat p(x^{(i)}=x^{*(i)}|y=c_k)</script></li>
</ul>
<p>在朴素贝叶斯算法思想下、一般来说会衍生出以下三种不同的模型：</p>
<ul>
<li>离散型朴素贝叶斯（MultinomialNB）：所有维度的特征都是离散型随机变量</li>
<li>连续型朴素贝叶斯（GaussianNB）：所有维度的特征都是连续型随机变量</li>
<li>混合型朴素贝叶斯（MergedNB）：各个维度的特征有离散型也有连续型</li>
</ul>
<p>接下来就简单<del>（并不简单啊喂）</del>讲讲朴素贝叶斯的数学背景。由浅入深，我们会用离散型朴素贝叶斯来说明一些普适性的概念，连续型和混合型的相关定义是类似的</p>
<a id="more"></a>
<h1 id="朴素贝叶斯与贝叶斯决策论的联系"><a href="#朴素贝叶斯与贝叶斯决策论的联系" class="headerlink" title="朴素贝叶斯与贝叶斯决策论的联系"></a>朴素贝叶斯与贝叶斯决策论的联系</h1><p>朴素贝叶斯的模型参数即是类别的选择空间：</p>
<script type="math/tex; mode=display">
\Theta = \left\{ y = c_{1},{y = c}_{2},\ldots,{y = c}_{K} \right\}</script><p>朴素贝叶斯总的参数空间<script type="math/tex">\tilde{\Theta}</script>本应包括模型参数的先验概率<script type="math/tex">p\left( \theta_{k} \right) = p(y = c_{k})</script>、样本空间在模型参数下的条件概率<script type="math/tex">p\left( X \middle| \theta_{k} \right) = p(X|y = c_{k})</script>和样本空间本身的概率<script type="math/tex">p(X)</script>；但由于我们采取样本空间的子集<script type="math/tex">\tilde{X}</script>作为训练集，所以在给定的<script type="math/tex">\tilde{X}</script>下、<script type="math/tex">p\left( X \right) = p(\tilde{X})</script>是常数，因此可以把它从参数空间中删去。换句话说，我们关心的东西只有模型参数的先验概率和样本空间在模型参数下的条件概率</p>
<script type="math/tex; mode=display">
\tilde{\Theta} = \left\{ p\left( \theta \right),p\left( X \middle| \theta \right):\theta \in \Theta \right\}</script><p>行动空间<script type="math/tex">A</script>就是朴素贝叶斯总的参数空间<script type="math/tex">\tilde{\Theta}</script></p>
<p>决策就是后验概率最大化（在<a href="/posts/e312d61a/" title="推导与推广">推导与推广</a>里，我们会证明该决策为贝叶斯决策）</p>
<script type="math/tex; mode=display">
\delta\left( \tilde{X} \right) = \hat{\theta} = \arg{\max_{\tilde\theta\in\tilde\Theta}{p\left( \tilde{\theta} \middle| \tilde{X} \right)}}</script><p>在<script type="math/tex">\hat{\theta}</script>确定后，模型的决策就可以具体写成（这一步用到了独立性假设）</p>
<script type="math/tex; mode=display">
\begin{align}
  f\left( x^{*} \right) &= \arg{\max_{c_k}{\hat{p}\left( c_{k} \middle| X = x^{*} \right)}} \\
  &= \arg{\max_{c_k}{\hat{p}\left( y = c_{k} \right)\prod_{j = 1}^{n}{\hat{p}\left( X^{\left( j \right)} = {x^{*}}^{\left( j \right)} \middle| y = c_{k} \right)}}}
\end{align}</script><p>损失函数会随模型的不同而不同。在离散型朴素贝叶斯中，损失函数就是比较简单的 0-1 损失函数</p>
<script type="math/tex; mode=display">
L\left( \theta,\delta\left( \tilde{X} \right) \right) = \sum_{i = 1}^{N}{\tilde{L}\left( y_{i},f\left( x_{i} \right) \right) =}\sum_{i = 1}^{N}{I(}y_{i} \neq f\left( x_{i} \right))</script><p>这里的<script type="math/tex">I</script>是示性函数，它满足：</p>
<script type="math/tex; mode=display">
I\left( y_{i} \neq f\left( x_{i} \right) \right) = \left\{ \begin{matrix}
1,\ if\ y_{i} \neq f\left( x_{i} \right) \\
0,if\ y_{i} \neq f\left( x_{i} \right) \\
\end{matrix} \right.\</script><p>从上述定义出发、可以利用<a href="/posts/d007d6bc/" title="上一篇文章">上一篇文章</a>中讲解的两种参数估计方法导出离散型朴素贝叶斯的算法（详见<a href="/posts/e312d61a/" title="推导与推广">推导与推广</a>）：</p>
<ol>
<li><strong>输入</strong>：训练数据集<script type="math/tex">D = \{\left( x_{1},y_{1} \right),\ldots,(x_{N},y_{N})\}</script></li>
<li><strong>过程</strong>（利用 ML 估计导出模型的具体参数）：<ol>
<li>计算先验概率<script type="math/tex">p(y = c_{k})</script>的极大似然估计：  <script type="math/tex; mode=display">
\hat{p}\left( y = c_{k} \right) = \frac{\sum_{i = 1}^{N}{I(y_{i} = c_{k})}}{N},\ k = 1,2,\ldots,K</script></li>
<li>计算条件概率<script type="math/tex">p(X^{\left( j \right)} = a_{jl}|y = c_{k})</script>的极大似然估计（设每一个单独输入的 n 维向量<script type="math/tex">x_{i}</script>的第 j 维特征<script type="math/tex">x^{\left( j \right)}</script>可能的取值集合为<script type="math/tex">\{ a_{j1},\ldots,a_{jS_{j}}\}</script>）：  <script type="math/tex; mode=display">
\hat{p}\left( X^{\left( j \right)} = a_{jl} \middle| y = c_{k} \right) = \frac{\sum_{i = 1}^{N}{I(x_{i}^{\left( j \right)} = a_{jl},y_{i} = c_{k})}}{\sum_{i = 1}^{N}{I(y_{i} = c_{k})}}</script></li>
</ol>
</li>
<li><strong>输出</strong>（利用 MAP 估计进行决策）：朴素贝叶斯模型，能够估计数据<script type="math/tex">x^{*} = \left( {x^{*}}^{\left( 1 \right)},\ldots,{x^{*}}^{\left( n \right)} \right)^{T}</script>的类别：  <script type="math/tex; mode=display">
y = f(x^{*}) = \arg{\max_{c_k}{\hat{p}\left( y = c_{k} \right)\prod_{j = 1}^{n}{\hat{p}(X^{\left( j \right)} = {x^{*}}^{\left( j \right)}|y = c_{k})}}}</script></li>
</ol>
<p>由上述算法可以清晰地梳理出朴素贝叶斯算法背后的数学思想：</p>
<ul>
<li>使用极大似然估计导出模型的具体参数（先验概率、条件概率）</li>
<li>使用极大后验概率估计作为模型的决策（输出使得数据后验概率最大化的类别）</li>
</ul>
<h1 id="离散型朴素贝叶斯实例"><a href="#离散型朴素贝叶斯实例" class="headerlink" title="离散型朴素贝叶斯实例"></a>离散型朴素贝叶斯实例</h1><p>接下来我们在一个简单、虚拟的数据集上应用离散型朴素贝叶斯算法以加深对算法的理解，该数据集（不妨称之为气球数据集 1.0）如下表所示（参考了 UCI 上相应的数据集）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>大小</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>小</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>小孩</td>
<td>用脚踩</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>成人</td>
<td>用手打</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>大</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>大</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>该数据集的电子版本可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/_Data/balloon1.0.txt" target="_blank" rel="external">这里</a>。我们想预测的是样本：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>大小</th>
<th>测试人员</th>
<th>测试动作</th>
</tr>
</thead>
<tbody>
<tr>
<td>  紫色</td>
<td>小</td>
<td>小孩</td>
<td>用脚踩</td>
</tr>
</tbody>
</table>
</div>
<p>所导致的结果。容易观察到的是、气球的颜色对结果不起丝毫影响，所以在算法中该项特征可以直接去掉。因此从直观上来说，该样本所导致的结果应该是“不爆炸”，我们用离散型朴素贝叶斯算法来看看是否确实如此。首先我们需要计算类别的先验概率，易得：</p>
<script type="math/tex; mode=display">
p\left( 不爆炸\right) = p\left( 爆炸\right) = 0.5</script><p>亦即类别的先验概率也对决策不起作用。继而我们需要依次求出第2、3、4个特征（大小、测试人员、测试动作）的条件概率，它们才是决定新样本所属类别的关键。易得：</p>
<script type="math/tex; mode=display">p\left( 小气球\middle| 不爆炸\right) = \frac{5}{6},\ \ p\left( 大气球\middle| 不爆炸\right) = \frac{1}{6}</script><script type="math/tex; mode=display">p\left( 小气球\middle| 爆炸\right) = \frac{1}{6},\ \ p\left( 大气球\middle| 爆炸\right) = \frac{5}{6}</script><script type="math/tex; mode=display">p\left( 成人\middle| 不爆炸\right) = \frac{1}{3},\ \ p\left( 小孩\middle| 不爆炸\right) = \frac{2}{3}</script><script type="math/tex; mode=display">p\left( 成人\middle| 爆炸\right) = \frac{2}{3},\ \ p\left( 小孩\middle| 爆炸\right) = \frac{1}{3}</script><script type="math/tex; mode=display">p\left( 用手打\middle| 不爆炸\right) = \frac{5}{6},\ \ p\left( 用脚踩\middle| 不爆炸\right) = \frac{1}{6}</script><script type="math/tex; mode=display">p\left( 用手打\middle| 爆炸\right) = \frac{1}{6},\ \ p\left( 用脚踩\middle| 爆炸\right) = \frac{5}{6}</script><p>那么在条件“紫色小气球、小孩用脚踩”下，知（注意我们可以忽略颜色和先验概率）：</p>
<script type="math/tex; mode=display">\hat{p}\left( 不爆炸\right) = p\left( 小气球\middle| 不爆炸\right) \times p\left( 小孩\middle| 不爆炸\right) \times p\left( 用脚踩\middle| 不爆炸\right) = \frac{5}{54}</script><script type="math/tex; mode=display">\hat{p}\left( 爆炸\right) = p\left( 小气球\middle| 爆炸\right) \times p\left( 小孩\middle| 爆炸\right) \times p\left( 用脚踩\middle| 爆炸\right) = \frac{5}{108}</script><p>所以我们确实应该认为给定样本所导致的结果是“不爆炸”。</p>
<h1 id="不足与改进"><a href="#不足与改进" class="headerlink" title="不足与改进"></a>不足与改进</h1><p>需要指出的是，目前为止的算法存在一个问题：如果训练集中某个类别<script type="math/tex">c_{k}</script>的数据没有涵盖第 j 维特征的第 l 个取值的话、相应估计的条件概率<script type="math/tex">\hat{p}\left( X^{\left( j \right)} = a_{jl} \middle| y = c_{k} \right)</script>就是 0、从而导致模型可能会在测试集上的分类产生误差。解决这个问题的办法是在各个估计中加入平滑项（也有这种做法就叫贝叶斯估计的说法）：</p>
<ul>
<li>过程：<ul>
<li>计算先验概率<script type="math/tex">p_{\lambda}(y = c_{k})</script>：  <script type="math/tex; mode=display">
p_{\lambda}\left( y = c_{k} \right) = \frac{\sum_{i = 1}^{N}{I\left( y_{i} = c_{k} \right) + \lambda}}{N + K\lambda},\ k = 1,2,\ldots,K</script></li>
<li>计算条件概率<script type="math/tex">p_{\lambda}(X^{\left( j \right)} = a_{jl}|y = c_{k})</script>：  <script type="math/tex; mode=display">
p_{\lambda}\left( X^{\left( j \right)} = a_{jl} \middle| y = c_{k} \right) = \frac{\sum_{i = 1}^{N}{I\left( x_{i}^{\left( j \right)} = a_{jl},y_{i} = c_{k} \right) + \lambda}}{\sum_{i = 1}^{N}{I(y_{i} = c_{k})} + S_{j}\lambda}</script></li>
</ul>
</li>
</ul>
<p>可见当<script type="math/tex">\lambda = 0</script>时就是极大似然估计，而当<script type="math/tex">\lambda = 1</script>时、一般可以称之为拉普拉斯平滑（Laplace Smoothing）。拉普拉斯平滑是常见的做法、我们的实现中也会默认使用它。可以将气球数据集 1.0 稍作变动以彰显加入平滑项的重要性（新数据集如下表所示，不妨称之为气球数据集 1.5）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>颜色</th>
<th>大小</th>
<th>测试人员</th>
<th>测试动作</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>  黄色</td>
<td>小</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>小</td>
<td>小孩</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>成人</td>
<td>用手打</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>成人</td>
<td>用脚踩</td>
<td>爆炸</td>
</tr>
<tr>
<td>  黄色</td>
<td>大</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小</td>
<td>成人</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>小</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
<tr>
<td>  紫色</td>
<td>大</td>
<td>小孩</td>
<td>用手打</td>
<td>不爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>该数据集的电子版本可以参见<a href="https://github.com/carefree0910/MachineLearning/blob/master/_Data/balloon1.5.txt" target="_blank" rel="external">这里</a>。可以看到这个数据集是“不太均衡”的：它对样本“黄色小气球，小孩用脚踩”重复进行了三次实验、而对所有紫色气球样本实验的结果都是“不爆炸”。如果我们此时想预测“紫色小气球，小孩用脚踩”的结果，虽然从直观上来说应该是“爆炸”，但我们会发现、此时由于</p>
<script type="math/tex; mode=display">
p\left( 用脚踩| 不爆炸\right) = p\left( 紫色| 爆炸\right) = 0</script><p>所以会直接导致</p>
<script type="math/tex; mode=display">
\hat{p}\left( 不爆炸\right) = \hat{p}\left( 爆炸\right) = 0</script><p>从而我们只能随机进行决策，这不是一个令人满意的结果。此时加入平滑项就显得比较重要了，我们以拉普拉斯平滑为例、知（注意类别的先验概率仍然不造成影响）：</p>
<script type="math/tex; mode=display">p\left( 黄色\middle| 不爆炸\right) = \frac{3 + 1}{6 + 2},\ \ p\left( 紫色\middle| 不爆炸\right) = \frac{3 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 黄色\middle| 爆炸\right) = \frac{6 + 1}{6 + 2},\ \ p\left( 紫色\middle| 爆炸\right) = \frac{0 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 小气球\middle| 不爆炸\right) = \frac{4 + 1}{6 + 2},\ \ p\left( 大气球\middle| 不爆炸不爆炸\right) = \frac{2 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 小气球\middle| 爆炸\right) = \frac{4 + 1}{6 + 2},\ \ p\left( 大气球\middle| 爆炸\right) = \frac{2 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 成人\middle| 不爆炸\right) = \frac{2 + 1}{6 + 2},\ \ p\left( 小孩\middle| 不爆炸\right) = \frac{4 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 成人\middle| 爆炸\right) = \frac{3 + 1}{6 + 2},\ \ p\left( 小孩\middle| 爆炸\right) = \frac{3 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 用手打\middle| 不爆炸\right) = \frac{6 + 1}{6 + 2},\ \ p\left( 用脚踩\middle| 不爆炸\right) = \frac{0 + 1}{6 + 2}</script><script type="math/tex; mode=display">p\left( 用手打\middle| 爆炸\right) = \frac{1 + 1}{6 + 2},\ \ p\left( 用脚踩\middle| 爆炸\right) = \frac{5 + 1}{6 + 2}</script><p>从而可算得：</p>
<script type="math/tex; mode=display">\hat{p}\left( 不爆炸\right) = \frac{25}{1024},\ \ \hat{p}\left( 爆炸\right) = \frac{15}{512}</script><p>因此我们确实应该认为给定样本所导致的结果是“爆炸”</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;首先要叙述朴素贝叶斯算法的基本假设：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;独立性假设&lt;/strong&gt;：假设单一样本&lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt;的 n 个维度&lt;script type=&quot;math/tex&quot;&gt;X_i^{(1)},...,X_i^{(n)}&lt;/script&gt;彼此之间在各种意义上相互独立&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这当然是很强的假设，在现实任务中也大多无法满足该假设。由此会衍生出所谓的半朴素贝叶斯和贝叶斯网，这里先按下不表&lt;/p&gt;
&lt;p&gt;然后就是算法。我们打算先只叙述它的基本思想和各个公式，相关的定义和证明会放在后面的文章中。不过其实仅对着接下来的公式敲代码的话、就已经可以实现一个朴素贝叶斯模型了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基本思想：&lt;strong&gt;后验概率最大化&lt;/strong&gt;、然后通过贝叶斯公式转换成先验概率乘条件概率最大化&lt;/li&gt;
&lt;li&gt;各个公式（假设输入有 N 个、单个样本是 n 维的、一共有 K 类：&lt;script type=&quot;math/tex&quot;&gt;c_1,...,c_K&lt;/script&gt;）&lt;ul&gt;
&lt;li&gt;计算先验概率的极大似然估计：  &lt;script type=&quot;math/tex; mode=display&quot;&gt;
\hat p(y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N},k=1,2,...,K&lt;/script&gt;&lt;/li&gt;
&lt;li&gt;计算条件概率的极大似然估计：  &lt;script type=&quot;math/tex; mode=display&quot;&gt;
\hat p(x^{(j)}=a_{jl}|y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}&lt;/script&gt;其中样本&lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;第 j 维&lt;script type=&quot;math/tex&quot;&gt;x_i^{(j)}&lt;/script&gt;的取值集合为&lt;script type=&quot;math/tex&quot;&gt;\{a_{j1},...,a_{jS_j}\}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;得到最终的分类器：  &lt;script type=&quot;math/tex; mode=display&quot;&gt;
y=f(x^*)=\arg\max_{c_k}\hat p(y=c_k)\prod_{i=1}^n\hat p(x^{(i)}=x^{*(i)}|y=c_k)&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在朴素贝叶斯算法思想下、一般来说会衍生出以下三种不同的模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;离散型朴素贝叶斯（MultinomialNB）：所有维度的特征都是离散型随机变量&lt;/li&gt;
&lt;li&gt;连续型朴素贝叶斯（GaussianNB）：所有维度的特征都是连续型随机变量&lt;/li&gt;
&lt;li&gt;混合型朴素贝叶斯（MergedNB）：各个维度的特征有离散型也有连续型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;接下来就简单&lt;del&gt;（并不简单啊喂）&lt;/del&gt;讲讲朴素贝叶斯的数学背景。由浅入深，我们会用离散型朴素贝叶斯来说明一些普适性的概念，连续型和混合型的相关定义是类似的&lt;/p&gt;
    
    </summary>
    
      <category term="朴素贝叶斯" scheme="http://www.carefree0910.com/categories/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="算法" scheme="http://www.carefree0910.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>参数估计</title>
    <link href="http://www.carefree0910.com/posts/d007d6bc/"/>
    <id>http://www.carefree0910.com/posts/d007d6bc/</id>
    <published>2017-04-20T03:52:09.000Z</published>
    <updated>2017-04-20T13:22:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>无论是贝叶斯学派还是频率学派，一个无法避开的问题就是如何从已有的样本中获取信息并据此估计目标模型的参数。比较有名的“频率近似概率”其实就是（基于大数定律的）相当合理的估计之一，本章所叙述的两种参数估计方法在最后也通常会归结于它</p>
<a id="more"></a>
<h1 id="极大似然估计（ML-估计）"><a href="#极大似然估计（ML-估计）" class="headerlink" title="极大似然估计（ML 估计）"></a>极大似然估计（ML 估计）</h1><p>如果把模型描述成一个概率模型的话，一个自然的想法是希望得到的模型参数<script type="math/tex">\theta</script>能够使得在训练集<script type="math/tex">\tilde{X}</script>作为输入时、模型输出的概率达到极大。这里就有一个似然函数的概念，它能够输出<script type="math/tex">\tilde{X} = \left( x_{1},\ldots,x_{N} \right)^{T}</script>在模型参数为<script type="math/tex">\theta</script>下的概率：</p>
<script type="math/tex; mode=display">
p\left( \tilde{X} \middle| \theta \right) = \prod_{i = 1}^{N}{p(x_{i}|\theta)}</script><p>我们希望找到的<script type="math/tex">\hat{\theta}</script>就是使得似然函数在<script type="math/tex">\tilde{X}</script>作为输入时达到极大的参数：</p>
<script type="math/tex; mode=display">
\hat{\theta} = \arg{\max_\theta{p\left( \tilde{X} \middle| \theta \right) = \arg{\max_\theta{\prod_{i = 1}^{N}{p(x_{i}|\theta)}}}}}</script><p>举个栗子：假设一个暗箱中有白球、黑球共两个，虽然不知道具体的颜色分布情况、但是知道这两个球是完全一样的。现在有放回地从箱子里抽了 2 个球，发现两次抽出来的结果是 1 黑 1 白，那么该如何估计箱子里面球的颜色？从直观上来说似乎箱子中也是 1 黑 1 白会比较合理，下面我们就来说明“1 黑 1 白”这个估计就是极大似然估计。</p>
<p>在这个问题中，模型的参数<script type="math/tex">\theta</script>可以设为从暗箱中抽出黑球的概率，样本<script type="math/tex">x_{i}</script>可以描述为第i次取出的球是否是黑球；如果是就取 1、否则取 0。这样的话，似然函数就可以描述为：</p>
<script type="math/tex; mode=display">
p\left( \tilde{X} \middle| \theta \right) = \theta^{x_{1} + x_{2}}\left( 1 - \theta \right)^{2 - x_{1} - x_{2}}</script><p>直接对它求极大值（虽然可行但是）不太方便，通常的做法是将似然函数取对数之后再进行极大值的求解：</p>
<script type="math/tex; mode=display">
\ln{p\left( \tilde{X} \middle| \theta \right) = \left( x_{1} + x_{2} \right)\ln{\theta + \left( 2 - x_{1} - x_{2} \right)\ln{(1 - \theta)}}} \Rightarrow \frac{\partial\ln p}{\partial\theta} = \frac{x_{1} + x_{2}}{\theta} - \frac{2 - x_{1} - x_{2}}{1 - \theta}</script><p>从而可知：</p>
<script type="math/tex; mode=display">
\frac{\partial\ln p}{\partial\theta} = 0 \Rightarrow \theta = \frac{x_{1} + x_{2}}{2}</script><p>由于<script type="math/tex">x_{1} + x_{2} = 1</script>，所以得<script type="math/tex">\hat{\theta} = 0.5</script>、亦即应该估计从暗箱中抽出黑球的概率是 50%；进一步地、既然暗箱中的两个球完全一样，我们应该估计暗箱中的颜色分布为 1 黑 1 白。</p>
<p>从以上的讨论可以看出，极大似然估计视待估参数为一个未知但固定的量、不考虑“观察者”的影响（亦即不考虑先验知识的影响），是传统的频率学派的做法</p>
<h1 id="极大后验概率估计（MAP估计）"><a href="#极大后验概率估计（MAP估计）" class="headerlink" title="极大后验概率估计（MAP估计）"></a>极大后验概率估计（MAP估计）</h1><p>相比起极大似然估计，极大后验概率估计是更贴合贝叶斯学派思想的做法；事实上、甚至也有不少人直接称其为“贝叶斯估计”（注：贝叶斯估计的定义有许多，本人接触到的就有 3、4 种；囿于实力，本人无法辨析哪种才是真正的贝叶斯估计、所以我们不会进行相关的讨论）</p>
<p>在讨论 MAP 估计之前，我们有必要先知道何为后验概率<script type="math/tex">p(\theta|\tilde{X})</script>：它可以理解为参数<script type="math/tex">\theta</script>在训练集<script type="math/tex">\tilde{X}</script>下所谓的“真实的出现概率”，能够利用参数的先验概率<script type="math/tex">p\left( \theta \right)</script>、样本的先验概率<script type="math/tex">p(\tilde{X})</script>和条件概率<script type="math/tex">p\left( \tilde{X}|\theta \right) = \prod_{i = 1}^{N}{p\left( x_{i}|\theta \right)}</script>通过贝叶斯公式导出（详见<a href="/posts/e312d61a/" title="推导与推广">推导与推广</a>）</p>
<p>而 MAP 估计的核心思想、就是将待估参数<script type="math/tex">\theta</script>看成是一个随机变量、从而引入了极大似然估计里面没有引入的、参数<script type="math/tex">\theta</script>的先验分布。MAP 估计<script type="math/tex">{\hat{\theta}}_{\text{MAP}}</script>的定义为：</p>
<script type="math/tex; mode=display">
{\hat{\theta}}_{\text{MAP}} = \arg{\max_\theta{p(\theta|\tilde{X}) = \arg{\max_\theta{p(\theta)\prod_{i = 1}^{N}{p(x_{i}|\theta)}}}}}</script><p>同样的，为了计算简洁，我们通常对上式取对数：</p>
<script type="math/tex; mode=display">
{\hat{\theta}}_{\text{MAP}} = \arg{\max_\theta{\ln{p(\theta|\tilde{X})} = \arg{\max_\theta\left\lbrack \ln{p\left( \theta \right)} + \sum_{i = 1}^{N}{\ln{p\left( x_{i} \middle| \theta \right)}} \right\rbrack}}}</script><p>可以看到，从形式上、极大后验概率估计只比极大似然估计多了<script type="math/tex">\ln{p(\theta)}</script>这一项，不过它们背后的思想却相当不同。不过有意思的是，在下一节具体讨论朴素贝叶斯算法时我们会看到、朴素贝叶斯在估计参数时选用了极大似然估计法、但是在做决策时则选用了 MAP 估计</p>
<p>和极大似然估计相比，MAP 估计的一个显著优势在于它可以引入所谓的“先验知识”，这正是贝叶斯学派的精髓。当然这个优势同时也伴随着劣势：它我们对模型参数有相对较好的认知、否则会相当大地影响到结果的合理性</p>
<p>既然先验分布如此重要，那么是否有比较合理的、先验分布的选取方法呢？事实上，如何确定先验分布这个问题，正是贝叶斯统计中最困难、最具有争议性却又必须解决的问题。虽然这个问题确实有许多现代的研究成果，但遗憾的是，尚未能有一个圆满的理论和普适的方法。这里拟介绍“协调性假说”这个相对而言拥有比较好的直观的理论：</p>
<ul>
<li>我们选择的参数<script type="math/tex">\theta</script>的先验分布、应该与由它和训练集确定的后验分布属同一类型</li>
</ul>
<p>此时先验分布又叫共轭先验分布。这里面所谓的“同一类型”其实又是难有恰当定义的概念，但是我们可以直观地理解为：概率性质相似的所有分布归为“同一类型”。比如，所有的正态分布都是“同一类型”的</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;无论是贝叶斯学派还是频率学派，一个无法避开的问题就是如何从已有的样本中获取信息并据此估计目标模型的参数。比较有名的“频率近似概率”其实就是（基于大数定律的）相当合理的估计之一，本章所叙述的两种参数估计方法在最后也通常会归结于它&lt;/p&gt;
    
    </summary>
    
      <category term="朴素贝叶斯" scheme="http://www.carefree0910.com/categories/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯决策论</title>
    <link href="http://www.carefree0910.com/posts/a0e8b2e/"/>
    <id>http://www.carefree0910.com/posts/a0e8b2e/</id>
    <published>2017-04-20T03:13:19.000Z</published>
    <updated>2017-04-20T12:50:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>贝叶斯决策论是在概率框架下进行决策的基本方法之一、更是统计模式识别的主要方法之一。从名字也许能看出来，贝叶斯决策论其实是贝叶斯统计学派进行决策的方法。为了更加深刻地理解贝叶斯分类器，我们需要先对贝叶斯学派和其决策理论有一个大致的认知</p>
<a id="more"></a>
<h1 id="贝叶斯学派与频率学派"><a href="#贝叶斯学派与频率学派" class="headerlink" title="贝叶斯学派与频率学派"></a>贝叶斯学派与频率学派</h1><p>贝叶斯学派强调概率的“主观性”，这一点和传统的、我们可能比较熟悉的频率学派不同。详细的论述牵扯到许多概率论和数理统计的知识，这里只说一个直观：</p>
<ul>
<li>频率学派强调频率的“自然属性”，认为应该使用事件在重复试验中发生的频率作为其发生的概率的估计</li>
<li>贝叶斯学派不强调事件的“客观随机性”，认为仅仅只是“观察者”不知道事件的结果。换句话说，贝叶斯学派认为：事件之所以具有随机性仅仅是因为“观察者”的知识不完备，对于“知情者”来说、该事件其实不具备随机性。随机性的根源不在于事件，而在于“观察者”对该事件的知识状态</li>
</ul>
<p>举个栗子：假设一个人抛了一枚均匀硬币到地上并迅速将其踩在脚底而在他面前从近到远坐了三个人。他本人看到了硬币是正面朝上的，而其他三个人也多多少少看到了一些信息，但显然坐得越远、看得就越模糊。频率学派会认为，该硬币是正是反、各自的概率都应该是 50%；但是贝叶斯学派会认为，对抛硬币的人来说、硬币是正面的概率就是 100%，然后可能对离他最近的人来说是 80%、对离他最远的人来说就可能还是 50%</p>
<p>所以相比起把模型参数固定、注重样本的随机性的频率学派而言，贝叶斯学派将样本视为是固定的、把模型的参数视为关键。在上面这个例子里面，样本就是抛出去的那枚硬币，模型的参数就是每个人从中获得的“信息”。对于频率学派而言，每个人获得的“信息”不应该有不同，所以自然会根据“均匀硬币抛出正面的概率是 50%”这个“样本的信息”来导出“硬币是正面的概率为 50%”这个结论。但是对贝叶斯学派而言，硬币抛出去就抛出去了，问题的关键在于模型的参数、亦即“观察者”从中获得的信息，所以会导出“对于抛硬币的人而言，硬币是正面的概率是 100%”这一类的结论</p>
<h1 id="贝叶斯决策论"><a href="#贝叶斯决策论" class="headerlink" title="贝叶斯决策论"></a>贝叶斯决策论</h1><p>大致知道贝叶斯学派的思想后，我们就可以介绍贝叶斯决策论了。这里不可避免地要牵扯到概率论和数理统计的相关定义和知识，但幸运的是它们都是比较基础且直观的部分、无需太多数学背景就可以知道它们的含义：</p>
<h2 id="行动空间"><a href="#行动空间" class="headerlink" title="行动空间"></a>行动空间</h2><p>行动空间（通常用<script type="math/tex">A</script>来表示）是某项实际工作中可能采取的各种“行动”所构成的集合。正如前文所提到的、贝叶斯学派注重的是模型参数，所以通常而言我们想要做出的“行动”是“决定模型的参数”。因此我们通常会将行动空间取为参数空间，亦即<script type="math/tex">A=\Theta</script></p>
<h2 id="决策"><a href="#决策" class="headerlink" title="决策"></a>决策</h2><p>决策（通常用<script type="math/tex">\delta(\tilde X)</script>来表示）是样本空间<script type="math/tex">X</script>到行动空间<script type="math/tex">A</script>的一个映射。换句话说，对于一个单一的样本<script type="math/tex">\tilde X</script>（<script type="math/tex">\tilde X\in X</script>），决策函数可以利用它得到<script type="math/tex">A</script>中的一个行动。需要注意的是，这里的样本<script type="math/tex">\tilde X</script>通常是高维的随机向量：<script type="math/tex">\tilde X=(x_1,...,x_N)^T</script>；尤其需要分清的是，这个（以及本小节之后的所有）<script type="math/tex">\tilde X</script>其实是一般意义上的“训练集”、<script type="math/tex">x_i</script>才是一般意义上的“样本”。这是因为本小节主要在叙述数理统计相关知识，所以在术语上和机器学习术语会有所冲突，需要分辨清它们的关系</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>损失函数（通常用<script type="math/tex">L(\theta,a)=L(\theta,\delta(\tilde X))</script>来表示）用于衡量当参数是<script type="math/tex">\theta</script>（<script type="math/tex">\theta\in\Theta</script>，<script type="math/tex">\Theta</script>是参数空间）时采取行动<script type="math/tex">a(a\in A)</script>所引起的损失</p>
<h2 id="决策风险"><a href="#决策风险" class="headerlink" title="决策风险"></a>决策风险</h2><p>决策风险（通常用<script type="math/tex">R(\theta,\delta)</script>来表示）是损失函数的期望：<script type="math/tex">R(\theta,\delta)=EL(\theta,\delta(\tilde X))</script></p>
<h2 id="先验分布"><a href="#先验分布" class="headerlink" title="先验分布"></a>先验分布</h2><p>先验分布描述了参数<script type="math/tex">\theta</script>在已知样本<script type="math/tex">\tilde X</script>中的分布</p>
<h2 id="平均风险"><a href="#平均风险" class="headerlink" title="平均风险"></a>平均风险</h2><p>平均风险（通常用<script type="math/tex">\rho(\delta)</script>来表示）定义为决策风险<script type="math/tex">R(\theta,\delta)</script>在先验分布下的期望：</p>
<script type="math/tex; mode=display">
\rho(\delta) = E_\xi R(\theta,\delta)</script><h2 id="贝叶斯决策"><a href="#贝叶斯决策" class="headerlink" title="贝叶斯决策"></a>贝叶斯决策</h2><p>贝叶斯决策（通常用<script type="math/tex">\delta^*</script>来表示）满足：</p>
<script type="math/tex; mode=display">
\rho(\delta^*)=\inf_\delta\rho(\delta)</script><p>换句话说，贝叶斯决策<script type="math/tex">\delta^*</script>是在某个先验分布下使得平均风险最小的决策</p>
<p>寻找一般意义下的贝叶斯决策是相当不平凡的数学问题，为简洁、我们需要结合具体的机器学习算法来推导相应的贝叶斯决策。相关的讨论会在<a href="/posts/ea9b7d09/" title="说明朴素贝叶斯算法的文章">说明朴素贝叶斯算法的文章</a>中进行，这里就暂时先按下不表</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;贝叶斯决策论是在概率框架下进行决策的基本方法之一、更是统计模式识别的主要方法之一。从名字也许能看出来，贝叶斯决策论其实是贝叶斯统计学派进行决策的方法。为了更加深刻地理解贝叶斯分类器，我们需要先对贝叶斯学派和其决策理论有一个大致的认知&lt;/p&gt;
    
    </summary>
    
      <category term="朴素贝叶斯" scheme="http://www.carefree0910.com/categories/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
    
      <category term="数学" scheme="http://www.carefree0910.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
</feed>
